Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPickAndPlace/config_TD3_BC/q_filter_False/prm_loss_weight_0.01/seed_1
-----------------------------------------------
| epoch                          | 0          |
| stats_o/mean                   | 0.20261343 |
| stats_o/std                    | 0.09423366 |
| test/episodes                  | 10         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.417      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0049    |
| test/info_shaping_reward_mean  | -0.106     |
| test/info_shaping_reward_min   | -0.412     |
| test/Q                         | -1.0190877 |
| test/Q_plus_P                  | -1.0190877 |
| test/reward_per_eps            | -23.3      |
| test/steps                     | 400        |
| train/episodes                 | 40         |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.21      |
| train/info_shaping_reward_min  | -0.442     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 1600       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 1          |
| stats_o/mean                   | 0.2004528  |
| stats_o/std                    | 0.11012056 |
| test/episodes                  | 20         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.35       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00235   |
| test/info_shaping_reward_mean  | -0.108     |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -1.3823624 |
| test/Q_plus_P                  | -1.3823624 |
| test/reward_per_eps            | -26        |
| test/steps                     | 800        |
| train/episodes                 | 80         |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.01       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.508     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 3200       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 2          |
| stats_o/mean                   | 0.19923437 |
| stats_o/std                    | 0.11819696 |
| test/episodes                  | 30         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.212      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00328   |
| test/info_shaping_reward_mean  | -0.128     |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -1.842022  |
| test/Q_plus_P                  | -1.842022  |
| test/reward_per_eps            | -31.5      |
| test/steps                     | 1200       |
| train/episodes                 | 120        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00813    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.114     |
| train/info_shaping_reward_mean | -0.202     |
| train/info_shaping_reward_min  | -0.39      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 4800       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.19917837 |
| stats_o/std                    | 0.11681805 |
| test/episodes                  | 40         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.427      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00369   |
| test/info_shaping_reward_mean  | -0.101     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -2.0447268 |
| test/Q_plus_P                  | -2.0447268 |
| test/reward_per_eps            | -22.9      |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 4          |
| stats_o/mean                   | 0.20091696 |
| stats_o/std                    | 0.1190814  |
| test/episodes                  | 50         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.343      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00485   |
| test/info_shaping_reward_mean  | -0.107     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -2.365507  |
| test/Q_plus_P                  | -2.365507  |
| test/reward_per_eps            | -26.3      |
| test/steps                     | 2000       |
| train/episodes                 | 200        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00187    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 8000       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 5          |
| stats_o/mean                   | 0.2013924  |
| stats_o/std                    | 0.11653186 |
| test/episodes                  | 60         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.343      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00436   |
| test/info_shaping_reward_mean  | -0.112     |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -2.7134795 |
| test/Q_plus_P                  | -2.7134795 |
| test/reward_per_eps            | -26.3      |
| test/steps                     | 2400       |
| train/episodes                 | 240        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00625    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.196     |
| train/info_shaping_reward_min  | -0.397     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 9600       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 6           |
| stats_o/mean                   | 0.20172997  |
| stats_o/std                    | 0.115110904 |
| test/episodes                  | 70          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.237       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00707    |
| test/info_shaping_reward_mean  | -0.123      |
| test/info_shaping_reward_min   | -0.201      |
| test/Q                         | -3.2623465  |
| test/Q_plus_P                  | -3.2623465  |
| test/reward_per_eps            | -30.5       |
| test/steps                     | 2800        |
| train/episodes                 | 280         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0106      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.115      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 11200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 7          |
| stats_o/mean                   | 0.20182589 |
| stats_o/std                    | 0.11098321 |
| test/episodes                  | 80         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0975     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0205    |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.338     |
| test/Q                         | -3.858177  |
| test/Q_plus_P                  | -3.858177  |
| test/reward_per_eps            | -36.1      |
| test/steps                     | 3200       |
| train/episodes                 | 320        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00937    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 12800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 8          |
| stats_o/mean                   | 0.20184809 |
| stats_o/std                    | 0.10804077 |
| test/episodes                  | 90         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.217      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00435   |
| test/info_shaping_reward_mean  | -0.128     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -3.9612103 |
| test/Q_plus_P                  | -3.9612103 |
| test/reward_per_eps            | -31.3      |
| test/steps                     | 3600       |
| train/episodes                 | 360        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.276     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 14400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 9          |
| stats_o/mean                   | 0.20188285 |
| stats_o/std                    | 0.10496501 |
| test/episodes                  | 100        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.113      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00402   |
| test/info_shaping_reward_mean  | -0.14      |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -4.577064  |
| test/Q_plus_P                  | -4.577064  |
| test/reward_per_eps            | -35.5      |
| test/steps                     | 4000       |
| train/episodes                 | 400        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 16000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 10          |
| stats_o/mean                   | 0.20179667  |
| stats_o/std                    | 0.102314174 |
| test/episodes                  | 110         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.268       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00937    |
| test/info_shaping_reward_mean  | -0.137      |
| test/info_shaping_reward_min   | -0.543      |
| test/Q                         | -4.722861   |
| test/Q_plus_P                  | -4.722861   |
| test/reward_per_eps            | -29.3       |
| test/steps                     | 4400        |
| train/episodes                 | 440         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.201      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 17600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 11         |
| stats_o/mean                   | 0.20208585 |
| stats_o/std                    | 0.10039293 |
| test/episodes                  | 120        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.145      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00443   |
| test/info_shaping_reward_mean  | -0.15      |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -5.2802563 |
| test/Q_plus_P                  | -5.2802563 |
| test/reward_per_eps            | -34.2      |
| test/steps                     | 4800       |
| train/episodes                 | 480        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 19200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 12         |
| stats_o/mean                   | 0.20194586 |
| stats_o/std                    | 0.09933751 |
| test/episodes                  | 130        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.155      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00306   |
| test/info_shaping_reward_mean  | -0.148     |
| test/info_shaping_reward_min   | -0.217     |
| test/Q                         | -5.598737  |
| test/Q_plus_P                  | -5.598737  |
| test/reward_per_eps            | -33.8      |
| test/steps                     | 5200       |
| train/episodes                 | 520        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.292     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 20800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 13         |
| stats_o/mean                   | 0.20214236 |
| stats_o/std                    | 0.09716939 |
| test/episodes                  | 140        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.165      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0134    |
| test/info_shaping_reward_mean  | -0.144     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -5.952107  |
| test/Q_plus_P                  | -5.952107  |
| test/reward_per_eps            | -33.4      |
| test/steps                     | 5600       |
| train/episodes                 | 560        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00375    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 22400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 14         |
| stats_o/mean                   | 0.20242126 |
| stats_o/std                    | 0.09517952 |
| test/episodes                  | 150        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.102      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00654   |
| test/info_shaping_reward_mean  | -0.151     |
| test/info_shaping_reward_min   | -0.218     |
| test/Q                         | -6.5153255 |
| test/Q_plus_P                  | -6.5153255 |
| test/reward_per_eps            | -35.9      |
| test/steps                     | 6000       |
| train/episodes                 | 600        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 24000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 15         |
| stats_o/mean                   | 0.20267418 |
| stats_o/std                    | 0.09382494 |
| test/episodes                  | 160        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.125     |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -7.26025   |
| test/Q_plus_P                  | -7.26025   |
| test/reward_per_eps            | -40        |
| test/steps                     | 6400       |
| train/episodes                 | 640        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 25600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 16         |
| stats_o/mean                   | 0.20247199 |
| stats_o/std                    | 0.09281084 |
| test/episodes                  | 170        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0475     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00652   |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -7.2933173 |
| test/Q_plus_P                  | -7.2933173 |
| test/reward_per_eps            | -38.1      |
| test/steps                     | 6800       |
| train/episodes                 | 680        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00562    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 27200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 17         |
| stats_o/mean                   | 0.20265694 |
| stats_o/std                    | 0.0914428  |
| test/episodes                  | 180        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.123     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -7.935594  |
| test/Q_plus_P                  | -7.935594  |
| test/reward_per_eps            | -40        |
| test/steps                     | 7200       |
| train/episodes                 | 720        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 28800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 18         |
| stats_o/mean                   | 0.20269482 |
| stats_o/std                    | 0.0901812  |
| test/episodes                  | 190        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0475     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0145    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -8.062878  |
| test/Q_plus_P                  | -8.062878  |
| test/reward_per_eps            | -38.1      |
| test/steps                     | 7600       |
| train/episodes                 | 760        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 30400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 19          |
| stats_o/mean                   | 0.20271544  |
| stats_o/std                    | 0.088886574 |
| test/episodes                  | 200         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.107       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00453    |
| test/info_shaping_reward_mean  | -0.149      |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -8.04125    |
| test/Q_plus_P                  | -8.04125    |
| test/reward_per_eps            | -35.7       |
| test/steps                     | 8000        |
| train/episodes                 | 800         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 32000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 20         |
| stats_o/mean                   | 0.20278156 |
| stats_o/std                    | 0.08762869 |
| test/episodes                  | 210        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.126     |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -8.9398155 |
| test/Q_plus_P                  | -8.9398155 |
| test/reward_per_eps            | -40        |
| test/steps                     | 8400       |
| train/episodes                 | 840        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0025     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 33600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 21         |
| stats_o/mean                   | 0.20277141 |
| stats_o/std                    | 0.08649563 |
| test/episodes                  | 220        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.18       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00453   |
| test/info_shaping_reward_mean  | -0.146     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -8.192225  |
| test/Q_plus_P                  | -8.192225  |
| test/reward_per_eps            | -32.8      |
| test/steps                     | 8800       |
| train/episodes                 | 880        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 35200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 22          |
| stats_o/mean                   | 0.20269944  |
| stats_o/std                    | 0.085413605 |
| test/episodes                  | 230         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.171      |
| test/info_shaping_reward_min   | -0.218      |
| test/Q                         | -9.688617   |
| test/Q_plus_P                  | -9.688617   |
| test/reward_per_eps            | -40         |
| test/steps                     | 9200        |
| train/episodes                 | 920         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 36800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 23         |
| stats_o/mean                   | 0.20276238 |
| stats_o/std                    | 0.08461478 |
| test/episodes                  | 240        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.065      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00739   |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.778993  |
| test/Q_plus_P                  | -9.778993  |
| test/reward_per_eps            | -37.4      |
| test/steps                     | 9600       |
| train/episodes                 | 960        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 38400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 24         |
| stats_o/mean                   | 0.20285921 |
| stats_o/std                    | 0.08408125 |
| test/episodes                  | 250        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.09       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00711   |
| test/info_shaping_reward_mean  | -0.148     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.459572  |
| test/Q_plus_P                  | -9.459572  |
| test/reward_per_eps            | -36.4      |
| test/steps                     | 10000      |
| train/episodes                 | 1000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 40000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 25         |
| stats_o/mean                   | 0.20304596 |
| stats_o/std                    | 0.08344188 |
| test/episodes                  | 260        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.12      |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -10.669958 |
| test/Q_plus_P                  | -10.669958 |
| test/reward_per_eps            | -40        |
| test/steps                     | 10400      |
| train/episodes                 | 1040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 41600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 26         |
| stats_o/mean                   | 0.20297062 |
| stats_o/std                    | 0.08289042 |
| test/episodes                  | 270        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.1        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0045    |
| test/info_shaping_reward_mean  | -0.149     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -10.004297 |
| test/Q_plus_P                  | -10.004297 |
| test/reward_per_eps            | -36        |
| test/steps                     | 10800      |
| train/episodes                 | 1080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 43200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 27         |
| stats_o/mean                   | 0.20305353 |
| stats_o/std                    | 0.0822977  |
| test/episodes                  | 280        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -11.299452 |
| test/Q_plus_P                  | -11.299452 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11200      |
| train/episodes                 | 1120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 44800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.20311657 |
| stats_o/std                    | 0.08169753 |
| test/episodes                  | 290        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.05       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.018     |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -11.300366 |
| test/Q_plus_P                  | -11.300366 |
| test/reward_per_eps            | -38        |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.20323765 |
| stats_o/std                    | 0.08103363 |
| test/episodes                  | 300        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.055      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0133    |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -11.46898  |
| test/Q_plus_P                  | -11.46898  |
| test/reward_per_eps            | -37.8      |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 30          |
| stats_o/mean                   | 0.2032571   |
| stats_o/std                    | 0.080497295 |
| test/episodes                  | 310         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.04        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0297     |
| test/info_shaping_reward_mean  | -0.16       |
| test/info_shaping_reward_min   | -0.197      |
| test/Q                         | -11.397087  |
| test/Q_plus_P                  | -11.397087  |
| test/reward_per_eps            | -38.4       |
| test/steps                     | 12400       |
| train/episodes                 | 1240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 49600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 31         |
| stats_o/mean                   | 0.20332764 |
| stats_o/std                    | 0.08009743 |
| test/episodes                  | 320        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.107      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00944   |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -11.522327 |
| test/Q_plus_P                  | -11.522327 |
| test/reward_per_eps            | -35.7      |
| test/steps                     | 12800      |
| train/episodes                 | 1280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 51200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 32         |
| stats_o/mean                   | 0.20348996 |
| stats_o/std                    | 0.08021597 |
| test/episodes                  | 330        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.015      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00662   |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -12.471872 |
| test/Q_plus_P                  | -12.471872 |
| test/reward_per_eps            | -39.4      |
| test/steps                     | 13200      |
| train/episodes                 | 1320       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 52800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 33         |
| stats_o/mean                   | 0.20364466 |
| stats_o/std                    | 0.07973549 |
| test/episodes                  | 340        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.125      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00712   |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -12.048834 |
| test/Q_plus_P                  | -12.048834 |
| test/reward_per_eps            | -35        |
| test/steps                     | 13600      |
| train/episodes                 | 1360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 54400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 34         |
| stats_o/mean                   | 0.20371275 |
| stats_o/std                    | 0.07943507 |
| test/episodes                  | 350        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.124     |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -13.333056 |
| test/Q_plus_P                  | -13.333056 |
| test/reward_per_eps            | -40        |
| test/steps                     | 14000      |
| train/episodes                 | 1400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 56000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.20381111 |
| stats_o/std                    | 0.0792369  |
| test/episodes                  | 360        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0525     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0394    |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -11.952989 |
| test/Q_plus_P                  | -11.952989 |
| test/reward_per_eps            | -37.9      |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 36         |
| stats_o/mean                   | 0.20384641 |
| stats_o/std                    | 0.07930867 |
| test/episodes                  | 370        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0475     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0336    |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -12.344685 |
| test/Q_plus_P                  | -12.344685 |
| test/reward_per_eps            | -38.1      |
| test/steps                     | 14800      |
| train/episodes                 | 1480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 59200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.20370348  |
| stats_o/std                    | 0.08003525  |
| test/episodes                  | 380         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.095       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00704    |
| test/info_shaping_reward_mean  | -0.19       |
| test/info_shaping_reward_min   | -0.789      |
| test/Q                         | -11.2416725 |
| test/Q_plus_P                  | -11.2416725 |
| test/reward_per_eps            | -36.2       |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00937     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.19       |
| train/info_shaping_reward_min  | -0.299      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 38         |
| stats_o/mean                   | 0.20333597 |
| stats_o/std                    | 0.08159104 |
| test/episodes                  | 390        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.256     |
| test/info_shaping_reward_min   | -0.614     |
| test/Q                         | -14.303218 |
| test/Q_plus_P                  | -14.303218 |
| test/reward_per_eps            | -40        |
| test/steps                     | 15600      |
| train/episodes                 | 1560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.401     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 62400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 39         |
| stats_o/mean                   | 0.2033434  |
| stats_o/std                    | 0.08235529 |
| test/episodes                  | 400        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0984    |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.398     |
| test/Q                         | -12.902946 |
| test/Q_plus_P                  | -12.902946 |
| test/reward_per_eps            | -40        |
| test/steps                     | 16000      |
| train/episodes                 | 1600       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00375    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.192     |
| train/info_shaping_reward_min  | -0.317     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 64000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 40         |
| stats_o/mean                   | 0.20309818 |
| stats_o/std                    | 0.08257286 |
| test/episodes                  | 410        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.085      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0258    |
| test/info_shaping_reward_mean  | -0.15      |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -10.043105 |
| test/Q_plus_P                  | -10.043105 |
| test/reward_per_eps            | -36.6      |
| test/steps                     | 16400      |
| train/episodes                 | 1640       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.02       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 65600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 41         |
| stats_o/mean                   | 0.20288394 |
| stats_o/std                    | 0.0829403  |
| test/episodes                  | 420        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0275     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0168    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.219     |
| test/Q                         | -11.872392 |
| test/Q_plus_P                  | -11.872392 |
| test/reward_per_eps            | -38.9      |
| test/steps                     | 16800      |
| train/episodes                 | 1680       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00125    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 67200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 42         |
| stats_o/mean                   | 0.20269217 |
| stats_o/std                    | 0.08291314 |
| test/episodes                  | 430        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.045      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0226    |
| test/info_shaping_reward_mean  | -0.182     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -12.89531  |
| test/Q_plus_P                  | -12.89531  |
| test/reward_per_eps            | -38.2      |
| test/steps                     | 17200      |
| train/episodes                 | 1720       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0187     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0972    |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 68800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 43          |
| stats_o/mean                   | 0.20259589  |
| stats_o/std                    | 0.083532974 |
| test/episodes                  | 440         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0875      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00252    |
| test/info_shaping_reward_mean  | -0.151      |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -11.057032  |
| test/Q_plus_P                  | -11.057032  |
| test/reward_per_eps            | -36.5       |
| test/steps                     | 17600       |
| train/episodes                 | 1760        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.015       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.114      |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.226      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 70400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 44         |
| stats_o/mean                   | 0.20266047 |
| stats_o/std                    | 0.08397953 |
| test/episodes                  | 450        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.175      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00568   |
| test/info_shaping_reward_mean  | -0.137     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -12.664541 |
| test/Q_plus_P                  | -12.664541 |
| test/reward_per_eps            | -33        |
| test/steps                     | 18000      |
| train/episodes                 | 1800       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00562    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0821    |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.283     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 72000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.20264883  |
| stats_o/std                    | 0.084359996 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.207       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00383    |
| test/info_shaping_reward_mean  | -0.136      |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -10.538854  |
| test/Q_plus_P                  | -10.538854  |
| test/reward_per_eps            | -31.7       |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0181      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.102      |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 46         |
| stats_o/mean                   | 0.20243758 |
| stats_o/std                    | 0.08464393 |
| test/episodes                  | 470        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.28       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00544   |
| test/info_shaping_reward_mean  | -0.121     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -10.755258 |
| test/Q_plus_P                  | -10.755258 |
| test/reward_per_eps            | -28.8      |
| test/steps                     | 18800      |
| train/episodes                 | 1880       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0194     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0781    |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 75200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 47         |
| stats_o/mean                   | 0.20227608 |
| stats_o/std                    | 0.0854     |
| test/episodes                  | 480        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.235      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00294   |
| test/info_shaping_reward_mean  | -0.137     |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -10.741743 |
| test/Q_plus_P                  | -10.741743 |
| test/reward_per_eps            | -30.6      |
| test/steps                     | 19200      |
| train/episodes                 | 1920       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0806     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0499    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.282     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.8      |
| train/steps                    | 76800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 48         |
| stats_o/mean                   | 0.20218134 |
| stats_o/std                    | 0.08533815 |
| test/episodes                  | 490        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.152      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0156    |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.525     |
| test/Q                         | -13.077981 |
| test/Q_plus_P                  | -13.077981 |
| test/reward_per_eps            | -33.9      |
| test/steps                     | 19600      |
| train/episodes                 | 1960       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0594     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0528    |
| train/info_shaping_reward_mean | -0.155     |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.6      |
| train/steps                    | 78400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 49         |
| stats_o/mean                   | 0.20204335 |
| stats_o/std                    | 0.0867629  |
| test/episodes                  | 500        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.223      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00307   |
| test/info_shaping_reward_mean  | -0.129     |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -9.832559  |
| test/Q_plus_P                  | -9.832559  |
| test/reward_per_eps            | -31.1      |
| test/steps                     | 20000      |
| train/episodes                 | 2000       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0762     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0345    |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37        |
| train/steps                    | 80000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.20191063 |
| stats_o/std                    | 0.08726021 |
| test/episodes                  | 510        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.172      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00997   |
| test/info_shaping_reward_mean  | -0.13      |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -11.650233 |
| test/Q_plus_P                  | -11.650233 |
| test/reward_per_eps            | -33.1      |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0731     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0603    |
| train/info_shaping_reward_mean | -0.157     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.1      |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.20180754 |
| stats_o/std                    | 0.08787161 |
| test/episodes                  | 520        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0875     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0119    |
| test/info_shaping_reward_mean  | -0.157     |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -13.679978 |
| test/Q_plus_P                  | -13.679978 |
| test/reward_per_eps            | -36.5      |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0869     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0552    |
| train/info_shaping_reward_mean | -0.167     |
| train/info_shaping_reward_min  | -0.294     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.5      |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.20167635 |
| stats_o/std                    | 0.08838346 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.128      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0111    |
| test/info_shaping_reward_mean  | -0.148     |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -12.417392 |
| test/Q_plus_P                  | -12.417392 |
| test/reward_per_eps            | -34.9      |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0544     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0613    |
| train/info_shaping_reward_mean | -0.156     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 53         |
| stats_o/mean                   | 0.20154162 |
| stats_o/std                    | 0.08901578 |
| test/episodes                  | 540        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.158      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0134    |
| test/info_shaping_reward_mean  | -0.156     |
| test/info_shaping_reward_min   | -0.598     |
| test/Q                         | -11.104253 |
| test/Q_plus_P                  | -11.104253 |
| test/reward_per_eps            | -33.7      |
| test/steps                     | 21600      |
| train/episodes                 | 2160       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.07       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0618    |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.2      |
| train/steps                    | 86400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 54         |
| stats_o/mean                   | 0.20135382 |
| stats_o/std                    | 0.09014055 |
| test/episodes                  | 550        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.23       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00398   |
| test/info_shaping_reward_mean  | -0.13      |
| test/info_shaping_reward_min   | -0.212     |
| test/Q                         | -11.690283 |
| test/Q_plus_P                  | -11.690283 |
| test/reward_per_eps            | -30.8      |
| test/steps                     | 22000      |
| train/episodes                 | 2200       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0769     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0426    |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.404     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.9      |
| train/steps                    | 88000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 55         |
| stats_o/mean                   | 0.20123094 |
| stats_o/std                    | 0.09015455 |
| test/episodes                  | 560        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.203      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00176   |
| test/info_shaping_reward_mean  | -0.125     |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -11.09062  |
| test/Q_plus_P                  | -11.09062  |
| test/reward_per_eps            | -31.9      |
| test/steps                     | 22400      |
| train/episodes                 | 2240       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.104      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0381    |
| train/info_shaping_reward_mean | -0.16      |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.8      |
| train/steps                    | 89600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 56         |
| stats_o/mean                   | 0.2009397  |
| stats_o/std                    | 0.0912917  |
| test/episodes                  | 570        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.383      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00234   |
| test/info_shaping_reward_mean  | -0.104     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -10.146471 |
| test/Q_plus_P                  | -10.146471 |
| test/reward_per_eps            | -24.7      |
| test/steps                     | 22800      |
| train/episodes                 | 2280       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0875     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.04      |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.409     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.5      |
| train/steps                    | 91200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 57         |
| stats_o/mean                   | 0.2007069  |
| stats_o/std                    | 0.09172757 |
| test/episodes                  | 580        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.472      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00537   |
| test/info_shaping_reward_mean  | -0.0905    |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -7.686924  |
| test/Q_plus_P                  | -7.686924  |
| test/reward_per_eps            | -21.1      |
| test/steps                     | 23200      |
| train/episodes                 | 2320       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.124      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.039     |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.362     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35        |
| train/steps                    | 92800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 58         |
| stats_o/mean                   | 0.2006937  |
| stats_o/std                    | 0.09177539 |
| test/episodes                  | 590        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.18       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.015     |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.843     |
| test/Q                         | -12.926863 |
| test/Q_plus_P                  | -12.926863 |
| test/reward_per_eps            | -32.8      |
| test/steps                     | 23600      |
| train/episodes                 | 2360       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.124      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0269    |
| train/info_shaping_reward_mean | -0.157     |
| train/info_shaping_reward_min  | -0.286     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35        |
| train/steps                    | 94400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 59         |
| stats_o/mean                   | 0.20063852 |
| stats_o/std                    | 0.09240048 |
| test/episodes                  | 600        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.588      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.0816    |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -6.069349  |
| test/Q_plus_P                  | -6.069349  |
| test/reward_per_eps            | -16.5      |
| test/steps                     | 24000      |
| train/episodes                 | 2400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.117      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0199    |
| train/info_shaping_reward_mean | -0.162     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.3      |
| train/steps                    | 96000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 60         |
| stats_o/mean                   | 0.20055854 |
| stats_o/std                    | 0.0928838  |
| test/episodes                  | 610        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.378      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.109     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -11.235141 |
| test/Q_plus_P                  | -11.235141 |
| test/reward_per_eps            | -24.9      |
| test/steps                     | 24400      |
| train/episodes                 | 2440       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0981     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0504    |
| train/info_shaping_reward_mean | -0.154     |
| train/info_shaping_reward_min  | -0.272     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.1      |
| train/steps                    | 97600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 61          |
| stats_o/mean                   | 0.20041586  |
| stats_o/std                    | 0.093369626 |
| test/episodes                  | 620         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.32        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00414    |
| test/info_shaping_reward_mean  | -0.147      |
| test/info_shaping_reward_min   | -0.755      |
| test/Q                         | -10.29728   |
| test/Q_plus_P                  | -10.29728   |
| test/reward_per_eps            | -27.2       |
| test/steps                     | 24800       |
| train/episodes                 | 2480        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.214       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0218     |
| train/info_shaping_reward_mean | -0.143      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.4       |
| train/steps                    | 99200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 62         |
| stats_o/mean                   | 0.20029333 |
| stats_o/std                    | 0.09439485 |
| test/episodes                  | 630        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.297      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.008     |
| test/info_shaping_reward_mean  | -0.144     |
| test/info_shaping_reward_min   | -0.796     |
| test/Q                         | -8.805708  |
| test/Q_plus_P                  | -8.805708  |
| test/reward_per_eps            | -28.1      |
| test/steps                     | 25200      |
| train/episodes                 | 2520       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.169      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0236    |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.47      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.2      |
| train/steps                    | 100800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 63         |
| stats_o/mean                   | 0.20010601 |
| stats_o/std                    | 0.09527315 |
| test/episodes                  | 640        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.32       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00838   |
| test/info_shaping_reward_mean  | -0.191     |
| test/info_shaping_reward_min   | -0.768     |
| test/Q                         | -11.539973 |
| test/Q_plus_P                  | -11.539973 |
| test/reward_per_eps            | -27.2      |
| test/steps                     | 25600      |
| train/episodes                 | 2560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.162      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0173    |
| train/info_shaping_reward_mean | -0.147     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.5      |
| train/steps                    | 102400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.19985363 |
| stats_o/std                    | 0.09603079 |
| test/episodes                  | 650        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.55       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00163   |
| test/info_shaping_reward_mean  | -0.103     |
| test/info_shaping_reward_min   | -0.547     |
| test/Q                         | -7.044031  |
| test/Q_plus_P                  | -7.044031  |
| test/reward_per_eps            | -18        |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.131      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0339    |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.396     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.8      |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 65         |
| stats_o/mean                   | 0.19967754 |
| stats_o/std                    | 0.09650439 |
| test/episodes                  | 660        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.545      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00126   |
| test/info_shaping_reward_mean  | -0.0813    |
| test/info_shaping_reward_min   | -0.208     |
| test/Q                         | -7.79018   |
| test/Q_plus_P                  | -7.79018   |
| test/reward_per_eps            | -18.2      |
| test/steps                     | 26400      |
| train/episodes                 | 2640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.262      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0084    |
| train/info_shaping_reward_mean | -0.141     |
| train/info_shaping_reward_min  | -0.311     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.5      |
| train/steps                    | 105600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 66         |
| stats_o/mean                   | 0.19971137 |
| stats_o/std                    | 0.09712138 |
| test/episodes                  | 670        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.53       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00199   |
| test/info_shaping_reward_mean  | -0.0781    |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -6.975299  |
| test/Q_plus_P                  | -6.975299  |
| test/reward_per_eps            | -18.8      |
| test/steps                     | 26800      |
| train/episodes                 | 2680       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.226      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0169    |
| train/info_shaping_reward_mean | -0.156     |
| train/info_shaping_reward_min  | -0.423     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.9      |
| train/steps                    | 107200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 67          |
| stats_o/mean                   | 0.1994817   |
| stats_o/std                    | 0.097740114 |
| test/episodes                  | 680         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.49        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00268    |
| test/info_shaping_reward_mean  | -0.0861     |
| test/info_shaping_reward_min   | -0.212      |
| test/Q                         | -6.7122636  |
| test/Q_plus_P                  | -6.7122636  |
| test/reward_per_eps            | -20.4       |
| test/steps                     | 27200       |
| train/episodes                 | 2720        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.176       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0187     |
| train/info_shaping_reward_mean | -0.165      |
| train/info_shaping_reward_min  | -0.42       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33         |
| train/steps                    | 108800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 68         |
| stats_o/mean                   | 0.19926041 |
| stats_o/std                    | 0.09831324 |
| test/episodes                  | 690        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.59       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00265   |
| test/info_shaping_reward_mean  | -0.134     |
| test/info_shaping_reward_min   | -1.09      |
| test/Q                         | -7.1695533 |
| test/Q_plus_P                  | -7.1695533 |
| test/reward_per_eps            | -16.4      |
| test/steps                     | 27600      |
| train/episodes                 | 2760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.299      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0083    |
| train/info_shaping_reward_mean | -0.12      |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28        |
| train/steps                    | 110400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 69         |
| stats_o/mean                   | 0.19910292 |
| stats_o/std                    | 0.09880106 |
| test/episodes                  | 700        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.463      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.104     |
| test/info_shaping_reward_min   | -0.565     |
| test/Q                         | -6.890482  |
| test/Q_plus_P                  | -6.890482  |
| test/reward_per_eps            | -21.5      |
| test/steps                     | 28000      |
| train/episodes                 | 2800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.312      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00794   |
| train/info_shaping_reward_mean | -0.153     |
| train/info_shaping_reward_min  | -0.48      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.5      |
| train/steps                    | 112000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.19882688 |
| stats_o/std                    | 0.09892211 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00249   |
| test/info_shaping_reward_mean  | -0.0658    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -5.816129  |
| test/Q_plus_P                  | -5.816129  |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.388      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00816   |
| train/info_shaping_reward_mean | -0.122     |
| train/info_shaping_reward_min  | -0.301     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.5      |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 71         |
| stats_o/mean                   | 0.1986338  |
| stats_o/std                    | 0.09925103 |
| test/episodes                  | 720        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.55       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00245   |
| test/info_shaping_reward_mean  | -0.0751    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -6.3411255 |
| test/Q_plus_P                  | -6.3411255 |
| test/reward_per_eps            | -18        |
| test/steps                     | 28800      |
| train/episodes                 | 2880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.326      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00757   |
| train/info_shaping_reward_mean | -0.119     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.9      |
| train/steps                    | 115200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 72         |
| stats_o/mean                   | 0.19848967 |
| stats_o/std                    | 0.0993289  |
| test/episodes                  | 730        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.63       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00312   |
| test/info_shaping_reward_mean  | -0.0639    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -5.3159227 |
| test/Q_plus_P                  | -5.3159227 |
| test/reward_per_eps            | -14.8      |
| test/steps                     | 29200      |
| train/episodes                 | 2920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.308      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00744   |
| train/info_shaping_reward_mean | -0.118     |
| train/info_shaping_reward_min  | -0.29      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.7      |
| train/steps                    | 116800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 73          |
| stats_o/mean                   | 0.1982812   |
| stats_o/std                    | 0.099452294 |
| test/episodes                  | 740         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.645       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.001      |
| test/info_shaping_reward_mean  | -0.0617     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -4.947057   |
| test/Q_plus_P                  | -4.947057   |
| test/reward_per_eps            | -14.2       |
| test/steps                     | 29600       |
| train/episodes                 | 2960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.348       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00998    |
| train/info_shaping_reward_mean | -0.105      |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.1       |
| train/steps                    | 118400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 74         |
| stats_o/mean                   | 0.19816515 |
| stats_o/std                    | 0.09933244 |
| test/episodes                  | 750        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00278   |
| test/info_shaping_reward_mean  | -0.0587    |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -4.7522626 |
| test/Q_plus_P                  | -4.7522626 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 30000      |
| train/episodes                 | 3000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.353      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00853   |
| train/info_shaping_reward_mean | -0.0987    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.9      |
| train/steps                    | 120000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 75         |
| stats_o/mean                   | 0.19807395 |
| stats_o/std                    | 0.09931046 |
| test/episodes                  | 760        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.588      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00327   |
| test/info_shaping_reward_mean  | -0.0751    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -6.686907  |
| test/Q_plus_P                  | -6.686907  |
| test/reward_per_eps            | -16.5      |
| test/steps                     | 30400      |
| train/episodes                 | 3040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.306      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00672   |
| train/info_shaping_reward_mean | -0.113     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.8      |
| train/steps                    | 121600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 76          |
| stats_o/mean                   | 0.19793579  |
| stats_o/std                    | 0.099516556 |
| test/episodes                  | 770         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000447   |
| test/info_shaping_reward_mean  | -0.0576     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -4.2991567  |
| test/Q_plus_P                  | -4.2991567  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 30800       |
| train/episodes                 | 3080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.399       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0072     |
| train/info_shaping_reward_mean | -0.101      |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.1       |
| train/steps                    | 123200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 77         |
| stats_o/mean                   | 0.19791287 |
| stats_o/std                    | 0.09951063 |
| test/episodes                  | 780        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00223   |
| test/info_shaping_reward_mean  | -0.0596    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -4.7274957 |
| test/Q_plus_P                  | -4.7274957 |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 31200      |
| train/episodes                 | 3120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.352      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0104    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.296     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.9      |
| train/steps                    | 124800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 78         |
| stats_o/mean                   | 0.19783434 |
| stats_o/std                    | 0.09936687 |
| test/episodes                  | 790        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.63       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00348   |
| test/info_shaping_reward_mean  | -0.0657    |
| test/info_shaping_reward_min   | -0.211     |
| test/Q                         | -4.9382677 |
| test/Q_plus_P                  | -4.9382677 |
| test/reward_per_eps            | -14.8      |
| test/steps                     | 31600      |
| train/episodes                 | 3160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.41       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.0976    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.6      |
| train/steps                    | 126400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 79         |
| stats_o/mean                   | 0.19771188 |
| stats_o/std                    | 0.09931731 |
| test/episodes                  | 800        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000598  |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -3.4789617 |
| test/Q_plus_P                  | -3.4789617 |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 32000      |
| train/episodes                 | 3200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.461      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00498   |
| train/info_shaping_reward_mean | -0.0909    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 128000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 80         |
| stats_o/mean                   | 0.19765161 |
| stats_o/std                    | 0.09939475 |
| test/episodes                  | 810        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00281   |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -3.7405775 |
| test/Q_plus_P                  | -3.7405775 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 32400      |
| train/episodes                 | 3240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.366      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00545   |
| train/info_shaping_reward_mean | -0.0985    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.4      |
| train/steps                    | 129600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 81         |
| stats_o/mean                   | 0.19755985 |
| stats_o/std                    | 0.09930842 |
| test/episodes                  | 820        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00183   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -3.6694279 |
| test/Q_plus_P                  | -3.6694279 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 32800      |
| train/episodes                 | 3280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.436      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00571   |
| train/info_shaping_reward_mean | -0.0892    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.6      |
| train/steps                    | 131200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 82         |
| stats_o/mean                   | 0.1974614  |
| stats_o/std                    | 0.09918117 |
| test/episodes                  | 830        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0025    |
| test/info_shaping_reward_mean  | -0.0587    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -3.6332672 |
| test/Q_plus_P                  | -3.6332672 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 33200      |
| train/episodes                 | 3320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.471      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00517   |
| train/info_shaping_reward_mean | -0.0878    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 132800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 83         |
| stats_o/mean                   | 0.19739613 |
| stats_o/std                    | 0.09906986 |
| test/episodes                  | 840        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00219   |
| test/info_shaping_reward_mean  | -0.0602    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -4.316382  |
| test/Q_plus_P                  | -4.316382  |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 33600      |
| train/episodes                 | 3360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.464      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0059    |
| train/info_shaping_reward_mean | -0.0897    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 134400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 84         |
| stats_o/mean                   | 0.19722515 |
| stats_o/std                    | 0.09918284 |
| test/episodes                  | 850        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00577   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.4497042 |
| test/Q_plus_P                  | -3.4497042 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 34000      |
| train/episodes                 | 3400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.408      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00715   |
| train/info_shaping_reward_mean | -0.0984    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.7      |
| train/steps                    | 136000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 85         |
| stats_o/mean                   | 0.19719206 |
| stats_o/std                    | 0.0989695  |
| test/episodes                  | 860        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.652      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00256   |
| test/info_shaping_reward_mean  | -0.0634    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -3.8772888 |
| test/Q_plus_P                  | -3.8772888 |
| test/reward_per_eps            | -13.9      |
| test/steps                     | 34400      |
| train/episodes                 | 3440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.456      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0067    |
| train/info_shaping_reward_mean | -0.084     |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 137600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 86         |
| stats_o/mean                   | 0.19713907 |
| stats_o/std                    | 0.0988068  |
| test/episodes                  | 870        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00208   |
| test/info_shaping_reward_mean  | -0.0557    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -3.4849055 |
| test/Q_plus_P                  | -3.4849055 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 34800      |
| train/episodes                 | 3480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.437      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00746   |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.272     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.5      |
| train/steps                    | 139200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 87          |
| stats_o/mean                   | 0.19699617  |
| stats_o/std                    | 0.098904654 |
| test/episodes                  | 880         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.657       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.002      |
| test/info_shaping_reward_mean  | -0.0605     |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -3.4189794  |
| test/Q_plus_P                  | -3.4189794  |
| test/reward_per_eps            | -13.7       |
| test/steps                     | 35200       |
| train/episodes                 | 3520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.428       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00458    |
| train/info_shaping_reward_mean | -0.103      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.9       |
| train/steps                    | 140800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 88          |
| stats_o/mean                   | 0.19695692  |
| stats_o/std                    | 0.098865554 |
| test/episodes                  | 890         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00223    |
| test/info_shaping_reward_mean  | -0.0554     |
| test/info_shaping_reward_min   | -0.191      |
| test/Q                         | -2.9211433  |
| test/Q_plus_P                  | -2.9211433  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 35600       |
| train/episodes                 | 3560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.439       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00538    |
| train/info_shaping_reward_mean | -0.0879     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.4       |
| train/steps                    | 142400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 89         |
| stats_o/mean                   | 0.19685641 |
| stats_o/std                    | 0.09883389 |
| test/episodes                  | 900        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00463   |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.154721  |
| test/Q_plus_P                  | -3.154721  |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 36000      |
| train/episodes                 | 3600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.458      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00417   |
| train/info_shaping_reward_mean | -0.091     |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.7      |
| train/steps                    | 144000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 90         |
| stats_o/mean                   | 0.19676965 |
| stats_o/std                    | 0.09864814 |
| test/episodes                  | 910        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0039    |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -2.3145323 |
| test/Q_plus_P                  | -2.3145323 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 36400      |
| train/episodes                 | 3640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.527      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00436   |
| train/info_shaping_reward_mean | -0.0797    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 145600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 91          |
| stats_o/mean                   | 0.1967305   |
| stats_o/std                    | 0.098570116 |
| test/episodes                  | 920         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00193    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -2.8152564  |
| test/Q_plus_P                  | -2.8152564  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 36800       |
| train/episodes                 | 3680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.444       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00588    |
| train/info_shaping_reward_mean | -0.0979     |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.2       |
| train/steps                    | 147200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 92         |
| stats_o/mean                   | 0.19656353 |
| stats_o/std                    | 0.09849336 |
| test/episodes                  | 930        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00488   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -2.5789325 |
| test/Q_plus_P                  | -2.5789325 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 37200      |
| train/episodes                 | 3720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.511      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00635   |
| train/info_shaping_reward_mean | -0.0813    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 148800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 93         |
| stats_o/mean                   | 0.19644494 |
| stats_o/std                    | 0.09838074 |
| test/episodes                  | 940        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00403   |
| test/info_shaping_reward_mean  | -0.0644    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -3.666376  |
| test/Q_plus_P                  | -3.666376  |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 37600      |
| train/episodes                 | 3760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00584   |
| train/info_shaping_reward_mean | -0.0795    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 150400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 94         |
| stats_o/mean                   | 0.19629861 |
| stats_o/std                    | 0.09848023 |
| test/episodes                  | 950        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -2.121499  |
| test/Q_plus_P                  | -2.121499  |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 38000      |
| train/episodes                 | 3800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.482      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.0995    |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.7      |
| train/steps                    | 152000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 95         |
| stats_o/mean                   | 0.19612598 |
| stats_o/std                    | 0.09860581 |
| test/episodes                  | 960        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00156   |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -2.254418  |
| test/Q_plus_P                  | -2.254418  |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 38400      |
| train/episodes                 | 3840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.473      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00578   |
| train/info_shaping_reward_mean | -0.0858    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 153600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 96         |
| stats_o/mean                   | 0.19599365 |
| stats_o/std                    | 0.09860519 |
| test/episodes                  | 970        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00192   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -2.818717  |
| test/Q_plus_P                  | -2.818717  |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 38800      |
| train/episodes                 | 3880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00593   |
| train/info_shaping_reward_mean | -0.0822    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 155200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 97         |
| stats_o/mean                   | 0.19598371 |
| stats_o/std                    | 0.09852141 |
| test/episodes                  | 980        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00277   |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -3.1973963 |
| test/Q_plus_P                  | -3.1973963 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 39200      |
| train/episodes                 | 3920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.508      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00565   |
| train/info_shaping_reward_mean | -0.0891    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.7      |
| train/steps                    | 156800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 98         |
| stats_o/mean                   | 0.19583523 |
| stats_o/std                    | 0.09855161 |
| test/episodes                  | 990        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000899  |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -2.121238  |
| test/Q_plus_P                  | -2.121238  |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 39600      |
| train/episodes                 | 3960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.493      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00492   |
| train/info_shaping_reward_mean | -0.0869    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.3      |
| train/steps                    | 158400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.1957635  |
| stats_o/std                    | 0.09844746 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00183   |
| test/info_shaping_reward_mean  | -0.0451    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.5313349 |
| test/Q_plus_P                  | -2.5313349 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00494   |
| train/info_shaping_reward_mean | -0.081     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 100         |
| stats_o/mean                   | 0.19565602  |
| stats_o/std                    | 0.098305725 |
| test/episodes                  | 1010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000859   |
| test/info_shaping_reward_mean  | -0.0554     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -2.7508764  |
| test/Q_plus_P                  | -2.7508764  |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 40400       |
| train/episodes                 | 4040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.573       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00451    |
| train/info_shaping_reward_mean | -0.0737     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 161600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 101        |
| stats_o/mean                   | 0.19555295 |
| stats_o/std                    | 0.09820251 |
| test/episodes                  | 1020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00246   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -2.3098044 |
| test/Q_plus_P                  | -2.3098044 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 40800      |
| train/episodes                 | 4080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.516      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00512   |
| train/info_shaping_reward_mean | -0.0818    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 163200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 102         |
| stats_o/mean                   | 0.19552249  |
| stats_o/std                    | 0.098061435 |
| test/episodes                  | 1030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00245    |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -2.4948568  |
| test/Q_plus_P                  | -2.4948568  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 41200       |
| train/episodes                 | 4120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.524       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00411    |
| train/info_shaping_reward_mean | -0.0762     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 164800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 103         |
| stats_o/mean                   | 0.19543988  |
| stats_o/std                    | 0.097936735 |
| test/episodes                  | 1040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.67        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00316    |
| test/info_shaping_reward_mean  | -0.0568     |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -2.7064273  |
| test/Q_plus_P                  | -2.7064273  |
| test/reward_per_eps            | -13.2       |
| test/steps                     | 41600       |
| train/episodes                 | 4160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.539       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00503    |
| train/info_shaping_reward_mean | -0.0816     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 166400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 104        |
| stats_o/mean                   | 0.19532463 |
| stats_o/std                    | 0.09784064 |
| test/episodes                  | 1050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00106   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -2.2006743 |
| test/Q_plus_P                  | -2.2006743 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 42000      |
| train/episodes                 | 4200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.511      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 168000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 105        |
| stats_o/mean                   | 0.19525419 |
| stats_o/std                    | 0.09771278 |
| test/episodes                  | 1060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00433   |
| test/info_shaping_reward_mean  | -0.0567    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -2.0694182 |
| test/Q_plus_P                  | -2.0694182 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 42400      |
| train/episodes                 | 4240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00435   |
| train/info_shaping_reward_mean | -0.0761    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 169600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 106        |
| stats_o/mean                   | 0.19517212 |
| stats_o/std                    | 0.09766807 |
| test/episodes                  | 1070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00177   |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -2.219934  |
| test/Q_plus_P                  | -2.219934  |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 42800      |
| train/episodes                 | 4280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.469      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00663   |
| train/info_shaping_reward_mean | -0.0811    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 171200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 107        |
| stats_o/mean                   | 0.1951008  |
| stats_o/std                    | 0.09760489 |
| test/episodes                  | 1080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00174   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.2035332 |
| test/Q_plus_P                  | -2.2035332 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 43200      |
| train/episodes                 | 4320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.46       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00384   |
| train/info_shaping_reward_mean | -0.0918    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 172800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 108        |
| stats_o/mean                   | 0.19505839 |
| stats_o/std                    | 0.09763701 |
| test/episodes                  | 1090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00134   |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -1.8888254 |
| test/Q_plus_P                  | -1.8888254 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 43600      |
| train/episodes                 | 4360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.501      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00502   |
| train/info_shaping_reward_mean | -0.0791    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 174400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 109        |
| stats_o/mean                   | 0.19495761 |
| stats_o/std                    | 0.09758503 |
| test/episodes                  | 1100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00161   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -1.7022831 |
| test/Q_plus_P                  | -1.7022831 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 44000      |
| train/episodes                 | 4400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.518      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00347   |
| train/info_shaping_reward_mean | -0.0781    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 176000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 110         |
| stats_o/mean                   | 0.1949082   |
| stats_o/std                    | 0.097438574 |
| test/episodes                  | 1110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0023     |
| test/info_shaping_reward_mean  | -0.0496     |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -1.5253077  |
| test/Q_plus_P                  | -1.5253077  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 44400       |
| train/episodes                 | 4440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.529       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00493    |
| train/info_shaping_reward_mean | -0.0751     |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.8       |
| train/steps                    | 177600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 111        |
| stats_o/mean                   | 0.19489557 |
| stats_o/std                    | 0.09743686 |
| test/episodes                  | 1120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000284  |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.9379767 |
| test/Q_plus_P                  | -1.9379767 |
| test/reward_per_eps            | -11        |
| test/steps                     | 44800      |
| train/episodes                 | 4480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.46       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00547   |
| train/info_shaping_reward_mean | -0.0875    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 179200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 112        |
| stats_o/mean                   | 0.19491044 |
| stats_o/std                    | 0.09731486 |
| test/episodes                  | 1130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0011    |
| test/info_shaping_reward_mean  | -0.0451    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -1.5877129 |
| test/Q_plus_P                  | -1.5877129 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 45200      |
| train/episodes                 | 4520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00264   |
| train/info_shaping_reward_mean | -0.076     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 180800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 113        |
| stats_o/mean                   | 0.19485408 |
| stats_o/std                    | 0.09720765 |
| test/episodes                  | 1140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00331   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.8497392 |
| test/Q_plus_P                  | -1.8497392 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 45600      |
| train/episodes                 | 4560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00351   |
| train/info_shaping_reward_mean | -0.0748    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 182400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 114        |
| stats_o/mean                   | 0.19479495 |
| stats_o/std                    | 0.09719739 |
| test/episodes                  | 1150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000774  |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.6821669 |
| test/Q_plus_P                  | -1.6821669 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 46000      |
| train/episodes                 | 4600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.525      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00298   |
| train/info_shaping_reward_mean | -0.0835    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 184000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 115         |
| stats_o/mean                   | 0.19477373  |
| stats_o/std                    | 0.097063154 |
| test/episodes                  | 1160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.0552     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -1.9169183  |
| test/Q_plus_P                  | -1.9169183  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 46400       |
| train/episodes                 | 4640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.517       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.0789     |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.3       |
| train/steps                    | 185600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 116         |
| stats_o/mean                   | 0.19471039  |
| stats_o/std                    | 0.096991085 |
| test/episodes                  | 1170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00128    |
| test/info_shaping_reward_mean  | -0.0563     |
| test/info_shaping_reward_min   | -0.19       |
| test/Q                         | -1.9757158  |
| test/Q_plus_P                  | -1.9757158  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 46800       |
| train/episodes                 | 4680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.553       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00528    |
| train/info_shaping_reward_mean | -0.0752     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 187200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 117        |
| stats_o/mean                   | 0.19469954 |
| stats_o/std                    | 0.09694263 |
| test/episodes                  | 1180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000967  |
| test/info_shaping_reward_mean  | -0.0447    |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -1.6322646 |
| test/Q_plus_P                  | -1.6322646 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 47200      |
| train/episodes                 | 4720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.507      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00333   |
| train/info_shaping_reward_mean | -0.0833    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.7      |
| train/steps                    | 188800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.19469336  |
| stats_o/std                    | 0.096986406 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0028     |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -1.3513849  |
| test/Q_plus_P                  | -1.3513849  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.537       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00557    |
| train/info_shaping_reward_mean | -0.0814     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.5       |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 119        |
| stats_o/mean                   | 0.19469574 |
| stats_o/std                    | 0.09694194 |
| test/episodes                  | 1200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000768  |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.4793216 |
| test/Q_plus_P                  | -1.4793216 |
| test/reward_per_eps            | -10        |
| test/steps                     | 48000      |
| train/episodes                 | 4800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0878    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 192000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 120         |
| stats_o/mean                   | 0.19464856  |
| stats_o/std                    | 0.096996784 |
| test/episodes                  | 1210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000598   |
| test/info_shaping_reward_mean  | -0.0501     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -1.4563613  |
| test/Q_plus_P                  | -1.4563613  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 48400       |
| train/episodes                 | 4840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.483       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00539    |
| train/info_shaping_reward_mean | -0.0988     |
| train/info_shaping_reward_min  | -0.306      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.7       |
| train/steps                    | 193600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.19461323 |
| stats_o/std                    | 0.09688003 |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00109   |
| test/info_shaping_reward_mean  | -0.0468    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.3206723 |
| test/Q_plus_P                  | -1.3206723 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00412   |
| train/info_shaping_reward_mean | -0.07      |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 122        |
| stats_o/mean                   | 0.1945806  |
| stats_o/std                    | 0.09685543 |
| test/episodes                  | 1230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00181   |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.202     |
| test/Q                         | -1.7359062 |
| test/Q_plus_P                  | -1.7359062 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 49200      |
| train/episodes                 | 4920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.501      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00518   |
| train/info_shaping_reward_mean | -0.0849    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20        |
| train/steps                    | 196800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 123         |
| stats_o/mean                   | 0.19452545  |
| stats_o/std                    | 0.096803285 |
| test/episodes                  | 1240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000401   |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.197      |
| test/Q                         | -1.4173214  |
| test/Q_plus_P                  | -1.4173214  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 49600       |
| train/episodes                 | 4960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.58        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0712     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 198400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.19455305  |
| stats_o/std                    | 0.096730135 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00298    |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -1.5544801  |
| test/Q_plus_P                  | -1.5544801  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.543       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0803     |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.3       |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 125        |
| stats_o/mean                   | 0.19449465 |
| stats_o/std                    | 0.09669154 |
| test/episodes                  | 1260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00248   |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1807057 |
| test/Q_plus_P                  | -1.1807057 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 50400      |
| train/episodes                 | 5040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.527      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0043    |
| train/info_shaping_reward_mean | -0.0766    |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 201600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 126        |
| stats_o/mean                   | 0.19449575 |
| stats_o/std                    | 0.09651868 |
| test/episodes                  | 1270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00267   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.309243  |
| test/Q_plus_P                  | -1.309243  |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 50800      |
| train/episodes                 | 5080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00387   |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 203200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 127        |
| stats_o/mean                   | 0.19443087 |
| stats_o/std                    | 0.09643355 |
| test/episodes                  | 1280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00121   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.8279997 |
| test/Q_plus_P                  | -1.8279997 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 51200      |
| train/episodes                 | 5120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0048    |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 204800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.19439623 |
| stats_o/std                    | 0.09632804 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.003     |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.6134518 |
| test/Q_plus_P                  | -1.6134518 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.511      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00424   |
| train/info_shaping_reward_mean | -0.0748    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 129        |
| stats_o/mean                   | 0.19434309 |
| stats_o/std                    | 0.09628425 |
| test/episodes                  | 1300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -2.975643  |
| test/Q_plus_P                  | -2.975643  |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 52000      |
| train/episodes                 | 5200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00378   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 208000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 130        |
| stats_o/mean                   | 0.19433947 |
| stats_o/std                    | 0.09612979 |
| test/episodes                  | 1310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00161   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -1.123546  |
| test/Q_plus_P                  | -1.123546  |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 52400      |
| train/episodes                 | 5240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00438   |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 209600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 131        |
| stats_o/mean                   | 0.19426434 |
| stats_o/std                    | 0.09609021 |
| test/episodes                  | 1320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000302  |
| test/info_shaping_reward_mean  | -0.0458    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -1.2276765 |
| test/Q_plus_P                  | -1.2276765 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 52800      |
| train/episodes                 | 5280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00362   |
| train/info_shaping_reward_mean | -0.0727    |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 211200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 132        |
| stats_o/mean                   | 0.19424628 |
| stats_o/std                    | 0.09606492 |
| test/episodes                  | 1330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00294   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.4811023 |
| test/Q_plus_P                  | -1.4811023 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 53200      |
| train/episodes                 | 5320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00434   |
| train/info_shaping_reward_mean | -0.0794    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 212800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 133         |
| stats_o/mean                   | 0.19421844  |
| stats_o/std                    | 0.095969684 |
| test/episodes                  | 1340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.201      |
| test/Q                         | -1.0181736  |
| test/Q_plus_P                  | -1.0181736  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 53600       |
| train/episodes                 | 5360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.497       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00442    |
| train/info_shaping_reward_mean | -0.0789     |
| train/info_shaping_reward_min  | -0.198      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 214400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 134        |
| stats_o/mean                   | 0.19416782 |
| stats_o/std                    | 0.09591923 |
| test/episodes                  | 1350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00185   |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -1.9515522 |
| test/Q_plus_P                  | -1.9515522 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 54000      |
| train/episodes                 | 5400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00552   |
| train/info_shaping_reward_mean | -0.0757    |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 216000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 135        |
| stats_o/mean                   | 0.19417545 |
| stats_o/std                    | 0.09600686 |
| test/episodes                  | 1360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00156   |
| test/info_shaping_reward_mean  | -0.0439    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.2363552 |
| test/Q_plus_P                  | -1.2363552 |
| test/reward_per_eps            | -9         |
| test/steps                     | 54400      |
| train/episodes                 | 5440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00379   |
| train/info_shaping_reward_mean | -0.081     |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 217600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 136        |
| stats_o/mean                   | 0.19413269 |
| stats_o/std                    | 0.09591445 |
| test/episodes                  | 1370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00172   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -1.4092795 |
| test/Q_plus_P                  | -1.4092795 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 54800      |
| train/episodes                 | 5480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.547      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00408   |
| train/info_shaping_reward_mean | -0.0803    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 219200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 137         |
| stats_o/mean                   | 0.19409716  |
| stats_o/std                    | 0.095872514 |
| test/episodes                  | 1380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000471   |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -1.4396529  |
| test/Q_plus_P                  | -1.4396529  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 55200       |
| train/episodes                 | 5520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00449    |
| train/info_shaping_reward_mean | -0.072      |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 220800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.1940711  |
| stats_o/std                    | 0.09584445 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00288   |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -1.595489  |
| test/Q_plus_P                  | -1.595489  |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00452   |
| train/info_shaping_reward_mean | -0.0796    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.19407274 |
| stats_o/std                    | 0.09581219 |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00328   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -1.3621386 |
| test/Q_plus_P                  | -1.3621386 |
| test/reward_per_eps            | -10        |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.0847    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 140        |
| stats_o/mean                   | 0.19405346 |
| stats_o/std                    | 0.09573016 |
| test/episodes                  | 1410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00124   |
| test/info_shaping_reward_mean  | -0.0443    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3882041 |
| test/Q_plus_P                  | -1.3882041 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 56400      |
| train/episodes                 | 5640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.565      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0045    |
| train/info_shaping_reward_mean | -0.0744    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 225600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.1940473  |
| stats_o/std                    | 0.09572484 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00185   |
| test/info_shaping_reward_mean  | -0.0437    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.2858672 |
| test/Q_plus_P                  | -1.2858672 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00356   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 142         |
| stats_o/mean                   | 0.1940302   |
| stats_o/std                    | 0.095579766 |
| test/episodes                  | 1430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00219    |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.193      |
| test/Q                         | -1.166841   |
| test/Q_plus_P                  | -1.166841   |
| test/reward_per_eps            | -9          |
| test/steps                     | 57200       |
| train/episodes                 | 5720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.0693     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 228800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 143        |
| stats_o/mean                   | 0.19402765 |
| stats_o/std                    | 0.09548399 |
| test/episodes                  | 1440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00212   |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -1.325687  |
| test/Q_plus_P                  | -1.325687  |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 57600      |
| train/episodes                 | 5760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 230400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.19404039  |
| stats_o/std                    | 0.095379286 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00112    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.355411   |
| test/Q_plus_P                  | -1.355411   |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.558       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00391    |
| train/info_shaping_reward_mean | -0.0714     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 145        |
| stats_o/mean                   | 0.19399542 |
| stats_o/std                    | 0.09536189 |
| test/episodes                  | 1460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.4634917 |
| test/Q_plus_P                  | -1.4634917 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 58400      |
| train/episodes                 | 5840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00388   |
| train/info_shaping_reward_mean | -0.084     |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 233600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 146        |
| stats_o/mean                   | 0.19395475 |
| stats_o/std                    | 0.09530146 |
| test/episodes                  | 1470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000116  |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.3689731 |
| test/Q_plus_P                  | -1.3689731 |
| test/reward_per_eps            | -10        |
| test/steps                     | 58800      |
| train/episodes                 | 5880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00325   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 235200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 147        |
| stats_o/mean                   | 0.1939402  |
| stats_o/std                    | 0.09521145 |
| test/episodes                  | 1480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00331   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.4727683 |
| test/Q_plus_P                  | -1.4727683 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 59200      |
| train/episodes                 | 5920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.55       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00377   |
| train/info_shaping_reward_mean | -0.0749    |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 236800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 148         |
| stats_o/mean                   | 0.19390902  |
| stats_o/std                    | 0.095125414 |
| test/episodes                  | 1490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00179    |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.193      |
| test/Q                         | -1.4196005  |
| test/Q_plus_P                  | -1.4196005  |
| test/reward_per_eps            | -10         |
| test/steps                     | 59600       |
| train/episodes                 | 5960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.549       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00416    |
| train/info_shaping_reward_mean | -0.0711     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 238400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 149        |
| stats_o/mean                   | 0.19391033 |
| stats_o/std                    | 0.09501111 |
| test/episodes                  | 1500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00428   |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.3585352 |
| test/Q_plus_P                  | -1.3585352 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 60000      |
| train/episodes                 | 6000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 240000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 150        |
| stats_o/mean                   | 0.19391659 |
| stats_o/std                    | 0.09506032 |
| test/episodes                  | 1510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00235   |
| test/info_shaping_reward_mean  | -0.0403    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.2207711 |
| test/Q_plus_P                  | -1.2207711 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 60400      |
| train/episodes                 | 6040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0049    |
| train/info_shaping_reward_mean | -0.0717    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 241600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 151        |
| stats_o/mean                   | 0.19392094 |
| stats_o/std                    | 0.09495651 |
| test/episodes                  | 1520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.5483391 |
| test/Q_plus_P                  | -1.5483391 |
| test/reward_per_eps            | -11        |
| test/steps                     | 60800      |
| train/episodes                 | 6080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00508   |
| train/info_shaping_reward_mean | -0.0724    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 243200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 152        |
| stats_o/mean                   | 0.1939063  |
| stats_o/std                    | 0.09493416 |
| test/episodes                  | 1530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.001     |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4952452 |
| test/Q_plus_P                  | -1.4952452 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 61200      |
| train/episodes                 | 6120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0034    |
| train/info_shaping_reward_mean | -0.0946    |
| train/info_shaping_reward_min  | -0.311     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 244800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 153        |
| stats_o/mean                   | 0.19385204 |
| stats_o/std                    | 0.09492996 |
| test/episodes                  | 1540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000897  |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2882992 |
| test/Q_plus_P                  | -1.2882992 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 61600      |
| train/episodes                 | 6160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0033    |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 246400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.19378786 |
| stats_o/std                    | 0.09489515 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00105   |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.2921264 |
| test/Q_plus_P                  | -1.2921264 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.575      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00446   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.19380984 |
| stats_o/std                    | 0.09484597 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00337   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2969397 |
| test/Q_plus_P                  | -1.2969397 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0031    |
| train/info_shaping_reward_mean | -0.0786    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 156         |
| stats_o/mean                   | 0.19377111  |
| stats_o/std                    | 0.09487649  |
| test/episodes                  | 1570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000791   |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -0.99652535 |
| test/Q_plus_P                  | -0.99652535 |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 62800       |
| train/episodes                 | 6280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.573       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00429    |
| train/info_shaping_reward_mean | -0.0709     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 251200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 157        |
| stats_o/mean                   | 0.1937407  |
| stats_o/std                    | 0.09477015 |
| test/episodes                  | 1580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00119   |
| test/info_shaping_reward_mean  | -0.041     |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -1.0730003 |
| test/Q_plus_P                  | -1.0730003 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 63200      |
| train/episodes                 | 6320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00411   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 252800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 158        |
| stats_o/mean                   | 0.19374071 |
| stats_o/std                    | 0.09471669 |
| test/episodes                  | 1590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000779  |
| test/info_shaping_reward_mean  | -0.0447    |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -1.1681623 |
| test/Q_plus_P                  | -1.1681623 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 63600      |
| train/episodes                 | 6360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00301   |
| train/info_shaping_reward_mean | -0.0715    |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 254400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.19371225  |
| stats_o/std                    | 0.094741434 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00155    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.2505828  |
| test/Q_plus_P                  | -1.2505828  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.503       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00391    |
| train/info_shaping_reward_mean | -0.0862     |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.9       |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 160        |
| stats_o/mean                   | 0.19371796 |
| stats_o/std                    | 0.0946513  |
| test/episodes                  | 1610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -1.0706167 |
| test/Q_plus_P                  | -1.0706167 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 64400      |
| train/episodes                 | 6440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.541      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00301   |
| train/info_shaping_reward_mean | -0.0756    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 257600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 161        |
| stats_o/mean                   | 0.19374491 |
| stats_o/std                    | 0.09453374 |
| test/episodes                  | 1620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000441  |
| test/info_shaping_reward_mean  | -0.0426    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -1.1069821 |
| test/Q_plus_P                  | -1.1069821 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 64800      |
| train/episodes                 | 6480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00368   |
| train/info_shaping_reward_mean | -0.0747    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 259200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 162        |
| stats_o/mean                   | 0.19377239 |
| stats_o/std                    | 0.09451761 |
| test/episodes                  | 1630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000454  |
| test/info_shaping_reward_mean  | -0.0451    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.2100601 |
| test/Q_plus_P                  | -1.2100601 |
| test/reward_per_eps            | -9         |
| test/steps                     | 65200      |
| train/episodes                 | 6520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.527      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00564   |
| train/info_shaping_reward_mean | -0.0784    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 260800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 163        |
| stats_o/mean                   | 0.19375141 |
| stats_o/std                    | 0.09462439 |
| test/episodes                  | 1640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00241   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.1802148 |
| test/Q_plus_P                  | -1.1802148 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 65600      |
| train/episodes                 | 6560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.0768    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 262400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 164        |
| stats_o/mean                   | 0.19376318 |
| stats_o/std                    | 0.09459156 |
| test/episodes                  | 1650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00174   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2120371 |
| test/Q_plus_P                  | -1.2120371 |
| test/reward_per_eps            | -10        |
| test/steps                     | 66000      |
| train/episodes                 | 6600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0039    |
| train/info_shaping_reward_mean | -0.0727    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 264000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 165        |
| stats_o/mean                   | 0.19378063 |
| stats_o/std                    | 0.0945349  |
| test/episodes                  | 1660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00453   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -0.9527943 |
| test/Q_plus_P                  | -0.9527943 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 66400      |
| train/episodes                 | 6640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00362   |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 265600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 166        |
| stats_o/mean                   | 0.19383204 |
| stats_o/std                    | 0.09450013 |
| test/episodes                  | 1670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00445   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.0465003 |
| test/Q_plus_P                  | -1.0465003 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 66800      |
| train/episodes                 | 6680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00274   |
| train/info_shaping_reward_mean | -0.0802    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 267200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.19382836 |
| stats_o/std                    | 0.09448163 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0026    |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1804162 |
| test/Q_plus_P                  | -1.1804162 |
| test/reward_per_eps            | -10        |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00322   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 168        |
| stats_o/mean                   | 0.193863   |
| stats_o/std                    | 0.09451345 |
| test/episodes                  | 1690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00131   |
| test/info_shaping_reward_mean  | -0.0398    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -0.8081207 |
| test/Q_plus_P                  | -0.8081207 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 67600      |
| train/episodes                 | 6760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.544      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00388   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 270400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.19386093 |
| stats_o/std                    | 0.09440174 |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00144   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -1.0607432 |
| test/Q_plus_P                  | -1.0607432 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00377   |
| train/info_shaping_reward_mean | -0.0666    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 170         |
| stats_o/mean                   | 0.1938458   |
| stats_o/std                    | 0.094304234 |
| test/episodes                  | 1710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0024     |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.3317821  |
| test/Q_plus_P                  | -1.3317821  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 68400       |
| train/episodes                 | 6840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00444    |
| train/info_shaping_reward_mean | -0.0704     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 273600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 171        |
| stats_o/mean                   | 0.19385631 |
| stats_o/std                    | 0.09424422 |
| test/episodes                  | 1720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00172   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.064689  |
| test/Q_plus_P                  | -1.064689  |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 68800      |
| train/episodes                 | 6880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00444   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 275200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 172         |
| stats_o/mean                   | 0.19386911  |
| stats_o/std                    | 0.094096735 |
| test/episodes                  | 1730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00265    |
| test/info_shaping_reward_mean  | -0.0447     |
| test/info_shaping_reward_min   | -0.19       |
| test/Q                         | -1.0164428  |
| test/Q_plus_P                  | -1.0164428  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 69200       |
| train/episodes                 | 6920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00344    |
| train/info_shaping_reward_mean | -0.0649     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 276800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 173        |
| stats_o/mean                   | 0.19383827 |
| stats_o/std                    | 0.09402162 |
| test/episodes                  | 1740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.0429    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2693056 |
| test/Q_plus_P                  | -1.2693056 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 69600      |
| train/episodes                 | 6960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0841    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 278400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 174        |
| stats_o/mean                   | 0.19382912 |
| stats_o/std                    | 0.0939536  |
| test/episodes                  | 1750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00373   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.219385  |
| test/Q_plus_P                  | -1.219385  |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 70000      |
| train/episodes                 | 7000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 280000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 175        |
| stats_o/mean                   | 0.19380075 |
| stats_o/std                    | 0.09387966 |
| test/episodes                  | 1760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0575219 |
| test/Q_plus_P                  | -1.0575219 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 70400      |
| train/episodes                 | 7040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.541      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00495   |
| train/info_shaping_reward_mean | -0.0717    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 281600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 176        |
| stats_o/mean                   | 0.1938366  |
| stats_o/std                    | 0.09374855 |
| test/episodes                  | 1770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00245   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.200298  |
| test/Q_plus_P                  | -1.200298  |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 70800      |
| train/episodes                 | 7080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00292   |
| train/info_shaping_reward_mean | -0.0673    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 283200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.19385585  |
| stats_o/std                    | 0.093688495 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000827   |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -1.3522071  |
| test/Q_plus_P                  | -1.3522071  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0726     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.1938458  |
| stats_o/std                    | 0.09368048 |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00218   |
| test/info_shaping_reward_mean  | -0.0443    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.0589397 |
| test/Q_plus_P                  | -1.0589397 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00417   |
| train/info_shaping_reward_mean | -0.0853    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 179        |
| stats_o/mean                   | 0.19384216 |
| stats_o/std                    | 0.09369605 |
| test/episodes                  | 1800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00048   |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1986365 |
| test/Q_plus_P                  | -1.1986365 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 72000      |
| train/episodes                 | 7200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00453   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 288000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 180        |
| stats_o/mean                   | 0.19382904 |
| stats_o/std                    | 0.09360411 |
| test/episodes                  | 1810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00134   |
| test/info_shaping_reward_mean  | -0.0441    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1649413 |
| test/Q_plus_P                  | -1.1649413 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 72400      |
| train/episodes                 | 7240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00264   |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 289600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 181        |
| stats_o/mean                   | 0.1938569  |
| stats_o/std                    | 0.09353995 |
| test/episodes                  | 1820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000868  |
| test/info_shaping_reward_mean  | -0.043     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1497692 |
| test/Q_plus_P                  | -1.1497692 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 72800      |
| train/episodes                 | 7280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00352   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 291200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 182         |
| stats_o/mean                   | 0.19390537  |
| stats_o/std                    | 0.093627416 |
| test/episodes                  | 1830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00191    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.2751548  |
| test/Q_plus_P                  | -1.2751548  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 73200       |
| train/episodes                 | 7320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00413    |
| train/info_shaping_reward_mean | -0.0661     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 292800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 183         |
| stats_o/mean                   | 0.19389765  |
| stats_o/std                    | 0.093642324 |
| test/episodes                  | 1840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00249    |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -1.1567147  |
| test/Q_plus_P                  | -1.1567147  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 73600       |
| train/episodes                 | 7360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00402    |
| train/info_shaping_reward_mean | -0.0755     |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 294400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 184        |
| stats_o/mean                   | 0.1939099  |
| stats_o/std                    | 0.09357019 |
| test/episodes                  | 1850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00473   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.3909732 |
| test/Q_plus_P                  | -1.3909732 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 74000      |
| train/episodes                 | 7400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00348   |
| train/info_shaping_reward_mean | -0.0801    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 296000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 185        |
| stats_o/mean                   | 0.19390418 |
| stats_o/std                    | 0.09346065 |
| test/episodes                  | 1860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00353   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.1631911 |
| test/Q_plus_P                  | -1.1631911 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 74400      |
| train/episodes                 | 7440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00409   |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 297600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 186        |
| stats_o/mean                   | 0.1939264  |
| stats_o/std                    | 0.09336159 |
| test/episodes                  | 1870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00555   |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.4209732 |
| test/Q_plus_P                  | -1.4209732 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 74800      |
| train/episodes                 | 7480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00387   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 299200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.19394529 |
| stats_o/std                    | 0.09323    |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00386   |
| test/info_shaping_reward_mean  | -0.0447    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2377743 |
| test/Q_plus_P                  | -1.2377743 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.645      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0651    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 188        |
| stats_o/mean                   | 0.1939487  |
| stats_o/std                    | 0.09311676 |
| test/episodes                  | 1890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00147   |
| test/info_shaping_reward_mean  | -0.0447    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.0675822 |
| test/Q_plus_P                  | -1.0675822 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 75600      |
| train/episodes                 | 7560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00383   |
| train/info_shaping_reward_mean | -0.0719    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 302400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.1939353   |
| stats_o/std                    | 0.093036585 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0042     |
| test/info_shaping_reward_mean  | -0.046      |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.0669588  |
| test/Q_plus_P                  | -1.0669588  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00411    |
| train/info_shaping_reward_mean | -0.0659     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 190        |
| stats_o/mean                   | 0.19394499 |
| stats_o/std                    | 0.09291897 |
| test/episodes                  | 1910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00549   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3539277 |
| test/Q_plus_P                  | -1.3539277 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 76400      |
| train/episodes                 | 7640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00502   |
| train/info_shaping_reward_mean | -0.066     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 305600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 191         |
| stats_o/mean                   | 0.19395499  |
| stats_o/std                    | 0.092896625 |
| test/episodes                  | 1920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00558    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -0.92619425 |
| test/Q_plus_P                  | -0.92619425 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 76800       |
| train/episodes                 | 7680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00436    |
| train/info_shaping_reward_mean | -0.0723     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 307200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 192        |
| stats_o/mean                   | 0.19397251 |
| stats_o/std                    | 0.09288532 |
| test/episodes                  | 1930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00579   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1657196 |
| test/Q_plus_P                  | -1.1657196 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 77200      |
| train/episodes                 | 7720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00559   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 308800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 193        |
| stats_o/mean                   | 0.19394481 |
| stats_o/std                    | 0.09279922 |
| test/episodes                  | 1940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00129   |
| test/info_shaping_reward_mean  | -0.0426    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0301218 |
| test/Q_plus_P                  | -1.0301218 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 77600      |
| train/episodes                 | 7760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00385   |
| train/info_shaping_reward_mean | -0.0631    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 310400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 194         |
| stats_o/mean                   | 0.19397043  |
| stats_o/std                    | 0.09276801  |
| test/episodes                  | 1950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00477    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -0.99563795 |
| test/Q_plus_P                  | -0.99563795 |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 78000       |
| train/episodes                 | 7800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.585       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00376    |
| train/info_shaping_reward_mean | -0.0669     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 312000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 195        |
| stats_o/mean                   | 0.19396412 |
| stats_o/std                    | 0.09277433 |
| test/episodes                  | 1960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00211   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2659572 |
| test/Q_plus_P                  | -1.2659572 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 78400      |
| train/episodes                 | 7840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0048    |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 313600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 196         |
| stats_o/mean                   | 0.1940102   |
| stats_o/std                    | 0.092679866 |
| test/episodes                  | 1970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00255    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.064772   |
| test/Q_plus_P                  | -1.064772   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 78800       |
| train/episodes                 | 7880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00444    |
| train/info_shaping_reward_mean | -0.0665     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 315200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 197        |
| stats_o/mean                   | 0.19400693 |
| stats_o/std                    | 0.09261194 |
| test/episodes                  | 1980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000181  |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.188638  |
| test/Q_plus_P                  | -1.188638  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 79200      |
| train/episodes                 | 7920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00355   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 316800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.1940207   |
| stats_o/std                    | 0.09251824  |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00113    |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.19       |
| test/Q                         | -0.98966545 |
| test/Q_plus_P                  | -0.98966545 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0688     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.19398545 |
| stats_o/std                    | 0.09247166 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00236   |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.1117027 |
| test/Q_plus_P                  | -1.1117027 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00352   |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 200         |
| stats_o/mean                   | 0.19402818  |
| stats_o/std                    | 0.092368454 |
| test/episodes                  | 2010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00272    |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.1966348  |
| test/Q_plus_P                  | -1.1966348  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 80400       |
| train/episodes                 | 8040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.574       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0034     |
| train/info_shaping_reward_mean | -0.0695     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 321600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 201         |
| stats_o/mean                   | 0.1940566   |
| stats_o/std                    | 0.09229746  |
| test/episodes                  | 2020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00219    |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -0.88910675 |
| test/Q_plus_P                  | -0.88910675 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 80800       |
| train/episodes                 | 8080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.595       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00484    |
| train/info_shaping_reward_mean | -0.0683     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 323200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 202         |
| stats_o/mean                   | 0.19405432  |
| stats_o/std                    | 0.092259824 |
| test/episodes                  | 2030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00553    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -1.081552   |
| test/Q_plus_P                  | -1.081552   |
| test/reward_per_eps            | -10         |
| test/steps                     | 81200       |
| train/episodes                 | 8120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00314    |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 324800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 203        |
| stats_o/mean                   | 0.1940749  |
| stats_o/std                    | 0.09219348 |
| test/episodes                  | 2040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00619   |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2037024 |
| test/Q_plus_P                  | -1.2037024 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 81600      |
| train/episodes                 | 8160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00491   |
| train/info_shaping_reward_mean | -0.0655    |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 326400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 204        |
| stats_o/mean                   | 0.19411325 |
| stats_o/std                    | 0.0921503  |
| test/episodes                  | 2050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00317   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.1140286 |
| test/Q_plus_P                  | -1.1140286 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 82000      |
| train/episodes                 | 8200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00311   |
| train/info_shaping_reward_mean | -0.0758    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 328000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 205        |
| stats_o/mean                   | 0.19413902 |
| stats_o/std                    | 0.09210373 |
| test/episodes                  | 2060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00262   |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.1653737 |
| test/Q_plus_P                  | -1.1653737 |
| test/reward_per_eps            | -9         |
| test/steps                     | 82400      |
| train/episodes                 | 8240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.643      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 329600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 206         |
| stats_o/mean                   | 0.194189    |
| stats_o/std                    | 0.091969214 |
| test/episodes                  | 2070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00494    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.6275052  |
| test/Q_plus_P                  | -1.6275052  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 82800       |
| train/episodes                 | 8280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00395    |
| train/info_shaping_reward_mean | -0.071      |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 331200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.1942388  |
| stats_o/std                    | 0.09188225 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00676   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1063678 |
| test/Q_plus_P                  | -1.1063678 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00398   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 208         |
| stats_o/mean                   | 0.19424514  |
| stats_o/std                    | 0.09181404  |
| test/episodes                  | 2090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000466   |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.97204554 |
| test/Q_plus_P                  | -0.97204554 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 83600       |
| train/episodes                 | 8360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00471    |
| train/info_shaping_reward_mean | -0.0701     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 334400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 209        |
| stats_o/mean                   | 0.19425374 |
| stats_o/std                    | 0.09186084 |
| test/episodes                  | 2100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00266   |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.6522084 |
| test/Q_plus_P                  | -1.6522084 |
| test/reward_per_eps            | -11        |
| test/steps                     | 84000      |
| train/episodes                 | 8400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00381   |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 336000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 210         |
| stats_o/mean                   | 0.19425823  |
| stats_o/std                    | 0.091821164 |
| test/episodes                  | 2110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00463    |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.1116141  |
| test/Q_plus_P                  | -1.1116141  |
| test/reward_per_eps            | -10         |
| test/steps                     | 84400       |
| train/episodes                 | 8440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00429    |
| train/info_shaping_reward_mean | -0.0705     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 337600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 211        |
| stats_o/mean                   | 0.19428663 |
| stats_o/std                    | 0.09177862 |
| test/episodes                  | 2120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00541   |
| test/info_shaping_reward_mean  | -0.0434    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0434557 |
| test/Q_plus_P                  | -1.0434557 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 84800      |
| train/episodes                 | 8480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0034    |
| train/info_shaping_reward_mean | -0.0686    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 339200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 212        |
| stats_o/mean                   | 0.19430345 |
| stats_o/std                    | 0.09173992 |
| test/episodes                  | 2130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00266   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.1204234 |
| test/Q_plus_P                  | -1.1204234 |
| test/reward_per_eps            | -9         |
| test/steps                     | 85200      |
| train/episodes                 | 8520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.582      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00293   |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 340800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 213         |
| stats_o/mean                   | 0.19429494  |
| stats_o/std                    | 0.09170211  |
| test/episodes                  | 2140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00396    |
| test/info_shaping_reward_mean  | -0.0496     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.97636694 |
| test/Q_plus_P                  | -0.97636694 |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 85600       |
| train/episodes                 | 8560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0041     |
| train/info_shaping_reward_mean | -0.0668     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 342400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 214        |
| stats_o/mean                   | 0.19431517 |
| stats_o/std                    | 0.0916055  |
| test/episodes                  | 2150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.0417    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1557468 |
| test/Q_plus_P                  | -1.1557468 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 86000      |
| train/episodes                 | 8600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0036    |
| train/info_shaping_reward_mean | -0.0628    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 344000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 215        |
| stats_o/mean                   | 0.19433442 |
| stats_o/std                    | 0.09152159 |
| test/episodes                  | 2160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00379   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.1810259 |
| test/Q_plus_P                  | -1.1810259 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 86400      |
| train/episodes                 | 8640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.0688    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 345600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 216        |
| stats_o/mean                   | 0.19431807 |
| stats_o/std                    | 0.09146081 |
| test/episodes                  | 2170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00373   |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1663283 |
| test/Q_plus_P                  | -1.1663283 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 86800      |
| train/episodes                 | 8680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00337   |
| train/info_shaping_reward_mean | -0.0639    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 347200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 217        |
| stats_o/mean                   | 0.19430725 |
| stats_o/std                    | 0.09138342 |
| test/episodes                  | 2180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0041    |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.0045377 |
| test/Q_plus_P                  | -1.0045377 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 87200      |
| train/episodes                 | 8720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0045    |
| train/info_shaping_reward_mean | -0.0646    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 348800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 218        |
| stats_o/mean                   | 0.19433299 |
| stats_o/std                    | 0.09134077 |
| test/episodes                  | 2190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00311   |
| test/info_shaping_reward_mean  | -0.0422    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0183352 |
| test/Q_plus_P                  | -1.0183352 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 87600      |
| train/episodes                 | 8760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00442   |
| train/info_shaping_reward_mean | -0.0813    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 350400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.1943335   |
| stats_o/std                    | 0.091246165 |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00329    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0306771  |
| test/Q_plus_P                  | -1.0306771  |
| test/reward_per_eps            | -9          |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00442    |
| train/info_shaping_reward_mean | -0.0683     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 220         |
| stats_o/mean                   | 0.19434771  |
| stats_o/std                    | 0.091172755 |
| test/episodes                  | 2210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00571    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.96433336 |
| test/Q_plus_P                  | -0.96433336 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 88400       |
| train/episodes                 | 8840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00448    |
| train/info_shaping_reward_mean | -0.0757     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 353600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 221        |
| stats_o/mean                   | 0.19437018 |
| stats_o/std                    | 0.09116601 |
| test/episodes                  | 2220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00572   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -0.9455126 |
| test/Q_plus_P                  | -0.9455126 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 88800      |
| train/episodes                 | 8880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00525   |
| train/info_shaping_reward_mean | -0.0819    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 355200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.19440332 |
| stats_o/std                    | 0.09107273 |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00336   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.2765758 |
| test/Q_plus_P                  | -1.2765758 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00291   |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 223        |
| stats_o/mean                   | 0.19439638 |
| stats_o/std                    | 0.09111531 |
| test/episodes                  | 2240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00718   |
| test/info_shaping_reward_mean  | -0.0468    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.0303079 |
| test/Q_plus_P                  | -1.0303079 |
| test/reward_per_eps            | -8         |
| test/steps                     | 89600      |
| train/episodes                 | 8960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00484   |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 358400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.19442642 |
| stats_o/std                    | 0.09101851 |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.007     |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.065923  |
| test/Q_plus_P                  | -1.065923  |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00359   |
| train/info_shaping_reward_mean | -0.0648    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 225         |
| stats_o/mean                   | 0.194467    |
| stats_o/std                    | 0.090943485 |
| test/episodes                  | 2260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00782    |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.99495816 |
| test/Q_plus_P                  | -0.99495816 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 90400       |
| train/episodes                 | 9040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.605       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.0673     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 361600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 226        |
| stats_o/mean                   | 0.19448698 |
| stats_o/std                    | 0.09084173 |
| test/episodes                  | 2270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0032    |
| test/info_shaping_reward_mean  | -0.046     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.120137  |
| test/Q_plus_P                  | -1.120137  |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 90800      |
| train/episodes                 | 9080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.63       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0049    |
| train/info_shaping_reward_mean | -0.0664    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 363200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 227        |
| stats_o/mean                   | 0.19451079 |
| stats_o/std                    | 0.0908132  |
| test/episodes                  | 2280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00163   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0320957 |
| test/Q_plus_P                  | -1.0320957 |
| test/reward_per_eps            | -9         |
| test/steps                     | 91200      |
| train/episodes                 | 9120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00487   |
| train/info_shaping_reward_mean | -0.0691    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 364800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 228        |
| stats_o/mean                   | 0.19453673 |
| stats_o/std                    | 0.09075131 |
| test/episodes                  | 2290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0454    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.961622  |
| test/Q_plus_P                  | -0.961622  |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 91600      |
| train/episodes                 | 9160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00328   |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 366400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 229        |
| stats_o/mean                   | 0.19457522 |
| stats_o/std                    | 0.0907062  |
| test/episodes                  | 2300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00634   |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1531426 |
| test/Q_plus_P                  | -1.1531426 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 92000      |
| train/episodes                 | 9200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00363   |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 368000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 230        |
| stats_o/mean                   | 0.19460067 |
| stats_o/std                    | 0.09069913 |
| test/episodes                  | 2310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00564   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9835918 |
| test/Q_plus_P                  | -0.9835918 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 92400      |
| train/episodes                 | 9240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00361   |
| train/info_shaping_reward_mean | -0.0688    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 369600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 231        |
| stats_o/mean                   | 0.19463289 |
| stats_o/std                    | 0.09060547 |
| test/episodes                  | 2320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00419   |
| test/info_shaping_reward_mean  | -0.0422    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0109702 |
| test/Q_plus_P                  | -1.0109702 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 92800      |
| train/episodes                 | 9280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0039    |
| train/info_shaping_reward_mean | -0.0683    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 371200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 232        |
| stats_o/mean                   | 0.1946702  |
| stats_o/std                    | 0.09052552 |
| test/episodes                  | 2330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00313   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.0480376 |
| test/Q_plus_P                  | -1.0480376 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 93200      |
| train/episodes                 | 9320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00528   |
| train/info_shaping_reward_mean | -0.0653    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 372800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 233        |
| stats_o/mean                   | 0.1946866  |
| stats_o/std                    | 0.09047062 |
| test/episodes                  | 2340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0027    |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.117785  |
| test/Q_plus_P                  | -1.117785  |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 93600      |
| train/episodes                 | 9360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00521   |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 374400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.1947181  |
| stats_o/std                    | 0.09040455 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00389   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.149612  |
| test/Q_plus_P                  | -1.149612  |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00467   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 235        |
| stats_o/mean                   | 0.1947366  |
| stats_o/std                    | 0.09034368 |
| test/episodes                  | 2360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00194   |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.1668214 |
| test/Q_plus_P                  | -1.1668214 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 94400      |
| train/episodes                 | 9440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.643      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00319   |
| train/info_shaping_reward_mean | -0.0646    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 377600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 236        |
| stats_o/mean                   | 0.19476815 |
| stats_o/std                    | 0.0902563  |
| test/episodes                  | 2370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00668   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0253301 |
| test/Q_plus_P                  | -1.0253301 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 94800      |
| train/episodes                 | 9480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00443   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 379200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 237        |
| stats_o/mean                   | 0.1948403  |
| stats_o/std                    | 0.09023761 |
| test/episodes                  | 2380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00522   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0316138 |
| test/Q_plus_P                  | -1.0316138 |
| test/reward_per_eps            | -9         |
| test/steps                     | 95200      |
| train/episodes                 | 9520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.0722    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 380800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 238        |
| stats_o/mean                   | 0.19484957 |
| stats_o/std                    | 0.09017353 |
| test/episodes                  | 2390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00435   |
| test/info_shaping_reward_mean  | -0.0575    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4208777 |
| test/Q_plus_P                  | -1.4208777 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 95600      |
| train/episodes                 | 9560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.068     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 382400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 239        |
| stats_o/mean                   | 0.19486739 |
| stats_o/std                    | 0.09012175 |
| test/episodes                  | 2400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0046    |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.3073272 |
| test/Q_plus_P                  | -1.3073272 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 96000      |
| train/episodes                 | 9600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00341   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 384000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 240         |
| stats_o/mean                   | 0.1948922   |
| stats_o/std                    | 0.090044044 |
| test/episodes                  | 2410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0044     |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.95059115 |
| test/Q_plus_P                  | -0.95059115 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 96400       |
| train/episodes                 | 9640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00404    |
| train/info_shaping_reward_mean | -0.0661     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 385600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 241        |
| stats_o/mean                   | 0.19489004 |
| stats_o/std                    | 0.0901116  |
| test/episodes                  | 2420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0813131 |
| test/Q_plus_P                  | -1.0813131 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 96800      |
| train/episodes                 | 9680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00371   |
| train/info_shaping_reward_mean | -0.0853    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 387200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 242        |
| stats_o/mean                   | 0.19490646 |
| stats_o/std                    | 0.09007797 |
| test/episodes                  | 2430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00375   |
| test/info_shaping_reward_mean  | -0.042     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.8673858 |
| test/Q_plus_P                  | -0.8673858 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 97200      |
| train/episodes                 | 9720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 388800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 243         |
| stats_o/mean                   | 0.1949204   |
| stats_o/std                    | 0.090071686 |
| test/episodes                  | 2440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00194    |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.9627473  |
| test/Q_plus_P                  | -0.9627473  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 97600       |
| train/episodes                 | 9760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.558       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00472    |
| train/info_shaping_reward_mean | -0.0768     |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 390400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.19494069 |
| stats_o/std                    | 0.08998521 |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00245   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.0547099 |
| test/Q_plus_P                  | -1.0547099 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00439   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 245        |
| stats_o/mean                   | 0.1949757  |
| stats_o/std                    | 0.08991326 |
| test/episodes                  | 2460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00305   |
| test/info_shaping_reward_mean  | -0.0433    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.966756  |
| test/Q_plus_P                  | -0.966756  |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 98400      |
| train/episodes                 | 9840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00437   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 393600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 246        |
| stats_o/mean                   | 0.19500855 |
| stats_o/std                    | 0.08989973 |
| test/episodes                  | 2470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00164   |
| test/info_shaping_reward_mean  | -0.0456    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.0928319 |
| test/Q_plus_P                  | -1.0928319 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 98800      |
| train/episodes                 | 9880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0782    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 395200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 247        |
| stats_o/mean                   | 0.19504195 |
| stats_o/std                    | 0.08993103 |
| test/episodes                  | 2480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00329   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0755225 |
| test/Q_plus_P                  | -1.0755225 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 99200      |
| train/episodes                 | 9920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00433   |
| train/info_shaping_reward_mean | -0.0792    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 396800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.19506405 |
| stats_o/std                    | 0.08984384 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.0443    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9908151 |
| test/Q_plus_P                  | -0.9908151 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00532   |
| train/info_shaping_reward_mean | -0.0669    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 249         |
| stats_o/mean                   | 0.19509044  |
| stats_o/std                    | 0.089771785 |
| test/episodes                  | 2500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00958    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.95203453 |
| test/Q_plus_P                  | -0.95203453 |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 100000      |
| train/episodes                 | 10000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0027     |
| train/info_shaping_reward_mean | -0.072      |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 400000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 250         |
| stats_o/mean                   | 0.19511448  |
| stats_o/std                    | 0.089689165 |
| test/episodes                  | 2510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00607    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -0.9503472  |
| test/Q_plus_P                  | -0.9503472  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 100400      |
| train/episodes                 | 10040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00434    |
| train/info_shaping_reward_mean | -0.0665     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 401600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 251        |
| stats_o/mean                   | 0.19513796 |
| stats_o/std                    | 0.089685   |
| test/episodes                  | 2520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00452   |
| test/info_shaping_reward_mean  | -0.0436    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -0.9755638 |
| test/Q_plus_P                  | -0.9755638 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 100800     |
| train/episodes                 | 10080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00518   |
| train/info_shaping_reward_mean | -0.0753    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 403200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 252         |
| stats_o/mean                   | 0.19516169  |
| stats_o/std                    | 0.089606486 |
| test/episodes                  | 2530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00305    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.9702407  |
| test/Q_plus_P                  | -0.9702407  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 101200      |
| train/episodes                 | 10120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00432    |
| train/info_shaping_reward_mean | -0.066      |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 404800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.19520257  |
| stats_o/std                    | 0.08951724  |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0106     |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.91445655 |
| test/Q_plus_P                  | -0.91445655 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00508    |
| train/info_shaping_reward_mean | -0.0669     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.19520842  |
| stats_o/std                    | 0.089517735 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00472    |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.88873184 |
| test/Q_plus_P                  | -0.88873184 |
| test/reward_per_eps            | -8          |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.0727     |
| train/info_shaping_reward_min  | -0.21       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 255         |
| stats_o/mean                   | 0.19524732  |
| stats_o/std                    | 0.089486815 |
| test/episodes                  | 2560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00329    |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.9952654  |
| test/Q_plus_P                  | -0.9952654  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 102400      |
| train/episodes                 | 10240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00526    |
| train/info_shaping_reward_mean | -0.0748     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 409600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 256        |
| stats_o/mean                   | 0.19524541 |
| stats_o/std                    | 0.0894804  |
| test/episodes                  | 2570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0128    |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1445053 |
| test/Q_plus_P                  | -1.1445053 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 102800     |
| train/episodes                 | 10280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00429   |
| train/info_shaping_reward_mean | -0.0673    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 411200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 257         |
| stats_o/mean                   | 0.19525461  |
| stats_o/std                    | 0.08941885  |
| test/episodes                  | 2580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0036     |
| test/info_shaping_reward_mean  | -0.043      |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.94105005 |
| test/Q_plus_P                  | -0.94105005 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 103200      |
| train/episodes                 | 10320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00486    |
| train/info_shaping_reward_mean | -0.0666     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 412800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 258         |
| stats_o/mean                   | 0.19527891  |
| stats_o/std                    | 0.089425944 |
| test/episodes                  | 2590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00483    |
| test/info_shaping_reward_mean  | -0.0472     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1766413  |
| test/Q_plus_P                  | -1.1766413  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 103600      |
| train/episodes                 | 10360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.605       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00538    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 414400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 259        |
| stats_o/mean                   | 0.19527893 |
| stats_o/std                    | 0.08936719 |
| test/episodes                  | 2600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00298   |
| test/info_shaping_reward_mean  | -0.0423    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0022746 |
| test/Q_plus_P                  | -1.0022746 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 104000     |
| train/episodes                 | 10400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00538   |
| train/info_shaping_reward_mean | -0.0655    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 416000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 260         |
| stats_o/mean                   | 0.19529824  |
| stats_o/std                    | 0.089404866 |
| test/episodes                  | 2610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00284    |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.1080042  |
| test/Q_plus_P                  | -1.1080042  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 104400      |
| train/episodes                 | 10440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00389    |
| train/info_shaping_reward_mean | -0.0822     |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 417600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 261        |
| stats_o/mean                   | 0.19532637 |
| stats_o/std                    | 0.08933485 |
| test/episodes                  | 2620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00284   |
| test/info_shaping_reward_mean  | -0.0457    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.0328168 |
| test/Q_plus_P                  | -1.0328168 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 104800     |
| train/episodes                 | 10480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0033    |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 419200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 262        |
| stats_o/mean                   | 0.19534245 |
| stats_o/std                    | 0.08926239 |
| test/episodes                  | 2630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00203   |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0588253 |
| test/Q_plus_P                  | -1.0588253 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 105200     |
| train/episodes                 | 10520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00339   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 420800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.19535996  |
| stats_o/std                    | 0.089199275 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00144    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.0539496  |
| test/Q_plus_P                  | -1.0539496  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00336    |
| train/info_shaping_reward_mean | -0.0639     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 264        |
| stats_o/mean                   | 0.19539383 |
| stats_o/std                    | 0.08916705 |
| test/episodes                  | 2650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0031    |
| test/info_shaping_reward_mean  | -0.0454    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0238366 |
| test/Q_plus_P                  | -1.0238366 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 106000     |
| train/episodes                 | 10600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00275   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 424000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 265         |
| stats_o/mean                   | 0.1954133   |
| stats_o/std                    | 0.08908638  |
| test/episodes                  | 2660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00411    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.98094046 |
| test/Q_plus_P                  | -0.98094046 |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 106400      |
| train/episodes                 | 10640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0047     |
| train/info_shaping_reward_mean | -0.0634     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 425600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.19543014  |
| stats_o/std                    | 0.089005984 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.003      |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.9843196  |
| test/Q_plus_P                  | -0.9843196  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00483    |
| train/info_shaping_reward_mean | -0.0646     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 267        |
| stats_o/mean                   | 0.19545591 |
| stats_o/std                    | 0.08895747 |
| test/episodes                  | 2680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00208   |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.0244702 |
| test/Q_plus_P                  | -1.0244702 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 107200     |
| train/episodes                 | 10720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00447   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 428800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.19546898  |
| stats_o/std                    | 0.088890344 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00338    |
| test/info_shaping_reward_mean  | -0.0434     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.97885203 |
| test/Q_plus_P                  | -0.97885203 |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00373    |
| train/info_shaping_reward_mean | -0.0676     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 269        |
| stats_o/mean                   | 0.19548264 |
| stats_o/std                    | 0.08884641 |
| test/episodes                  | 2700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00405   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0624905 |
| test/Q_plus_P                  | -1.0624905 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 108000     |
| train/episodes                 | 10800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0665    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 432000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 270        |
| stats_o/mean                   | 0.19548608 |
| stats_o/std                    | 0.08876497 |
| test/episodes                  | 2710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00473   |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1530871 |
| test/Q_plus_P                  | -1.1530871 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 108400     |
| train/episodes                 | 10840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0044    |
| train/info_shaping_reward_mean | -0.0626    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 433600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 271        |
| stats_o/mean                   | 0.19550428 |
| stats_o/std                    | 0.08870165 |
| test/episodes                  | 2720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00208   |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1526332 |
| test/Q_plus_P                  | -1.1526332 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 108800     |
| train/episodes                 | 10880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00432   |
| train/info_shaping_reward_mean | -0.0643    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 435200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.19551092  |
| stats_o/std                    | 0.088655956 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00518    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.1175216  |
| test/Q_plus_P                  | -1.1175216  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00367    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.209      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 273         |
| stats_o/mean                   | 0.19553289  |
| stats_o/std                    | 0.08862843  |
| test/episodes                  | 2740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00263    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -0.96917444 |
| test/Q_plus_P                  | -0.96917444 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 109600      |
| train/episodes                 | 10960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.595       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00417    |
| train/info_shaping_reward_mean | -0.0782     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 438400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 274        |
| stats_o/mean                   | 0.19557029 |
| stats_o/std                    | 0.08861459 |
| test/episodes                  | 2750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00626   |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.0540127 |
| test/Q_plus_P                  | -1.0540127 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 110000     |
| train/episodes                 | 11000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00384   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 440000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 275        |
| stats_o/mean                   | 0.19559233 |
| stats_o/std                    | 0.08856658 |
| test/episodes                  | 2760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00442   |
| test/info_shaping_reward_mean  | -0.0467    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0234692 |
| test/Q_plus_P                  | -1.0234692 |
| test/reward_per_eps            | -9         |
| test/steps                     | 110400     |
| train/episodes                 | 11040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00474   |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 441600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 276        |
| stats_o/mean                   | 0.19562544 |
| stats_o/std                    | 0.08849554 |
| test/episodes                  | 2770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0036    |
| test/info_shaping_reward_mean  | -0.0419    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8475576 |
| test/Q_plus_P                  | -0.8475576 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 110800     |
| train/episodes                 | 11080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00339   |
| train/info_shaping_reward_mean | -0.0639    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 443200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 277        |
| stats_o/mean                   | 0.19565539 |
| stats_o/std                    | 0.08843008 |
| test/episodes                  | 2780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00127   |
| test/info_shaping_reward_mean  | -0.0454    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.065558  |
| test/Q_plus_P                  | -1.065558  |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 111200     |
| train/episodes                 | 11120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00515   |
| train/info_shaping_reward_mean | -0.0647    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 444800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 278        |
| stats_o/mean                   | 0.19568546 |
| stats_o/std                    | 0.08834996 |
| test/episodes                  | 2790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00241   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.0133    |
| test/Q_plus_P                  | -1.0133    |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 111600     |
| train/episodes                 | 11160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.0613    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 446400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 279        |
| stats_o/mean                   | 0.19568478 |
| stats_o/std                    | 0.08837084 |
| test/episodes                  | 2800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0032    |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.1110859 |
| test/Q_plus_P                  | -1.1110859 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 112000     |
| train/episodes                 | 11200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00563   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 448000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 280         |
| stats_o/mean                   | 0.19571808  |
| stats_o/std                    | 0.088366956 |
| test/episodes                  | 2810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0021     |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.94491684 |
| test/Q_plus_P                  | -0.94491684 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 112400      |
| train/episodes                 | 11240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00365    |
| train/info_shaping_reward_mean | -0.0758     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 449600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.19572753 |
| stats_o/std                    | 0.08830287 |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00576   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0812998 |
| test/Q_plus_P                  | -1.0812998 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.062     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.19574554 |
| stats_o/std                    | 0.08823772 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0011    |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2168832 |
| test/Q_plus_P                  | -1.2168832 |
| test/reward_per_eps            | -10        |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00375   |
| train/info_shaping_reward_mean | -0.0683    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 283         |
| stats_o/mean                   | 0.19578244  |
| stats_o/std                    | 0.08823974  |
| test/episodes                  | 2840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00199    |
| test/info_shaping_reward_mean  | -0.0434     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.94171005 |
| test/Q_plus_P                  | -0.94171005 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 113600      |
| train/episodes                 | 11360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00405    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 454400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 284        |
| stats_o/mean                   | 0.19579478 |
| stats_o/std                    | 0.08818801 |
| test/episodes                  | 2850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00265   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.1096321 |
| test/Q_plus_P                  | -1.1096321 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 114000     |
| train/episodes                 | 11400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 456000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 285        |
| stats_o/mean                   | 0.19582994 |
| stats_o/std                    | 0.08816039 |
| test/episodes                  | 2860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00539   |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.9212349 |
| test/Q_plus_P                  | -0.9212349 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 114400     |
| train/episodes                 | 11440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 457600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 286        |
| stats_o/mean                   | 0.19582912 |
| stats_o/std                    | 0.08808524 |
| test/episodes                  | 2870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00317   |
| test/info_shaping_reward_mean  | -0.0414    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.9121617 |
| test/Q_plus_P                  | -0.9121617 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 114800     |
| train/episodes                 | 11480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00503   |
| train/info_shaping_reward_mean | -0.0667    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 459200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 287         |
| stats_o/mean                   | 0.1958339   |
| stats_o/std                    | 0.088031195 |
| test/episodes                  | 2880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00274    |
| test/info_shaping_reward_mean  | -0.0432     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.94331986 |
| test/Q_plus_P                  | -0.94331986 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 115200      |
| train/episodes                 | 11520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00546    |
| train/info_shaping_reward_mean | -0.068      |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 460800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 288         |
| stats_o/mean                   | 0.19584702  |
| stats_o/std                    | 0.087963715 |
| test/episodes                  | 2890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0038     |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.86866724 |
| test/Q_plus_P                  | -0.86866724 |
| test/reward_per_eps            | -8          |
| test/steps                     | 115600      |
| train/episodes                 | 11560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00502    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 462400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.1958606   |
| stats_o/std                    | 0.087948665 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.9348275  |
| test/Q_plus_P                  | -0.9348275  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.0673     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 290        |
| stats_o/mean                   | 0.19587933 |
| stats_o/std                    | 0.08790428 |
| test/episodes                  | 2910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00519   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2397858 |
| test/Q_plus_P                  | -1.2397858 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 116400     |
| train/episodes                 | 11640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.669      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0043    |
| train/info_shaping_reward_mean | -0.0618    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 465600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.19590487  |
| stats_o/std                    | 0.08788286  |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00367    |
| test/info_shaping_reward_mean  | -0.043      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.97722346 |
| test/Q_plus_P                  | -0.97722346 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00469    |
| train/info_shaping_reward_mean | -0.0716     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 292        |
| stats_o/mean                   | 0.19594474 |
| stats_o/std                    | 0.08787212 |
| test/episodes                  | 2930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0876461 |
| test/Q_plus_P                  | -1.0876461 |
| test/reward_per_eps            | -9         |
| test/steps                     | 117200     |
| train/episodes                 | 11720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00582   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 468800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 293        |
| stats_o/mean                   | 0.19596426 |
| stats_o/std                    | 0.08782706 |
| test/episodes                  | 2940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0041    |
| test/info_shaping_reward_mean  | -0.046     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9599038 |
| test/Q_plus_P                  | -0.9599038 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 117600     |
| train/episodes                 | 11760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00385   |
| train/info_shaping_reward_mean | -0.0647    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 470400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.1959978   |
| stats_o/std                    | 0.08778476  |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00492    |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.97599465 |
| test/Q_plus_P                  | -0.97599465 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0042     |
| train/info_shaping_reward_mean | -0.0693     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 295        |
| stats_o/mean                   | 0.19600639 |
| stats_o/std                    | 0.08775067 |
| test/episodes                  | 2960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00369   |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.954301  |
| test/Q_plus_P                  | -0.954301  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 118400     |
| train/episodes                 | 11840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00619   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 473600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 296        |
| stats_o/mean                   | 0.19604819 |
| stats_o/std                    | 0.08775493 |
| test/episodes                  | 2970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00234   |
| test/info_shaping_reward_mean  | -0.0423    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -0.8198284 |
| test/Q_plus_P                  | -0.8198284 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 118800     |
| train/episodes                 | 11880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00596   |
| train/info_shaping_reward_mean | -0.0775    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 475200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 297        |
| stats_o/mean                   | 0.19604847 |
| stats_o/std                    | 0.08770831 |
| test/episodes                  | 2980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00656   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1657972 |
| test/Q_plus_P                  | -1.1657972 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 119200     |
| train/episodes                 | 11920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.64       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00495   |
| train/info_shaping_reward_mean | -0.0654    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 476800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 298        |
| stats_o/mean                   | 0.19606173 |
| stats_o/std                    | 0.08771632 |
| test/episodes                  | 2990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00793   |
| test/info_shaping_reward_mean  | -0.0467    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1123872 |
| test/Q_plus_P                  | -1.1123872 |
| test/reward_per_eps            | -9         |
| test/steps                     | 119600     |
| train/episodes                 | 11960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00529   |
| train/info_shaping_reward_mean | -0.0743    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 478400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 299        |
| stats_o/mean                   | 0.19608694 |
| stats_o/std                    | 0.08765865 |
| test/episodes                  | 3000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000626  |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.8943163 |
| test/Q_plus_P                  | -0.8943163 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 120000     |
| train/episodes                 | 12000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00584   |
| train/info_shaping_reward_mean | -0.0624    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 480000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 300        |
| stats_o/mean                   | 0.19611563 |
| stats_o/std                    | 0.08758331 |
| test/episodes                  | 3010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00596   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9868938 |
| test/Q_plus_P                  | -0.9868938 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 120400     |
| train/episodes                 | 12040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.643      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.005     |
| train/info_shaping_reward_mean | -0.0632    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 481600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.19614518 |
| stats_o/std                    | 0.08756188 |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00742   |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9916415 |
| test/Q_plus_P                  | -0.9916415 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00561   |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 302        |
| stats_o/mean                   | 0.1961505  |
| stats_o/std                    | 0.08752332 |
| test/episodes                  | 3030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00439   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0987853 |
| test/Q_plus_P                  | -1.0987853 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 121200     |
| train/episodes                 | 12120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00458   |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 484800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 303        |
| stats_o/mean                   | 0.19617662 |
| stats_o/std                    | 0.08749133 |
| test/episodes                  | 3040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00109   |
| test/info_shaping_reward_mean  | -0.0433    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0507917 |
| test/Q_plus_P                  | -1.0507917 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 121600     |
| train/episodes                 | 12160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00438   |
| train/info_shaping_reward_mean | -0.0613    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 486400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.19618683  |
| stats_o/std                    | 0.087421626 |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000872   |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.88542175 |
| test/Q_plus_P                  | -0.88542175 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0036     |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 305         |
| stats_o/mean                   | 0.19620873  |
| stats_o/std                    | 0.087360665 |
| test/episodes                  | 3060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00451    |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.0019985  |
| test/Q_plus_P                  | -1.0019985  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 122400      |
| train/episodes                 | 12240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00485    |
| train/info_shaping_reward_mean | -0.0631     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 489600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 306         |
| stats_o/mean                   | 0.19622189  |
| stats_o/std                    | 0.087283954 |
| test/episodes                  | 3070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00689    |
| test/info_shaping_reward_mean  | -0.046      |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.1517203  |
| test/Q_plus_P                  | -1.1517203  |
| test/reward_per_eps            | -9          |
| test/steps                     | 122800      |
| train/episodes                 | 12280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00527    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 491200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 307         |
| stats_o/mean                   | 0.19625819  |
| stats_o/std                    | 0.08726383  |
| test/episodes                  | 3080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00235    |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.97677016 |
| test/Q_plus_P                  | -0.97677016 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 123200      |
| train/episodes                 | 12320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00387    |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 492800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 308        |
| stats_o/mean                   | 0.19626357 |
| stats_o/std                    | 0.08722254 |
| test/episodes                  | 3090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00817   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9485413 |
| test/Q_plus_P                  | -0.9485413 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 123600     |
| train/episodes                 | 12360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.645      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 494400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 309        |
| stats_o/mean                   | 0.19628525 |
| stats_o/std                    | 0.08717681 |
| test/episodes                  | 3100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00768   |
| test/info_shaping_reward_mean  | -0.0458    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.8604064 |
| test/Q_plus_P                  | -0.8604064 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 124000     |
| train/episodes                 | 12400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00547   |
| train/info_shaping_reward_mean | -0.0722    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 496000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.19630516  |
| stats_o/std                    | 0.087139465 |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0103     |
| test/info_shaping_reward_mean  | -0.0489     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.9215946  |
| test/Q_plus_P                  | -0.9215946  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.007      |
| train/info_shaping_reward_mean | -0.0698     |
| train/info_shaping_reward_min  | -0.194      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.19630636 |
| stats_o/std                    | 0.08712865 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00234   |
| test/info_shaping_reward_mean  | -0.0421    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.9935476 |
| test/Q_plus_P                  | -0.9935476 |
| test/reward_per_eps            | -8         |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.595      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00627   |
| train/info_shaping_reward_mean | -0.0688    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 312         |
| stats_o/mean                   | 0.19633956  |
| stats_o/std                    | 0.087065294 |
| test/episodes                  | 3130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0134     |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.921238   |
| test/Q_plus_P                  | -0.921238   |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 125200      |
| train/episodes                 | 12520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00466    |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 500800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 313        |
| stats_o/mean                   | 0.19635881 |
| stats_o/std                    | 0.08702716 |
| test/episodes                  | 3140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00334   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0065571 |
| test/Q_plus_P                  | -1.0065571 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 125600     |
| train/episodes                 | 12560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00538   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 502400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 314        |
| stats_o/mean                   | 0.19637795 |
| stats_o/std                    | 0.08697638 |
| test/episodes                  | 3150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00992   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0107032 |
| test/Q_plus_P                  | -1.0107032 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 126000     |
| train/episodes                 | 12600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0051    |
| train/info_shaping_reward_mean | -0.0634    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 504000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 315        |
| stats_o/mean                   | 0.19640107 |
| stats_o/std                    | 0.08695459 |
| test/episodes                  | 3160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00672   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0451373 |
| test/Q_plus_P                  | -1.0451373 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 126400     |
| train/episodes                 | 12640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00532   |
| train/info_shaping_reward_mean | -0.0762    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 505600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 316        |
| stats_o/mean                   | 0.19641563 |
| stats_o/std                    | 0.0869526  |
| test/episodes                  | 3170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00542   |
| test/info_shaping_reward_mean  | -0.0436    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.8952421 |
| test/Q_plus_P                  | -0.8952421 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 126800     |
| train/episodes                 | 12680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.0774    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 507200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 317         |
| stats_o/mean                   | 0.19644117  |
| stats_o/std                    | 0.086908296 |
| test/episodes                  | 3180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1153792  |
| test/Q_plus_P                  | -1.1153792  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 127200      |
| train/episodes                 | 12720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00524    |
| train/info_shaping_reward_mean | -0.0649     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 508800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 318        |
| stats_o/mean                   | 0.19645964 |
| stats_o/std                    | 0.0868738  |
| test/episodes                  | 3190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00538   |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -0.9122039 |
| test/Q_plus_P                  | -0.9122039 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 127600     |
| train/episodes                 | 12760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00454   |
| train/info_shaping_reward_mean | -0.0669    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 510400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 319        |
| stats_o/mean                   | 0.196477   |
| stats_o/std                    | 0.0868528  |
| test/episodes                  | 3200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00909   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8812782 |
| test/Q_plus_P                  | -0.8812782 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 128000     |
| train/episodes                 | 12800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00413   |
| train/info_shaping_reward_mean | -0.0744    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 512000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 320        |
| stats_o/mean                   | 0.19651583 |
| stats_o/std                    | 0.08685354 |
| test/episodes                  | 3210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00219   |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0163572 |
| test/Q_plus_P                  | -1.0163572 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 128400     |
| train/episodes                 | 12840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00387   |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 513600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 321        |
| stats_o/mean                   | 0.19653772 |
| stats_o/std                    | 0.08678597 |
| test/episodes                  | 3220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00547   |
| test/info_shaping_reward_mean  | -0.0446    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.934627  |
| test/Q_plus_P                  | -0.934627  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 128800     |
| train/episodes                 | 12880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00585   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 515200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 322        |
| stats_o/mean                   | 0.1965538  |
| stats_o/std                    | 0.08676399 |
| test/episodes                  | 3230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00778   |
| test/info_shaping_reward_mean  | -0.0621    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.2694178 |
| test/Q_plus_P                  | -2.2694178 |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 129200     |
| train/episodes                 | 12920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00684   |
| train/info_shaping_reward_mean | -0.072     |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 516800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 323        |
| stats_o/mean                   | 0.19657242 |
| stats_o/std                    | 0.08672659 |
| test/episodes                  | 3240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00177   |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.208448  |
| test/Q_plus_P                  | -1.208448  |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 129600     |
| train/episodes                 | 12960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00499   |
| train/info_shaping_reward_mean | -0.0642    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 518400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.19662136  |
| stats_o/std                    | 0.086750954 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0166     |
| test/info_shaping_reward_mean  | -0.0558     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0946479  |
| test/Q_plus_P                  | -1.0946479  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00486    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 325        |
| stats_o/mean                   | 0.19664127 |
| stats_o/std                    | 0.08668366 |
| test/episodes                  | 3260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00764   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -0.9875076 |
| test/Q_plus_P                  | -0.9875076 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 130400     |
| train/episodes                 | 13040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.653      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00555   |
| train/info_shaping_reward_mean | -0.068     |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 521600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.19665879  |
| stats_o/std                    | 0.08665443  |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00268    |
| test/info_shaping_reward_mean  | -0.043      |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -0.94303477 |
| test/Q_plus_P                  | -0.94303477 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00473    |
| train/info_shaping_reward_mean | -0.0614     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 327        |
| stats_o/mean                   | 0.19666946 |
| stats_o/std                    | 0.08659566 |
| test/episodes                  | 3280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0017    |
| test/info_shaping_reward_mean  | -0.0429    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.9849603 |
| test/Q_plus_P                  | -0.9849603 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 131200     |
| train/episodes                 | 13120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.665      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00358   |
| train/info_shaping_reward_mean | -0.0636    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 524800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 328        |
| stats_o/mean                   | 0.19669357 |
| stats_o/std                    | 0.08660579 |
| test/episodes                  | 3290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9466549 |
| test/Q_plus_P                  | -0.9466549 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 131600     |
| train/episodes                 | 13160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00434   |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 526400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 329         |
| stats_o/mean                   | 0.19671728  |
| stats_o/std                    | 0.086548306 |
| test/episodes                  | 3300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00303    |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.0073018  |
| test/Q_plus_P                  | -1.0073018  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 132000      |
| train/episodes                 | 13200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.0709     |
| train/info_shaping_reward_min  | -0.198      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 528000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 330        |
| stats_o/mean                   | 0.19672468 |
| stats_o/std                    | 0.08650974 |
| test/episodes                  | 3310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00806   |
| test/info_shaping_reward_mean  | -0.0451    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8817886 |
| test/Q_plus_P                  | -0.8817886 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 132400     |
| train/episodes                 | 13240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00671   |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 529600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 331        |
| stats_o/mean                   | 0.19672333 |
| stats_o/std                    | 0.08645249 |
| test/episodes                  | 3320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00715   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.8967086 |
| test/Q_plus_P                  | -0.8967086 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 132800     |
| train/episodes                 | 13280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00585   |
| train/info_shaping_reward_mean | -0.0615    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 531200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 332         |
| stats_o/mean                   | 0.19674562  |
| stats_o/std                    | 0.086393125 |
| test/episodes                  | 3330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0062     |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.92997694 |
| test/Q_plus_P                  | -0.92997694 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 133200      |
| train/episodes                 | 13320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00674    |
| train/info_shaping_reward_mean | -0.0643     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 532800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 333         |
| stats_o/mean                   | 0.1967588   |
| stats_o/std                    | 0.086352795 |
| test/episodes                  | 3340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00575    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.9786277  |
| test/Q_plus_P                  | -0.9786277  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 133600      |
| train/episodes                 | 13360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00506    |
| train/info_shaping_reward_mean | -0.0649     |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 534400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 334         |
| stats_o/mean                   | 0.19678533  |
| stats_o/std                    | 0.086343765 |
| test/episodes                  | 3350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00686    |
| test/info_shaping_reward_mean  | -0.0496     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.9343002  |
| test/Q_plus_P                  | -0.9343002  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 134000      |
| train/episodes                 | 13400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00572    |
| train/info_shaping_reward_mean | -0.0764     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 536000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 335         |
| stats_o/mean                   | 0.19679092  |
| stats_o/std                    | 0.08631264  |
| test/episodes                  | 3360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00645    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.91465896 |
| test/Q_plus_P                  | -0.91465896 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 134400      |
| train/episodes                 | 13440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00506    |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 537600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.19681385  |
| stats_o/std                    | 0.086289495 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00765    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.83925813 |
| test/Q_plus_P                  | -0.83925813 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0068     |
| train/info_shaping_reward_mean | -0.0769     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 337        |
| stats_o/mean                   | 0.19681026 |
| stats_o/std                    | 0.08630125 |
| test/episodes                  | 3380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.0432    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.8296855 |
| test/Q_plus_P                  | -0.8296855 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 135200     |
| train/episodes                 | 13520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00567   |
| train/info_shaping_reward_mean | -0.0738    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 540800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 338         |
| stats_o/mean                   | 0.19683492  |
| stats_o/std                    | 0.086259335 |
| test/episodes                  | 3390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0109     |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.000635   |
| test/Q_plus_P                  | -1.000635   |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 135600      |
| train/episodes                 | 13560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00606    |
| train/info_shaping_reward_mean | -0.0655     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 542400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.19685411  |
| stats_o/std                    | 0.086192004 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00251    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.9478503  |
| test/Q_plus_P                  | -0.9478503  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00556    |
| train/info_shaping_reward_mean | -0.0641     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 340        |
| stats_o/mean                   | 0.19687673 |
| stats_o/std                    | 0.08615065 |
| test/episodes                  | 3410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00747   |
| test/info_shaping_reward_mean  | -0.0431    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8194712 |
| test/Q_plus_P                  | -0.8194712 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 136400     |
| train/episodes                 | 13640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00421   |
| train/info_shaping_reward_mean | -0.0674    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 545600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 341         |
| stats_o/mean                   | 0.1968934   |
| stats_o/std                    | 0.086095765 |
| test/episodes                  | 3420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00182    |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.2828652  |
| test/Q_plus_P                  | -1.2828652  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 136800      |
| train/episodes                 | 13680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00452    |
| train/info_shaping_reward_mean | -0.0679     |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 547200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 342        |
| stats_o/mean                   | 0.19692065 |
| stats_o/std                    | 0.08602312 |
| test/episodes                  | 3430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000115  |
| test/info_shaping_reward_mean  | -0.0451    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0088533 |
| test/Q_plus_P                  | -1.0088533 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 137200     |
| train/episodes                 | 13720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00595   |
| train/info_shaping_reward_mean | -0.0653    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 548800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 343        |
| stats_o/mean                   | 0.19693935 |
| stats_o/std                    | 0.08603729 |
| test/episodes                  | 3440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0101    |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0489007 |
| test/Q_plus_P                  | -1.0489007 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 137600     |
| train/episodes                 | 13760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00514   |
| train/info_shaping_reward_mean | -0.0755    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 550400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 344         |
| stats_o/mean                   | 0.19695453  |
| stats_o/std                    | 0.08602368  |
| test/episodes                  | 3450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0025     |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.89376915 |
| test/Q_plus_P                  | -0.89376915 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 138000      |
| train/episodes                 | 13800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00554    |
| train/info_shaping_reward_mean | -0.0749     |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 552000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.1969811   |
| stats_o/std                    | 0.08595248  |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.011      |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.85288215 |
| test/Q_plus_P                  | -0.85288215 |
| test/reward_per_eps            | -8          |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00588    |
| train/info_shaping_reward_mean | -0.0621     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 346        |
| stats_o/mean                   | 0.19699623 |
| stats_o/std                    | 0.08591304 |
| test/episodes                  | 3470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00183   |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1679075 |
| test/Q_plus_P                  | -1.1679075 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 138800     |
| train/episodes                 | 13880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.662      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00633   |
| train/info_shaping_reward_mean | -0.0614    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 555200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 347        |
| stats_o/mean                   | 0.19700219 |
| stats_o/std                    | 0.08590119 |
| test/episodes                  | 3480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0106    |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.93087   |
| test/Q_plus_P                  | -0.93087   |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 139200     |
| train/episodes                 | 13920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00559   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 556800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.19700725  |
| stats_o/std                    | 0.085861295 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00893    |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1402655  |
| test/Q_plus_P                  | -1.1402655  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00648    |
| train/info_shaping_reward_mean | -0.0635     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 349        |
| stats_o/mean                   | 0.19701271 |
| stats_o/std                    | 0.08585621 |
| test/episodes                  | 3500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00589   |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9926297 |
| test/Q_plus_P                  | -0.9926297 |
| test/reward_per_eps            | -9         |
| test/steps                     | 140000     |
| train/episodes                 | 14000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00769   |
| train/info_shaping_reward_mean | -0.0818    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 560000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 350         |
| stats_o/mean                   | 0.197003    |
| stats_o/std                    | 0.085826695 |
| test/episodes                  | 3510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00351    |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.9530782  |
| test/Q_plus_P                  | -0.9530782  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 140400      |
| train/episodes                 | 14040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00766    |
| train/info_shaping_reward_mean | -0.0676     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 561600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 351         |
| stats_o/mean                   | 0.19702303  |
| stats_o/std                    | 0.08581736  |
| test/episodes                  | 3520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00353    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.78836584 |
| test/Q_plus_P                  | -0.78836584 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 140800      |
| train/episodes                 | 14080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00575    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 563200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 352        |
| stats_o/mean                   | 0.19705234 |
| stats_o/std                    | 0.08578567 |
| test/episodes                  | 3530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00384   |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9634173 |
| test/Q_plus_P                  | -0.9634173 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 141200     |
| train/episodes                 | 14120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00534   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 564800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 353         |
| stats_o/mean                   | 0.19706747  |
| stats_o/std                    | 0.085710086 |
| test/episodes                  | 3540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00424    |
| test/info_shaping_reward_mean  | -0.0438     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.82594156 |
| test/Q_plus_P                  | -0.82594156 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 141600      |
| train/episodes                 | 14160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00514    |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 566400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 354         |
| stats_o/mean                   | 0.19707882  |
| stats_o/std                    | 0.085667714 |
| test/episodes                  | 3550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00461    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.9355441  |
| test/Q_plus_P                  | -0.9355441  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 142000      |
| train/episodes                 | 14200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00696    |
| train/info_shaping_reward_mean | -0.0671     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 568000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.19709095  |
| stats_o/std                    | 0.085615724 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0143     |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.8955109  |
| test/Q_plus_P                  | -0.8955109  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00695    |
| train/info_shaping_reward_mean | -0.0658     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 356         |
| stats_o/mean                   | 0.19710368  |
| stats_o/std                    | 0.085592605 |
| test/episodes                  | 3570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00692    |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.90558493 |
| test/Q_plus_P                  | -0.90558493 |
| test/reward_per_eps            | -8          |
| test/steps                     | 142800      |
| train/episodes                 | 14280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00647    |
| train/info_shaping_reward_mean | -0.0689     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 571200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 357        |
| stats_o/mean                   | 0.19711867 |
| stats_o/std                    | 0.08555352 |
| test/episodes                  | 3580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00635   |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9310682 |
| test/Q_plus_P                  | -0.9310682 |
| test/reward_per_eps            | -9         |
| test/steps                     | 143200     |
| train/episodes                 | 14320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00521   |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 572800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 358        |
| stats_o/mean                   | 0.19712563 |
| stats_o/std                    | 0.08552156 |
| test/episodes                  | 3590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00434   |
| test/info_shaping_reward_mean  | -0.0458    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.0983583 |
| test/Q_plus_P                  | -1.0983583 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 143600     |
| train/episodes                 | 14360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00505   |
| train/info_shaping_reward_mean | -0.0681    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 574400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 359        |
| stats_o/mean                   | 0.19714962 |
| stats_o/std                    | 0.08548941 |
| test/episodes                  | 3600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0129    |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0478833 |
| test/Q_plus_P                  | -1.0478833 |
| test/reward_per_eps            | -9         |
| test/steps                     | 144000     |
| train/episodes                 | 14400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00561   |
| train/info_shaping_reward_mean | -0.0684    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 576000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 360        |
| stats_o/mean                   | 0.19716632 |
| stats_o/std                    | 0.08546843 |
| test/episodes                  | 3610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0061    |
| test/info_shaping_reward_mean  | -0.0423    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.8491336 |
| test/Q_plus_P                  | -0.8491336 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 144400     |
| train/episodes                 | 14440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00686   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 577600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 361         |
| stats_o/mean                   | 0.19718087  |
| stats_o/std                    | 0.085408114 |
| test/episodes                  | 3620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00193    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.8301064  |
| test/Q_plus_P                  | -0.8301064  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 144800      |
| train/episodes                 | 14480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00799    |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 579200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 362        |
| stats_o/mean                   | 0.1971948  |
| stats_o/std                    | 0.08538182 |
| test/episodes                  | 3630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0989144 |
| test/Q_plus_P                  | -1.0989144 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 145200     |
| train/episodes                 | 14520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00693   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 580800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 363        |
| stats_o/mean                   | 0.19720915 |
| stats_o/std                    | 0.08530685 |
| test/episodes                  | 3640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00344   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1077435 |
| test/Q_plus_P                  | -1.1077435 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 145600     |
| train/episodes                 | 14560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.67       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0062    |
| train/info_shaping_reward_mean | -0.0613    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 582400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 364         |
| stats_o/mean                   | 0.19722882  |
| stats_o/std                    | 0.085260525 |
| test/episodes                  | 3650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00957    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.87878627 |
| test/Q_plus_P                  | -0.87878627 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 146000      |
| train/episodes                 | 14600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00771    |
| train/info_shaping_reward_mean | -0.0663     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 584000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.19725104  |
| stats_o/std                    | 0.085239455 |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00337    |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.2015455  |
| test/Q_plus_P                  | -1.2015455  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00538    |
| train/info_shaping_reward_mean | -0.0745     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 366         |
| stats_o/mean                   | 0.19727555  |
| stats_o/std                    | 0.085265696 |
| test/episodes                  | 3670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00694    |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.87692255 |
| test/Q_plus_P                  | -0.87692255 |
| test/reward_per_eps            | -9          |
| test/steps                     | 146800      |
| train/episodes                 | 14680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00401    |
| train/info_shaping_reward_mean | -0.0631     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 587200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.19729085  |
| stats_o/std                    | 0.085254565 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00489    |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.9756248  |
| test/Q_plus_P                  | -0.9756248  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00352    |
| train/info_shaping_reward_mean | -0.0721     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 368        |
| stats_o/mean                   | 0.19732317 |
| stats_o/std                    | 0.08521513 |
| test/episodes                  | 3690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00494   |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0304164 |
| test/Q_plus_P                  | -1.0304164 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 147600     |
| train/episodes                 | 14760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0059    |
| train/info_shaping_reward_mean | -0.0673    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 590400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 369        |
| stats_o/mean                   | 0.19733469 |
| stats_o/std                    | 0.08519168 |
| test/episodes                  | 3700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0055    |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.9751899 |
| test/Q_plus_P                  | -0.9751899 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 148000     |
| train/episodes                 | 14800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00452   |
| train/info_shaping_reward_mean | -0.0656    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 592000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 370        |
| stats_o/mean                   | 0.1973618  |
| stats_o/std                    | 0.08514797 |
| test/episodes                  | 3710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00429   |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -0.9479569 |
| test/Q_plus_P                  | -0.9479569 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 148400     |
| train/episodes                 | 14840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00603   |
| train/info_shaping_reward_mean | -0.0662    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 593600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 371        |
| stats_o/mean                   | 0.19737644 |
| stats_o/std                    | 0.0851009  |
| test/episodes                  | 3720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00438   |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1458372 |
| test/Q_plus_P                  | -1.1458372 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 148800     |
| train/episodes                 | 14880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00482   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 595200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 372         |
| stats_o/mean                   | 0.19740543  |
| stats_o/std                    | 0.085042365 |
| test/episodes                  | 3730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0039     |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.00867    |
| test/Q_plus_P                  | -1.00867    |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 149200      |
| train/episodes                 | 14920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0069     |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 596800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 373        |
| stats_o/mean                   | 0.19741541 |
| stats_o/std                    | 0.08497319 |
| test/episodes                  | 3740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00818   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0839924 |
| test/Q_plus_P                  | -1.0839924 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 149600     |
| train/episodes                 | 14960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00791   |
| train/info_shaping_reward_mean | -0.0642    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 598400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.19744648 |
| stats_o/std                    | 0.0849137  |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00157   |
| test/info_shaping_reward_mean  | -0.043     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.8553012 |
| test/Q_plus_P                  | -0.8553012 |
| test/reward_per_eps            | -8         |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.675      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0627    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.19747481  |
| stats_o/std                    | 0.08489545  |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00725    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.88577545 |
| test/Q_plus_P                  | -0.88577545 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00614    |
| train/info_shaping_reward_mean | -0.0739     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.1974911  |
| stats_o/std                    | 0.08486124 |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00655   |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0587718 |
| test/Q_plus_P                  | -1.0587718 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00672   |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.19751467  |
| stats_o/std                    | 0.084837824 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00514    |
| test/info_shaping_reward_mean  | -0.0533     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1092074  |
| test/Q_plus_P                  | -1.1092074  |
| test/reward_per_eps            | -10         |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00553    |
| train/info_shaping_reward_mean | -0.072      |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 378        |
| stats_o/mean                   | 0.19754745 |
| stats_o/std                    | 0.0849016  |
| test/episodes                  | 3790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00609   |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9875667 |
| test/Q_plus_P                  | -0.9875667 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 151600     |
| train/episodes                 | 15160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.575      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00557   |
| train/info_shaping_reward_mean | -0.0863    |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 606400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 379        |
| stats_o/mean                   | 0.1975567  |
| stats_o/std                    | 0.08485299 |
| test/episodes                  | 3800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00636   |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9564479 |
| test/Q_plus_P                  | -0.9564479 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 152000     |
| train/episodes                 | 15200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.645      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00615   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 608000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 380        |
| stats_o/mean                   | 0.19758087 |
| stats_o/std                    | 0.08481079 |
| test/episodes                  | 3810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00728   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0289717 |
| test/Q_plus_P                  | -1.0289717 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 152400     |
| train/episodes                 | 15240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00468   |
| train/info_shaping_reward_mean | -0.0654    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 609600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 381         |
| stats_o/mean                   | 0.1976043   |
| stats_o/std                    | 0.084780484 |
| test/episodes                  | 3820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00607    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.9972895  |
| test/Q_plus_P                  | -0.9972895  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 152800      |
| train/episodes                 | 15280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00528    |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 611200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.19762732 |
| stats_o/std                    | 0.08479188 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00631   |
| test/info_shaping_reward_mean  | -0.0427    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8600507 |
| test/Q_plus_P                  | -0.8600507 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00545   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 383        |
| stats_o/mean                   | 0.19763416 |
| stats_o/std                    | 0.08477997 |
| test/episodes                  | 3840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0105    |
| test/info_shaping_reward_mean  | -0.0454    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.8107155 |
| test/Q_plus_P                  | -0.8107155 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 153600     |
| train/episodes                 | 15360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00515   |
| train/info_shaping_reward_mean | -0.0666    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 614400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 384        |
| stats_o/mean                   | 0.19765137 |
| stats_o/std                    | 0.08472997 |
| test/episodes                  | 3850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00291   |
| test/info_shaping_reward_mean  | -0.0439    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8825502 |
| test/Q_plus_P                  | -0.8825502 |
| test/reward_per_eps            | -8         |
| test/steps                     | 154000     |
| train/episodes                 | 15400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.652      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00566   |
| train/info_shaping_reward_mean | -0.0646    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 616000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 385        |
| stats_o/mean                   | 0.19766977 |
| stats_o/std                    | 0.08465846 |
| test/episodes                  | 3860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00544   |
| test/info_shaping_reward_mean  | -0.0531    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2523116 |
| test/Q_plus_P                  | -1.2523116 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 154400     |
| train/episodes                 | 15440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0603    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 617600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 386         |
| stats_o/mean                   | 0.19767827  |
| stats_o/std                    | 0.084602125 |
| test/episodes                  | 3870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00925    |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.172553   |
| test/Q_plus_P                  | -1.172553   |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 154800      |
| train/episodes                 | 15480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0053     |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 619200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.19769982  |
| stats_o/std                    | 0.084617525 |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00705    |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1503752  |
| test/Q_plus_P                  | -1.1503752  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00619    |
| train/info_shaping_reward_mean | -0.0889     |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 388        |
| stats_o/mean                   | 0.19771685 |
| stats_o/std                    | 0.08462573 |
| test/episodes                  | 3890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00668   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0526694 |
| test/Q_plus_P                  | -1.0526694 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 155600     |
| train/episodes                 | 15560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00522   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 622400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 389         |
| stats_o/mean                   | 0.19772954  |
| stats_o/std                    | 0.0846747   |
| test/episodes                  | 3900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00995    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.94097006 |
| test/Q_plus_P                  | -0.94097006 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 156000      |
| train/episodes                 | 15600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00697    |
| train/info_shaping_reward_mean | -0.084      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 624000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 390        |
| stats_o/mean                   | 0.19774918 |
| stats_o/std                    | 0.08466707 |
| test/episodes                  | 3910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0107    |
| test/info_shaping_reward_mean  | -0.0457    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0054564 |
| test/Q_plus_P                  | -1.0054564 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 156400     |
| train/episodes                 | 15640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.565      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00606   |
| train/info_shaping_reward_mean | -0.0719    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 625600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.19776344  |
| stats_o/std                    | 0.08463993  |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00577    |
| test/info_shaping_reward_mean  | -0.046      |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.85641676 |
| test/Q_plus_P                  | -0.85641676 |
| test/reward_per_eps            | -8          |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00467    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 392        |
| stats_o/mean                   | 0.19778451 |
| stats_o/std                    | 0.08460272 |
| test/episodes                  | 3930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0044    |
| test/info_shaping_reward_mean  | -0.0468    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.9373136 |
| test/Q_plus_P                  | -0.9373136 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 157200     |
| train/episodes                 | 15720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00716   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 628800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 393        |
| stats_o/mean                   | 0.19779417 |
| stats_o/std                    | 0.08457361 |
| test/episodes                  | 3940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0129    |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9883194 |
| test/Q_plus_P                  | -0.9883194 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 157600     |
| train/episodes                 | 15760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00531   |
| train/info_shaping_reward_mean | -0.0768    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 630400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 394         |
| stats_o/mean                   | 0.1978112   |
| stats_o/std                    | 0.08453024  |
| test/episodes                  | 3950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00653    |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.85352266 |
| test/Q_plus_P                  | -0.85352266 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 158000      |
| train/episodes                 | 15800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0051     |
| train/info_shaping_reward_mean | -0.0765     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 632000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 395        |
| stats_o/mean                   | 0.19783644 |
| stats_o/std                    | 0.08453304 |
| test/episodes                  | 3960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00688   |
| test/info_shaping_reward_mean  | -0.046     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.9583618 |
| test/Q_plus_P                  | -0.9583618 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 158400     |
| train/episodes                 | 15840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.662      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00521   |
| train/info_shaping_reward_mean | -0.0622    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 633600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 396         |
| stats_o/mean                   | 0.19786529  |
| stats_o/std                    | 0.084495954 |
| test/episodes                  | 3970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0126     |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.204      |
| test/Q                         | -1.0750662  |
| test/Q_plus_P                  | -1.0750662  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 158800      |
| train/episodes                 | 15880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.635       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00582    |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 635200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 397        |
| stats_o/mean                   | 0.19787347 |
| stats_o/std                    | 0.08449277 |
| test/episodes                  | 3980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0104    |
| test/info_shaping_reward_mean  | -0.0586    |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -1.7192082 |
| test/Q_plus_P                  | -1.7192082 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 159200     |
| train/episodes                 | 15920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00705   |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 636800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 398        |
| stats_o/mean                   | 0.19788742 |
| stats_o/std                    | 0.08445447 |
| test/episodes                  | 3990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00169   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2015475 |
| test/Q_plus_P                  | -1.2015475 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 159600     |
| train/episodes                 | 15960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0073    |
| train/info_shaping_reward_mean | -0.07      |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 638400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 399         |
| stats_o/mean                   | 0.19789511  |
| stats_o/std                    | 0.084409654 |
| test/episodes                  | 4000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00204    |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0493733  |
| test/Q_plus_P                  | -1.0493733  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 160000      |
| train/episodes                 | 16000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00492    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 640000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 400         |
| stats_o/mean                   | 0.19791597  |
| stats_o/std                    | 0.084359095 |
| test/episodes                  | 4010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00791    |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.146511   |
| test/Q_plus_P                  | -1.146511   |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 160400      |
| train/episodes                 | 16040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00458    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 641600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 401        |
| stats_o/mean                   | 0.19792694 |
| stats_o/std                    | 0.0843142  |
| test/episodes                  | 4020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00942   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.890732  |
| test/Q_plus_P                  | -0.890732  |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 160800     |
| train/episodes                 | 16080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00731   |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 643200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.19795948 |
| stats_o/std                    | 0.08428937 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00549   |
| test/info_shaping_reward_mean  | -0.047     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0657443 |
| test/Q_plus_P                  | -1.0657443 |
| test/reward_per_eps            | -9         |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00604   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 403         |
| stats_o/mean                   | 0.19795369  |
| stats_o/std                    | 0.08429048  |
| test/episodes                  | 4040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0118     |
| test/info_shaping_reward_mean  | -0.0445     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.71655595 |
| test/Q_plus_P                  | -0.71655595 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 161600      |
| train/episodes                 | 16160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00588    |
| train/info_shaping_reward_mean | -0.0763     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 646400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.19795927  |
| stats_o/std                    | 0.08427084  |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00488    |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.73800516 |
| test/Q_plus_P                  | -0.73800516 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0065     |
| train/info_shaping_reward_mean | -0.0658     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.19797753  |
| stats_o/std                    | 0.084309645 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00458    |
| test/info_shaping_reward_mean  | -0.043      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.9998836  |
| test/Q_plus_P                  | -0.9998836  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00685    |
| train/info_shaping_reward_mean | -0.0747     |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 406        |
| stats_o/mean                   | 0.19799043 |
| stats_o/std                    | 0.08428152 |
| test/episodes                  | 4070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00292   |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8901194 |
| test/Q_plus_P                  | -0.8901194 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 162800     |
| train/episodes                 | 16280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00712   |
| train/info_shaping_reward_mean | -0.0628    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 651200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 407        |
| stats_o/mean                   | 0.1980077  |
| stats_o/std                    | 0.08424803 |
| test/episodes                  | 4080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0412    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.8285658 |
| test/Q_plus_P                  | -0.8285658 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 163200     |
| train/episodes                 | 16320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.647      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00719   |
| train/info_shaping_reward_mean | -0.0643    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 652800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 408        |
| stats_o/mean                   | 0.19802482 |
| stats_o/std                    | 0.08422093 |
| test/episodes                  | 4090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00387   |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0684083 |
| test/Q_plus_P                  | -1.0684083 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 163600     |
| train/episodes                 | 16360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00694   |
| train/info_shaping_reward_mean | -0.0651    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 654400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 409        |
| stats_o/mean                   | 0.19804779 |
| stats_o/std                    | 0.08418363 |
| test/episodes                  | 4100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00428   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0642337 |
| test/Q_plus_P                  | -1.0642337 |
| test/reward_per_eps            | -9         |
| test/steps                     | 164000     |
| train/episodes                 | 16400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.643      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00734   |
| train/info_shaping_reward_mean | -0.0663    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 656000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 410        |
| stats_o/mean                   | 0.19806017 |
| stats_o/std                    | 0.08416012 |
| test/episodes                  | 4110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00953   |
| test/info_shaping_reward_mean  | -0.0456    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8118698 |
| test/Q_plus_P                  | -0.8118698 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 164400     |
| train/episodes                 | 16440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.671      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00624   |
| train/info_shaping_reward_mean | -0.0629    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 657600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 411        |
| stats_o/mean                   | 0.19807345 |
| stats_o/std                    | 0.08412704 |
| test/episodes                  | 4120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00923   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9724619 |
| test/Q_plus_P                  | -0.9724619 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 164800     |
| train/episodes                 | 16480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00653   |
| train/info_shaping_reward_mean | -0.063     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 659200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 412        |
| stats_o/mean                   | 0.19807468 |
| stats_o/std                    | 0.08408919 |
| test/episodes                  | 4130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00477   |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.012827  |
| test/Q_plus_P                  | -1.012827  |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 165200     |
| train/episodes                 | 16520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00804   |
| train/info_shaping_reward_mean | -0.0658    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 660800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.19809486  |
| stats_o/std                    | 0.0840664   |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00243    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.89272225 |
| test/Q_plus_P                  | -0.89272225 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00894    |
| train/info_shaping_reward_mean | -0.0666     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 414         |
| stats_o/mean                   | 0.1981024   |
| stats_o/std                    | 0.084027804 |
| test/episodes                  | 4150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00765    |
| test/info_shaping_reward_mean  | -0.0475     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1025386  |
| test/Q_plus_P                  | -1.1025386  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 166000      |
| train/episodes                 | 16600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00535    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 664000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 415        |
| stats_o/mean                   | 0.19811486 |
| stats_o/std                    | 0.08400417 |
| test/episodes                  | 4160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00817   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9215007 |
| test/Q_plus_P                  | -0.9215007 |
| test/reward_per_eps            | -9         |
| test/steps                     | 166400     |
| train/episodes                 | 16640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00594   |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 665600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 416        |
| stats_o/mean                   | 0.19813591 |
| stats_o/std                    | 0.08396062 |
| test/episodes                  | 4170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00752   |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.8604872 |
| test/Q_plus_P                  | -0.8604872 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 166800     |
| train/episodes                 | 16680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0074    |
| train/info_shaping_reward_mean | -0.063     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 667200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 417         |
| stats_o/mean                   | 0.19815753  |
| stats_o/std                    | 0.08392514  |
| test/episodes                  | 4180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0111     |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.92298025 |
| test/Q_plus_P                  | -0.92298025 |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 167200      |
| train/episodes                 | 16720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00705    |
| train/info_shaping_reward_mean | -0.0623     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 668800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.1981663   |
| stats_o/std                    | 0.083887026 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0089     |
| test/info_shaping_reward_mean  | -0.0496     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.83976626 |
| test/Q_plus_P                  | -0.83976626 |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00476    |
| train/info_shaping_reward_mean | -0.0664     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 419         |
| stats_o/mean                   | 0.19818754  |
| stats_o/std                    | 0.083869606 |
| test/episodes                  | 4200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0111     |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.113649   |
| test/Q_plus_P                  | -1.113649   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 168000      |
| train/episodes                 | 16800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00595    |
| train/info_shaping_reward_mean | -0.073      |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 672000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 420         |
| stats_o/mean                   | 0.19821268  |
| stats_o/std                    | 0.083843604 |
| test/episodes                  | 4210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00793    |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.8711554  |
| test/Q_plus_P                  | -0.8711554  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 168400      |
| train/episodes                 | 16840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00503    |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 673600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 421        |
| stats_o/mean                   | 0.19822387 |
| stats_o/std                    | 0.08386836 |
| test/episodes                  | 4220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00979   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8971538 |
| test/Q_plus_P                  | -0.8971538 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 168800     |
| train/episodes                 | 16880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00634   |
| train/info_shaping_reward_mean | -0.0792    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 675200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.19824518  |
| stats_o/std                    | 0.083892405 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000705   |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0698415  |
| test/Q_plus_P                  | -1.0698415  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00636    |
| train/info_shaping_reward_mean | -0.0728     |
| train/info_shaping_reward_min  | -0.209      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.19825548  |
| stats_o/std                    | 0.083842844 |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.017      |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.90006465 |
| test/Q_plus_P                  | -0.90006465 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00717    |
| train/info_shaping_reward_mean | -0.0683     |
| train/info_shaping_reward_min  | -0.194      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 424        |
| stats_o/mean                   | 0.19828011 |
| stats_o/std                    | 0.08381609 |
| test/episodes                  | 4250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00518   |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0097355 |
| test/Q_plus_P                  | -1.0097355 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 170000     |
| train/episodes                 | 17000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00895   |
| train/info_shaping_reward_mean | -0.0755    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 680000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 425        |
| stats_o/mean                   | 0.19829135 |
| stats_o/std                    | 0.08377583 |
| test/episodes                  | 4260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00812   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.060158  |
| test/Q_plus_P                  | -1.060158  |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 170400     |
| train/episodes                 | 17040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00471   |
| train/info_shaping_reward_mean | -0.0628    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 681600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.19830535  |
| stats_o/std                    | 0.083729774 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0136     |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.83798546 |
| test/Q_plus_P                  | -0.83798546 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00612    |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 427         |
| stats_o/mean                   | 0.19831948  |
| stats_o/std                    | 0.083709106 |
| test/episodes                  | 4280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0118     |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.9187615  |
| test/Q_plus_P                  | -0.9187615  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 171200      |
| train/episodes                 | 17120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00445    |
| train/info_shaping_reward_mean | -0.0675     |
| train/info_shaping_reward_min  | -0.208      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 684800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 428        |
| stats_o/mean                   | 0.19832869 |
| stats_o/std                    | 0.08369446 |
| test/episodes                  | 4290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0109    |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0473473 |
| test/Q_plus_P                  | -1.0473473 |
| test/reward_per_eps            | -9         |
| test/steps                     | 171600     |
| train/episodes                 | 17160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.647      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00591   |
| train/info_shaping_reward_mean | -0.0778    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 686400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 429        |
| stats_o/mean                   | 0.1983309  |
| stats_o/std                    | 0.08370549 |
| test/episodes                  | 4300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0154    |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8717752 |
| test/Q_plus_P                  | -0.8717752 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 172000     |
| train/episodes                 | 17200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0667    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 688000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 430        |
| stats_o/mean                   | 0.19834954 |
| stats_o/std                    | 0.08367382 |
| test/episodes                  | 4310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00508   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9527498 |
| test/Q_plus_P                  | -0.9527498 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 172400     |
| train/episodes                 | 17240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.63       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 689600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 431        |
| stats_o/mean                   | 0.19835335 |
| stats_o/std                    | 0.0836468  |
| test/episodes                  | 4320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.8610278 |
| test/Q_plus_P                  | -0.8610278 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 172800     |
| train/episodes                 | 17280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.635      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00654   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 691200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 432         |
| stats_o/mean                   | 0.19838022  |
| stats_o/std                    | 0.0836437   |
| test/episodes                  | 4330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0124     |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.98915774 |
| test/Q_plus_P                  | -0.98915774 |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 173200      |
| train/episodes                 | 17320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00562    |
| train/info_shaping_reward_mean | -0.0732     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 692800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 433         |
| stats_o/mean                   | 0.19840246  |
| stats_o/std                    | 0.08360399  |
| test/episodes                  | 4340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0126     |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.85259634 |
| test/Q_plus_P                  | -0.85259634 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 173600      |
| train/episodes                 | 17360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00581    |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 694400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 434        |
| stats_o/mean                   | 0.1984106  |
| stats_o/std                    | 0.08364421 |
| test/episodes                  | 4350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00599   |
| test/info_shaping_reward_mean  | -0.0444    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.7979597 |
| test/Q_plus_P                  | -0.7979597 |
| test/reward_per_eps            | -8         |
| test/steps                     | 174000     |
| train/episodes                 | 17400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00729   |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.296     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 696000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 435         |
| stats_o/mean                   | 0.1984304   |
| stats_o/std                    | 0.083612    |
| test/episodes                  | 4360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0106     |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.85451436 |
| test/Q_plus_P                  | -0.85451436 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 174400      |
| train/episodes                 | 17440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00885    |
| train/info_shaping_reward_mean | -0.0646     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 697600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 436         |
| stats_o/mean                   | 0.19844261  |
| stats_o/std                    | 0.0835763   |
| test/episodes                  | 4370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000575   |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.85456103 |
| test/Q_plus_P                  | -0.85456103 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 174800      |
| train/episodes                 | 17480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00634    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 699200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 437        |
| stats_o/mean                   | 0.1984388  |
| stats_o/std                    | 0.08357298 |
| test/episodes                  | 4380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.818      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000735  |
| test/info_shaping_reward_mean  | -0.0386    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.7951104 |
| test/Q_plus_P                  | -0.7951104 |
| test/reward_per_eps            | -7.3       |
| test/steps                     | 175200     |
| train/episodes                 | 17520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00719   |
| train/info_shaping_reward_mean | -0.0666    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 700800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 438        |
| stats_o/mean                   | 0.19845866 |
| stats_o/std                    | 0.08355227 |
| test/episodes                  | 4390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00979   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.8795066 |
| test/Q_plus_P                  | -0.8795066 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 175600     |
| train/episodes                 | 17560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.01      |
| train/info_shaping_reward_mean | -0.0654    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 702400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 439        |
| stats_o/mean                   | 0.19845451 |
| stats_o/std                    | 0.08351663 |
| test/episodes                  | 4400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00825   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8726002 |
| test/Q_plus_P                  | -0.8726002 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 176000     |
| train/episodes                 | 17600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00735   |
| train/info_shaping_reward_mean | -0.0623    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 704000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.19847202  |
| stats_o/std                    | 0.083522804 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00212    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.921532   |
| test/Q_plus_P                  | -0.921532   |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00607    |
| train/info_shaping_reward_mean | -0.0759     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.1984855   |
| stats_o/std                    | 0.08347106  |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000894   |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.97250795 |
| test/Q_plus_P                  | -0.97250795 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00855    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 442        |
| stats_o/mean                   | 0.1984939  |
| stats_o/std                    | 0.08341725 |
| test/episodes                  | 4430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00412   |
| test/info_shaping_reward_mean  | -0.0426    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.8526242 |
| test/Q_plus_P                  | -0.8526242 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 177200     |
| train/episodes                 | 17720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.666      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00481   |
| train/info_shaping_reward_mean | -0.062     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 708800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 443        |
| stats_o/mean                   | 0.19850248 |
| stats_o/std                    | 0.08343565 |
| test/episodes                  | 4440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0066    |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1005608 |
| test/Q_plus_P                  | -1.1005608 |
| test/reward_per_eps            | -9         |
| test/steps                     | 177600     |
| train/episodes                 | 17760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00726   |
| train/info_shaping_reward_mean | -0.0769    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 710400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 444        |
| stats_o/mean                   | 0.19850701 |
| stats_o/std                    | 0.08343748 |
| test/episodes                  | 4450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00484   |
| test/info_shaping_reward_mean  | -0.0547    |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -1.183817  |
| test/Q_plus_P                  | -1.183817  |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 178000     |
| train/episodes                 | 17800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00662   |
| train/info_shaping_reward_mean | -0.0805    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 712000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.1985203   |
| stats_o/std                    | 0.08339507  |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00607    |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.94223243 |
| test/Q_plus_P                  | -0.94223243 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00775    |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 446        |
| stats_o/mean                   | 0.19852968 |
| stats_o/std                    | 0.08338439 |
| test/episodes                  | 4470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00777   |
| test/info_shaping_reward_mean  | -0.0456    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.975012  |
| test/Q_plus_P                  | -0.975012  |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 178800     |
| train/episodes                 | 17880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00765   |
| train/info_shaping_reward_mean | -0.0767    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 715200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 447        |
| stats_o/mean                   | 0.19853292 |
| stats_o/std                    | 0.08342695 |
| test/episodes                  | 4480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.012     |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8951155 |
| test/Q_plus_P                  | -0.8951155 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 179200     |
| train/episodes                 | 17920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.582      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00902   |
| train/info_shaping_reward_mean | -0.0778    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 716800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 448        |
| stats_o/mean                   | 0.19854465 |
| stats_o/std                    | 0.08340158 |
| test/episodes                  | 4490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00858   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.879448  |
| test/Q_plus_P                  | -0.879448  |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 179600     |
| train/episodes                 | 17960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00602   |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 718400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 449        |
| stats_o/mean                   | 0.19856966 |
| stats_o/std                    | 0.08338633 |
| test/episodes                  | 4500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00943   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0266159 |
| test/Q_plus_P                  | -1.0266159 |
| test/reward_per_eps            | -9         |
| test/steps                     | 180000     |
| train/episodes                 | 18000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00642   |
| train/info_shaping_reward_mean | -0.0681    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 720000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.19858438 |
| stats_o/std                    | 0.08336696 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0106    |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0123253 |
| test/Q_plus_P                  | -1.0123253 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00689   |
| train/info_shaping_reward_mean | -0.0643    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 451        |
| stats_o/mean                   | 0.19860806 |
| stats_o/std                    | 0.08336238 |
| test/episodes                  | 4520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00311   |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9689832 |
| test/Q_plus_P                  | -0.9689832 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 180800     |
| train/episodes                 | 18080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00546   |
| train/info_shaping_reward_mean | -0.0691    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 723200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 452        |
| stats_o/mean                   | 0.19860882 |
| stats_o/std                    | 0.08335225 |
| test/episodes                  | 4530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0105    |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0752703 |
| test/Q_plus_P                  | -1.0752703 |
| test/reward_per_eps            | -10        |
| test/steps                     | 181200     |
| train/episodes                 | 18120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00676   |
| train/info_shaping_reward_mean | -0.0638    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 724800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 453        |
| stats_o/mean                   | 0.19860892 |
| stats_o/std                    | 0.08332758 |
| test/episodes                  | 4540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00624   |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0367314 |
| test/Q_plus_P                  | -1.0367314 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 181600     |
| train/episodes                 | 18160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00711   |
| train/info_shaping_reward_mean | -0.0653    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 726400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.19861153  |
| stats_o/std                    | 0.083337024 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00187    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.9244513  |
| test/Q_plus_P                  | -0.9244513  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.574       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00538    |
| train/info_shaping_reward_mean | -0.0785     |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 455        |
| stats_o/mean                   | 0.19862674 |
| stats_o/std                    | 0.08331592 |
| test/episodes                  | 4560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00646   |
| test/info_shaping_reward_mean  | -0.047     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.050236  |
| test/Q_plus_P                  | -1.050236  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 182400     |
| train/episodes                 | 18240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0051    |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 729600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 456         |
| stats_o/mean                   | 0.19864054  |
| stats_o/std                    | 0.083280966 |
| test/episodes                  | 4570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00764    |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.9799249  |
| test/Q_plus_P                  | -0.9799249  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 182800      |
| train/episodes                 | 18280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00657    |
| train/info_shaping_reward_mean | -0.0663     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 731200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 457        |
| stats_o/mean                   | 0.19864586 |
| stats_o/std                    | 0.08324926 |
| test/episodes                  | 4580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0119    |
| test/info_shaping_reward_mean  | -0.05      |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0021609 |
| test/Q_plus_P                  | -1.0021609 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 183200     |
| train/episodes                 | 18320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.67       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00593   |
| train/info_shaping_reward_mean | -0.0633    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 732800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.1986628   |
| stats_o/std                    | 0.083230704 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0116     |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.9478649  |
| test/Q_plus_P                  | -0.9478649  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00676    |
| train/info_shaping_reward_mean | -0.0673     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 459         |
| stats_o/mean                   | 0.19869334  |
| stats_o/std                    | 0.083213665 |
| test/episodes                  | 4600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0129     |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.2311883  |
| test/Q_plus_P                  | -1.2311883  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 184000      |
| train/episodes                 | 18400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0066     |
| train/info_shaping_reward_mean | -0.0634     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 736000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.19871342  |
| stats_o/std                    | 0.083208315 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.015      |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.910802   |
| test/Q_plus_P                  | -0.910802   |
| test/reward_per_eps            | -8          |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00927    |
| train/info_shaping_reward_mean | -0.074      |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 461        |
| stats_o/mean                   | 0.19872263 |
| stats_o/std                    | 0.08319193 |
| test/episodes                  | 4620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0026    |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.899722  |
| test/Q_plus_P                  | -0.899722  |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 184800     |
| train/episodes                 | 18480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.669      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00607   |
| train/info_shaping_reward_mean | -0.0638    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 739200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 462        |
| stats_o/mean                   | 0.19874966 |
| stats_o/std                    | 0.08319947 |
| test/episodes                  | 4630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00815   |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0338389 |
| test/Q_plus_P                  | -1.0338389 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 185200     |
| train/episodes                 | 18520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00975   |
| train/info_shaping_reward_mean | -0.0858    |
| train/info_shaping_reward_min  | -0.276     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 740800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 463        |
| stats_o/mean                   | 0.19877408 |
| stats_o/std                    | 0.08323152 |
| test/episodes                  | 4640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0134    |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9659063 |
| test/Q_plus_P                  | -0.9659063 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 185600     |
| train/episodes                 | 18560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.587      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00816   |
| train/info_shaping_reward_mean | -0.0723    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 742400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 464        |
| stats_o/mean                   | 0.19880126 |
| stats_o/std                    | 0.08326134 |
| test/episodes                  | 4650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0123    |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8535884 |
| test/Q_plus_P                  | -0.8535884 |
| test/reward_per_eps            | -8         |
| test/steps                     | 186000     |
| train/episodes                 | 18600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.657      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00714   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 744000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 465         |
| stats_o/mean                   | 0.19881965  |
| stats_o/std                    | 0.08325134  |
| test/episodes                  | 4660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00488    |
| test/info_shaping_reward_mean  | -0.0462     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.91310275 |
| test/Q_plus_P                  | -0.91310275 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 186400      |
| train/episodes                 | 18640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.578       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00573    |
| train/info_shaping_reward_mean | -0.0734     |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 745600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.19882159  |
| stats_o/std                    | 0.08323102  |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00764    |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.96297187 |
| test/Q_plus_P                  | -0.96297187 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00792    |
| train/info_shaping_reward_mean | -0.0666     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 467         |
| stats_o/mean                   | 0.19885351  |
| stats_o/std                    | 0.083279036 |
| test/episodes                  | 4680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0103     |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.8616534  |
| test/Q_plus_P                  | -0.8616534  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 187200      |
| train/episodes                 | 18720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00526    |
| train/info_shaping_reward_mean | -0.0842     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 748800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 468         |
| stats_o/mean                   | 0.1988665   |
| stats_o/std                    | 0.083243094 |
| test/episodes                  | 4690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0115     |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.8035705  |
| test/Q_plus_P                  | -0.8035705  |
| test/reward_per_eps            | -8          |
| test/steps                     | 187600      |
| train/episodes                 | 18760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00694    |
| train/info_shaping_reward_mean | -0.0635     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 750400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 469         |
| stats_o/mean                   | 0.19887003  |
| stats_o/std                    | 0.083204724 |
| test/episodes                  | 4700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00702    |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.0554042  |
| test/Q_plus_P                  | -1.0554042  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 188000      |
| train/episodes                 | 18800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00768    |
| train/info_shaping_reward_mean | -0.0714     |
| train/info_shaping_reward_min  | -0.197      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 752000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.19888158 |
| stats_o/std                    | 0.08319852 |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00972   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8899822 |
| test/Q_plus_P                  | -0.8899822 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00668   |
| train/info_shaping_reward_mean | -0.0653    |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.19889891 |
| stats_o/std                    | 0.0832043  |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00665   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1991432 |
| test/Q_plus_P                  | -1.1991432 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00685   |
| train/info_shaping_reward_mean | -0.0717    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.19892153 |
| stats_o/std                    | 0.08320966 |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0156    |
| test/info_shaping_reward_mean  | -0.0567    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9362786 |
| test/Q_plus_P                  | -0.9362786 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00986   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 473        |
| stats_o/mean                   | 0.19892931 |
| stats_o/std                    | 0.08319507 |
| test/episodes                  | 4740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00511   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.976677  |
| test/Q_plus_P                  | -0.976677  |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 189600     |
| train/episodes                 | 18960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00626   |
| train/info_shaping_reward_mean | -0.0645    |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 758400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 474        |
| stats_o/mean                   | 0.19892845 |
| stats_o/std                    | 0.08321911 |
| test/episodes                  | 4750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0147    |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9873745 |
| test/Q_plus_P                  | -0.9873745 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 190000     |
| train/episodes                 | 19000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00724   |
| train/info_shaping_reward_mean | -0.0874    |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 760000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 475         |
| stats_o/mean                   | 0.19894135  |
| stats_o/std                    | 0.08322272  |
| test/episodes                  | 4760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.003      |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.87708414 |
| test/Q_plus_P                  | -0.87708414 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 190400      |
| train/episodes                 | 19040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0071     |
| train/info_shaping_reward_mean | -0.0754     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 761600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 476        |
| stats_o/mean                   | 0.1989567  |
| stats_o/std                    | 0.08319228 |
| test/episodes                  | 4770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0172    |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1635652 |
| test/Q_plus_P                  | -1.1635652 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 190800     |
| train/episodes                 | 19080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00652   |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 763200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.19896938  |
| stats_o/std                    | 0.0831927   |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0124     |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.99067503 |
| test/Q_plus_P                  | -0.99067503 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00603    |
| train/info_shaping_reward_mean | -0.0725     |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 478         |
| stats_o/mean                   | 0.19899225  |
| stats_o/std                    | 0.08318558  |
| test/episodes                  | 4790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00638    |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.86454463 |
| test/Q_plus_P                  | -0.86454463 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 191600      |
| train/episodes                 | 19160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00669    |
| train/info_shaping_reward_mean | -0.0741     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 766400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.19900766  |
| stats_o/std                    | 0.08320501  |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00658    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.96560454 |
| test/Q_plus_P                  | -0.96560454 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00611    |
| train/info_shaping_reward_mean | -0.0797     |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 480        |
| stats_o/mean                   | 0.19902305 |
| stats_o/std                    | 0.0831954  |
| test/episodes                  | 4810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.8206055 |
| test/Q_plus_P                  | -0.8206055 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 192400     |
| train/episodes                 | 19240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0051    |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 769600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 481        |
| stats_o/mean                   | 0.19904369 |
| stats_o/std                    | 0.08318656 |
| test/episodes                  | 4820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00748   |
| test/info_shaping_reward_mean  | -0.0586    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3128909 |
| test/Q_plus_P                  | -1.3128909 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 192800     |
| train/episodes                 | 19280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00567   |
| train/info_shaping_reward_mean | -0.0673    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 771200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 482        |
| stats_o/mean                   | 0.19905081 |
| stats_o/std                    | 0.08315168 |
| test/episodes                  | 4830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00217   |
| test/info_shaping_reward_mean  | -0.0597    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3673103 |
| test/Q_plus_P                  | -1.3673103 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 193200     |
| train/episodes                 | 19320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00614   |
| train/info_shaping_reward_mean | -0.0679    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 772800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 483        |
| stats_o/mean                   | 0.19905876 |
| stats_o/std                    | 0.08314784 |
| test/episodes                  | 4840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00483   |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8881605 |
| test/Q_plus_P                  | -0.8881605 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 193600     |
| train/episodes                 | 19360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00495   |
| train/info_shaping_reward_mean | -0.0779    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 774400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.19905695  |
| stats_o/std                    | 0.08314712  |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00262    |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.92387414 |
| test/Q_plus_P                  | -0.92387414 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00732    |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.19905971  |
| stats_o/std                    | 0.0831163   |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00313    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.93633753 |
| test/Q_plus_P                  | -0.93633753 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00509    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 486        |
| stats_o/mean                   | 0.19906716 |
| stats_o/std                    | 0.08312737 |
| test/episodes                  | 4870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00739   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0830569 |
| test/Q_plus_P                  | -1.0830569 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 194800     |
| train/episodes                 | 19480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.59       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00577   |
| train/info_shaping_reward_mean | -0.074     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 779200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 487        |
| stats_o/mean                   | 0.19908218 |
| stats_o/std                    | 0.08313452 |
| test/episodes                  | 4880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00705   |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.9694184 |
| test/Q_plus_P                  | -0.9694184 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 195200     |
| train/episodes                 | 19520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00594   |
| train/info_shaping_reward_mean | -0.085     |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 780800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 488         |
| stats_o/mean                   | 0.19909021  |
| stats_o/std                    | 0.083145104 |
| test/episodes                  | 4890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0035     |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.97640234 |
| test/Q_plus_P                  | -0.97640234 |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 195600      |
| train/episodes                 | 19560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.556       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00648    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 782400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 489         |
| stats_o/mean                   | 0.19908959  |
| stats_o/std                    | 0.08312882  |
| test/episodes                  | 4900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00734    |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.92503023 |
| test/Q_plus_P                  | -0.92503023 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 196000      |
| train/episodes                 | 19600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00713    |
| train/info_shaping_reward_mean | -0.0639     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 784000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 490        |
| stats_o/mean                   | 0.19909047 |
| stats_o/std                    | 0.08310899 |
| test/episodes                  | 4910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00481   |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.146048  |
| test/Q_plus_P                  | -1.146048  |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 196400     |
| train/episodes                 | 19640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00692   |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 785600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.1990985   |
| stats_o/std                    | 0.083111614 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00358    |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.97368586 |
| test/Q_plus_P                  | -0.97368586 |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00581    |
| train/info_shaping_reward_mean | -0.0746     |
| train/info_shaping_reward_min  | -0.226      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 492        |
| stats_o/mean                   | 0.19911787 |
| stats_o/std                    | 0.08311228 |
| test/episodes                  | 4930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00392   |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.8600692 |
| test/Q_plus_P                  | -0.8600692 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 197200     |
| train/episodes                 | 19720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00702   |
| train/info_shaping_reward_mean | -0.0713    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 788800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.19911765 |
| stats_o/std                    | 0.08312231 |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00642   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0285268 |
| test/Q_plus_P                  | -1.0285268 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00596   |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 494        |
| stats_o/mean                   | 0.19913585 |
| stats_o/std                    | 0.08310416 |
| test/episodes                  | 4950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.008     |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.8259229 |
| test/Q_plus_P                  | -0.8259229 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 198000     |
| train/episodes                 | 19800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.666      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00554   |
| train/info_shaping_reward_mean | -0.0632    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 792000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 495        |
| stats_o/mean                   | 0.19914326 |
| stats_o/std                    | 0.08307701 |
| test/episodes                  | 4960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0085    |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0461181 |
| test/Q_plus_P                  | -1.0461181 |
| test/reward_per_eps            | -9         |
| test/steps                     | 198400     |
| train/episodes                 | 19840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00468   |
| train/info_shaping_reward_mean | -0.0643    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 793600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 496        |
| stats_o/mean                   | 0.19914748 |
| stats_o/std                    | 0.08305921 |
| test/episodes                  | 4970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00464   |
| test/info_shaping_reward_mean  | -0.0531    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0790805 |
| test/Q_plus_P                  | -1.0790805 |
| test/reward_per_eps            | -10        |
| test/steps                     | 198800     |
| train/episodes                 | 19880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00654   |
| train/info_shaping_reward_mean | -0.0733    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 795200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 497         |
| stats_o/mean                   | 0.19916253  |
| stats_o/std                    | 0.083058625 |
| test/episodes                  | 4980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00289    |
| test/info_shaping_reward_mean  | -0.056      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1718998  |
| test/Q_plus_P                  | -1.1718998  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 199200      |
| train/episodes                 | 19920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00676    |
| train/info_shaping_reward_mean | -0.078      |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 796800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 498         |
| stats_o/mean                   | 0.19917619  |
| stats_o/std                    | 0.08306595  |
| test/episodes                  | 4990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00326    |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.93096596 |
| test/Q_plus_P                  | -0.93096596 |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 199600      |
| train/episodes                 | 19960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00611    |
| train/info_shaping_reward_mean | -0.0705     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 798400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.19916943  |
| stats_o/std                    | 0.083040945 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00085    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.867648   |
| test/Q_plus_P                  | -0.867648   |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00501    |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 500        |
| stats_o/mean                   | 0.19918892 |
| stats_o/std                    | 0.08305842 |
| test/episodes                  | 5010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00937   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.9222387 |
| test/Q_plus_P                  | -0.9222387 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 200400     |
| train/episodes                 | 20040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00635   |
| train/info_shaping_reward_mean | -0.0806    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 801600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 501        |
| stats_o/mean                   | 0.19920732 |
| stats_o/std                    | 0.0830646  |
| test/episodes                  | 5020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00422   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0052856 |
| test/Q_plus_P                  | -1.0052856 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 200800     |
| train/episodes                 | 20080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00594   |
| train/info_shaping_reward_mean | -0.0808    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 803200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.19920519  |
| stats_o/std                    | 0.0830439   |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00519    |
| test/info_shaping_reward_mean  | -0.0475     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.86182195 |
| test/Q_plus_P                  | -0.86182195 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00611    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 503        |
| stats_o/mean                   | 0.19922186 |
| stats_o/std                    | 0.08303692 |
| test/episodes                  | 5040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00677   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.7939647 |
| test/Q_plus_P                  | -0.7939647 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 201600     |
| train/episodes                 | 20160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00611   |
| train/info_shaping_reward_mean | -0.0738    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 806400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 504        |
| stats_o/mean                   | 0.19922265 |
| stats_o/std                    | 0.0830079  |
| test/episodes                  | 5050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00741   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9634737 |
| test/Q_plus_P                  | -0.9634737 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 202000     |
| train/episodes                 | 20200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00542   |
| train/info_shaping_reward_mean | -0.0605    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 808000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 505         |
| stats_o/mean                   | 0.19923654  |
| stats_o/std                    | 0.08298231  |
| test/episodes                  | 5060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00441    |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.92235273 |
| test/Q_plus_P                  | -0.92235273 |
| test/reward_per_eps            | -8          |
| test/steps                     | 202400      |
| train/episodes                 | 20240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0051     |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 809600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.19924244  |
| stats_o/std                    | 0.083002634 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0029     |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.863919   |
| test/Q_plus_P                  | -0.863919   |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00799    |
| train/info_shaping_reward_mean | -0.0735     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 507        |
| stats_o/mean                   | 0.1992391  |
| stats_o/std                    | 0.08296708 |
| test/episodes                  | 5080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00514   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0691484 |
| test/Q_plus_P                  | -1.0691484 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 203200     |
| train/episodes                 | 20320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.663      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00537   |
| train/info_shaping_reward_mean | -0.0626    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 812800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 508        |
| stats_o/mean                   | 0.19924557 |
| stats_o/std                    | 0.08292372 |
| test/episodes                  | 5090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00493   |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1095384 |
| test/Q_plus_P                  | -1.1095384 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 203600     |
| train/episodes                 | 20360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.677      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00441   |
| train/info_shaping_reward_mean | -0.0602    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.9      |
| train/steps                    | 814400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 509         |
| stats_o/mean                   | 0.1992503   |
| stats_o/std                    | 0.08291592  |
| test/episodes                  | 5100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00458    |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.98788923 |
| test/Q_plus_P                  | -0.98788923 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 204000      |
| train/episodes                 | 20400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00672    |
| train/info_shaping_reward_mean | -0.0706     |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 816000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 510        |
| stats_o/mean                   | 0.1992641  |
| stats_o/std                    | 0.08288482 |
| test/episodes                  | 5110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0126    |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0243427 |
| test/Q_plus_P                  | -1.0243427 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 204400     |
| train/episodes                 | 20440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00588   |
| train/info_shaping_reward_mean | -0.0645    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 817600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 511        |
| stats_o/mean                   | 0.19926636 |
| stats_o/std                    | 0.08287365 |
| test/episodes                  | 5120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00325   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.0351877 |
| test/Q_plus_P                  | -1.0351877 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 204800     |
| train/episodes                 | 20480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0647    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 819200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 512         |
| stats_o/mean                   | 0.19926712  |
| stats_o/std                    | 0.082894124 |
| test/episodes                  | 5130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00919    |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.90071696 |
| test/Q_plus_P                  | -0.90071696 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 205200      |
| train/episodes                 | 20520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00715    |
| train/info_shaping_reward_mean | -0.0806     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 820800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 513        |
| stats_o/mean                   | 0.19928102 |
| stats_o/std                    | 0.08289825 |
| test/episodes                  | 5140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00441   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.947376  |
| test/Q_plus_P                  | -0.947376  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 205600     |
| train/episodes                 | 20560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00596   |
| train/info_shaping_reward_mean | -0.0723    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 822400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 514         |
| stats_o/mean                   | 0.1992892   |
| stats_o/std                    | 0.082926385 |
| test/episodes                  | 5150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00608    |
| test/info_shaping_reward_mean  | -0.0447     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.8265544  |
| test/Q_plus_P                  | -0.8265544  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 206000      |
| train/episodes                 | 20600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.583       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00921    |
| train/info_shaping_reward_mean | -0.0895     |
| train/info_shaping_reward_min  | -0.284      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 824000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 515        |
| stats_o/mean                   | 0.19929272 |
| stats_o/std                    | 0.08290356 |
| test/episodes                  | 5160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00106   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0089331 |
| test/Q_plus_P                  | -1.0089331 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 206400     |
| train/episodes                 | 20640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00747   |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 825600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 516         |
| stats_o/mean                   | 0.19930081  |
| stats_o/std                    | 0.08288803  |
| test/episodes                  | 5170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0152     |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.98787236 |
| test/Q_plus_P                  | -0.98787236 |
| test/reward_per_eps            | -9          |
| test/steps                     | 206800      |
| train/episodes                 | 20680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00709    |
| train/info_shaping_reward_mean | -0.0664     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 827200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 517         |
| stats_o/mean                   | 0.19931637  |
| stats_o/std                    | 0.082883865 |
| test/episodes                  | 5180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00606    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.753648   |
| test/Q_plus_P                  | -0.753648   |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 207200      |
| train/episodes                 | 20720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00621    |
| train/info_shaping_reward_mean | -0.0709     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 828800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 518         |
| stats_o/mean                   | 0.19931702  |
| stats_o/std                    | 0.082864076 |
| test/episodes                  | 5190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00512    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.0897815  |
| test/Q_plus_P                  | -1.0897815  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 207600      |
| train/episodes                 | 20760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00558    |
| train/info_shaping_reward_mean | -0.0694     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 830400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 519         |
| stats_o/mean                   | 0.19932878  |
| stats_o/std                    | 0.08283567  |
| test/episodes                  | 5200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00317    |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.82031214 |
| test/Q_plus_P                  | -0.82031214 |
| test/reward_per_eps            | -8          |
| test/steps                     | 208000      |
| train/episodes                 | 20800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00733    |
| train/info_shaping_reward_mean | -0.0623     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 832000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.1993298   |
| stats_o/std                    | 0.082832776 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00542    |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.0934414  |
| test/Q_plus_P                  | -1.0934414  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00659    |
| train/info_shaping_reward_mean | -0.0687     |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 521        |
| stats_o/mean                   | 0.1993308  |
| stats_o/std                    | 0.08284225 |
| test/episodes                  | 5220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00569   |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.179105  |
| test/Q_plus_P                  | -1.179105  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 208800     |
| train/episodes                 | 20880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00732   |
| train/info_shaping_reward_mean | -0.0674    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 835200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 522        |
| stats_o/mean                   | 0.1993465  |
| stats_o/std                    | 0.08283734 |
| test/episodes                  | 5230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00687   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0253173 |
| test/Q_plus_P                  | -1.0253173 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 209200     |
| train/episodes                 | 20920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0064    |
| train/info_shaping_reward_mean | -0.0777    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 836800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 523         |
| stats_o/mean                   | 0.19936806  |
| stats_o/std                    | 0.082873344 |
| test/episodes                  | 5240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0105     |
| test/info_shaping_reward_mean  | -0.0527     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0326802  |
| test/Q_plus_P                  | -1.0326802  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 209600      |
| train/episodes                 | 20960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.58        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00536    |
| train/info_shaping_reward_mean | -0.0772     |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 838400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 524         |
| stats_o/mean                   | 0.19937535  |
| stats_o/std                    | 0.082850054 |
| test/episodes                  | 5250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0147     |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.98031217 |
| test/Q_plus_P                  | -0.98031217 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 210000      |
| train/episodes                 | 21000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00887    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 840000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 525         |
| stats_o/mean                   | 0.19938433  |
| stats_o/std                    | 0.082842074 |
| test/episodes                  | 5260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00986    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.8631498  |
| test/Q_plus_P                  | -0.8631498  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 210400      |
| train/episodes                 | 21040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00592    |
| train/info_shaping_reward_mean | -0.0688     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 841600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.1993954   |
| stats_o/std                    | 0.082811095 |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0178     |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0212975  |
| test/Q_plus_P                  | -1.0212975  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00493    |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 527        |
| stats_o/mean                   | 0.19940041 |
| stats_o/std                    | 0.08280938 |
| test/episodes                  | 5280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00868   |
| test/info_shaping_reward_mean  | -0.0568    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1370535 |
| test/Q_plus_P                  | -1.1370535 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 211200     |
| train/episodes                 | 21120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0066    |
| train/info_shaping_reward_mean | -0.0726    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 844800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 528        |
| stats_o/mean                   | 0.19941194 |
| stats_o/std                    | 0.08283646 |
| test/episodes                  | 5290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0184    |
| test/info_shaping_reward_mean  | -0.0564    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0673099 |
| test/Q_plus_P                  | -1.0673099 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 211600     |
| train/episodes                 | 21160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00668   |
| train/info_shaping_reward_mean | -0.0775    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 846400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 529        |
| stats_o/mean                   | 0.19942132 |
| stats_o/std                    | 0.08282839 |
| test/episodes                  | 5300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0051    |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8807843 |
| test/Q_plus_P                  | -0.8807843 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 212000     |
| train/episodes                 | 21200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00793   |
| train/info_shaping_reward_mean | -0.0686    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 848000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 530         |
| stats_o/mean                   | 0.19944203  |
| stats_o/std                    | 0.082808666 |
| test/episodes                  | 5310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00775    |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0359055  |
| test/Q_plus_P                  | -1.0359055  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 212400      |
| train/episodes                 | 21240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00651    |
| train/info_shaping_reward_mean | -0.0713     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 849600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 531        |
| stats_o/mean                   | 0.19944616 |
| stats_o/std                    | 0.08278257 |
| test/episodes                  | 5320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00796   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0429667 |
| test/Q_plus_P                  | -1.0429667 |
| test/reward_per_eps            | -9         |
| test/steps                     | 212800     |
| train/episodes                 | 21280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00683   |
| train/info_shaping_reward_mean | -0.0676    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 851200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 532        |
| stats_o/mean                   | 0.19946007 |
| stats_o/std                    | 0.08277773 |
| test/episodes                  | 5330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00539   |
| test/info_shaping_reward_mean  | -0.0541    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1209958 |
| test/Q_plus_P                  | -1.1209958 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 213200     |
| train/episodes                 | 21320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00721   |
| train/info_shaping_reward_mean | -0.0745    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 852800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.19946474  |
| stats_o/std                    | 0.08274987  |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00378    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.99735886 |
| test/Q_plus_P                  | -0.99735886 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00623    |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 534        |
| stats_o/mean                   | 0.1994798  |
| stats_o/std                    | 0.08271348 |
| test/episodes                  | 5350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00771   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9843876 |
| test/Q_plus_P                  | -0.9843876 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 214000     |
| train/episodes                 | 21400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.658      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0059    |
| train/info_shaping_reward_mean | -0.0648    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 856000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 535         |
| stats_o/mean                   | 0.19949509  |
| stats_o/std                    | 0.082684495 |
| test/episodes                  | 5360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00259    |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.8618591  |
| test/Q_plus_P                  | -0.8618591  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 214400      |
| train/episodes                 | 21440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00511    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 857600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 536        |
| stats_o/mean                   | 0.19950992 |
| stats_o/std                    | 0.08268724 |
| test/episodes                  | 5370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00313   |
| test/info_shaping_reward_mean  | -0.0472    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9647561 |
| test/Q_plus_P                  | -0.9647561 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 214800     |
| train/episodes                 | 21480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00931   |
| train/info_shaping_reward_mean | -0.0724    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 859200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 537         |
| stats_o/mean                   | 0.19951911  |
| stats_o/std                    | 0.082665145 |
| test/episodes                  | 5380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00916    |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.94289815 |
| test/Q_plus_P                  | -0.94289815 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 215200      |
| train/episodes                 | 21520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00727    |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 860800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 538         |
| stats_o/mean                   | 0.19953753  |
| stats_o/std                    | 0.08265053  |
| test/episodes                  | 5390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00364    |
| test/info_shaping_reward_mean  | -0.0488     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.94361985 |
| test/Q_plus_P                  | -0.94361985 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 215600      |
| train/episodes                 | 21560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00592    |
| train/info_shaping_reward_mean | -0.0672     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 862400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 539        |
| stats_o/mean                   | 0.19954647 |
| stats_o/std                    | 0.08266731 |
| test/episodes                  | 5400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00399   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9794124 |
| test/Q_plus_P                  | -0.9794124 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 216000     |
| train/episodes                 | 21600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00655   |
| train/info_shaping_reward_mean | -0.0861    |
| train/info_shaping_reward_min  | -0.28      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 864000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 540        |
| stats_o/mean                   | 0.19955635 |
| stats_o/std                    | 0.08262901 |
| test/episodes                  | 5410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00103   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9367435 |
| test/Q_plus_P                  | -0.9367435 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 216400     |
| train/episodes                 | 21640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00549   |
| train/info_shaping_reward_mean | -0.0722    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 865600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 541        |
| stats_o/mean                   | 0.19956984 |
| stats_o/std                    | 0.08260604 |
| test/episodes                  | 5420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00843   |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.8710975 |
| test/Q_plus_P                  | -0.8710975 |
| test/reward_per_eps            | -8         |
| test/steps                     | 216800     |
| train/episodes                 | 21680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00651   |
| train/info_shaping_reward_mean | -0.0663    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 867200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 542        |
| stats_o/mean                   | 0.19958113 |
| stats_o/std                    | 0.082566   |
| test/episodes                  | 5430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00256   |
| test/info_shaping_reward_mean  | -0.047     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.8658676 |
| test/Q_plus_P                  | -0.8658676 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 217200     |
| train/episodes                 | 21720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.652      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00745   |
| train/info_shaping_reward_mean | -0.0636    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 868800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 543        |
| stats_o/mean                   | 0.19958101 |
| stats_o/std                    | 0.08255948 |
| test/episodes                  | 5440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00286   |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.8553526 |
| test/Q_plus_P                  | -0.8553526 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 217600     |
| train/episodes                 | 21760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00506   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 870400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 544        |
| stats_o/mean                   | 0.1995986  |
| stats_o/std                    | 0.0825744  |
| test/episodes                  | 5450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00993   |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9006562 |
| test/Q_plus_P                  | -0.9006562 |
| test/reward_per_eps            | -8         |
| test/steps                     | 218000     |
| train/episodes                 | 21800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00515   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 872000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 545        |
| stats_o/mean                   | 0.19960381 |
| stats_o/std                    | 0.08254106 |
| test/episodes                  | 5460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00721   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8922797 |
| test/Q_plus_P                  | -0.8922797 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 218400     |
| train/episodes                 | 21840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00607   |
| train/info_shaping_reward_mean | -0.0648    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 873600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.19962315  |
| stats_o/std                    | 0.08253973  |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0121     |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.82989067 |
| test/Q_plus_P                  | -0.82989067 |
| test/reward_per_eps            | -8          |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00638    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.211      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 547         |
| stats_o/mean                   | 0.199637    |
| stats_o/std                    | 0.082498886 |
| test/episodes                  | 5480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0105     |
| test/info_shaping_reward_mean  | -0.0567     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1719949  |
| test/Q_plus_P                  | -1.1719949  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 219200      |
| train/episodes                 | 21920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00522    |
| train/info_shaping_reward_mean | -0.0665     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 876800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 548        |
| stats_o/mean                   | 0.19964442 |
| stats_o/std                    | 0.08250072 |
| test/episodes                  | 5490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00895   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8060641 |
| test/Q_plus_P                  | -0.8060641 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 219600     |
| train/episodes                 | 21960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00352   |
| train/info_shaping_reward_mean | -0.0713    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 878400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 549        |
| stats_o/mean                   | 0.19966087 |
| stats_o/std                    | 0.08246319 |
| test/episodes                  | 5500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00616   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.8307375 |
| test/Q_plus_P                  | -0.8307375 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 220000     |
| train/episodes                 | 22000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.664      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00919   |
| train/info_shaping_reward_mean | -0.0638    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 880000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 550         |
| stats_o/mean                   | 0.19966011  |
| stats_o/std                    | 0.0824277   |
| test/episodes                  | 5510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00198    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.93165606 |
| test/Q_plus_P                  | -0.93165606 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 220400      |
| train/episodes                 | 22040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0065     |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 881600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 551         |
| stats_o/mean                   | 0.1996718   |
| stats_o/std                    | 0.08239786  |
| test/episodes                  | 5520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00272    |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.84715194 |
| test/Q_plus_P                  | -0.84715194 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 220800      |
| train/episodes                 | 22080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00541    |
| train/info_shaping_reward_mean | -0.0662     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 883200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 552        |
| stats_o/mean                   | 0.19968161 |
| stats_o/std                    | 0.08238414 |
| test/episodes                  | 5530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00759   |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.8320486 |
| test/Q_plus_P                  | -0.8320486 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 221200     |
| train/episodes                 | 22120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00606   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 884800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 553        |
| stats_o/mean                   | 0.19968382 |
| stats_o/std                    | 0.0823707  |
| test/episodes                  | 5540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0101    |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.8531551 |
| test/Q_plus_P                  | -0.8531551 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 221600     |
| train/episodes                 | 22160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.657      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00721   |
| train/info_shaping_reward_mean | -0.0637    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 886400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 554         |
| stats_o/mean                   | 0.19969706  |
| stats_o/std                    | 0.082351744 |
| test/episodes                  | 5550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00242    |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0399421  |
| test/Q_plus_P                  | -1.0399421  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 222000      |
| train/episodes                 | 22200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00883    |
| train/info_shaping_reward_mean | -0.0715     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 888000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 555         |
| stats_o/mean                   | 0.19969973  |
| stats_o/std                    | 0.082326114 |
| test/episodes                  | 5560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00694    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.84799886 |
| test/Q_plus_P                  | -0.84799886 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 222400      |
| train/episodes                 | 22240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00917    |
| train/info_shaping_reward_mean | -0.0695     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 889600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 556         |
| stats_o/mean                   | 0.19970903  |
| stats_o/std                    | 0.082281105 |
| test/episodes                  | 5570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00389    |
| test/info_shaping_reward_mean  | -0.0488     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.9522397  |
| test/Q_plus_P                  | -0.9522397  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 222800      |
| train/episodes                 | 22280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00683    |
| train/info_shaping_reward_mean | -0.0646     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 891200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 557        |
| stats_o/mean                   | 0.19971877 |
| stats_o/std                    | 0.08224481 |
| test/episodes                  | 5580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0134    |
| test/info_shaping_reward_mean  | -0.0582    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.911653  |
| test/Q_plus_P                  | -0.911653  |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 223200     |
| train/episodes                 | 22320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00787   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 892800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 558        |
| stats_o/mean                   | 0.19972979 |
| stats_o/std                    | 0.08222624 |
| test/episodes                  | 5590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.812      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0104    |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.7921609 |
| test/Q_plus_P                  | -0.7921609 |
| test/reward_per_eps            | -7.5       |
| test/steps                     | 223600     |
| train/episodes                 | 22360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00504   |
| train/info_shaping_reward_mean | -0.0639    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 894400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 559        |
| stats_o/mean                   | 0.19973491 |
| stats_o/std                    | 0.08220904 |
| test/episodes                  | 5600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00579   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.7666234 |
| test/Q_plus_P                  | -0.7666234 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 224000     |
| train/episodes                 | 22400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00595   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 896000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 560         |
| stats_o/mean                   | 0.19974701  |
| stats_o/std                    | 0.08223603  |
| test/episodes                  | 5610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00807    |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.92262113 |
| test/Q_plus_P                  | -0.92262113 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 224400      |
| train/episodes                 | 22440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00755    |
| train/info_shaping_reward_mean | -0.0819     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 897600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 561        |
| stats_o/mean                   | 0.19975479 |
| stats_o/std                    | 0.08220124 |
| test/episodes                  | 5620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00372   |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1972753 |
| test/Q_plus_P                  | -1.1972753 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 224800     |
| train/episodes                 | 22480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00673   |
| train/info_shaping_reward_mean | -0.0656    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 899200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.19976711  |
| stats_o/std                    | 0.082207166 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00793    |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.9877259  |
| test/Q_plus_P                  | -0.9877259  |
| test/reward_per_eps            | -9          |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00699    |
| train/info_shaping_reward_mean | -0.0688     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.19978489  |
| stats_o/std                    | 0.08219862  |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0041     |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.79925156 |
| test/Q_plus_P                  | -0.79925156 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00695    |
| train/info_shaping_reward_mean | -0.0751     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 564        |
| stats_o/mean                   | 0.19978148 |
| stats_o/std                    | 0.08219823 |
| test/episodes                  | 5650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00921   |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0389514 |
| test/Q_plus_P                  | -1.0389514 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 226000     |
| train/episodes                 | 22600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00527   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 904000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 565         |
| stats_o/mean                   | 0.19978803  |
| stats_o/std                    | 0.08215628  |
| test/episodes                  | 5660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00197    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.76239973 |
| test/Q_plus_P                  | -0.76239973 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 226400      |
| train/episodes                 | 22640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00996    |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 905600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 566        |
| stats_o/mean                   | 0.19979598 |
| stats_o/std                    | 0.08215061 |
| test/episodes                  | 5670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00608   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.9046088 |
| test/Q_plus_P                  | -0.9046088 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 226800     |
| train/episodes                 | 22680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00881   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 907200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 567        |
| stats_o/mean                   | 0.1998022  |
| stats_o/std                    | 0.08211682 |
| test/episodes                  | 5680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00602   |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9113529 |
| test/Q_plus_P                  | -0.9113529 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 227200     |
| train/episodes                 | 22720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00701   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 908800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 568        |
| stats_o/mean                   | 0.19981194 |
| stats_o/std                    | 0.08208972 |
| test/episodes                  | 5690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0109    |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0127294 |
| test/Q_plus_P                  | -1.0127294 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 227600     |
| train/episodes                 | 22760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00601   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 910400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.19981425  |
| stats_o/std                    | 0.08206102  |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00216    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.92099166 |
| test/Q_plus_P                  | -0.92099166 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00751    |
| train/info_shaping_reward_mean | -0.0643     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 570        |
| stats_o/mean                   | 0.19981663 |
| stats_o/std                    | 0.08204113 |
| test/episodes                  | 5710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00373   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.954588  |
| test/Q_plus_P                  | -0.954588  |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 228400     |
| train/episodes                 | 22840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00911   |
| train/info_shaping_reward_mean | -0.0684    |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 913600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 571         |
| stats_o/mean                   | 0.19981778  |
| stats_o/std                    | 0.082008556 |
| test/episodes                  | 5720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00853    |
| test/info_shaping_reward_mean  | -0.0501     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.9822786  |
| test/Q_plus_P                  | -0.9822786  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 228800      |
| train/episodes                 | 22880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00558    |
| train/info_shaping_reward_mean | -0.0733     |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 915200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.19983338  |
| stats_o/std                    | 0.081989765 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000354   |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.8744     |
| test/Q_plus_P                  | -0.8744     |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00607    |
| train/info_shaping_reward_mean | -0.066      |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 573        |
| stats_o/mean                   | 0.19982833 |
| stats_o/std                    | 0.08196428 |
| test/episodes                  | 5740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0103    |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9789491 |
| test/Q_plus_P                  | -0.9789491 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 229600     |
| train/episodes                 | 22960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00607   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 918400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.19983183  |
| stats_o/std                    | 0.08195049  |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0079     |
| test/info_shaping_reward_mean  | -0.0501     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.88026243 |
| test/Q_plus_P                  | -0.88026243 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00498    |
| train/info_shaping_reward_mean | -0.0658     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.19983667  |
| stats_o/std                    | 0.081934385 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00498    |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.165188   |
| test/Q_plus_P                  | -1.165188   |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00633    |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.19984056  |
| stats_o/std                    | 0.081905134 |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0062     |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.802515   |
| test/Q_plus_P                  | -0.802515   |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00833    |
| train/info_shaping_reward_mean | -0.0623     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 577         |
| stats_o/mean                   | 0.19984478  |
| stats_o/std                    | 0.081884295 |
| test/episodes                  | 5780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00763    |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0025729  |
| test/Q_plus_P                  | -1.0025729  |
| test/reward_per_eps            | -9          |
| test/steps                     | 231200      |
| train/episodes                 | 23120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00636    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 924800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.19984512 |
| stats_o/std                    | 0.08188228 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.812      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00384   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.7053577 |
| test/Q_plus_P                  | -0.7053577 |
| test/reward_per_eps            | -7.5       |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00809   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 579         |
| stats_o/mean                   | 0.19984293  |
| stats_o/std                    | 0.08188047  |
| test/episodes                  | 5800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00835    |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.97132635 |
| test/Q_plus_P                  | -0.97132635 |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 232000      |
| train/episodes                 | 23200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00646    |
| train/info_shaping_reward_mean | -0.0813     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 928000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 580         |
| stats_o/mean                   | 0.1998554   |
| stats_o/std                    | 0.08185288  |
| test/episodes                  | 5810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00185    |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -0.92712766 |
| test/Q_plus_P                  | -0.92712766 |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 232400      |
| train/episodes                 | 23240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00566    |
| train/info_shaping_reward_mean | -0.0663     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 929600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 581        |
| stats_o/mean                   | 0.19986373 |
| stats_o/std                    | 0.08182485 |
| test/episodes                  | 5820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00433   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.0719061 |
| test/Q_plus_P                  | -1.0719061 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 232800     |
| train/episodes                 | 23280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00612   |
| train/info_shaping_reward_mean | -0.0653    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 931200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 582         |
| stats_o/mean                   | 0.19987111  |
| stats_o/std                    | 0.081818655 |
| test/episodes                  | 5830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00136    |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.8816629  |
| test/Q_plus_P                  | -0.8816629  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 233200      |
| train/episodes                 | 23320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00614    |
| train/info_shaping_reward_mean | -0.0679     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 932800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 583        |
| stats_o/mean                   | 0.19987398 |
| stats_o/std                    | 0.08179885 |
| test/episodes                  | 5840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00646   |
| test/info_shaping_reward_mean  | -0.0578    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1991055 |
| test/Q_plus_P                  | -1.1991055 |
| test/reward_per_eps            | -10        |
| test/steps                     | 233600     |
| train/episodes                 | 23360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.655      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00522   |
| train/info_shaping_reward_mean | -0.0639    |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 934400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 584         |
| stats_o/mean                   | 0.19988337  |
| stats_o/std                    | 0.081798    |
| test/episodes                  | 5850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0149     |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.90473366 |
| test/Q_plus_P                  | -0.90473366 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 234000      |
| train/episodes                 | 23400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00626    |
| train/info_shaping_reward_mean | -0.0694     |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 936000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 585         |
| stats_o/mean                   | 0.19988933  |
| stats_o/std                    | 0.081822135 |
| test/episodes                  | 5860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000603   |
| test/info_shaping_reward_mean  | -0.0576     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0540342  |
| test/Q_plus_P                  | -1.0540342  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 234400      |
| train/episodes                 | 23440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.592       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00681    |
| train/info_shaping_reward_mean | -0.0692     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 937600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.199897    |
| stats_o/std                    | 0.081814595 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0153     |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.8778776  |
| test/Q_plus_P                  | -0.8778776  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00671    |
| train/info_shaping_reward_mean | -0.0745     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 587        |
| stats_o/mean                   | 0.19990347 |
| stats_o/std                    | 0.0818146  |
| test/episodes                  | 5880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00984   |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9620787 |
| test/Q_plus_P                  | -0.9620787 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 235200     |
| train/episodes                 | 23520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00659   |
| train/info_shaping_reward_mean | -0.073     |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 940800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 588         |
| stats_o/mean                   | 0.19990736  |
| stats_o/std                    | 0.08181777  |
| test/episodes                  | 5890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0077     |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.86854124 |
| test/Q_plus_P                  | -0.86854124 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 235600      |
| train/episodes                 | 23560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00412    |
| train/info_shaping_reward_mean | -0.0913     |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 942400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 589        |
| stats_o/mean                   | 0.19992109 |
| stats_o/std                    | 0.08180761 |
| test/episodes                  | 5900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00566   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0355872 |
| test/Q_plus_P                  | -1.0355872 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 236000     |
| train/episodes                 | 23600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.63       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00695   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 944000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 590        |
| stats_o/mean                   | 0.19993085 |
| stats_o/std                    | 0.0817815  |
| test/episodes                  | 5910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0102    |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0277618 |
| test/Q_plus_P                  | -1.0277618 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 236400     |
| train/episodes                 | 23640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.669      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00559   |
| train/info_shaping_reward_mean | -0.0641    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 945600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 591         |
| stats_o/mean                   | 0.19994321  |
| stats_o/std                    | 0.08176164  |
| test/episodes                  | 5920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.007      |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.88235146 |
| test/Q_plus_P                  | -0.88235146 |
| test/reward_per_eps            | -8          |
| test/steps                     | 236800      |
| train/episodes                 | 23680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00829    |
| train/info_shaping_reward_mean | -0.069      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 947200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 592         |
| stats_o/mean                   | 0.19995672  |
| stats_o/std                    | 0.08175428  |
| test/episodes                  | 5930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0146     |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.96924347 |
| test/Q_plus_P                  | -0.96924347 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 237200      |
| train/episodes                 | 23720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00739    |
| train/info_shaping_reward_mean | -0.0716     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 948800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.19996352  |
| stats_o/std                    | 0.08174357  |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00867    |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.87956166 |
| test/Q_plus_P                  | -0.87956166 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00665    |
| train/info_shaping_reward_mean | -0.0663     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 594        |
| stats_o/mean                   | 0.1999646  |
| stats_o/std                    | 0.08174759 |
| test/episodes                  | 5950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00944   |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -1.0305063 |
| test/Q_plus_P                  | -1.0305063 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 238000     |
| train/episodes                 | 23800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00677   |
| train/info_shaping_reward_mean | -0.0693    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 952000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 595        |
| stats_o/mean                   | 0.19996366 |
| stats_o/std                    | 0.0817243  |
| test/episodes                  | 5960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00452   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8456546 |
| test/Q_plus_P                  | -0.8456546 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 238400     |
| train/episodes                 | 23840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00552   |
| train/info_shaping_reward_mean | -0.0664    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 953600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 596        |
| stats_o/mean                   | 0.19997534 |
| stats_o/std                    | 0.08170367 |
| test/episodes                  | 5970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0107    |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.039074  |
| test/Q_plus_P                  | -1.039074  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 238800     |
| train/episodes                 | 23880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.635      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00717   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 955200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 597         |
| stats_o/mean                   | 0.19998251  |
| stats_o/std                    | 0.08169228  |
| test/episodes                  | 5980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00669    |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.88173413 |
| test/Q_plus_P                  | -0.88173413 |
| test/reward_per_eps            | -8          |
| test/steps                     | 239200      |
| train/episodes                 | 23920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00653    |
| train/info_shaping_reward_mean | -0.066      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 956800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.19999199  |
| stats_o/std                    | 0.08169391  |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00557    |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.86580503 |
| test/Q_plus_P                  | -0.86580503 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00471    |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.19999878 |
| stats_o/std                    | 0.08170373 |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0142    |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.8322108 |
| test/Q_plus_P                  | -0.8322108 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0088    |
| train/info_shaping_reward_mean | -0.0856    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
