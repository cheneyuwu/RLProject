Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPickAndPlace/config_TD3_BC/q_filter_False/prm_loss_weight_0.01/seed_0
-----------------------------------------------
| epoch                          | 0          |
| stats_o/mean                   | 0.20327999 |
| stats_o/std                    | 0.09487798 |
| test/episodes                  | 10         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.345      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00436   |
| test/info_shaping_reward_mean  | -0.14      |
| test/info_shaping_reward_min   | -0.627     |
| test/Q                         | -1.2227176 |
| test/Q_plus_P                  | -1.2227176 |
| test/reward_per_eps            | -26.2      |
| test/steps                     | 400        |
| train/episodes                 | 40         |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0206     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0889    |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.483     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 1600       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 1          |
| stats_o/mean                   | 0.20150708 |
| stats_o/std                    | 0.10607382 |
| test/episodes                  | 20         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.328      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00199   |
| test/info_shaping_reward_mean  | -0.11      |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.4537642 |
| test/Q_plus_P                  | -1.4537642 |
| test/reward_per_eps            | -26.9      |
| test/steps                     | 800        |
| train/episodes                 | 80         |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0269     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0792    |
| train/info_shaping_reward_mean | -0.207     |
| train/info_shaping_reward_min  | -0.496     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 3200       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 2          |
| stats_o/mean                   | 0.20320502 |
| stats_o/std                    | 0.10632515 |
| test/episodes                  | 30         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.273      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00463   |
| test/info_shaping_reward_mean  | -0.115     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.7207614 |
| test/Q_plus_P                  | -1.7207614 |
| test/reward_per_eps            | -29.1      |
| test/steps                     | 1200       |
| train/episodes                 | 120        |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0156     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0863    |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 4800       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.2034094  |
| stats_o/std                    | 0.10348092 |
| test/episodes                  | 40         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.393      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00277   |
| test/info_shaping_reward_mean  | -0.105     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.8799762 |
| test/Q_plus_P                  | -1.8799762 |
| test/reward_per_eps            | -24.3      |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0131     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.088     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 4           |
| stats_o/mean                   | 0.20234068  |
| stats_o/std                    | 0.101373084 |
| test/episodes                  | 50          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.105       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0116     |
| test/info_shaping_reward_mean  | -0.145      |
| test/info_shaping_reward_min   | -0.204      |
| test/Q                         | -2.6649494  |
| test/Q_plus_P                  | -2.6649494  |
| test/reward_per_eps            | -35.8       |
| test/steps                     | 2000        |
| train/episodes                 | 200         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.005       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.104      |
| train/info_shaping_reward_mean | -0.183      |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.8       |
| train/steps                    | 8000        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 5          |
| stats_o/mean                   | 0.20262107 |
| stats_o/std                    | 0.09863604 |
| test/episodes                  | 60         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.075      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00802   |
| test/info_shaping_reward_mean  | -0.133     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.9444659 |
| test/Q_plus_P                  | -2.9444659 |
| test/reward_per_eps            | -37        |
| test/steps                     | 2400       |
| train/episodes                 | 240        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 9600       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 6           |
| stats_o/mean                   | 0.20268857  |
| stats_o/std                    | 0.094862655 |
| test/episodes                  | 70          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.095       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0159     |
| test/info_shaping_reward_mean  | -0.147      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -3.4626215  |
| test/Q_plus_P                  | -3.4626215  |
| test/reward_per_eps            | -36.2       |
| test/steps                     | 2800        |
| train/episodes                 | 280         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 11200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 7          |
| stats_o/mean                   | 0.20316517 |
| stats_o/std                    | 0.09130649 |
| test/episodes                  | 80         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.105      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0232    |
| test/info_shaping_reward_mean  | -0.155     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -3.9358513 |
| test/Q_plus_P                  | -3.9358513 |
| test/reward_per_eps            | -35.8      |
| test/steps                     | 3200       |
| train/episodes                 | 320        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 12800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 8          |
| stats_o/mean                   | 0.20335905 |
| stats_o/std                    | 0.08850697 |
| test/episodes                  | 90         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.141     |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -4.441606  |
| test/Q_plus_P                  | -4.441606  |
| test/reward_per_eps            | -40        |
| test/steps                     | 3600       |
| train/episodes                 | 360        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 14400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 9          |
| stats_o/mean                   | 0.20312713 |
| stats_o/std                    | 0.08636429 |
| test/episodes                  | 100        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.04       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0347    |
| test/info_shaping_reward_mean  | -0.155     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -4.761404  |
| test/Q_plus_P                  | -4.761404  |
| test/reward_per_eps            | -38.4      |
| test/steps                     | 4000       |
| train/episodes                 | 400        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 16000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 10         |
| stats_o/mean                   | 0.20339224 |
| stats_o/std                    | 0.08410195 |
| test/episodes                  | 110        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -5.3286114 |
| test/Q_plus_P                  | -5.3286114 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4400       |
| train/episodes                 | 440        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 17600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 11         |
| stats_o/mean                   | 0.20331907 |
| stats_o/std                    | 0.08203677 |
| test/episodes                  | 120        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.105      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00883   |
| test/info_shaping_reward_mean  | -0.155     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -5.333069  |
| test/Q_plus_P                  | -5.333069  |
| test/reward_per_eps            | -35.8      |
| test/steps                     | 4800       |
| train/episodes                 | 480        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 19200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 12         |
| stats_o/mean                   | 0.20335186 |
| stats_o/std                    | 0.08030844 |
| test/episodes                  | 130        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.113      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0313    |
| test/info_shaping_reward_mean  | -0.157     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -5.7280226 |
| test/Q_plus_P                  | -5.7280226 |
| test/reward_per_eps            | -35.5      |
| test/steps                     | 5200       |
| train/episodes                 | 520        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 20800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 13         |
| stats_o/mean                   | 0.20341678 |
| stats_o/std                    | 0.078967   |
| test/episodes                  | 140        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.055      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0173    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -6.177611  |
| test/Q_plus_P                  | -6.177611  |
| test/reward_per_eps            | -37.8      |
| test/steps                     | 5600       |
| train/episodes                 | 560        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 22400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 14         |
| stats_o/mean                   | 0.20339678 |
| stats_o/std                    | 0.07767564 |
| test/episodes                  | 150        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -6.779994  |
| test/Q_plus_P                  | -6.779994  |
| test/reward_per_eps            | -40        |
| test/steps                     | 6000       |
| train/episodes                 | 600        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 24000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 15         |
| stats_o/mean                   | 0.2035854  |
| stats_o/std                    | 0.07741281 |
| test/episodes                  | 160        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.055      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0467    |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -6.913108  |
| test/Q_plus_P                  | -6.913108  |
| test/reward_per_eps            | -37.8      |
| test/steps                     | 6400       |
| train/episodes                 | 640        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 25600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 16         |
| stats_o/mean                   | 0.2036188  |
| stats_o/std                    | 0.07658633 |
| test/episodes                  | 170        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0593    |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -7.440162  |
| test/Q_plus_P                  | -7.440162  |
| test/reward_per_eps            | -40        |
| test/steps                     | 6800       |
| train/episodes                 | 680        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 27200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 17         |
| stats_o/mean                   | 0.20356704 |
| stats_o/std                    | 0.07576372 |
| test/episodes                  | 180        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -7.790381  |
| test/Q_plus_P                  | -7.790381  |
| test/reward_per_eps            | -40        |
| test/steps                     | 7200       |
| train/episodes                 | 720        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 28800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 18         |
| stats_o/mean                   | 0.20368037 |
| stats_o/std                    | 0.07515995 |
| test/episodes                  | 190        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -8.090774  |
| test/Q_plus_P                  | -8.090774  |
| test/reward_per_eps            | -40        |
| test/steps                     | 7600       |
| train/episodes                 | 760        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 30400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.20375898 |
| stats_o/std                    | 0.0745735  |
| test/episodes                  | 200        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.182     |
| test/info_shaping_reward_min   | -0.292     |
| test/Q                         | -8.462607  |
| test/Q_plus_P                  | -8.462607  |
| test/reward_per_eps            | -40        |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 20          |
| stats_o/mean                   | 0.2037449   |
| stats_o/std                    | 0.074015595 |
| test/episodes                  | 210         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -8.529993   |
| test/Q_plus_P                  | -8.529993   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8400        |
| train/episodes                 | 840         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 33600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 21          |
| stats_o/mean                   | 0.20372097  |
| stats_o/std                    | 0.073618144 |
| test/episodes                  | 220         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0625      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00558    |
| test/info_shaping_reward_mean  | -0.163      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -8.314581   |
| test/Q_plus_P                  | -8.314581   |
| test/reward_per_eps            | -37.5       |
| test/steps                     | 8800        |
| train/episodes                 | 880         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 35200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 22         |
| stats_o/mean                   | 0.20363149 |
| stats_o/std                    | 0.07322863 |
| test/episodes                  | 230        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.352418  |
| test/Q_plus_P                  | -9.352418  |
| test/reward_per_eps            | -40        |
| test/steps                     | 9200       |
| train/episodes                 | 920        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 36800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 23          |
| stats_o/mean                   | 0.20356455  |
| stats_o/std                    | 0.072762854 |
| test/episodes                  | 240         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -9.521969   |
| test/Q_plus_P                  | -9.521969   |
| test/reward_per_eps            | -40         |
| test/steps                     | 9600        |
| train/episodes                 | 960         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 38400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 24         |
| stats_o/mean                   | 0.2036923  |
| stats_o/std                    | 0.07226731 |
| test/episodes                  | 250        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.05       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0139    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -9.563258  |
| test/Q_plus_P                  | -9.563258  |
| test/reward_per_eps            | -38        |
| test/steps                     | 10000      |
| train/episodes                 | 1000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 40000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 25         |
| stats_o/mean                   | 0.20364863 |
| stats_o/std                    | 0.07176841 |
| test/episodes                  | 260        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.085      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0216    |
| test/info_shaping_reward_mean  | -0.156     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -9.081999  |
| test/Q_plus_P                  | -9.081999  |
| test/reward_per_eps            | -36.6      |
| test/steps                     | 10400      |
| train/episodes                 | 1040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 41600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 26          |
| stats_o/mean                   | 0.20364626  |
| stats_o/std                    | 0.071733974 |
| test/episodes                  | 270         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -10.301661  |
| test/Q_plus_P                  | -10.301661  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10800       |
| train/episodes                 | 1080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.21       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 43200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 27         |
| stats_o/mean                   | 0.20377637 |
| stats_o/std                    | 0.07205744 |
| test/episodes                  | 280        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.206     |
| test/info_shaping_reward_min   | -0.613     |
| test/Q                         | -10.849959 |
| test/Q_plus_P                  | -10.849959 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11200      |
| train/episodes                 | 1120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 44800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 28          |
| stats_o/mean                   | 0.2037302   |
| stats_o/std                    | 0.071802236 |
| test/episodes                  | 290         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.194      |
| test/info_shaping_reward_min   | -0.502      |
| test/Q                         | -11.067161  |
| test/Q_plus_P                  | -11.067161  |
| test/reward_per_eps            | -40         |
| test/steps                     | 11600       |
| train/episodes                 | 1160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 46400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.20363337 |
| stats_o/std                    | 0.07225358 |
| test/episodes                  | 300        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.055      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00717   |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -10.756469 |
| test/Q_plus_P                  | -10.756469 |
| test/reward_per_eps            | -37.8      |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00625    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 30         |
| stats_o/mean                   | 0.20380887 |
| stats_o/std                    | 0.07338996 |
| test/episodes                  | 310        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.133     |
| test/info_shaping_reward_mean  | -0.197     |
| test/info_shaping_reward_min   | -0.521     |
| test/Q                         | -11.791648 |
| test/Q_plus_P                  | -11.791648 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12400      |
| train/episodes                 | 1240       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.01       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.287     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 49600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 31         |
| stats_o/mean                   | 0.20372654 |
| stats_o/std                    | 0.07475857 |
| test/episodes                  | 320        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.055      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0167    |
| test/info_shaping_reward_mean  | -0.207     |
| test/info_shaping_reward_min   | -0.642     |
| test/Q                         | -10.971554 |
| test/Q_plus_P                  | -10.971554 |
| test/reward_per_eps            | -37.8      |
| test/steps                     | 12800      |
| train/episodes                 | 1280       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.025      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.129     |
| train/info_shaping_reward_mean | -0.2       |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 51200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 32         |
| stats_o/mean                   | 0.2039244  |
| stats_o/std                    | 0.07582458 |
| test/episodes                  | 330        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.223      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00133   |
| test/info_shaping_reward_mean  | -0.127     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -8.375868  |
| test/Q_plus_P                  | -8.375868  |
| test/reward_per_eps            | -31.1      |
| test/steps                     | 13200      |
| train/episodes                 | 1320       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.02       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.102     |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 52800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 33         |
| stats_o/mean                   | 0.20403485 |
| stats_o/std                    | 0.07724521 |
| test/episodes                  | 340        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.147      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.568     |
| test/Q                         | -10.10598  |
| test/Q_plus_P                  | -10.10598  |
| test/reward_per_eps            | -34.1      |
| test/steps                     | 13600      |
| train/episodes                 | 1360       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.377     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 54400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 34         |
| stats_o/mean                   | 0.2041267  |
| stats_o/std                    | 0.07967514 |
| test/episodes                  | 350        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.312      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00257   |
| test/info_shaping_reward_mean  | -0.122     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -8.660559  |
| test/Q_plus_P                  | -8.660559  |
| test/reward_per_eps            | -27.5      |
| test/steps                     | 14000      |
| train/episodes                 | 1400       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.02       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.11      |
| train/info_shaping_reward_mean | -0.2       |
| train/info_shaping_reward_min  | -0.354     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 56000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.2042821  |
| stats_o/std                    | 0.08081308 |
| test/episodes                  | 360        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00699   |
| test/info_shaping_reward_mean  | -0.148     |
| test/info_shaping_reward_min   | -0.228     |
| test/Q                         | -10.284802 |
| test/Q_plus_P                  | -10.284802 |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0281     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.105     |
| train/info_shaping_reward_mean | -0.212     |
| train/info_shaping_reward_min  | -0.428     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 36         |
| stats_o/mean                   | 0.20425212 |
| stats_o/std                    | 0.08270957 |
| test/episodes                  | 370        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.193      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00358   |
| test/info_shaping_reward_mean  | -0.178     |
| test/info_shaping_reward_min   | -0.754     |
| test/Q                         | -10.867794 |
| test/Q_plus_P                  | -10.867794 |
| test/reward_per_eps            | -32.3      |
| test/steps                     | 14800      |
| train/episodes                 | 1480       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0544     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0589    |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.321     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 59200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 37         |
| stats_o/mean                   | 0.20428026 |
| stats_o/std                    | 0.0854012  |
| test/episodes                  | 380        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.315      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00563   |
| test/info_shaping_reward_mean  | -0.131     |
| test/info_shaping_reward_min   | -0.313     |
| test/Q                         | -8.064653  |
| test/Q_plus_P                  | -8.064653  |
| test/reward_per_eps            | -27.4      |
| test/steps                     | 15200      |
| train/episodes                 | 1520       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00937    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.223     |
| train/info_shaping_reward_min  | -0.454     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 60800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 38          |
| stats_o/mean                   | 0.20401621  |
| stats_o/std                    | 0.086240895 |
| test/episodes                  | 390         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.315       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00447    |
| test/info_shaping_reward_mean  | -0.129      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -8.263328   |
| test/Q_plus_P                  | -8.263328   |
| test/reward_per_eps            | -27.4       |
| test/steps                     | 15600       |
| train/episodes                 | 1560        |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0594      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0952     |
| train/info_shaping_reward_mean | -0.199      |
| train/info_shaping_reward_min  | -0.436      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.6       |
| train/steps                    | 62400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 39         |
| stats_o/mean                   | 0.20419273 |
| stats_o/std                    | 0.08768705 |
| test/episodes                  | 400        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.188      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00911   |
| test/info_shaping_reward_mean  | -0.15      |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -10.79831  |
| test/Q_plus_P                  | -10.79831  |
| test/reward_per_eps            | -32.5      |
| test/steps                     | 16000      |
| train/episodes                 | 1600       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0412     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.102     |
| train/info_shaping_reward_mean | -0.211     |
| train/info_shaping_reward_min  | -0.399     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 64000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.20413938  |
| stats_o/std                    | 0.088163175 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.372       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000496   |
| test/info_shaping_reward_mean  | -0.097      |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -7.057019   |
| test/Q_plus_P                  | -7.057019   |
| test/reward_per_eps            | -25.1       |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.0881      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0479     |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.38       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.5       |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.20425676  |
| stats_o/std                    | 0.088220395 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.52        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00441    |
| test/info_shaping_reward_mean  | -0.0865     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -5.340555   |
| test/Q_plus_P                  | -5.340555   |
| test/reward_per_eps            | -19.2       |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.129       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0409     |
| train/info_shaping_reward_mean | -0.149      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.9       |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 42         |
| stats_o/mean                   | 0.20427772 |
| stats_o/std                    | 0.08875923 |
| test/episodes                  | 430        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.333      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00249   |
| test/info_shaping_reward_mean  | -0.1       |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -7.6463623 |
| test/Q_plus_P                  | -7.6463623 |
| test/reward_per_eps            | -26.7      |
| test/steps                     | 17200      |
| train/episodes                 | 1720       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.146      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0433    |
| train/info_shaping_reward_mean | -0.152     |
| train/info_shaping_reward_min  | -0.285     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.2      |
| train/steps                    | 68800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 43         |
| stats_o/mean                   | 0.20428354 |
| stats_o/std                    | 0.08855649 |
| test/episodes                  | 440        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.412      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00258   |
| test/info_shaping_reward_mean  | -0.103     |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -6.7528257 |
| test/Q_plus_P                  | -6.7528257 |
| test/reward_per_eps            | -23.5      |
| test/steps                     | 17600      |
| train/episodes                 | 1760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.142      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0122    |
| train/info_shaping_reward_mean | -0.143     |
| train/info_shaping_reward_min  | -0.275     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.3      |
| train/steps                    | 70400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 44         |
| stats_o/mean                   | 0.20440893 |
| stats_o/std                    | 0.08881054 |
| test/episodes                  | 450        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.422      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00157   |
| test/info_shaping_reward_mean  | -0.127     |
| test/info_shaping_reward_min   | -1.06      |
| test/Q                         | -8.023368  |
| test/Q_plus_P                  | -8.023368  |
| test/reward_per_eps            | -23.1      |
| test/steps                     | 18000      |
| train/episodes                 | 1800       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.136      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0197    |
| train/info_shaping_reward_mean | -0.153     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.5      |
| train/steps                    | 72000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.20453687  |
| stats_o/std                    | 0.089085504 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.588       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00235    |
| test/info_shaping_reward_mean  | -0.0732     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -5.3988795  |
| test/Q_plus_P                  | -5.3988795  |
| test/reward_per_eps            | -16.5       |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.213       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0102     |
| train/info_shaping_reward_mean | -0.14       |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.5       |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 46         |
| stats_o/mean                   | 0.20463905 |
| stats_o/std                    | 0.0889526  |
| test/episodes                  | 470        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.57       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000733  |
| test/info_shaping_reward_mean  | -0.0743    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -4.73535   |
| test/Q_plus_P                  | -4.73535   |
| test/reward_per_eps            | -17.2      |
| test/steps                     | 18800      |
| train/episodes                 | 1880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.266      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00756   |
| train/info_shaping_reward_mean | -0.122     |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.4      |
| train/steps                    | 75200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 47          |
| stats_o/mean                   | 0.20468993  |
| stats_o/std                    | 0.089110576 |
| test/episodes                  | 480         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.623       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00093    |
| test/info_shaping_reward_mean  | -0.0584     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -3.3854842  |
| test/Q_plus_P                  | -3.3854842  |
| test/reward_per_eps            | -15.1       |
| test/steps                     | 19200       |
| train/episodes                 | 1920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.289       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00697    |
| train/info_shaping_reward_mean | -0.124      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.4       |
| train/steps                    | 76800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 48         |
| stats_o/mean                   | 0.20469545 |
| stats_o/std                    | 0.08888145 |
| test/episodes                  | 490        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000633  |
| test/info_shaping_reward_mean  | -0.0557    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.7500412 |
| test/Q_plus_P                  | -2.7500412 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 19600      |
| train/episodes                 | 1960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.35       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00668   |
| train/info_shaping_reward_mean | -0.106     |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26        |
| train/steps                    | 78400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 49         |
| stats_o/mean                   | 0.204773   |
| stats_o/std                    | 0.08863774 |
| test/episodes                  | 500        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.652      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00353   |
| test/info_shaping_reward_mean  | -0.0627    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -3.4239771 |
| test/Q_plus_P                  | -3.4239771 |
| test/reward_per_eps            | -13.9      |
| test/steps                     | 20000      |
| train/episodes                 | 2000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.397      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00504   |
| train/info_shaping_reward_mean | -0.106     |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.1      |
| train/steps                    | 80000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.20481795 |
| stats_o/std                    | 0.08896173 |
| test/episodes                  | 510        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.555      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00159   |
| test/info_shaping_reward_mean  | -0.0692    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -4.117004  |
| test/Q_plus_P                  | -4.117004  |
| test/reward_per_eps            | -17.8      |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.304      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00672   |
| train/info_shaping_reward_mean | -0.137     |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.8      |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 51          |
| stats_o/mean                   | 0.20480004  |
| stats_o/std                    | 0.089200325 |
| test/episodes                  | 520         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.004      |
| test/info_shaping_reward_mean  | -0.0547     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -2.2752213  |
| test/Q_plus_P                  | -2.2752213  |
| test/reward_per_eps            | -12         |
| test/steps                     | 20800       |
| train/episodes                 | 2080        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.331       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0195     |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.8       |
| train/steps                    | 83200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.20480993 |
| stats_o/std                    | 0.08907137 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000938  |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.5900915 |
| test/Q_plus_P                  | -2.5900915 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.423      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00629   |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.1      |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 53         |
| stats_o/mean                   | 0.2048562  |
| stats_o/std                    | 0.08872789 |
| test/episodes                  | 540        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0014    |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -2.372845  |
| test/Q_plus_P                  | -2.372845  |
| test/reward_per_eps            | -11        |
| test/steps                     | 21600      |
| train/episodes                 | 2160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.372      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00524   |
| train/info_shaping_reward_mean | -0.0979    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.1      |
| train/steps                    | 86400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 54          |
| stats_o/mean                   | 0.20481378  |
| stats_o/std                    | 0.088289574 |
| test/episodes                  | 550         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.4252937  |
| test/Q_plus_P                  | -2.4252937  |
| test/reward_per_eps            | -12         |
| test/steps                     | 22000       |
| train/episodes                 | 2200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.494       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00541    |
| train/info_shaping_reward_mean | -0.0818     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.2       |
| train/steps                    | 88000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 55          |
| stats_o/mean                   | 0.20484987  |
| stats_o/std                    | 0.087859064 |
| test/episodes                  | 560         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00113    |
| test/info_shaping_reward_mean  | -0.0497     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.9143242  |
| test/Q_plus_P                  | -1.9143242  |
| test/reward_per_eps            | -11         |
| test/steps                     | 22400       |
| train/episodes                 | 2240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.482       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00466    |
| train/info_shaping_reward_mean | -0.0843     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.7       |
| train/steps                    | 89600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 56         |
| stats_o/mean                   | 0.20476027 |
| stats_o/std                    | 0.0877863  |
| test/episodes                  | 570        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0021    |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.2917578 |
| test/Q_plus_P                  | -2.2917578 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 22800      |
| train/episodes                 | 2280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.468      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00343   |
| train/info_shaping_reward_mean | -0.0973    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.3      |
| train/steps                    | 91200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 57         |
| stats_o/mean                   | 0.20476075 |
| stats_o/std                    | 0.08743675 |
| test/episodes                  | 580        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0024    |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.3824475 |
| test/Q_plus_P                  | -2.3824475 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 23200      |
| train/episodes                 | 2320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.479      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00523   |
| train/info_shaping_reward_mean | -0.085     |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 92800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 58         |
| stats_o/mean                   | 0.20478986 |
| stats_o/std                    | 0.08701308 |
| test/episodes                  | 590        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000887  |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5573235 |
| test/Q_plus_P                  | -1.5573235 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 23600      |
| train/episodes                 | 2360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.485      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00464   |
| train/info_shaping_reward_mean | -0.0803    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 94400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 59         |
| stats_o/mean                   | 0.20473091 |
| stats_o/std                    | 0.08672044 |
| test/episodes                  | 600        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.8296393 |
| test/Q_plus_P                  | -1.8296393 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 24000      |
| train/episodes                 | 2400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.479      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0054    |
| train/info_shaping_reward_mean | -0.0879    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 96000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 60         |
| stats_o/mean                   | 0.20476939 |
| stats_o/std                    | 0.08625067 |
| test/episodes                  | 610        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00101   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.9085822 |
| test/Q_plus_P                  | -1.9085822 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 24400      |
| train/episodes                 | 2440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00428   |
| train/info_shaping_reward_mean | -0.0756    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 97600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 61         |
| stats_o/mean                   | 0.20481287 |
| stats_o/std                    | 0.08586536 |
| test/episodes                  | 620        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00339   |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.8578496 |
| test/Q_plus_P                  | -1.8578496 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 24800      |
| train/episodes                 | 2480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.497      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00442   |
| train/info_shaping_reward_mean | -0.0825    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 99200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 62         |
| stats_o/mean                   | 0.2048043  |
| stats_o/std                    | 0.085439   |
| test/episodes                  | 630        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00103   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.9465388 |
| test/Q_plus_P                  | -1.9465388 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 25200      |
| train/episodes                 | 2520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.523      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0778    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 100800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 63         |
| stats_o/mean                   | 0.20478867 |
| stats_o/std                    | 0.08503727 |
| test/episodes                  | 640        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00123   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6080152 |
| test/Q_plus_P                  | -1.6080152 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 25600      |
| train/episodes                 | 2560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.504      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00397   |
| train/info_shaping_reward_mean | -0.0799    |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 102400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.20480095 |
| stats_o/std                    | 0.08465112 |
| test/episodes                  | 650        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00155   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4199729 |
| test/Q_plus_P                  | -1.4199729 |
| test/reward_per_eps            | -10        |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00532   |
| train/info_shaping_reward_mean | -0.0758    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 65         |
| stats_o/mean                   | 0.20484899 |
| stats_o/std                    | 0.0844362  |
| test/episodes                  | 660        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00143   |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.211     |
| test/Q                         | -2.8792443 |
| test/Q_plus_P                  | -2.8792443 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 26400      |
| train/episodes                 | 2640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.0895    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 105600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 66         |
| stats_o/mean                   | 0.20481513 |
| stats_o/std                    | 0.08410856 |
| test/episodes                  | 670        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00127   |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5333751 |
| test/Q_plus_P                  | -1.5333751 |
| test/reward_per_eps            | -11        |
| test/steps                     | 26800      |
| train/episodes                 | 2680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.53       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0043    |
| train/info_shaping_reward_mean | -0.0855    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 107200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 67         |
| stats_o/mean                   | 0.20485593 |
| stats_o/std                    | 0.08386805 |
| test/episodes                  | 680        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00104   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5145013 |
| test/Q_plus_P                  | -1.5145013 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 27200      |
| train/episodes                 | 2720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.563      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.0738    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 108800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 68         |
| stats_o/mean                   | 0.20486173 |
| stats_o/std                    | 0.08362732 |
| test/episodes                  | 690        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00132   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6685998 |
| test/Q_plus_P                  | -1.6685998 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 27600      |
| train/episodes                 | 2760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.499      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00526   |
| train/info_shaping_reward_mean | -0.0817    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20        |
| train/steps                    | 110400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 69         |
| stats_o/mean                   | 0.20488405 |
| stats_o/std                    | 0.08324167 |
| test/episodes                  | 700        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.67       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0019    |
| test/info_shaping_reward_mean  | -0.062     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.7655303 |
| test/Q_plus_P                  | -1.7655303 |
| test/reward_per_eps            | -13.2      |
| test/steps                     | 28000      |
| train/episodes                 | 2800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.541      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00546   |
| train/info_shaping_reward_mean | -0.0752    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 112000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 70          |
| stats_o/mean                   | 0.20492807  |
| stats_o/std                    | 0.083171934 |
| test/episodes                  | 710         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000877   |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.3119524  |
| test/Q_plus_P                  | -1.3119524  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 28400       |
| train/episodes                 | 2840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.514       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00583    |
| train/info_shaping_reward_mean | -0.0889     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.4       |
| train/steps                    | 113600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 71         |
| stats_o/mean                   | 0.20492391 |
| stats_o/std                    | 0.08298655 |
| test/episodes                  | 720        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00212   |
| test/info_shaping_reward_mean  | -0.0515    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4879247 |
| test/Q_plus_P                  | -1.4879247 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 28800      |
| train/episodes                 | 2880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00401   |
| train/info_shaping_reward_mean | -0.0836    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 115200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 72         |
| stats_o/mean                   | 0.2049046  |
| stats_o/std                    | 0.08272545 |
| test/episodes                  | 730        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00164   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2670276 |
| test/Q_plus_P                  | -1.2670276 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 29200      |
| train/episodes                 | 2920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0774    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 116800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.20490521 |
| stats_o/std                    | 0.08242528 |
| test/episodes                  | 740        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00112   |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.4586344 |
| test/Q_plus_P                  | -1.4586344 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.492      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00356   |
| train/info_shaping_reward_mean | -0.0799    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.3      |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 74         |
| stats_o/mean                   | 0.20491813 |
| stats_o/std                    | 0.08215207 |
| test/episodes                  | 750        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00142   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2786938 |
| test/Q_plus_P                  | -1.2786938 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 30000      |
| train/episodes                 | 3000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.544      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00391   |
| train/info_shaping_reward_mean | -0.0839    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 120000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 75         |
| stats_o/mean                   | 0.20488806 |
| stats_o/std                    | 0.08212321 |
| test/episodes                  | 760        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00139   |
| test/info_shaping_reward_mean  | -0.0419    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1380901 |
| test/Q_plus_P                  | -1.1380901 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 30400      |
| train/episodes                 | 3040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.514      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00502   |
| train/info_shaping_reward_mean | -0.0854    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 121600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 76         |
| stats_o/mean                   | 0.20483488 |
| stats_o/std                    | 0.08185953 |
| test/episodes                  | 770        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00116   |
| test/info_shaping_reward_mean  | -0.0431    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1851865 |
| test/Q_plus_P                  | -1.1851865 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 30800      |
| train/episodes                 | 3080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00416   |
| train/info_shaping_reward_mean | -0.0824    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 123200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 77         |
| stats_o/mean                   | 0.20483577 |
| stats_o/std                    | 0.08178148 |
| test/episodes                  | 780        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00286   |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -1.3482361 |
| test/Q_plus_P                  | -1.3482361 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 31200      |
| train/episodes                 | 3120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00425   |
| train/info_shaping_reward_mean | -0.0869    |
| train/info_shaping_reward_min  | -0.283     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 124800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 78         |
| stats_o/mean                   | 0.20484598 |
| stats_o/std                    | 0.08176765 |
| test/episodes                  | 790        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00124   |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4798758 |
| test/Q_plus_P                  | -1.4798758 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 31600      |
| train/episodes                 | 3160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.514      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00384   |
| train/info_shaping_reward_mean | -0.0927    |
| train/info_shaping_reward_min  | -0.277     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 126400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 79          |
| stats_o/mean                   | 0.20483315  |
| stats_o/std                    | 0.081425495 |
| test/episodes                  | 800         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0602     |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -2.5835092  |
| test/Q_plus_P                  | -2.5835092  |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 32000       |
| train/episodes                 | 3200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.552       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00269    |
| train/info_shaping_reward_mean | -0.0749     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 128000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 80         |
| stats_o/mean                   | 0.20479937 |
| stats_o/std                    | 0.08118329 |
| test/episodes                  | 810        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00214   |
| test/info_shaping_reward_mean  | -0.0457    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3038384 |
| test/Q_plus_P                  | -1.3038384 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 32400      |
| train/episodes                 | 3240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00339   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 129600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 81          |
| stats_o/mean                   | 0.20477703  |
| stats_o/std                    | 0.081133805 |
| test/episodes                  | 820         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.3189567  |
| test/Q_plus_P                  | -1.3189567  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 32800       |
| train/episodes                 | 3280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.485       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00411    |
| train/info_shaping_reward_mean | -0.088      |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.6       |
| train/steps                    | 131200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 82         |
| stats_o/mean                   | 0.20476572 |
| stats_o/std                    | 0.08084048 |
| test/episodes                  | 830        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00135   |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4645196 |
| test/Q_plus_P                  | -1.4645196 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 33200      |
| train/episodes                 | 3320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00321   |
| train/info_shaping_reward_mean | -0.0772    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 132800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 83         |
| stats_o/mean                   | 0.2047434  |
| stats_o/std                    | 0.08056753 |
| test/episodes                  | 840        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2744824 |
| test/Q_plus_P                  | -1.2744824 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 33600      |
| train/episodes                 | 3360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.0766    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 134400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 84          |
| stats_o/mean                   | 0.20474896  |
| stats_o/std                    | 0.080495864 |
| test/episodes                  | 850         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0018     |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0573957  |
| test/Q_plus_P                  | -1.0573957  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 34000       |
| train/episodes                 | 3400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.562       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00453    |
| train/info_shaping_reward_mean | -0.0722     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 136000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.20479661  |
| stats_o/std                    | 0.080437206 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00156    |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1276081  |
| test/Q_plus_P                  | -1.1276081  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.537       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00403    |
| train/info_shaping_reward_mean | -0.0909     |
| train/info_shaping_reward_min  | -0.287      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.5       |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 86         |
| stats_o/mean                   | 0.20485362 |
| stats_o/std                    | 0.08024453 |
| test/episodes                  | 870        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00338   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3766261 |
| test/Q_plus_P                  | -1.3766261 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 34800      |
| train/episodes                 | 3480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.543      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00453   |
| train/info_shaping_reward_mean | -0.0796    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 139200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 87          |
| stats_o/mean                   | 0.2048568   |
| stats_o/std                    | 0.079940535 |
| test/episodes                  | 880         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.003      |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.325187   |
| test/Q_plus_P                  | -1.325187   |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 35200       |
| train/episodes                 | 3520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.583       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0714     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 140800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 88          |
| stats_o/mean                   | 0.20482275  |
| stats_o/std                    | 0.079729535 |
| test/episodes                  | 890         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00172    |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.5244     |
| test/Q_plus_P                  | -1.5244     |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 35600       |
| train/episodes                 | 3560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.549       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0038     |
| train/info_shaping_reward_mean | -0.0728     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 142400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 89         |
| stats_o/mean                   | 0.2048444  |
| stats_o/std                    | 0.07955515 |
| test/episodes                  | 900        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.1368464 |
| test/Q_plus_P                  | -1.1368464 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 36000      |
| train/episodes                 | 3600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00334   |
| train/info_shaping_reward_mean | -0.0837    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 144000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 90         |
| stats_o/mean                   | 0.20483978 |
| stats_o/std                    | 0.07946795 |
| test/episodes                  | 910        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1357497 |
| test/Q_plus_P                  | -1.1357497 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 36400      |
| train/episodes                 | 3640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00367   |
| train/info_shaping_reward_mean | -0.0768    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 145600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 91          |
| stats_o/mean                   | 0.20487827  |
| stats_o/std                    | 0.079312034 |
| test/episodes                  | 920         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00281    |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2694784  |
| test/Q_plus_P                  | -1.2694784  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 36800       |
| train/episodes                 | 3680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.538       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00425    |
| train/info_shaping_reward_mean | -0.0772     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.5       |
| train/steps                    | 147200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 92         |
| stats_o/mean                   | 0.2049101  |
| stats_o/std                    | 0.07927467 |
| test/episodes                  | 930        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2413256 |
| test/Q_plus_P                  | -1.2413256 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 37200      |
| train/episodes                 | 3720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.538      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0036    |
| train/info_shaping_reward_mean | -0.0861    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 148800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.20491724  |
| stats_o/std                    | 0.079038985 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00322    |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0698322  |
| test/Q_plus_P                  | -1.0698322  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00401    |
| train/info_shaping_reward_mean | -0.0786     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 94          |
| stats_o/mean                   | 0.20492269  |
| stats_o/std                    | 0.078996375 |
| test/episodes                  | 950         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.625       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00576    |
| test/info_shaping_reward_mean  | -0.067      |
| test/info_shaping_reward_min   | -0.197      |
| test/Q                         | -2.884927   |
| test/Q_plus_P                  | -2.884927   |
| test/reward_per_eps            | -15         |
| test/steps                     | 38000       |
| train/episodes                 | 3800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.58        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00372    |
| train/info_shaping_reward_mean | -0.0813     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 152000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 95         |
| stats_o/mean                   | 0.20495638 |
| stats_o/std                    | 0.07875788 |
| test/episodes                  | 960        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00275   |
| test/info_shaping_reward_mean  | -0.0633    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2745783 |
| test/Q_plus_P                  | -1.2745783 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 38400      |
| train/episodes                 | 3840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0044    |
| train/info_shaping_reward_mean | -0.066     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 153600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 96         |
| stats_o/mean                   | 0.20499448 |
| stats_o/std                    | 0.07867714 |
| test/episodes                  | 970        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00377   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2178774 |
| test/Q_plus_P                  | -1.2178774 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 38800      |
| train/episodes                 | 3880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00409   |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 155200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 97         |
| stats_o/mean                   | 0.20499189 |
| stats_o/std                    | 0.07840936 |
| test/episodes                  | 980        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00601   |
| test/info_shaping_reward_mean  | -0.05      |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1912544 |
| test/Q_plus_P                  | -1.1912544 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 39200      |
| train/episodes                 | 3920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.587      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00404   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 156800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 98          |
| stats_o/mean                   | 0.20496634  |
| stats_o/std                    | 0.078403205 |
| test/episodes                  | 990         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00629    |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1020683  |
| test/Q_plus_P                  | -1.1020683  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 39600       |
| train/episodes                 | 3960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.547       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00344    |
| train/info_shaping_reward_mean | -0.074      |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 158400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.20495789 |
| stats_o/std                    | 0.07844813 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00981   |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3026903 |
| test/Q_plus_P                  | -1.3026903 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.527      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.0852    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 100        |
| stats_o/mean                   | 0.2049542  |
| stats_o/std                    | 0.07853901 |
| test/episodes                  | 1010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00449   |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3007538 |
| test/Q_plus_P                  | -1.3007538 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 40400      |
| train/episodes                 | 4040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.497      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00406   |
| train/info_shaping_reward_mean | -0.095     |
| train/info_shaping_reward_min  | -0.283     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 161600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.20494305  |
| stats_o/std                    | 0.078407764 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.002      |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.0432711  |
| test/Q_plus_P                  | -1.0432711  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.551       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00514    |
| train/info_shaping_reward_mean | -0.0727     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18         |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 102        |
| stats_o/mean                   | 0.20496437 |
| stats_o/std                    | 0.07819673 |
| test/episodes                  | 1030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00242   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2550359 |
| test/Q_plus_P                  | -1.2550359 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 41200      |
| train/episodes                 | 4120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00453   |
| train/info_shaping_reward_mean | -0.0665    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 164800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 103        |
| stats_o/mean                   | 0.20494555 |
| stats_o/std                    | 0.07817694 |
| test/episodes                  | 1040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00424   |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1819173 |
| test/Q_plus_P                  | -1.1819173 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 41600      |
| train/episodes                 | 4160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.517      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0036    |
| train/info_shaping_reward_mean | -0.0772    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 166400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 104        |
| stats_o/mean                   | 0.20495887 |
| stats_o/std                    | 0.07819724 |
| test/episodes                  | 1050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00301   |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.3151397 |
| test/Q_plus_P                  | -1.3151397 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 42000      |
| train/episodes                 | 4200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.477      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.0847    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 168000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 105        |
| stats_o/mean                   | 0.20492037 |
| stats_o/std                    | 0.07815669 |
| test/episodes                  | 1060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00394   |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2712739 |
| test/Q_plus_P                  | -1.2712739 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 42400      |
| train/episodes                 | 4240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.072     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 169600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 106        |
| stats_o/mean                   | 0.20493294 |
| stats_o/std                    | 0.07799011 |
| test/episodes                  | 1070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00702   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.0747589 |
| test/Q_plus_P                  | -1.0747589 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 42800      |
| train/episodes                 | 4280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00506   |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 171200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 107        |
| stats_o/mean                   | 0.20492812 |
| stats_o/std                    | 0.07792873 |
| test/episodes                  | 1080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00417   |
| test/info_shaping_reward_mean  | -0.0612    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -2.8659904 |
| test/Q_plus_P                  | -2.8659904 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 43200      |
| train/episodes                 | 4320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00424   |
| train/info_shaping_reward_mean | -0.0734    |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 172800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 108        |
| stats_o/mean                   | 0.20491987 |
| stats_o/std                    | 0.07789264 |
| test/episodes                  | 1090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00182   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.2632676 |
| test/Q_plus_P                  | -1.2632676 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 43600      |
| train/episodes                 | 4360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.511      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0841    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 174400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 109        |
| stats_o/mean                   | 0.20490342 |
| stats_o/std                    | 0.07774882 |
| test/episodes                  | 1100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00355   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0018553 |
| test/Q_plus_P                  | -1.0018553 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 44000      |
| train/episodes                 | 4400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0036    |
| train/info_shaping_reward_mean | -0.0676    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 176000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 110        |
| stats_o/mean                   | 0.20491853 |
| stats_o/std                    | 0.07758599 |
| test/episodes                  | 1110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00437   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2032009 |
| test/Q_plus_P                  | -1.2032009 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 44400      |
| train/episodes                 | 4440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00383   |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 177600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 111        |
| stats_o/mean                   | 0.20494033 |
| stats_o/std                    | 0.07763816 |
| test/episodes                  | 1120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00852   |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.1811547 |
| test/Q_plus_P                  | -1.1811547 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 44800      |
| train/episodes                 | 4480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00408   |
| train/info_shaping_reward_mean | -0.093     |
| train/info_shaping_reward_min  | -0.267     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 179200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 112        |
| stats_o/mean                   | 0.2049432  |
| stats_o/std                    | 0.07754623 |
| test/episodes                  | 1130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00542   |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2787771 |
| test/Q_plus_P                  | -1.2787771 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 45200      |
| train/episodes                 | 4520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00448   |
| train/info_shaping_reward_mean | -0.0743    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 180800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 113        |
| stats_o/mean                   | 0.20495458 |
| stats_o/std                    | 0.07744486 |
| test/episodes                  | 1140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0013    |
| test/info_shaping_reward_mean  | -0.0591    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.7415237 |
| test/Q_plus_P                  | -1.7415237 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 45600      |
| train/episodes                 | 4560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.587      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00374   |
| train/info_shaping_reward_mean | -0.0775    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 182400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 114        |
| stats_o/mean                   | 0.2049579  |
| stats_o/std                    | 0.07737693 |
| test/episodes                  | 1150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00821   |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1118897 |
| test/Q_plus_P                  | -1.1118897 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 46000      |
| train/episodes                 | 4600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00364   |
| train/info_shaping_reward_mean | -0.0741    |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 184000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 115        |
| stats_o/mean                   | 0.20496517 |
| stats_o/std                    | 0.07751501 |
| test/episodes                  | 1160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.675      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0022    |
| test/info_shaping_reward_mean  | -0.0669    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -2.9309256 |
| test/Q_plus_P                  | -2.9309256 |
| test/reward_per_eps            | -13        |
| test/steps                     | 46400      |
| train/episodes                 | 4640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.485      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00571   |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.278     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 185600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 116        |
| stats_o/mean                   | 0.20495626 |
| stats_o/std                    | 0.07732227 |
| test/episodes                  | 1170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00541   |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1926377 |
| test/Q_plus_P                  | -1.1926377 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 46800      |
| train/episodes                 | 4680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0048    |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 187200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 117        |
| stats_o/mean                   | 0.20500839 |
| stats_o/std                    | 0.07743923 |
| test/episodes                  | 1180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00441   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4238185 |
| test/Q_plus_P                  | -1.4238185 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 47200      |
| train/episodes                 | 4720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.509      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00413   |
| train/info_shaping_reward_mean | -0.0884    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 188800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.20505165  |
| stats_o/std                    | 0.077480264 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00433    |
| test/info_shaping_reward_mean  | -0.0551     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.1547395  |
| test/Q_plus_P                  | -1.1547395  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.0857     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 119         |
| stats_o/mean                   | 0.20506781  |
| stats_o/std                    | 0.077440165 |
| test/episodes                  | 1200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00462    |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.236829   |
| test/Q_plus_P                  | -1.236829   |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 48000       |
| train/episodes                 | 4800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00548    |
| train/info_shaping_reward_mean | -0.081      |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 192000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 120        |
| stats_o/mean                   | 0.2050482  |
| stats_o/std                    | 0.07739017 |
| test/episodes                  | 1210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00507   |
| test/info_shaping_reward_mean  | -0.0591    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3399042 |
| test/Q_plus_P                  | -1.3399042 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 48400      |
| train/episodes                 | 4840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00465   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 193600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.20504637 |
| stats_o/std                    | 0.07731606 |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00594   |
| test/info_shaping_reward_mean  | -0.0531    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1437805 |
| test/Q_plus_P                  | -1.1437805 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00395   |
| train/info_shaping_reward_mean | -0.0782    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 122        |
| stats_o/mean                   | 0.20506254 |
| stats_o/std                    | 0.07728143 |
| test/episodes                  | 1230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00628   |
| test/info_shaping_reward_mean  | -0.0569    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9512751 |
| test/Q_plus_P                  | -0.9512751 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 49200      |
| train/episodes                 | 4920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00442   |
| train/info_shaping_reward_mean | -0.0863    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 196800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 123         |
| stats_o/mean                   | 0.2050598   |
| stats_o/std                    | 0.077137664 |
| test/episodes                  | 1240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00153    |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.990488   |
| test/Q_plus_P                  | -0.990488   |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 49600       |
| train/episodes                 | 4960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.561       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00449    |
| train/info_shaping_reward_mean | -0.075      |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 198400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.20504513  |
| stats_o/std                    | 0.077004306 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00723    |
| test/info_shaping_reward_mean  | -0.0627     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1288868  |
| test/Q_plus_P                  | -1.1288868  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00363    |
| train/info_shaping_reward_mean | -0.0727     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 125        |
| stats_o/mean                   | 0.20505653 |
| stats_o/std                    | 0.0769264  |
| test/episodes                  | 1260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00532   |
| test/info_shaping_reward_mean  | -0.0581    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -0.9928703 |
| test/Q_plus_P                  | -0.9928703 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 50400      |
| train/episodes                 | 5040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00539   |
| train/info_shaping_reward_mean | -0.0804    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 201600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 126        |
| stats_o/mean                   | 0.20507054 |
| stats_o/std                    | 0.07677413 |
| test/episodes                  | 1270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0038    |
| test/info_shaping_reward_mean  | -0.0601    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.7399633 |
| test/Q_plus_P                  | -2.7399633 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 50800      |
| train/episodes                 | 5080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.587      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00465   |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 203200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 127        |
| stats_o/mean                   | 0.20506646 |
| stats_o/std                    | 0.07670501 |
| test/episodes                  | 1280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00501   |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1971335 |
| test/Q_plus_P                  | -1.1971335 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 51200      |
| train/episodes                 | 5120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00519   |
| train/info_shaping_reward_mean | -0.0811    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 204800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.20505802 |
| stats_o/std                    | 0.07653944 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00558   |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1662273 |
| test/Q_plus_P                  | -1.1662273 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00473   |
| train/info_shaping_reward_mean | -0.072     |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 129         |
| stats_o/mean                   | 0.20504713  |
| stats_o/std                    | 0.076577455 |
| test/episodes                  | 1300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00296    |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.94779646 |
| test/Q_plus_P                  | -0.94779646 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 52000       |
| train/episodes                 | 5200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.573       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00488    |
| train/info_shaping_reward_mean | -0.0862     |
| train/info_shaping_reward_min  | -0.291      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 208000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 130        |
| stats_o/mean                   | 0.20502576 |
| stats_o/std                    | 0.07650182 |
| test/episodes                  | 1310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00475   |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.137429  |
| test/Q_plus_P                  | -1.137429  |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 52400      |
| train/episodes                 | 5240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0046    |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 209600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 131        |
| stats_o/mean                   | 0.20505764 |
| stats_o/std                    | 0.0766074  |
| test/episodes                  | 1320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00608   |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2191172 |
| test/Q_plus_P                  | -1.2191172 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 52800      |
| train/episodes                 | 5280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00523   |
| train/info_shaping_reward_mean | -0.0843    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 211200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 132         |
| stats_o/mean                   | 0.20506315  |
| stats_o/std                    | 0.076695055 |
| test/episodes                  | 1330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00637    |
| test/info_shaping_reward_mean  | -0.0577     |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -1.0477043  |
| test/Q_plus_P                  | -1.0477043  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 53200       |
| train/episodes                 | 5320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.524       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0933     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19         |
| train/steps                    | 212800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 133        |
| stats_o/mean                   | 0.20508468 |
| stats_o/std                    | 0.07661397 |
| test/episodes                  | 1340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00713   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.0791811 |
| test/Q_plus_P                  | -1.0791811 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 53600      |
| train/episodes                 | 5360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00467   |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 214400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 134        |
| stats_o/mean                   | 0.20508617 |
| stats_o/std                    | 0.07656825 |
| test/episodes                  | 1350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00529   |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.200985  |
| test/Q_plus_P                  | -1.200985  |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 54000      |
| train/episodes                 | 5400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00593   |
| train/info_shaping_reward_mean | -0.0877    |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 216000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 135        |
| stats_o/mean                   | 0.20508066 |
| stats_o/std                    | 0.07652784 |
| test/episodes                  | 1360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00904   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0104985 |
| test/Q_plus_P                  | -1.0104985 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 54400      |
| train/episodes                 | 5440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0062    |
| train/info_shaping_reward_mean | -0.0789    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 217600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 136        |
| stats_o/mean                   | 0.2050828  |
| stats_o/std                    | 0.07648988 |
| test/episodes                  | 1370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00742   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.0075269 |
| test/Q_plus_P                  | -1.0075269 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 54800      |
| train/episodes                 | 5480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00487   |
| train/info_shaping_reward_mean | -0.0785    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 219200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 137        |
| stats_o/mean                   | 0.205101   |
| stats_o/std                    | 0.07640271 |
| test/episodes                  | 1380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00216   |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0646449 |
| test/Q_plus_P                  | -1.0646449 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 55200      |
| train/episodes                 | 5520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00473   |
| train/info_shaping_reward_mean | -0.0757    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 220800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.20506969 |
| stats_o/std                    | 0.07636848 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0116    |
| test/info_shaping_reward_mean  | -0.0634    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.1937844 |
| test/Q_plus_P                  | -1.1937844 |
| test/reward_per_eps            | -10        |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00533   |
| train/info_shaping_reward_mean | -0.0786    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 139         |
| stats_o/mean                   | 0.20505962  |
| stats_o/std                    | 0.076379545 |
| test/episodes                  | 1400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00868    |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.94176084 |
| test/Q_plus_P                  | -0.94176084 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 56000       |
| train/episodes                 | 5600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.583       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0038     |
| train/info_shaping_reward_mean | -0.0823     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 224000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 140        |
| stats_o/mean                   | 0.20504494 |
| stats_o/std                    | 0.07622102 |
| test/episodes                  | 1410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00725   |
| test/info_shaping_reward_mean  | -0.0564    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.0929871 |
| test/Q_plus_P                  | -1.0929871 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 56400      |
| train/episodes                 | 5640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0047    |
| train/info_shaping_reward_mean | -0.0688    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 225600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.20504494 |
| stats_o/std                    | 0.07620115 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00509   |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -0.9980745 |
| test/Q_plus_P                  | -0.9980745 |
| test/reward_per_eps            | -9         |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00541   |
| train/info_shaping_reward_mean | -0.0803    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 142        |
| stats_o/mean                   | 0.20502688 |
| stats_o/std                    | 0.07603674 |
| test/episodes                  | 1430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00696   |
| test/info_shaping_reward_mean  | -0.0642    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5104669 |
| test/Q_plus_P                  | -1.5104669 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 57200      |
| train/episodes                 | 5720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00426   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 228800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 143         |
| stats_o/mean                   | 0.20503403  |
| stats_o/std                    | 0.076032914 |
| test/episodes                  | 1440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0135     |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.7553     |
| test/Q_plus_P                  | -0.7553     |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 57600       |
| train/episodes                 | 5760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.551       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00543    |
| train/info_shaping_reward_mean | -0.0747     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18         |
| train/steps                    | 230400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.20505106  |
| stats_o/std                    | 0.076045156 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00508    |
| test/info_shaping_reward_mean  | -0.0497     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.9545809  |
| test/Q_plus_P                  | -0.9545809  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.555       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00464    |
| train/info_shaping_reward_mean | -0.085      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 145        |
| stats_o/mean                   | 0.20501707 |
| stats_o/std                    | 0.07600254 |
| test/episodes                  | 1460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00267   |
| test/info_shaping_reward_mean  | -0.0547    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0173668 |
| test/Q_plus_P                  | -1.0173668 |
| test/reward_per_eps            | -10        |
| test/steps                     | 58400      |
| train/episodes                 | 5840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00481   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 233600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 146         |
| stats_o/mean                   | 0.2050196   |
| stats_o/std                    | 0.075851075 |
| test/episodes                  | 1470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00828    |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.9382747  |
| test/Q_plus_P                  | -0.9382747  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 58800       |
| train/episodes                 | 5880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00539    |
| train/info_shaping_reward_mean | -0.0687     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 235200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 147        |
| stats_o/mean                   | 0.20501159 |
| stats_o/std                    | 0.0758724  |
| test/episodes                  | 1480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0124    |
| test/info_shaping_reward_mean  | -0.0594    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -0.9598911 |
| test/Q_plus_P                  | -0.9598911 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 59200      |
| train/episodes                 | 5920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.534      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00649   |
| train/info_shaping_reward_mean | -0.0785    |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 236800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 148        |
| stats_o/mean                   | 0.20503035 |
| stats_o/std                    | 0.07595286 |
| test/episodes                  | 1490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00747   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1754895 |
| test/Q_plus_P                  | -1.1754895 |
| test/reward_per_eps            | -9         |
| test/steps                     | 59600      |
| train/episodes                 | 5960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.516      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00487   |
| train/info_shaping_reward_mean | -0.0819    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 238400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 149         |
| stats_o/mean                   | 0.20502977  |
| stats_o/std                    | 0.075885236 |
| test/episodes                  | 1500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00381    |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.0702094  |
| test/Q_plus_P                  | -1.0702094  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 60000       |
| train/episodes                 | 6000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.572       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00407    |
| train/info_shaping_reward_mean | -0.0804     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 240000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 150        |
| stats_o/mean                   | 0.20506124 |
| stats_o/std                    | 0.07604918 |
| test/episodes                  | 1510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0152    |
| test/info_shaping_reward_mean  | -0.0595    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1593784 |
| test/Q_plus_P                  | -1.1593784 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 60400      |
| train/episodes                 | 6040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00469   |
| train/info_shaping_reward_mean | -0.0866    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 241600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 151         |
| stats_o/mean                   | 0.20508987  |
| stats_o/std                    | 0.075947545 |
| test/episodes                  | 1520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00443    |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1496198  |
| test/Q_plus_P                  | -1.1496198  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 60800       |
| train/episodes                 | 6080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.005      |
| train/info_shaping_reward_mean | -0.0689     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 243200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 152         |
| stats_o/mean                   | 0.20508745  |
| stats_o/std                    | 0.07581029  |
| test/episodes                  | 1530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00844    |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -0.93872607 |
| test/Q_plus_P                  | -0.93872607 |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 61200       |
| train/episodes                 | 6120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.58        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0034     |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 244800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 153         |
| stats_o/mean                   | 0.2051004   |
| stats_o/std                    | 0.075671785 |
| test/episodes                  | 1540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0115     |
| test/info_shaping_reward_mean  | -0.0622     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2134349  |
| test/Q_plus_P                  | -1.2134349  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 61600       |
| train/episodes                 | 6160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0056     |
| train/info_shaping_reward_mean | -0.0653     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 246400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.20510401 |
| stats_o/std                    | 0.07554498 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0108    |
| test/info_shaping_reward_mean  | -0.0589    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1680955 |
| test/Q_plus_P                  | -1.1680955 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00509   |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.20510319 |
| stats_o/std                    | 0.0754047  |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0107    |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.9850121 |
| test/Q_plus_P                  | -0.9850121 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0038    |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 156         |
| stats_o/mean                   | 0.20510273  |
| stats_o/std                    | 0.075292766 |
| test/episodes                  | 1570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0133     |
| test/info_shaping_reward_mean  | -0.0596     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0012238  |
| test/Q_plus_P                  | -1.0012238  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 62800       |
| train/episodes                 | 6280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.615       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0728     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 251200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 157        |
| stats_o/mean                   | 0.20511205 |
| stats_o/std                    | 0.07526572 |
| test/episodes                  | 1580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.015     |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.7154279 |
| test/Q_plus_P                  | -0.7154279 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 63200      |
| train/episodes                 | 6320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.524      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0061    |
| train/info_shaping_reward_mean | -0.0863    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 252800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 158        |
| stats_o/mean                   | 0.20510176 |
| stats_o/std                    | 0.07514777 |
| test/episodes                  | 1590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00811   |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0313616 |
| test/Q_plus_P                  | -1.0313616 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 63600      |
| train/episodes                 | 6360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00417   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 254400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.20510808  |
| stats_o/std                    | 0.074991725 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0148     |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1802249  |
| test/Q_plus_P                  | -1.1802249  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00657    |
| train/info_shaping_reward_mean | -0.0662     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 160        |
| stats_o/mean                   | 0.20510449 |
| stats_o/std                    | 0.07486735 |
| test/episodes                  | 1610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00828   |
| test/info_shaping_reward_mean  | -0.0629    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3800021 |
| test/Q_plus_P                  | -1.3800021 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 64400      |
| train/episodes                 | 6440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00429   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 257600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.20509304  |
| stats_o/std                    | 0.07473503  |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00669    |
| test/info_shaping_reward_mean  | -0.0518     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.98373324 |
| test/Q_plus_P                  | -0.98373324 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.58        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00615    |
| train/info_shaping_reward_mean | -0.0711     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 162        |
| stats_o/mean                   | 0.20510367 |
| stats_o/std                    | 0.07475063 |
| test/episodes                  | 1630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000277  |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.9707501 |
| test/Q_plus_P                  | -0.9707501 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 65200      |
| train/episodes                 | 6520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00467   |
| train/info_shaping_reward_mean | -0.0796    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 260800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 163        |
| stats_o/mean                   | 0.20509487 |
| stats_o/std                    | 0.07474985 |
| test/episodes                  | 1640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00825   |
| test/info_shaping_reward_mean  | -0.0581    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1895105 |
| test/Q_plus_P                  | -1.1895105 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 65600      |
| train/episodes                 | 6560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.555      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00757   |
| train/info_shaping_reward_mean | -0.0846    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 262400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 164        |
| stats_o/mean                   | 0.20509619 |
| stats_o/std                    | 0.07463379 |
| test/episodes                  | 1650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0126    |
| test/info_shaping_reward_mean  | -0.0587    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1660724 |
| test/Q_plus_P                  | -1.1660724 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 66000      |
| train/episodes                 | 6600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00567   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 264000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 165        |
| stats_o/mean                   | 0.20509748 |
| stats_o/std                    | 0.07455811 |
| test/episodes                  | 1660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0118    |
| test/info_shaping_reward_mean  | -0.0603    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1736919 |
| test/Q_plus_P                  | -1.1736919 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 66400      |
| train/episodes                 | 6640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00535   |
| train/info_shaping_reward_mean | -0.0782    |
| train/info_shaping_reward_min  | -0.274     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 265600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 166        |
| stats_o/mean                   | 0.2050845  |
| stats_o/std                    | 0.07458572 |
| test/episodes                  | 1670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0118    |
| test/info_shaping_reward_mean  | -0.0604    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0196452 |
| test/Q_plus_P                  | -1.0196452 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 66800      |
| train/episodes                 | 6680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00496   |
| train/info_shaping_reward_mean | -0.0915    |
| train/info_shaping_reward_min  | -0.283     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 267200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.2050654  |
| stats_o/std                    | 0.07455093 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0088    |
| test/info_shaping_reward_mean  | -0.0565    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8925161 |
| test/Q_plus_P                  | -0.8925161 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00541   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 168        |
| stats_o/mean                   | 0.20507145 |
| stats_o/std                    | 0.07456532 |
| test/episodes                  | 1690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0137    |
| test/info_shaping_reward_mean  | -0.0636    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0808462 |
| test/Q_plus_P                  | -1.0808462 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 67600      |
| train/episodes                 | 6760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0047    |
| train/info_shaping_reward_mean | -0.0819    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 270400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.20506829 |
| stats_o/std                    | 0.07447449 |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00767   |
| test/info_shaping_reward_mean  | -0.0592    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1370556 |
| test/Q_plus_P                  | -1.1370556 |
| test/reward_per_eps            | -10        |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00765   |
| train/info_shaping_reward_mean | -0.0741    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 170        |
| stats_o/mean                   | 0.20506307 |
| stats_o/std                    | 0.07440844 |
| test/episodes                  | 1710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0156    |
| test/info_shaping_reward_mean  | -0.0611    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0586972 |
| test/Q_plus_P                  | -1.0586972 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 68400      |
| train/episodes                 | 6840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.0696    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 273600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 171        |
| stats_o/mean                   | 0.20507404 |
| stats_o/std                    | 0.0743233  |
| test/episodes                  | 1720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0106    |
| test/info_shaping_reward_mean  | -0.0626    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0073915 |
| test/Q_plus_P                  | -1.0073915 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 68800      |
| train/episodes                 | 6880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00489   |
| train/info_shaping_reward_mean | -0.0787    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 275200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 172         |
| stats_o/mean                   | 0.20508571  |
| stats_o/std                    | 0.074265756 |
| test/episodes                  | 1730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.014      |
| test/info_shaping_reward_mean  | -0.0613     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0785508  |
| test/Q_plus_P                  | -1.0785508  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 69200       |
| train/episodes                 | 6920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00414    |
| train/info_shaping_reward_mean | -0.0699     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 276800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 173        |
| stats_o/mean                   | 0.20508724 |
| stats_o/std                    | 0.07423631 |
| test/episodes                  | 1740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0158    |
| test/info_shaping_reward_mean  | -0.0697    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2758766 |
| test/Q_plus_P                  | -1.2758766 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 69600      |
| train/episodes                 | 6960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00647   |
| train/info_shaping_reward_mean | -0.0756    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 278400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 174        |
| stats_o/mean                   | 0.20508894 |
| stats_o/std                    | 0.07413946 |
| test/episodes                  | 1750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0145    |
| test/info_shaping_reward_mean  | -0.0617    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1250434 |
| test/Q_plus_P                  | -1.1250434 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 70000      |
| train/episodes                 | 7000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00567   |
| train/info_shaping_reward_mean | -0.0746    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 280000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 175         |
| stats_o/mean                   | 0.2051045   |
| stats_o/std                    | 0.07406697  |
| test/episodes                  | 1760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.93874115 |
| test/Q_plus_P                  | -0.93874115 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 70400       |
| train/episodes                 | 7040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.62        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00395    |
| train/info_shaping_reward_mean | -0.0753     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 281600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 176        |
| stats_o/mean                   | 0.20506737 |
| stats_o/std                    | 0.07404125 |
| test/episodes                  | 1770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0124    |
| test/info_shaping_reward_mean  | -0.0597    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9528343 |
| test/Q_plus_P                  | -0.9528343 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 70800      |
| train/episodes                 | 7080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00458   |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 283200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.20504402  |
| stats_o/std                    | 0.073978364 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0164     |
| test/info_shaping_reward_mean  | -0.0618     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.0707498  |
| test/Q_plus_P                  | -1.0707498  |
| test/reward_per_eps            | -9          |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00748    |
| train/info_shaping_reward_mean | -0.0808     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.20505583 |
| stats_o/std                    | 0.07400894 |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0154    |
| test/info_shaping_reward_mean  | -0.0603    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9711885 |
| test/Q_plus_P                  | -0.9711885 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00483   |
| train/info_shaping_reward_mean | -0.0752    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 179        |
| stats_o/mean                   | 0.20502998 |
| stats_o/std                    | 0.07396101 |
| test/episodes                  | 1800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00929   |
| test/info_shaping_reward_mean  | -0.0557    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0008012 |
| test/Q_plus_P                  | -1.0008012 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 72000      |
| train/episodes                 | 7200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00526   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 288000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 180         |
| stats_o/mean                   | 0.20501934  |
| stats_o/std                    | 0.073890164 |
| test/episodes                  | 1810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0155     |
| test/info_shaping_reward_mean  | -0.0565     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.9912216  |
| test/Q_plus_P                  | -0.9912216  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 72400       |
| train/episodes                 | 7240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.559       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00467    |
| train/info_shaping_reward_mean | -0.0784     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 289600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 181        |
| stats_o/mean                   | 0.20500381 |
| stats_o/std                    | 0.07387643 |
| test/episodes                  | 1820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0135    |
| test/info_shaping_reward_mean  | -0.0602    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0904832 |
| test/Q_plus_P                  | -1.0904832 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 72800      |
| train/episodes                 | 7280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00514   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 291200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 182        |
| stats_o/mean                   | 0.20500055 |
| stats_o/std                    | 0.07383273 |
| test/episodes                  | 1830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00867   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.8929932 |
| test/Q_plus_P                  | -0.8929932 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 73200      |
| train/episodes                 | 7320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0681    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 292800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 183        |
| stats_o/mean                   | 0.20499493 |
| stats_o/std                    | 0.0737432  |
| test/episodes                  | 1840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0123    |
| test/info_shaping_reward_mean  | -0.0628    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1774979 |
| test/Q_plus_P                  | -1.1774979 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 73600      |
| train/episodes                 | 7360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.0656    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 294400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 184        |
| stats_o/mean                   | 0.20499864 |
| stats_o/std                    | 0.07365434 |
| test/episodes                  | 1850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0123    |
| test/info_shaping_reward_mean  | -0.068     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2542051 |
| test/Q_plus_P                  | -1.2542051 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 74000      |
| train/episodes                 | 7400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00593   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 296000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 185        |
| stats_o/mean                   | 0.20500505 |
| stats_o/std                    | 0.073654   |
| test/episodes                  | 1860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00886   |
| test/info_shaping_reward_mean  | -0.0574    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0308152 |
| test/Q_plus_P                  | -1.0308152 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 74400      |
| train/episodes                 | 7440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.572      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00504   |
| train/info_shaping_reward_mean | -0.0764    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 297600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 186        |
| stats_o/mean                   | 0.20500942 |
| stats_o/std                    | 0.07362988 |
| test/episodes                  | 1870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0222    |
| test/info_shaping_reward_mean  | -0.0575    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0581822 |
| test/Q_plus_P                  | -1.0581822 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 74800      |
| train/episodes                 | 7480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00446   |
| train/info_shaping_reward_mean | -0.0741    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 299200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.20502558 |
| stats_o/std                    | 0.0735998  |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0171    |
| test/info_shaping_reward_mean  | -0.0613    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1806642 |
| test/Q_plus_P                  | -1.1806642 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00538   |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 188         |
| stats_o/mean                   | 0.20502132  |
| stats_o/std                    | 0.07351186  |
| test/episodes                  | 1890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0096     |
| test/info_shaping_reward_mean  | -0.057      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.99514824 |
| test/Q_plus_P                  | -0.99514824 |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 75600       |
| train/episodes                 | 7560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.552       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00663    |
| train/info_shaping_reward_mean | -0.0769     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 302400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.20499413  |
| stats_o/std                    | 0.073494785 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0144     |
| test/info_shaping_reward_mean  | -0.0569     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1428231  |
| test/Q_plus_P                  | -1.1428231  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0713     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 190         |
| stats_o/mean                   | 0.20498432  |
| stats_o/std                    | 0.07338913  |
| test/episodes                  | 1910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.013      |
| test/info_shaping_reward_mean  | -0.059      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.92432135 |
| test/Q_plus_P                  | -0.92432135 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 76400       |
| train/episodes                 | 7640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00663    |
| train/info_shaping_reward_mean | -0.0711     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 305600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 191        |
| stats_o/mean                   | 0.20498346 |
| stats_o/std                    | 0.07326935 |
| test/episodes                  | 1920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.015     |
| test/info_shaping_reward_mean  | -0.0602    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0509446 |
| test/Q_plus_P                  | -1.0509446 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 76800      |
| train/episodes                 | 7680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00596   |
| train/info_shaping_reward_mean | -0.07      |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 307200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.20496908  |
| stats_o/std                    | 0.073200844 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0069     |
| test/info_shaping_reward_mean  | -0.0626     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0709621  |
| test/Q_plus_P                  | -1.0709621  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.589       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00748    |
| train/info_shaping_reward_mean | -0.0715     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 193         |
| stats_o/mean                   | 0.20496674  |
| stats_o/std                    | 0.073201664 |
| test/episodes                  | 1940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0112     |
| test/info_shaping_reward_mean  | -0.0577     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.9931591  |
| test/Q_plus_P                  | -0.9931591  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 77600       |
| train/episodes                 | 7760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.583       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00779    |
| train/info_shaping_reward_mean | -0.0844     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 310400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 194        |
| stats_o/mean                   | 0.20497297 |
| stats_o/std                    | 0.07314929 |
| test/episodes                  | 1950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0183    |
| test/info_shaping_reward_mean  | -0.0567    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0100381 |
| test/Q_plus_P                  | -1.0100381 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 78000      |
| train/episodes                 | 7800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00807   |
| train/info_shaping_reward_mean | -0.073     |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 312000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 195        |
| stats_o/mean                   | 0.20497392 |
| stats_o/std                    | 0.07305012 |
| test/episodes                  | 1960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0131    |
| test/info_shaping_reward_mean  | -0.0594    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0612571 |
| test/Q_plus_P                  | -1.0612571 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 78400      |
| train/episodes                 | 7840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00687   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 313600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 196        |
| stats_o/mean                   | 0.20497622 |
| stats_o/std                    | 0.07302514 |
| test/episodes                  | 1970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0146    |
| test/info_shaping_reward_mean  | -0.0586    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.119264  |
| test/Q_plus_P                  | -1.119264  |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 78800      |
| train/episodes                 | 7880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00609   |
| train/info_shaping_reward_mean | -0.0814    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 315200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 197        |
| stats_o/mean                   | 0.20497504 |
| stats_o/std                    | 0.07300301 |
| test/episodes                  | 1980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0208    |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9042893 |
| test/Q_plus_P                  | -0.9042893 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 79200      |
| train/episodes                 | 7920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0053    |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 316800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.2049843   |
| stats_o/std                    | 0.072948225 |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0132     |
| test/info_shaping_reward_mean  | -0.0628     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2906533  |
| test/Q_plus_P                  | -1.2906533  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00559    |
| train/info_shaping_reward_mean | -0.0716     |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 199         |
| stats_o/mean                   | 0.20498775  |
| stats_o/std                    | 0.072902344 |
| test/episodes                  | 2000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00761    |
| test/info_shaping_reward_mean  | -0.0631     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.272598   |
| test/Q_plus_P                  | -1.272598   |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 80000       |
| train/episodes                 | 8000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.547       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00593    |
| train/info_shaping_reward_mean | -0.0789     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 320000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 200        |
| stats_o/mean                   | 0.20498203 |
| stats_o/std                    | 0.07285447 |
| test/episodes                  | 2010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00886   |
| test/info_shaping_reward_mean  | -0.0622    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2207918 |
| test/Q_plus_P                  | -1.2207918 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 80400      |
| train/episodes                 | 8040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00568   |
| train/info_shaping_reward_mean | -0.0724    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 321600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 201         |
| stats_o/mean                   | 0.20500031  |
| stats_o/std                    | 0.07280907  |
| test/episodes                  | 2020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0174     |
| test/info_shaping_reward_mean  | -0.0588     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.94942987 |
| test/Q_plus_P                  | -0.94942987 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 80800       |
| train/episodes                 | 8080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00762    |
| train/info_shaping_reward_mean | -0.0752     |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 323200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 202        |
| stats_o/mean                   | 0.20501047 |
| stats_o/std                    | 0.07285835 |
| test/episodes                  | 2030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0165    |
| test/info_shaping_reward_mean  | -0.0607    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0997282 |
| test/Q_plus_P                  | -1.0997282 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 81200      |
| train/episodes                 | 8120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00577   |
| train/info_shaping_reward_mean | -0.0892    |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 324800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 203        |
| stats_o/mean                   | 0.20501728 |
| stats_o/std                    | 0.07277354 |
| test/episodes                  | 2040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0201    |
| test/info_shaping_reward_mean  | -0.0589    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9787589 |
| test/Q_plus_P                  | -0.9787589 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 81600      |
| train/episodes                 | 8160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00689   |
| train/info_shaping_reward_mean | -0.0676    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 326400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 204        |
| stats_o/mean                   | 0.20502555 |
| stats_o/std                    | 0.07270536 |
| test/episodes                  | 2050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0105    |
| test/info_shaping_reward_mean  | -0.0563    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0491087 |
| test/Q_plus_P                  | -1.0491087 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 82000      |
| train/episodes                 | 8200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00489   |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 328000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 205        |
| stats_o/mean                   | 0.20502724 |
| stats_o/std                    | 0.07262866 |
| test/episodes                  | 2060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0207    |
| test/info_shaping_reward_mean  | -0.0578    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.018919  |
| test/Q_plus_P                  | -1.018919  |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 82400      |
| train/episodes                 | 8240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00636   |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 329600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 206        |
| stats_o/mean                   | 0.20502347 |
| stats_o/std                    | 0.07259302 |
| test/episodes                  | 2070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00903   |
| test/info_shaping_reward_mean  | -0.0598    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2148011 |
| test/Q_plus_P                  | -1.2148011 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 82800      |
| train/episodes                 | 8280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00612   |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 331200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.20501935 |
| stats_o/std                    | 0.07258231 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0159    |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0078492 |
| test/Q_plus_P                  | -1.0078492 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00537   |
| train/info_shaping_reward_mean | -0.076     |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 208        |
| stats_o/mean                   | 0.20503056 |
| stats_o/std                    | 0.07250154 |
| test/episodes                  | 2090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00126   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.985753  |
| test/Q_plus_P                  | -0.985753  |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 83600      |
| train/episodes                 | 8360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00755   |
| train/info_shaping_reward_mean | -0.0684    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 334400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 209        |
| stats_o/mean                   | 0.20501879 |
| stats_o/std                    | 0.07245692 |
| test/episodes                  | 2100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0112    |
| test/info_shaping_reward_mean  | -0.0569    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0664763 |
| test/Q_plus_P                  | -1.0664763 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 84000      |
| train/episodes                 | 8400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00544   |
| train/info_shaping_reward_mean | -0.0749    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 336000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 210        |
| stats_o/mean                   | 0.20501302 |
| stats_o/std                    | 0.07234045 |
| test/episodes                  | 2110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0134    |
| test/info_shaping_reward_mean  | -0.0549    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0657318 |
| test/Q_plus_P                  | -1.0657318 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 84400      |
| train/episodes                 | 8440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00652   |
| train/info_shaping_reward_mean | -0.0633    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 337600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 211        |
| stats_o/mean                   | 0.20499277 |
| stats_o/std                    | 0.0722863  |
| test/episodes                  | 2120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00835   |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1754963 |
| test/Q_plus_P                  | -1.1754963 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 84800      |
| train/episodes                 | 8480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00648   |
| train/info_shaping_reward_mean | -0.0741    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 339200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 212         |
| stats_o/mean                   | 0.20497999  |
| stats_o/std                    | 0.07223463  |
| test/episodes                  | 2130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0146     |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.90932906 |
| test/Q_plus_P                  | -0.90932906 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 85200       |
| train/episodes                 | 8520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.557       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00555    |
| train/info_shaping_reward_mean | -0.0711     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 340800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 213        |
| stats_o/mean                   | 0.20498535 |
| stats_o/std                    | 0.0721957  |
| test/episodes                  | 2140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0119    |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9561898 |
| test/Q_plus_P                  | -0.9561898 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 85600      |
| train/episodes                 | 8560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00607   |
| train/info_shaping_reward_mean | -0.0825    |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 342400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 214        |
| stats_o/mean                   | 0.20499489 |
| stats_o/std                    | 0.07210538 |
| test/episodes                  | 2150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00541   |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0243584 |
| test/Q_plus_P                  | -1.0243584 |
| test/reward_per_eps            | -9         |
| test/steps                     | 86000      |
| train/episodes                 | 8600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00681   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 344000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 215        |
| stats_o/mean                   | 0.20500456 |
| stats_o/std                    | 0.07202119 |
| test/episodes                  | 2160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00953   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0693848 |
| test/Q_plus_P                  | -1.0693848 |
| test/reward_per_eps            | -9         |
| test/steps                     | 86400      |
| train/episodes                 | 8640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00516   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 345600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 216         |
| stats_o/mean                   | 0.20501196  |
| stats_o/std                    | 0.071979254 |
| test/episodes                  | 2170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0137     |
| test/info_shaping_reward_mean  | -0.0546     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0482337  |
| test/Q_plus_P                  | -1.0482337  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 86800       |
| train/episodes                 | 8680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.544       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00616    |
| train/info_shaping_reward_mean | -0.0756     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 347200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 217        |
| stats_o/mean                   | 0.20502177 |
| stats_o/std                    | 0.07197979 |
| test/episodes                  | 2180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0111    |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.8635295 |
| test/Q_plus_P                  | -0.8635295 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 87200      |
| train/episodes                 | 8720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.57       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00562   |
| train/info_shaping_reward_mean | -0.0771    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 348800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 218        |
| stats_o/mean                   | 0.20502222 |
| stats_o/std                    | 0.07194685 |
| test/episodes                  | 2190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0205    |
| test/info_shaping_reward_mean  | -0.0654    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0859075 |
| test/Q_plus_P                  | -1.0859075 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 87600      |
| train/episodes                 | 8760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00542   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 350400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.2050232   |
| stats_o/std                    | 0.07193129  |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00453    |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.83808196 |
| test/Q_plus_P                  | -0.83808196 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00587    |
| train/info_shaping_reward_mean | -0.0727     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 220        |
| stats_o/mean                   | 0.20503424 |
| stats_o/std                    | 0.0718587  |
| test/episodes                  | 2210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0163    |
| test/info_shaping_reward_mean  | -0.0585    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2412473 |
| test/Q_plus_P                  | -1.2412473 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 88400      |
| train/episodes                 | 8840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00542   |
| train/info_shaping_reward_mean | -0.0665    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 353600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 221        |
| stats_o/mean                   | 0.20504177 |
| stats_o/std                    | 0.07183621 |
| test/episodes                  | 2220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0119    |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0861852 |
| test/Q_plus_P                  | -1.0861852 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 88800      |
| train/episodes                 | 8880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00613   |
| train/info_shaping_reward_mean | -0.0757    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 355200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 222         |
| stats_o/mean                   | 0.205038    |
| stats_o/std                    | 0.071794145 |
| test/episodes                  | 2230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0101     |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0372897  |
| test/Q_plus_P                  | -1.0372897  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 89200       |
| train/episodes                 | 8920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00595    |
| train/info_shaping_reward_mean | -0.0721     |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 356800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 223         |
| stats_o/mean                   | 0.20502064  |
| stats_o/std                    | 0.071767084 |
| test/episodes                  | 2240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.015      |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -0.9900776  |
| test/Q_plus_P                  | -0.9900776  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 89600       |
| train/episodes                 | 8960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00591    |
| train/info_shaping_reward_mean | -0.069      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 358400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 224         |
| stats_o/mean                   | 0.20498925  |
| stats_o/std                    | 0.071803086 |
| test/episodes                  | 2250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0102     |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0761856  |
| test/Q_plus_P                  | -1.0761856  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 90000       |
| train/episodes                 | 9000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00684    |
| train/info_shaping_reward_mean | -0.0818     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 360000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 225        |
| stats_o/mean                   | 0.20500028 |
| stats_o/std                    | 0.07190368 |
| test/episodes                  | 2260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0132    |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.04509   |
| test/Q_plus_P                  | -1.04509   |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 90400      |
| train/episodes                 | 9040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.534      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00727   |
| train/info_shaping_reward_mean | -0.0846    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 361600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 226         |
| stats_o/mean                   | 0.20501524  |
| stats_o/std                    | 0.071863554 |
| test/episodes                  | 2270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.688       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00514    |
| test/info_shaping_reward_mean  | -0.0625     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.7177576  |
| test/Q_plus_P                  | -1.7177576  |
| test/reward_per_eps            | -12.5       |
| test/steps                     | 90800       |
| train/episodes                 | 9080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00574    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 363200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 227         |
| stats_o/mean                   | 0.20501778  |
| stats_o/std                    | 0.07179825  |
| test/episodes                  | 2280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.017      |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.96273154 |
| test/Q_plus_P                  | -0.96273154 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 91200       |
| train/episodes                 | 9120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.592       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00404    |
| train/info_shaping_reward_mean | -0.0772     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 364800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 228        |
| stats_o/mean                   | 0.20501703 |
| stats_o/std                    | 0.07178399 |
| test/episodes                  | 2290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00798   |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9620223 |
| test/Q_plus_P                  | -0.9620223 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 91600      |
| train/episodes                 | 9160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00652   |
| train/info_shaping_reward_mean | -0.0759    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 366400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 229        |
| stats_o/mean                   | 0.20501062 |
| stats_o/std                    | 0.07170547 |
| test/episodes                  | 2300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0134    |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1407369 |
| test/Q_plus_P                  | -1.1407369 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 92000      |
| train/episodes                 | 9200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.658      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00542   |
| train/info_shaping_reward_mean | -0.0622    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 368000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 230         |
| stats_o/mean                   | 0.20501873  |
| stats_o/std                    | 0.07161657  |
| test/episodes                  | 2310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0142     |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -0.90555483 |
| test/Q_plus_P                  | -0.90555483 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 92400       |
| train/episodes                 | 9240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00567    |
| train/info_shaping_reward_mean | -0.0642     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 369600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 231         |
| stats_o/mean                   | 0.20500189  |
| stats_o/std                    | 0.071596324 |
| test/episodes                  | 2320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0157     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.9000012  |
| test/Q_plus_P                  | -0.9000012  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 92800       |
| train/episodes                 | 9280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00504    |
| train/info_shaping_reward_mean | -0.0782     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 371200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 232        |
| stats_o/mean                   | 0.20500794 |
| stats_o/std                    | 0.07149726 |
| test/episodes                  | 2330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0131    |
| test/info_shaping_reward_mean  | -0.0614    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0921361 |
| test/Q_plus_P                  | -1.0921361 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 93200      |
| train/episodes                 | 9320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.63       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00614   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 372800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 233         |
| stats_o/mean                   | 0.20501326  |
| stats_o/std                    | 0.071526796 |
| test/episodes                  | 2340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0093     |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0301697  |
| test/Q_plus_P                  | -1.0301697  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 93600       |
| train/episodes                 | 9360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.571       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00721    |
| train/info_shaping_reward_mean | -0.0782     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 374400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.20502412 |
| stats_o/std                    | 0.0715235  |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0139    |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0217975 |
| test/Q_plus_P                  | -1.0217975 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0052    |
| train/info_shaping_reward_mean | -0.0811    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.20501257  |
| stats_o/std                    | 0.071596384 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0202     |
| test/info_shaping_reward_mean  | -0.0572     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0391917  |
| test/Q_plus_P                  | -1.0391917  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00677    |
| train/info_shaping_reward_mean | -0.0936     |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 236        |
| stats_o/mean                   | 0.20501453 |
| stats_o/std                    | 0.07156621 |
| test/episodes                  | 2370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00863   |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0947497 |
| test/Q_plus_P                  | -1.0947497 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 94800      |
| train/episodes                 | 9480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00649   |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 379200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 237         |
| stats_o/mean                   | 0.20501989  |
| stats_o/std                    | 0.071506746 |
| test/episodes                  | 2380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0122     |
| test/info_shaping_reward_mean  | -0.0663     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -2.2079325  |
| test/Q_plus_P                  | -2.2079325  |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 95200       |
| train/episodes                 | 9520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00727    |
| train/info_shaping_reward_mean | -0.071      |
| train/info_shaping_reward_min  | -0.219      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 380800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 238        |
| stats_o/mean                   | 0.20502083 |
| stats_o/std                    | 0.07147297 |
| test/episodes                  | 2390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0122    |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8170179 |
| test/Q_plus_P                  | -0.8170179 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 95600      |
| train/episodes                 | 9560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00673   |
| train/info_shaping_reward_mean | -0.0822    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 382400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 239         |
| stats_o/mean                   | 0.20500591  |
| stats_o/std                    | 0.07144134  |
| test/episodes                  | 2400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0138     |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.81373125 |
| test/Q_plus_P                  | -0.81373125 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 96000       |
| train/episodes                 | 9600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00742    |
| train/info_shaping_reward_mean | -0.0716     |
| train/info_shaping_reward_min  | -0.194      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 384000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 240        |
| stats_o/mean                   | 0.20499161 |
| stats_o/std                    | 0.07135432 |
| test/episodes                  | 2410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0188    |
| test/info_shaping_reward_mean  | -0.0619    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0511461 |
| test/Q_plus_P                  | -1.0511461 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 96400      |
| train/episodes                 | 9640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00537   |
| train/info_shaping_reward_mean | -0.0669    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 385600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 241         |
| stats_o/mean                   | 0.20499888  |
| stats_o/std                    | 0.071278445 |
| test/episodes                  | 2420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0105     |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.94705856 |
| test/Q_plus_P                  | -0.94705856 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 96800       |
| train/episodes                 | 9680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0052     |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 387200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 242        |
| stats_o/mean                   | 0.20498991 |
| stats_o/std                    | 0.0712319  |
| test/episodes                  | 2430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0186    |
| test/info_shaping_reward_mean  | -0.0597    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1111333 |
| test/Q_plus_P                  | -1.1111333 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 97200      |
| train/episodes                 | 9720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00582   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 388800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 243        |
| stats_o/mean                   | 0.20498818 |
| stats_o/std                    | 0.07116773 |
| test/episodes                  | 2440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0142    |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9596634 |
| test/Q_plus_P                  | -0.9596634 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 97600      |
| train/episodes                 | 9760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00599   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 390400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.20499031 |
| stats_o/std                    | 0.07107322 |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00243   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0724819 |
| test/Q_plus_P                  | -1.0724819 |
| test/reward_per_eps            | -9         |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0047    |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 245        |
| stats_o/mean                   | 0.20499025 |
| stats_o/std                    | 0.07108803 |
| test/episodes                  | 2460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00922   |
| test/info_shaping_reward_mean  | -0.0601    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.9394679 |
| test/Q_plus_P                  | -0.9394679 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 98400      |
| train/episodes                 | 9840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00555   |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 393600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 246        |
| stats_o/mean                   | 0.20498331 |
| stats_o/std                    | 0.07103763 |
| test/episodes                  | 2470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00377   |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.043163  |
| test/Q_plus_P                  | -1.043163  |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 98800      |
| train/episodes                 | 9880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00661   |
| train/info_shaping_reward_mean | -0.0669    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 395200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 247         |
| stats_o/mean                   | 0.20497322  |
| stats_o/std                    | 0.071024224 |
| test/episodes                  | 2480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0158     |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.0476854  |
| test/Q_plus_P                  | -1.0476854  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 99200       |
| train/episodes                 | 9920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0079     |
| train/info_shaping_reward_mean | -0.0713     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 396800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.20496702 |
| stats_o/std                    | 0.07099643 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0174    |
| test/info_shaping_reward_mean  | -0.0614    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9544925 |
| test/Q_plus_P                  | -0.9544925 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00464   |
| train/info_shaping_reward_mean | -0.0669    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 249         |
| stats_o/mean                   | 0.2049758   |
| stats_o/std                    | 0.070972845 |
| test/episodes                  | 2500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00645    |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.0440339  |
| test/Q_plus_P                  | -1.0440339  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 100000      |
| train/episodes                 | 10000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00809    |
| train/info_shaping_reward_mean | -0.0713     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 400000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 250        |
| stats_o/mean                   | 0.20498836 |
| stats_o/std                    | 0.07097029 |
| test/episodes                  | 2510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0137    |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0006884 |
| test/Q_plus_P                  | -1.0006884 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 100400     |
| train/episodes                 | 10040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00587   |
| train/info_shaping_reward_mean | -0.0808    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 401600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 251        |
| stats_o/mean                   | 0.2049856  |
| stats_o/std                    | 0.07092639 |
| test/episodes                  | 2520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0073    |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1197593 |
| test/Q_plus_P                  | -1.1197593 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 100800     |
| train/episodes                 | 10080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00664   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 403200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 252         |
| stats_o/mean                   | 0.20499998  |
| stats_o/std                    | 0.070911236 |
| test/episodes                  | 2530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.019      |
| test/info_shaping_reward_mean  | -0.0583     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -0.9382088  |
| test/Q_plus_P                  | -0.9382088  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 101200      |
| train/episodes                 | 10120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.567       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00723    |
| train/info_shaping_reward_mean | -0.074      |
| train/info_shaping_reward_min  | -0.199      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 404800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.20500319  |
| stats_o/std                    | 0.07087355  |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00984    |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.93937844 |
| test/Q_plus_P                  | -0.93937844 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00597    |
| train/info_shaping_reward_mean | -0.0639     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 254        |
| stats_o/mean                   | 0.20499898 |
| stats_o/std                    | 0.07079132 |
| test/episodes                  | 2550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00738   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.8956789 |
| test/Q_plus_P                  | -0.8956789 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 102000     |
| train/episodes                 | 10200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00692   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 408000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 255        |
| stats_o/mean                   | 0.20500924 |
| stats_o/std                    | 0.07082046 |
| test/episodes                  | 2560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00825   |
| test/info_shaping_reward_mean  | -0.0569    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1746632 |
| test/Q_plus_P                  | -1.1746632 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 102400     |
| train/episodes                 | 10240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00732   |
| train/info_shaping_reward_mean | -0.0948    |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 409600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 256         |
| stats_o/mean                   | 0.20500751  |
| stats_o/std                    | 0.070737496 |
| test/episodes                  | 2570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00363    |
| test/info_shaping_reward_mean  | -0.0475     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.99729097 |
| test/Q_plus_P                  | -0.99729097 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 102800      |
| train/episodes                 | 10280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00656    |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 411200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 257        |
| stats_o/mean                   | 0.20501532 |
| stats_o/std                    | 0.07073047 |
| test/episodes                  | 2580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0146    |
| test/info_shaping_reward_mean  | -0.0682    |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -2.0127046 |
| test/Q_plus_P                  | -2.0127046 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 103200     |
| train/episodes                 | 10320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.567      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00669   |
| train/info_shaping_reward_mean | -0.0762    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 412800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 258        |
| stats_o/mean                   | 0.20501284 |
| stats_o/std                    | 0.07075437 |
| test/episodes                  | 2590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0142    |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0250216 |
| test/Q_plus_P                  | -1.0250216 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 103600     |
| train/episodes                 | 10360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00654   |
| train/info_shaping_reward_mean | -0.083     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 414400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 259        |
| stats_o/mean                   | 0.20500389 |
| stats_o/std                    | 0.07073378 |
| test/episodes                  | 2600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0125    |
| test/info_shaping_reward_mean  | -0.0582    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3020118 |
| test/Q_plus_P                  | -1.3020118 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 104000     |
| train/episodes                 | 10400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00752   |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 416000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 260        |
| stats_o/mean                   | 0.20500165 |
| stats_o/std                    | 0.07066971 |
| test/episodes                  | 2610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00873   |
| test/info_shaping_reward_mean  | -0.0541    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1827148 |
| test/Q_plus_P                  | -1.1827148 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 104400     |
| train/episodes                 | 10440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0055    |
| train/info_shaping_reward_mean | -0.0739    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 417600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 261        |
| stats_o/mean                   | 0.20500585 |
| stats_o/std                    | 0.07059311 |
| test/episodes                  | 2620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00429   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2014165 |
| test/Q_plus_P                  | -1.2014165 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 104800     |
| train/episodes                 | 10480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0057    |
| train/info_shaping_reward_mean | -0.0664    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 419200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 262        |
| stats_o/mean                   | 0.20499967 |
| stats_o/std                    | 0.07052321 |
| test/episodes                  | 2630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00686   |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3573809 |
| test/Q_plus_P                  | -1.3573809 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 105200     |
| train/episodes                 | 10520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.652      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00786   |
| train/info_shaping_reward_mean | -0.0645    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 420800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.20499296  |
| stats_o/std                    | 0.07048136  |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00548    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.97446024 |
| test/Q_plus_P                  | -0.97446024 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00341    |
| train/info_shaping_reward_mean | -0.0742     |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 264        |
| stats_o/mean                   | 0.2049616  |
| stats_o/std                    | 0.07058328 |
| test/episodes                  | 2650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00948   |
| test/info_shaping_reward_mean  | -0.0541    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0389456 |
| test/Q_plus_P                  | -1.0389456 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 106000     |
| train/episodes                 | 10600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.528      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0073    |
| train/info_shaping_reward_mean | -0.0961    |
| train/info_shaping_reward_min  | -0.301     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 424000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 265         |
| stats_o/mean                   | 0.20496322  |
| stats_o/std                    | 0.070513204 |
| test/episodes                  | 2660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00839    |
| test/info_shaping_reward_mean  | -0.0574     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.1629292  |
| test/Q_plus_P                  | -1.1629292  |
| test/reward_per_eps            | -10         |
| test/steps                     | 106400      |
| train/episodes                 | 10640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00455    |
| train/info_shaping_reward_mean | -0.0655     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 425600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 266        |
| stats_o/mean                   | 0.20497157 |
| stats_o/std                    | 0.07047435 |
| test/episodes                  | 2670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00356   |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2466111 |
| test/Q_plus_P                  | -1.2466111 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 106800     |
| train/episodes                 | 10680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00753   |
| train/info_shaping_reward_mean | -0.0708    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 427200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 267        |
| stats_o/mean                   | 0.20496838 |
| stats_o/std                    | 0.0704135  |
| test/episodes                  | 2680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00576   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1458368 |
| test/Q_plus_P                  | -1.1458368 |
| test/reward_per_eps            | -9         |
| test/steps                     | 107200     |
| train/episodes                 | 10720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00586   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 428800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.20497712  |
| stats_o/std                    | 0.070409834 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00685    |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.101456   |
| test/Q_plus_P                  | -1.101456   |
| test/reward_per_eps            | -9          |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00477    |
| train/info_shaping_reward_mean | -0.0871     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 269        |
| stats_o/mean                   | 0.2049747  |
| stats_o/std                    | 0.07035414 |
| test/episodes                  | 2700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00506   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9067261 |
| test/Q_plus_P                  | -0.9067261 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 108000     |
| train/episodes                 | 10800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00674   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 432000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.20497881  |
| stats_o/std                    | 0.070352815 |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00772    |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1077778  |
| test/Q_plus_P                  | -1.1077778  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00556    |
| train/info_shaping_reward_mean | -0.084      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 271        |
| stats_o/mean                   | 0.20497382 |
| stats_o/std                    | 0.07032855 |
| test/episodes                  | 2720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00473   |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.1562859 |
| test/Q_plus_P                  | -1.1562859 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 108800     |
| train/episodes                 | 10880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00653   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 435200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 272        |
| stats_o/mean                   | 0.20497552 |
| stats_o/std                    | 0.07032383 |
| test/episodes                  | 2730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00772   |
| test/info_shaping_reward_mean  | -0.0613    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2613472 |
| test/Q_plus_P                  | -1.2613472 |
| test/reward_per_eps            | -11        |
| test/steps                     | 109200     |
| train/episodes                 | 10920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00691   |
| train/info_shaping_reward_mean | -0.0781    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 436800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 273        |
| stats_o/mean                   | 0.20498206 |
| stats_o/std                    | 0.07023686 |
| test/episodes                  | 2740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0171    |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9720845 |
| test/Q_plus_P                  | -0.9720845 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 109600     |
| train/episodes                 | 10960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.688      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00523   |
| train/info_shaping_reward_mean | -0.0599    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.5      |
| train/steps                    | 438400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 274         |
| stats_o/mean                   | 0.20496689  |
| stats_o/std                    | 0.070193656 |
| test/episodes                  | 2750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00783    |
| test/info_shaping_reward_mean  | -0.0575     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1874558  |
| test/Q_plus_P                  | -1.1874558  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 110000      |
| train/episodes                 | 11000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0059     |
| train/info_shaping_reward_mean | -0.0712     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 440000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 275        |
| stats_o/mean                   | 0.2049564  |
| stats_o/std                    | 0.07012337 |
| test/episodes                  | 2760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00959   |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1033051 |
| test/Q_plus_P                  | -1.1033051 |
| test/reward_per_eps            | -9         |
| test/steps                     | 110400     |
| train/episodes                 | 11040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0075    |
| train/info_shaping_reward_mean | -0.062     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 441600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 276         |
| stats_o/mean                   | 0.20495687  |
| stats_o/std                    | 0.070128515 |
| test/episodes                  | 2770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00559    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0203143  |
| test/Q_plus_P                  | -1.0203143  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 110800      |
| train/episodes                 | 11080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00559    |
| train/info_shaping_reward_mean | -0.0743     |
| train/info_shaping_reward_min  | -0.194      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 443200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 277        |
| stats_o/mean                   | 0.20495638 |
| stats_o/std                    | 0.07004888 |
| test/episodes                  | 2780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0112    |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9722477 |
| test/Q_plus_P                  | -0.9722477 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 111200     |
| train/episodes                 | 11120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00679   |
| train/info_shaping_reward_mean | -0.0665    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 444800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 278         |
| stats_o/mean                   | 0.2049434   |
| stats_o/std                    | 0.070008546 |
| test/episodes                  | 2790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0103     |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.89160705 |
| test/Q_plus_P                  | -0.89160705 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 111600      |
| train/episodes                 | 11160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00524    |
| train/info_shaping_reward_mean | -0.0682     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 446400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 279        |
| stats_o/mean                   | 0.20495069 |
| stats_o/std                    | 0.06994239 |
| test/episodes                  | 2800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00831   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9012361 |
| test/Q_plus_P                  | -0.9012361 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 112000     |
| train/episodes                 | 11200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00566   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 448000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 280        |
| stats_o/mean                   | 0.20495304 |
| stats_o/std                    | 0.06992974 |
| test/episodes                  | 2810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0059    |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.264343  |
| test/Q_plus_P                  | -1.264343  |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 112400     |
| train/episodes                 | 11240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00543   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 449600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.20495826 |
| stats_o/std                    | 0.06988159 |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00992   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0036175 |
| test/Q_plus_P                  | -1.0036175 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00664   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.20494889 |
| stats_o/std                    | 0.06990105 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0177    |
| test/info_shaping_reward_mean  | -0.0582    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9809192 |
| test/Q_plus_P                  | -0.9809192 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00958   |
| train/info_shaping_reward_mean | -0.0792    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.20495178 |
| stats_o/std                    | 0.06987083 |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0171    |
| test/info_shaping_reward_mean  | -0.0568    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0923606 |
| test/Q_plus_P                  | -1.0923606 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.595      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00545   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 284        |
| stats_o/mean                   | 0.20495781 |
| stats_o/std                    | 0.06978955 |
| test/episodes                  | 2850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0122    |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0618516 |
| test/Q_plus_P                  | -1.0618516 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 114000     |
| train/episodes                 | 11400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00566   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 456000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 285        |
| stats_o/mean                   | 0.20495284 |
| stats_o/std                    | 0.06972063 |
| test/episodes                  | 2860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0145    |
| test/info_shaping_reward_mean  | -0.0565    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9701532 |
| test/Q_plus_P                  | -0.9701532 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 114400     |
| train/episodes                 | 11440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00687   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 457600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 286        |
| stats_o/mean                   | 0.20494808 |
| stats_o/std                    | 0.06965713 |
| test/episodes                  | 2870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0207    |
| test/info_shaping_reward_mean  | -0.0596    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9563952 |
| test/Q_plus_P                  | -0.9563952 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 114800     |
| train/episodes                 | 11480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0083    |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 459200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.20496015 |
| stats_o/std                    | 0.0695933  |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0165    |
| test/info_shaping_reward_mean  | -0.0587    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1396464 |
| test/Q_plus_P                  | -1.1396464 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.661      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00582   |
| train/info_shaping_reward_mean | -0.0631    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 288        |
| stats_o/mean                   | 0.2049601  |
| stats_o/std                    | 0.06952526 |
| test/episodes                  | 2890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0111    |
| test/info_shaping_reward_mean  | -0.0549    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.8373136 |
| test/Q_plus_P                  | -0.8373136 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 115600     |
| train/episodes                 | 11560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00563   |
| train/info_shaping_reward_mean | -0.0658    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 462400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 289        |
| stats_o/mean                   | 0.20495258 |
| stats_o/std                    | 0.06949811 |
| test/episodes                  | 2900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0236    |
| test/info_shaping_reward_mean  | -0.0589    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0106399 |
| test/Q_plus_P                  | -1.0106399 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 116000     |
| train/episodes                 | 11600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00682   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 464000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 290        |
| stats_o/mean                   | 0.2049542  |
| stats_o/std                    | 0.06943132 |
| test/episodes                  | 2910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0103    |
| test/info_shaping_reward_mean  | -0.0564    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0169129 |
| test/Q_plus_P                  | -1.0169129 |
| test/reward_per_eps            | -9         |
| test/steps                     | 116400     |
| train/episodes                 | 11640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00738   |
| train/info_shaping_reward_mean | -0.0666    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 465600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 291        |
| stats_o/mean                   | 0.20494545 |
| stats_o/std                    | 0.06938701 |
| test/episodes                  | 2920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0162    |
| test/info_shaping_reward_mean  | -0.0623    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0059184 |
| test/Q_plus_P                  | -1.0059184 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 116800     |
| train/episodes                 | 11680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00546   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 467200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 292         |
| stats_o/mean                   | 0.20492902  |
| stats_o/std                    | 0.06937219  |
| test/episodes                  | 2930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0146     |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -0.85073084 |
| test/Q_plus_P                  | -0.85073084 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 117200      |
| train/episodes                 | 11720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00753    |
| train/info_shaping_reward_mean | -0.0787     |
| train/info_shaping_reward_min  | -0.199      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 468800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.2049192   |
| stats_o/std                    | 0.069364354 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00613    |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0392271  |
| test/Q_plus_P                  | -1.0392271  |
| test/reward_per_eps            | -9          |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00597    |
| train/info_shaping_reward_mean | -0.0659     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.20491813  |
| stats_o/std                    | 0.069321334 |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0139     |
| test/info_shaping_reward_mean  | -0.0629     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2596171  |
| test/Q_plus_P                  | -1.2596171  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00776    |
| train/info_shaping_reward_mean | -0.0671     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 295        |
| stats_o/mean                   | 0.20491718 |
| stats_o/std                    | 0.06924928 |
| test/episodes                  | 2960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.011     |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0201744 |
| test/Q_plus_P                  | -1.0201744 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 118400     |
| train/episodes                 | 11840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00426   |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 473600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 296         |
| stats_o/mean                   | 0.20490576  |
| stats_o/std                    | 0.069275595 |
| test/episodes                  | 2970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0112     |
| test/info_shaping_reward_mean  | -0.0555     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.8746627  |
| test/Q_plus_P                  | -0.8746627  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 118800      |
| train/episodes                 | 11880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.558       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00695    |
| train/info_shaping_reward_mean | -0.0829     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 475200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 297        |
| stats_o/mean                   | 0.20490976 |
| stats_o/std                    | 0.06923451 |
| test/episodes                  | 2980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0114    |
| test/info_shaping_reward_mean  | -0.0602    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0203485 |
| test/Q_plus_P                  | -1.0203485 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 119200     |
| train/episodes                 | 11920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00519   |
| train/info_shaping_reward_mean | -0.0674    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 476800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 298         |
| stats_o/mean                   | 0.20490324  |
| stats_o/std                    | 0.069198415 |
| test/episodes                  | 2990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0185     |
| test/info_shaping_reward_mean  | -0.0579     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.92657936 |
| test/Q_plus_P                  | -0.92657936 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 119600      |
| train/episodes                 | 11960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00905    |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.205      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 478400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 299        |
| stats_o/mean                   | 0.2049123  |
| stats_o/std                    | 0.06917687 |
| test/episodes                  | 3000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0147    |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9305607 |
| test/Q_plus_P                  | -0.9305607 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 120000     |
| train/episodes                 | 12000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00799   |
| train/info_shaping_reward_mean | -0.075     |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 480000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 300        |
| stats_o/mean                   | 0.2049127  |
| stats_o/std                    | 0.06914696 |
| test/episodes                  | 3010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0132    |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.9266189 |
| test/Q_plus_P                  | -0.9266189 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 120400     |
| train/episodes                 | 12040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00582   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 481600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.2049015  |
| stats_o/std                    | 0.069131   |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0115    |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0204158 |
| test/Q_plus_P                  | -1.0204158 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00645   |
| train/info_shaping_reward_mean | -0.0662    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 302         |
| stats_o/mean                   | 0.20489511  |
| stats_o/std                    | 0.069072455 |
| test/episodes                  | 3030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0161     |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.8702422  |
| test/Q_plus_P                  | -0.8702422  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 121200      |
| train/episodes                 | 12120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00428    |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.205      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 484800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 303        |
| stats_o/mean                   | 0.2048957  |
| stats_o/std                    | 0.06902789 |
| test/episodes                  | 3040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0115    |
| test/info_shaping_reward_mean  | -0.0589    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.095095  |
| test/Q_plus_P                  | -1.095095  |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 121600     |
| train/episodes                 | 12160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 486400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 304        |
| stats_o/mean                   | 0.20490904 |
| stats_o/std                    | 0.06899285 |
| test/episodes                  | 3050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00685   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.9517243 |
| test/Q_plus_P                  | -0.9517243 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 122000     |
| train/episodes                 | 12200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00617   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 488000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 305         |
| stats_o/mean                   | 0.20491429  |
| stats_o/std                    | 0.06895139  |
| test/episodes                  | 3060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0104     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.86135894 |
| test/Q_plus_P                  | -0.86135894 |
| test/reward_per_eps            | -8          |
| test/steps                     | 122400      |
| train/episodes                 | 12240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0082     |
| train/info_shaping_reward_mean | -0.071      |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 489600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 306         |
| stats_o/mean                   | 0.20490931  |
| stats_o/std                    | 0.068882495 |
| test/episodes                  | 3070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0101     |
| test/info_shaping_reward_mean  | -0.0597     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.308145   |
| test/Q_plus_P                  | -1.308145   |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 122800      |
| train/episodes                 | 12280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00569    |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 491200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.20490675 |
| stats_o/std                    | 0.06886503 |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0177    |
| test/info_shaping_reward_mean  | -0.0607    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2112869 |
| test/Q_plus_P                  | -1.2112869 |
| test/reward_per_eps            | -10        |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00571   |
| train/info_shaping_reward_mean | -0.075     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 308         |
| stats_o/mean                   | 0.20491321  |
| stats_o/std                    | 0.068838954 |
| test/episodes                  | 3090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0121     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0487709  |
| test/Q_plus_P                  | -1.0487709  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 123600      |
| train/episodes                 | 12360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.615       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00553    |
| train/info_shaping_reward_mean | -0.0749     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 494400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 309         |
| stats_o/mean                   | 0.20492184  |
| stats_o/std                    | 0.068872586 |
| test/episodes                  | 3100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0113     |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.974933   |
| test/Q_plus_P                  | -0.974933   |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 124000      |
| train/episodes                 | 12400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00457    |
| train/info_shaping_reward_mean | -0.0792     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 496000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.20491625  |
| stats_o/std                    | 0.068933815 |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.95574576 |
| test/Q_plus_P                  | -0.95574576 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00512    |
| train/info_shaping_reward_mean | -0.0766     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.20493244 |
| stats_o/std                    | 0.06889685 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00801   |
| test/info_shaping_reward_mean  | -0.0582    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1715851 |
| test/Q_plus_P                  | -1.1715851 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0066    |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 312         |
| stats_o/mean                   | 0.20493032  |
| stats_o/std                    | 0.0688419   |
| test/episodes                  | 3130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00849    |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.97864884 |
| test/Q_plus_P                  | -0.97864884 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 125200      |
| train/episodes                 | 12520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00566    |
| train/info_shaping_reward_mean | -0.0664     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 500800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 313        |
| stats_o/mean                   | 0.20492081 |
| stats_o/std                    | 0.06894033 |
| test/episodes                  | 3140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.812      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0135    |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8159136 |
| test/Q_plus_P                  | -0.8159136 |
| test/reward_per_eps            | -7.5       |
| test/steps                     | 125600     |
| train/episodes                 | 12560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00654   |
| train/info_shaping_reward_mean | -0.0861    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 502400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 314        |
| stats_o/mean                   | 0.20493355 |
| stats_o/std                    | 0.06893411 |
| test/episodes                  | 3150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0105    |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.9459725 |
| test/Q_plus_P                  | -0.9459725 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 126000     |
| train/episodes                 | 12600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00787   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 504000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 315         |
| stats_o/mean                   | 0.2049312   |
| stats_o/std                    | 0.068901755 |
| test/episodes                  | 3160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0112     |
| test/info_shaping_reward_mean  | -0.0625     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0105036  |
| test/Q_plus_P                  | -1.0105036  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 126400      |
| train/episodes                 | 12640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00814    |
| train/info_shaping_reward_mean | -0.0763     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 505600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 316        |
| stats_o/mean                   | 0.20492554 |
| stats_o/std                    | 0.06888235 |
| test/episodes                  | 3170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0144    |
| test/info_shaping_reward_mean  | -0.062     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0564013 |
| test/Q_plus_P                  | -1.0564013 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 126800     |
| train/episodes                 | 12680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00709   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 507200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 317        |
| stats_o/mean                   | 0.20493434 |
| stats_o/std                    | 0.06883181 |
| test/episodes                  | 3180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0104    |
| test/info_shaping_reward_mean  | -0.0597    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1662068 |
| test/Q_plus_P                  | -1.1662068 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 127200     |
| train/episodes                 | 12720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00604   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 508800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 318        |
| stats_o/mean                   | 0.2049301  |
| stats_o/std                    | 0.06882345 |
| test/episodes                  | 3190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0192    |
| test/info_shaping_reward_mean  | -0.0567    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8235611 |
| test/Q_plus_P                  | -0.8235611 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 127600     |
| train/episodes                 | 12760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00595   |
| train/info_shaping_reward_mean | -0.0691    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 510400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 319         |
| stats_o/mean                   | 0.2049314   |
| stats_o/std                    | 0.068837315 |
| test/episodes                  | 3200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0149     |
| test/info_shaping_reward_mean  | -0.0592     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.9005475  |
| test/Q_plus_P                  | -0.9005475  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 128000      |
| train/episodes                 | 12800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.561       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00603    |
| train/info_shaping_reward_mean | -0.084      |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 512000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 320        |
| stats_o/mean                   | 0.20492716 |
| stats_o/std                    | 0.06884243 |
| test/episodes                  | 3210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0134    |
| test/info_shaping_reward_mean  | -0.0547    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.8387937 |
| test/Q_plus_P                  | -0.8387937 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 128400     |
| train/episodes                 | 12840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00737   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 513600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.2049242   |
| stats_o/std                    | 0.0688057   |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0141     |
| test/info_shaping_reward_mean  | -0.0564     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.91279894 |
| test/Q_plus_P                  | -0.91279894 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00615    |
| train/info_shaping_reward_mean | -0.0689     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 322         |
| stats_o/mean                   | 0.20493406  |
| stats_o/std                    | 0.06883188  |
| test/episodes                  | 3230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0105     |
| test/info_shaping_reward_mean  | -0.0559     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.96709734 |
| test/Q_plus_P                  | -0.96709734 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 129200      |
| train/episodes                 | 12920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00606    |
| train/info_shaping_reward_mean | -0.0773     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 516800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 323        |
| stats_o/mean                   | 0.20493583 |
| stats_o/std                    | 0.0688248  |
| test/episodes                  | 3240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0135    |
| test/info_shaping_reward_mean  | -0.0577    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.013712  |
| test/Q_plus_P                  | -1.013712  |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 129600     |
| train/episodes                 | 12960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00719   |
| train/info_shaping_reward_mean | -0.0794    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 518400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 324        |
| stats_o/mean                   | 0.20494007 |
| stats_o/std                    | 0.06881353 |
| test/episodes                  | 3250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0165    |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.8539423 |
| test/Q_plus_P                  | -0.8539423 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 130000     |
| train/episodes                 | 13000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00601   |
| train/info_shaping_reward_mean | -0.0639    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 520000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 325         |
| stats_o/mean                   | 0.20493771  |
| stats_o/std                    | 0.068793006 |
| test/episodes                  | 3260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0165     |
| test/info_shaping_reward_mean  | -0.0576     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.9713304  |
| test/Q_plus_P                  | -0.9713304  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 130400      |
| train/episodes                 | 13040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00515    |
| train/info_shaping_reward_mean | -0.0681     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 521600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.20494647  |
| stats_o/std                    | 0.068744056 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00719    |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.98025674 |
| test/Q_plus_P                  | -0.98025674 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00719    |
| train/info_shaping_reward_mean | -0.0656     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 327        |
| stats_o/mean                   | 0.20493834 |
| stats_o/std                    | 0.06871191 |
| test/episodes                  | 3280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00974   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1703335 |
| test/Q_plus_P                  | -1.1703335 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 131200     |
| train/episodes                 | 13120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00542   |
| train/info_shaping_reward_mean | -0.0713    |
| train/info_shaping_reward_min  | -0.276     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 524800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 328        |
| stats_o/mean                   | 0.20492895 |
| stats_o/std                    | 0.06873025 |
| test/episodes                  | 3290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0115    |
| test/info_shaping_reward_mean  | -0.0547    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0331734 |
| test/Q_plus_P                  | -1.0331734 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 131600     |
| train/episodes                 | 13160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0064    |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 526400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 329        |
| stats_o/mean                   | 0.20492844 |
| stats_o/std                    | 0.06869237 |
| test/episodes                  | 3300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00787   |
| test/info_shaping_reward_mean  | -0.0582    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.014545  |
| test/Q_plus_P                  | -1.014545  |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 132000     |
| train/episodes                 | 13200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.678      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00643   |
| train/info_shaping_reward_mean | -0.0625    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.9      |
| train/steps                    | 528000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.20492832  |
| stats_o/std                    | 0.06868802  |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00886    |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.79672694 |
| test/Q_plus_P                  | -0.79672694 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00482    |
| train/info_shaping_reward_mean | -0.0653     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 331        |
| stats_o/mean                   | 0.20493037 |
| stats_o/std                    | 0.06863401 |
| test/episodes                  | 3320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0165    |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.1376266 |
| test/Q_plus_P                  | -1.1376266 |
| test/reward_per_eps            | -9         |
| test/steps                     | 132800     |
| train/episodes                 | 13280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00637   |
| train/info_shaping_reward_mean | -0.0639    |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 531200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 332        |
| stats_o/mean                   | 0.20492476 |
| stats_o/std                    | 0.06864448 |
| test/episodes                  | 3330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0141    |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9598909 |
| test/Q_plus_P                  | -0.9598909 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 133200     |
| train/episodes                 | 13320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00935   |
| train/info_shaping_reward_mean | -0.0822    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 532800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 333        |
| stats_o/mean                   | 0.20493181 |
| stats_o/std                    | 0.06870318 |
| test/episodes                  | 3340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0107    |
| test/info_shaping_reward_mean  | -0.0549    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.017187  |
| test/Q_plus_P                  | -1.017187  |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 133600     |
| train/episodes                 | 13360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00607   |
| train/info_shaping_reward_mean | -0.0818    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 534400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 334         |
| stats_o/mean                   | 0.20493929  |
| stats_o/std                    | 0.0686431   |
| test/episodes                  | 3350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00169    |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.99459267 |
| test/Q_plus_P                  | -0.99459267 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 134000      |
| train/episodes                 | 13400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.635       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00649    |
| train/info_shaping_reward_mean | -0.067      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 536000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.20494086 |
| stats_o/std                    | 0.06862331 |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00898   |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1145942 |
| test/Q_plus_P                  | -1.1145942 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.655      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00687   |
| train/info_shaping_reward_mean | -0.0726    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 336        |
| stats_o/mean                   | 0.2049363  |
| stats_o/std                    | 0.06862059 |
| test/episodes                  | 3370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0119    |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0069022 |
| test/Q_plus_P                  | -1.0069022 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 134800     |
| train/episodes                 | 13480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00747   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 539200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.20493998  |
| stats_o/std                    | 0.06862638  |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00675    |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -0.91675264 |
| test/Q_plus_P                  | -0.91675264 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00689    |
| train/info_shaping_reward_mean | -0.0742     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.20494883 |
| stats_o/std                    | 0.06859317 |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0113    |
| test/info_shaping_reward_mean  | -0.0613    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.048784  |
| test/Q_plus_P                  | -1.048784  |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00575   |
| train/info_shaping_reward_mean | -0.0771    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.20494732  |
| stats_o/std                    | 0.068548806 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.015      |
| test/info_shaping_reward_mean  | -0.0546     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.9108457  |
| test/Q_plus_P                  | -0.9108457  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00549    |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 340         |
| stats_o/mean                   | 0.2049408   |
| stats_o/std                    | 0.06850178  |
| test/episodes                  | 3410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0179     |
| test/info_shaping_reward_mean  | -0.0579     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.94694716 |
| test/Q_plus_P                  | -0.94694716 |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 136400      |
| train/episodes                 | 13640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00696    |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 545600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 341         |
| stats_o/mean                   | 0.20493907  |
| stats_o/std                    | 0.06851131  |
| test/episodes                  | 3420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0146     |
| test/info_shaping_reward_mean  | -0.0601     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.97525513 |
| test/Q_plus_P                  | -0.97525513 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 136800      |
| train/episodes                 | 13680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00978    |
| train/info_shaping_reward_mean | -0.0703     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 547200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 342        |
| stats_o/mean                   | 0.20493214 |
| stats_o/std                    | 0.06849401 |
| test/episodes                  | 3430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00406   |
| test/info_shaping_reward_mean  | -0.0604    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0825821 |
| test/Q_plus_P                  | -1.0825821 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 137200     |
| train/episodes                 | 13720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00867   |
| train/info_shaping_reward_mean | -0.0863    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 548800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 343         |
| stats_o/mean                   | 0.20493834  |
| stats_o/std                    | 0.06854778  |
| test/episodes                  | 3440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0109     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.92784905 |
| test/Q_plus_P                  | -0.92784905 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 137600      |
| train/episodes                 | 13760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00816    |
| train/info_shaping_reward_mean | -0.0721     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 550400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 344         |
| stats_o/mean                   | 0.2049465   |
| stats_o/std                    | 0.06853674  |
| test/episodes                  | 3450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.017      |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.88087976 |
| test/Q_plus_P                  | -0.88087976 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 138000      |
| train/episodes                 | 13800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0049     |
| train/info_shaping_reward_mean | -0.0731     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 552000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 345        |
| stats_o/mean                   | 0.2049386  |
| stats_o/std                    | 0.06854854 |
| test/episodes                  | 3460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00762   |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9283896 |
| test/Q_plus_P                  | -0.9283896 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 138400     |
| train/episodes                 | 13840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00717   |
| train/info_shaping_reward_mean | -0.072     |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 553600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 346         |
| stats_o/mean                   | 0.20493576  |
| stats_o/std                    | 0.06850034  |
| test/episodes                  | 3470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0216     |
| test/info_shaping_reward_mean  | -0.0607     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.86414754 |
| test/Q_plus_P                  | -0.86414754 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 138800      |
| train/episodes                 | 13880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00699    |
| train/info_shaping_reward_mean | -0.0719     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 555200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 347         |
| stats_o/mean                   | 0.20492731  |
| stats_o/std                    | 0.068496525 |
| test/episodes                  | 3480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00966    |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.90590906 |
| test/Q_plus_P                  | -0.90590906 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 139200      |
| train/episodes                 | 13920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.56        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00614    |
| train/info_shaping_reward_mean | -0.078      |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 556800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 348        |
| stats_o/mean                   | 0.20492685 |
| stats_o/std                    | 0.06852141 |
| test/episodes                  | 3490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00959   |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0575606 |
| test/Q_plus_P                  | -1.0575606 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 139600     |
| train/episodes                 | 13960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00802   |
| train/info_shaping_reward_mean | -0.0806    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 558400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 349        |
| stats_o/mean                   | 0.20493098 |
| stats_o/std                    | 0.068535   |
| test/episodes                  | 3500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0167    |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0472468 |
| test/Q_plus_P                  | -1.0472468 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 140000     |
| train/episodes                 | 14000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00619   |
| train/info_shaping_reward_mean | -0.0805    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 560000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 350        |
| stats_o/mean                   | 0.2049455  |
| stats_o/std                    | 0.06857101 |
| test/episodes                  | 3510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00284   |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9302604 |
| test/Q_plus_P                  | -0.9302604 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 140400     |
| train/episodes                 | 14040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.582      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00657   |
| train/info_shaping_reward_mean | -0.0833    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 561600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 351        |
| stats_o/mean                   | 0.20492336 |
| stats_o/std                    | 0.06856719 |
| test/episodes                  | 3520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00844   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9498785 |
| test/Q_plus_P                  | -0.9498785 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 140800     |
| train/episodes                 | 14080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00597   |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 563200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 352        |
| stats_o/mean                   | 0.2049218  |
| stats_o/std                    | 0.06854938 |
| test/episodes                  | 3530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0166    |
| test/info_shaping_reward_mean  | -0.0628    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1607234 |
| test/Q_plus_P                  | -1.1607234 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 141200     |
| train/episodes                 | 14120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0064    |
| train/info_shaping_reward_mean | -0.0798    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 564800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 353         |
| stats_o/mean                   | 0.20493385  |
| stats_o/std                    | 0.068528876 |
| test/episodes                  | 3540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0159     |
| test/info_shaping_reward_mean  | -0.069      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3486134  |
| test/Q_plus_P                  | -1.3486134  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 141600      |
| train/episodes                 | 14160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00746    |
| train/info_shaping_reward_mean | -0.0775     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 566400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 354        |
| stats_o/mean                   | 0.20493302 |
| stats_o/std                    | 0.06854967 |
| test/episodes                  | 3550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0104    |
| test/info_shaping_reward_mean  | -0.0623    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1233051 |
| test/Q_plus_P                  | -1.1233051 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 142000     |
| train/episodes                 | 14200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.563      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0064    |
| train/info_shaping_reward_mean | -0.0777    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 568000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 355        |
| stats_o/mean                   | 0.20493086 |
| stats_o/std                    | 0.06851764 |
| test/episodes                  | 3560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00486   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9387304 |
| test/Q_plus_P                  | -0.9387304 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 142400     |
| train/episodes                 | 14240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00576   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 569600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 356        |
| stats_o/mean                   | 0.20491752 |
| stats_o/std                    | 0.06857138 |
| test/episodes                  | 3570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00368   |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0496535 |
| test/Q_plus_P                  | -1.0496535 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 142800     |
| train/episodes                 | 14280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00624   |
| train/info_shaping_reward_mean | -0.0896    |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 571200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 357         |
| stats_o/mean                   | 0.20491518  |
| stats_o/std                    | 0.068584405 |
| test/episodes                  | 3580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0146     |
| test/info_shaping_reward_mean  | -0.0607     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1556349  |
| test/Q_plus_P                  | -1.1556349  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 143200      |
| train/episodes                 | 14320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00799    |
| train/info_shaping_reward_mean | -0.0749     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 572800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 358        |
| stats_o/mean                   | 0.20491274 |
| stats_o/std                    | 0.06855723 |
| test/episodes                  | 3590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00901   |
| test/info_shaping_reward_mean  | -0.0628    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9628399 |
| test/Q_plus_P                  | -0.9628399 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 143600     |
| train/episodes                 | 14360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00675   |
| train/info_shaping_reward_mean | -0.0762    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 574400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 359        |
| stats_o/mean                   | 0.20490478 |
| stats_o/std                    | 0.06858882 |
| test/episodes                  | 3600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0214    |
| test/info_shaping_reward_mean  | -0.0611    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0268688 |
| test/Q_plus_P                  | -1.0268688 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 144000     |
| train/episodes                 | 14400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00783   |
| train/info_shaping_reward_mean | -0.081     |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 576000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 360         |
| stats_o/mean                   | 0.20490755  |
| stats_o/std                    | 0.068538964 |
| test/episodes                  | 3610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00403    |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0145826  |
| test/Q_plus_P                  | -1.0145826  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 144400      |
| train/episodes                 | 14440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00637    |
| train/info_shaping_reward_mean | -0.0665     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 577600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 361        |
| stats_o/mean                   | 0.20490667 |
| stats_o/std                    | 0.06855344 |
| test/episodes                  | 3620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00742   |
| test/info_shaping_reward_mean  | -0.0549    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0099518 |
| test/Q_plus_P                  | -1.0099518 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 144800     |
| train/episodes                 | 14480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0076    |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 579200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 362        |
| stats_o/mean                   | 0.204909   |
| stats_o/std                    | 0.06850209 |
| test/episodes                  | 3630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0171    |
| test/info_shaping_reward_mean  | -0.0608    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0583612 |
| test/Q_plus_P                  | -1.0583612 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 145200     |
| train/episodes                 | 14520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.673      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00652   |
| train/info_shaping_reward_mean | -0.0634    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 580800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.20491675  |
| stats_o/std                    | 0.06848279  |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00709    |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.89894706 |
| test/Q_plus_P                  | -0.89894706 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00779    |
| train/info_shaping_reward_mean | -0.0704     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 364        |
| stats_o/mean                   | 0.20492022 |
| stats_o/std                    | 0.06849353 |
| test/episodes                  | 3650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0117    |
| test/info_shaping_reward_mean  | -0.0541    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1307344 |
| test/Q_plus_P                  | -1.1307344 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 146000     |
| train/episodes                 | 14600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0084    |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 584000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 365        |
| stats_o/mean                   | 0.20491192 |
| stats_o/std                    | 0.0684751  |
| test/episodes                  | 3660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0137    |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0536666 |
| test/Q_plus_P                  | -1.0536666 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 146400     |
| train/episodes                 | 14640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00871   |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 585600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 366        |
| stats_o/mean                   | 0.20491046 |
| stats_o/std                    | 0.06847455 |
| test/episodes                  | 3670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00796   |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0745083 |
| test/Q_plus_P                  | -1.0745083 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 146800     |
| train/episodes                 | 14680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.575      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00755   |
| train/info_shaping_reward_mean | -0.0857    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 587200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 367        |
| stats_o/mean                   | 0.2048981  |
| stats_o/std                    | 0.06845282 |
| test/episodes                  | 3680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00737   |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9310656 |
| test/Q_plus_P                  | -0.9310656 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 147200     |
| train/episodes                 | 14720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00818   |
| train/info_shaping_reward_mean | -0.0679    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 588800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 368         |
| stats_o/mean                   | 0.2049007   |
| stats_o/std                    | 0.068409145 |
| test/episodes                  | 3690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00927    |
| test/info_shaping_reward_mean  | -0.0547     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.93536913 |
| test/Q_plus_P                  | -0.93536913 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 147600      |
| train/episodes                 | 14760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00683    |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 590400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 369         |
| stats_o/mean                   | 0.20489234  |
| stats_o/std                    | 0.068387195 |
| test/episodes                  | 3700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0124     |
| test/info_shaping_reward_mean  | -0.0579     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0014405  |
| test/Q_plus_P                  | -1.0014405  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 148000      |
| train/episodes                 | 14800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00651    |
| train/info_shaping_reward_mean | -0.0711     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 592000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 370         |
| stats_o/mean                   | 0.20488901  |
| stats_o/std                    | 0.068344615 |
| test/episodes                  | 3710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0144     |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0087954  |
| test/Q_plus_P                  | -1.0087954  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 148400      |
| train/episodes                 | 14840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00703    |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 593600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 371        |
| stats_o/mean                   | 0.20489094 |
| stats_o/std                    | 0.06836124 |
| test/episodes                  | 3720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00559   |
| test/info_shaping_reward_mean  | -0.0693    |
| test/info_shaping_reward_min   | -0.53      |
| test/Q                         | -2.422368  |
| test/Q_plus_P                  | -2.422368  |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 148800     |
| train/episodes                 | 14880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00674   |
| train/info_shaping_reward_mean | -0.0843    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 595200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 372         |
| stats_o/mean                   | 0.2049044   |
| stats_o/std                    | 0.06837287  |
| test/episodes                  | 3730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0129     |
| test/info_shaping_reward_mean  | -0.055      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.91200966 |
| test/Q_plus_P                  | -0.91200966 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 149200      |
| train/episodes                 | 14920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00581    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 596800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 373         |
| stats_o/mean                   | 0.20490797  |
| stats_o/std                    | 0.06843207  |
| test/episodes                  | 3740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.02       |
| test/info_shaping_reward_mean  | -0.0604     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.98458284 |
| test/Q_plus_P                  | -0.98458284 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 149600      |
| train/episodes                 | 14960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00722    |
| train/info_shaping_reward_mean | -0.0863     |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 598400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.204897   |
| stats_o/std                    | 0.06844458 |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00449   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9407316 |
| test/Q_plus_P                  | -0.9407316 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0057    |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 375        |
| stats_o/mean                   | 0.20489858 |
| stats_o/std                    | 0.0683979  |
| test/episodes                  | 3760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00652   |
| test/info_shaping_reward_mean  | -0.0557    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9630783 |
| test/Q_plus_P                  | -0.9630783 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 150400     |
| train/episodes                 | 15040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00793   |
| train/info_shaping_reward_mean | -0.0656    |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 601600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.20490645 |
| stats_o/std                    | 0.06841197 |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0118    |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0974164 |
| test/Q_plus_P                  | -1.0974164 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00614   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.20489427  |
| stats_o/std                    | 0.068399385 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00449    |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.9908373  |
| test/Q_plus_P                  | -0.9908373  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00745    |
| train/info_shaping_reward_mean | -0.0709     |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 378         |
| stats_o/mean                   | 0.20489688  |
| stats_o/std                    | 0.068386726 |
| test/episodes                  | 3790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00914    |
| test/info_shaping_reward_mean  | -0.0584     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.213626   |
| test/Q_plus_P                  | -1.213626   |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 151600      |
| train/episodes                 | 15160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00504    |
| train/info_shaping_reward_mean | -0.0732     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 606400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 379        |
| stats_o/mean                   | 0.20489913 |
| stats_o/std                    | 0.0683374  |
| test/episodes                  | 3800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00297   |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1477205 |
| test/Q_plus_P                  | -1.1477205 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 152000     |
| train/episodes                 | 15200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00668   |
| train/info_shaping_reward_mean | -0.0665    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 608000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.2048985   |
| stats_o/std                    | 0.06834285  |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00287    |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -0.92749274 |
| test/Q_plus_P                  | -0.92749274 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.005      |
| train/info_shaping_reward_mean | -0.0669     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 381         |
| stats_o/mean                   | 0.20489591  |
| stats_o/std                    | 0.0683578   |
| test/episodes                  | 3820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00552    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.84167653 |
| test/Q_plus_P                  | -0.84167653 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 152800      |
| train/episodes                 | 15280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00616    |
| train/info_shaping_reward_mean | -0.0716     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 611200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.20488948 |
| stats_o/std                    | 0.06833796 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0125    |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0364543 |
| test/Q_plus_P                  | -1.0364543 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00701   |
| train/info_shaping_reward_mean | -0.0719    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 383        |
| stats_o/mean                   | 0.20489329 |
| stats_o/std                    | 0.06830002 |
| test/episodes                  | 3840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0145    |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0256205 |
| test/Q_plus_P                  | -1.0256205 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 153600     |
| train/episodes                 | 15360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00584   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 614400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 384        |
| stats_o/mean                   | 0.20489946 |
| stats_o/std                    | 0.06825557 |
| test/episodes                  | 3850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0058    |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9252243 |
| test/Q_plus_P                  | -0.9252243 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 154000     |
| train/episodes                 | 15400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.667      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0063    |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 616000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 385        |
| stats_o/mean                   | 0.2049139  |
| stats_o/std                    | 0.06827555 |
| test/episodes                  | 3860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00521   |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.9709491 |
| test/Q_plus_P                  | -0.9709491 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 154400     |
| train/episodes                 | 15440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.008     |
| train/info_shaping_reward_mean | -0.0806    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 617600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 386        |
| stats_o/mean                   | 0.20491254 |
| stats_o/std                    | 0.06822117 |
| test/episodes                  | 3870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0105    |
| test/info_shaping_reward_mean  | -0.0564    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9600238 |
| test/Q_plus_P                  | -0.9600238 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 154800     |
| train/episodes                 | 15480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.689      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00801   |
| train/info_shaping_reward_mean | -0.0601    |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 619200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 387        |
| stats_o/mean                   | 0.20491028 |
| stats_o/std                    | 0.06823099 |
| test/episodes                  | 3880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00399   |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.1739004 |
| test/Q_plus_P                  | -1.1739004 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 155200     |
| train/episodes                 | 15520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00716   |
| train/info_shaping_reward_mean | -0.0748    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 620800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 388        |
| stats_o/mean                   | 0.20490682 |
| stats_o/std                    | 0.06821453 |
| test/episodes                  | 3890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00465   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9867729 |
| test/Q_plus_P                  | -0.9867729 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 155600     |
| train/episodes                 | 15560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00602   |
| train/info_shaping_reward_mean | -0.0681    |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 622400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 389         |
| stats_o/mean                   | 0.20491914  |
| stats_o/std                    | 0.068216585 |
| test/episodes                  | 3900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0111     |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1036439  |
| test/Q_plus_P                  | -1.1036439  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 156000      |
| train/episodes                 | 15600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00646    |
| train/info_shaping_reward_mean | -0.0753     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 624000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 390        |
| stats_o/mean                   | 0.20491599 |
| stats_o/std                    | 0.0681927  |
| test/episodes                  | 3910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00892   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9027243 |
| test/Q_plus_P                  | -0.9027243 |
| test/reward_per_eps            | -8         |
| test/steps                     | 156400     |
| train/episodes                 | 15640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00685   |
| train/info_shaping_reward_mean | -0.0669    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 625600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 391        |
| stats_o/mean                   | 0.20491575 |
| stats_o/std                    | 0.06825135 |
| test/episodes                  | 3920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00588   |
| test/info_shaping_reward_mean  | -0.057     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0372111 |
| test/Q_plus_P                  | -1.0372111 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 156800     |
| train/episodes                 | 15680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00887   |
| train/info_shaping_reward_mean | -0.0801    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 627200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 392         |
| stats_o/mean                   | 0.20491101  |
| stats_o/std                    | 0.068208806 |
| test/episodes                  | 3930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0177     |
| test/info_shaping_reward_mean  | -0.0611     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.121537   |
| test/Q_plus_P                  | -1.121537   |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 157200      |
| train/episodes                 | 15720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00645    |
| train/info_shaping_reward_mean | -0.0672     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 628800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 393        |
| stats_o/mean                   | 0.20491797 |
| stats_o/std                    | 0.06820319 |
| test/episodes                  | 3940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00154   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.906217  |
| test/Q_plus_P                  | -0.906217  |
| test/reward_per_eps            | -8         |
| test/steps                     | 157600     |
| train/episodes                 | 15760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00623   |
| train/info_shaping_reward_mean | -0.0745    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 630400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 394         |
| stats_o/mean                   | 0.20489901  |
| stats_o/std                    | 0.068220824 |
| test/episodes                  | 3950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0145     |
| test/info_shaping_reward_mean  | -0.0565     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.8740238  |
| test/Q_plus_P                  | -0.8740238  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 158000      |
| train/episodes                 | 15800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00795    |
| train/info_shaping_reward_mean | -0.0835     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 632000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 395         |
| stats_o/mean                   | 0.20490065  |
| stats_o/std                    | 0.068212554 |
| test/episodes                  | 3960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0188     |
| test/info_shaping_reward_mean  | -0.0558     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.93267995 |
| test/Q_plus_P                  | -0.93267995 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 158400      |
| train/episodes                 | 15840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00614    |
| train/info_shaping_reward_mean | -0.077      |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 633600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 396         |
| stats_o/mean                   | 0.2049111   |
| stats_o/std                    | 0.06821937  |
| test/episodes                  | 3970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0133     |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -0.97513115 |
| test/Q_plus_P                  | -0.97513115 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 158800      |
| train/episodes                 | 15880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00719    |
| train/info_shaping_reward_mean | -0.0867     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 635200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 397        |
| stats_o/mean                   | 0.20491119 |
| stats_o/std                    | 0.06818392 |
| test/episodes                  | 3980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00254   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.143278  |
| test/Q_plus_P                  | -1.143278  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 159200     |
| train/episodes                 | 15920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00748   |
| train/info_shaping_reward_mean | -0.0625    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 636800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 398        |
| stats_o/mean                   | 0.20490757 |
| stats_o/std                    | 0.06814655 |
| test/episodes                  | 3990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00439   |
| test/info_shaping_reward_mean  | -0.0568    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0298692 |
| test/Q_plus_P                  | -1.0298692 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 159600     |
| train/episodes                 | 15960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00681   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 638400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 399         |
| stats_o/mean                   | 0.20490164  |
| stats_o/std                    | 0.068108015 |
| test/episodes                  | 4000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00367    |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.8899285  |
| test/Q_plus_P                  | -0.8899285  |
| test/reward_per_eps            | -8          |
| test/steps                     | 160000      |
| train/episodes                 | 16000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00855    |
| train/info_shaping_reward_mean | -0.0692     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 640000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 400        |
| stats_o/mean                   | 0.20489456 |
| stats_o/std                    | 0.06809307 |
| test/episodes                  | 4010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00229   |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0342101 |
| test/Q_plus_P                  | -1.0342101 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 160400     |
| train/episodes                 | 16040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.652      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00599   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 641600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 401         |
| stats_o/mean                   | 0.20489377  |
| stats_o/std                    | 0.06809091  |
| test/episodes                  | 4020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00616    |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.90979934 |
| test/Q_plus_P                  | -0.90979934 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 160800      |
| train/episodes                 | 16080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00623    |
| train/info_shaping_reward_mean | -0.07       |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 643200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.20489688 |
| stats_o/std                    | 0.06805096 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0208    |
| test/info_shaping_reward_mean  | -0.0591    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1621121 |
| test/Q_plus_P                  | -1.1621121 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00771   |
| train/info_shaping_reward_mean | -0.0691    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 403         |
| stats_o/mean                   | 0.20489508  |
| stats_o/std                    | 0.06800468  |
| test/episodes                  | 4040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00884    |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.99760115 |
| test/Q_plus_P                  | -0.99760115 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 161600      |
| train/episodes                 | 16160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00566    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 646400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.20490552  |
| stats_o/std                    | 0.068013534 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00814    |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.98679954 |
| test/Q_plus_P                  | -0.98679954 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00583    |
| train/info_shaping_reward_mean | -0.0642     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 405        |
| stats_o/mean                   | 0.20491397 |
| stats_o/std                    | 0.06801341 |
| test/episodes                  | 4060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0129    |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.8900707 |
| test/Q_plus_P                  | -0.8900707 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 162400     |
| train/episodes                 | 16240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00609   |
| train/info_shaping_reward_mean | -0.0644    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 649600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 406         |
| stats_o/mean                   | 0.2049041   |
| stats_o/std                    | 0.06798566  |
| test/episodes                  | 4070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0143     |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -0.99330866 |
| test/Q_plus_P                  | -0.99330866 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 162800      |
| train/episodes                 | 16280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00713    |
| train/info_shaping_reward_mean | -0.0676     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 651200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 407        |
| stats_o/mean                   | 0.20489997 |
| stats_o/std                    | 0.0679716  |
| test/episodes                  | 4080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0117    |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9665546 |
| test/Q_plus_P                  | -0.9665546 |
| test/reward_per_eps            | -8         |
| test/steps                     | 163200     |
| train/episodes                 | 16320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0058    |
| train/info_shaping_reward_mean | -0.0766    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 652800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 408        |
| stats_o/mean                   | 0.20490819 |
| stats_o/std                    | 0.06795692 |
| test/episodes                  | 4090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00309   |
| test/info_shaping_reward_mean  | -0.0564    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1054368 |
| test/Q_plus_P                  | -1.1054368 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 163600     |
| train/episodes                 | 16360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00769   |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 654400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 409        |
| stats_o/mean                   | 0.2049124  |
| stats_o/std                    | 0.06794001 |
| test/episodes                  | 4100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0127    |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1081947 |
| test/Q_plus_P                  | -1.1081947 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 164000     |
| train/episodes                 | 16400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00586   |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 656000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 410        |
| stats_o/mean                   | 0.20491832 |
| stats_o/std                    | 0.06797265 |
| test/episodes                  | 4110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00833   |
| test/info_shaping_reward_mean  | -0.0605    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.113999  |
| test/Q_plus_P                  | -1.113999  |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 164400     |
| train/episodes                 | 16440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00526   |
| train/info_shaping_reward_mean | -0.0776    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 657600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 411        |
| stats_o/mean                   | 0.20491165 |
| stats_o/std                    | 0.06793583 |
| test/episodes                  | 4120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0144    |
| test/info_shaping_reward_mean  | -0.0612    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3549942 |
| test/Q_plus_P                  | -1.3549942 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 164800     |
| train/episodes                 | 16480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00801   |
| train/info_shaping_reward_mean | -0.0663    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 659200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 412         |
| stats_o/mean                   | 0.20491478  |
| stats_o/std                    | 0.067942865 |
| test/episodes                  | 4130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00215    |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.122268   |
| test/Q_plus_P                  | -1.122268   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 165200      |
| train/episodes                 | 16520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.602       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00794    |
| train/info_shaping_reward_mean | -0.0738     |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 660800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.20490733  |
| stats_o/std                    | 0.06791852  |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0041     |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.99275815 |
| test/Q_plus_P                  | -0.99275815 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00881    |
| train/info_shaping_reward_mean | -0.0663     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 414         |
| stats_o/mean                   | 0.20490777  |
| stats_o/std                    | 0.06792234  |
| test/episodes                  | 4150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00444    |
| test/info_shaping_reward_mean  | -0.0488     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.92182505 |
| test/Q_plus_P                  | -0.92182505 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 166000      |
| train/episodes                 | 16600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0067     |
| train/info_shaping_reward_mean | -0.0691     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 664000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 415         |
| stats_o/mean                   | 0.20492105  |
| stats_o/std                    | 0.067974456 |
| test/episodes                  | 4160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00537    |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.92048347 |
| test/Q_plus_P                  | -0.92048347 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 166400      |
| train/episodes                 | 16640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.578       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00588    |
| train/info_shaping_reward_mean | -0.0907     |
| train/info_shaping_reward_min  | -0.276      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 665600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 416        |
| stats_o/mean                   | 0.20492221 |
| stats_o/std                    | 0.06797991 |
| test/episodes                  | 4170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00523   |
| test/info_shaping_reward_mean  | -0.0541    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0507207 |
| test/Q_plus_P                  | -1.0507207 |
| test/reward_per_eps            | -9         |
| test/steps                     | 166800     |
| train/episodes                 | 16680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00787   |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 667200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 417         |
| stats_o/mean                   | 0.20493001  |
| stats_o/std                    | 0.068004705 |
| test/episodes                  | 4180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00925    |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.9403968  |
| test/Q_plus_P                  | -0.9403968  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 167200      |
| train/episodes                 | 16720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00817    |
| train/info_shaping_reward_mean | -0.0786     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 668800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.2049321   |
| stats_o/std                    | 0.06806184  |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00922    |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.93859994 |
| test/Q_plus_P                  | -0.93859994 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.582       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00855    |
| train/info_shaping_reward_mean | -0.0912     |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 419        |
| stats_o/mean                   | 0.2049414  |
| stats_o/std                    | 0.06804773 |
| test/episodes                  | 4200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00345   |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8969742 |
| test/Q_plus_P                  | -0.8969742 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 168000     |
| train/episodes                 | 16800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.63       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00668   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 672000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 420         |
| stats_o/mean                   | 0.2049446   |
| stats_o/std                    | 0.06807628  |
| test/episodes                  | 4210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0128     |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.89367247 |
| test/Q_plus_P                  | -0.89367247 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 168400      |
| train/episodes                 | 16840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00569    |
| train/info_shaping_reward_mean | -0.0759     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 673600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 421         |
| stats_o/mean                   | 0.20494513  |
| stats_o/std                    | 0.068076625 |
| test/episodes                  | 4220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000708   |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.96551645 |
| test/Q_plus_P                  | -0.96551645 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 168800      |
| train/episodes                 | 16880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00579    |
| train/info_shaping_reward_mean | -0.0664     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 675200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.20494395  |
| stats_o/std                    | 0.068064384 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0113     |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -0.9201374  |
| test/Q_plus_P                  | -0.9201374  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.62        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00913    |
| train/info_shaping_reward_mean | -0.0763     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 423        |
| stats_o/mean                   | 0.20495571 |
| stats_o/std                    | 0.06804881 |
| test/episodes                  | 4240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0121    |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -0.9354289 |
| test/Q_plus_P                  | -0.9354289 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 169600     |
| train/episodes                 | 16960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0061    |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 678400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 424        |
| stats_o/mean                   | 0.20495692 |
| stats_o/std                    | 0.06801836 |
| test/episodes                  | 4250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0124    |
| test/info_shaping_reward_mean  | -0.103     |
| test/info_shaping_reward_min   | -0.661     |
| test/Q                         | -4.6362414 |
| test/Q_plus_P                  | -4.6362414 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 170000     |
| train/episodes                 | 17000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00811   |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 680000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.20495723  |
| stats_o/std                    | 0.067988284 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000565   |
| test/info_shaping_reward_mean  | -0.0563     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.1265229  |
| test/Q_plus_P                  | -1.1265229  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00653    |
| train/info_shaping_reward_mean | -0.0655     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.20496564  |
| stats_o/std                    | 0.067958094 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0155     |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.191246   |
| test/Q_plus_P                  | -1.191246   |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00866    |
| train/info_shaping_reward_mean | -0.0717     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 427        |
| stats_o/mean                   | 0.2049541  |
| stats_o/std                    | 0.06797168 |
| test/episodes                  | 4280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00781   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.864629  |
| test/Q_plus_P                  | -0.864629  |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 171200     |
| train/episodes                 | 17120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00784   |
| train/info_shaping_reward_mean | -0.0784    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 684800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 428        |
| stats_o/mean                   | 0.20494516 |
| stats_o/std                    | 0.06798582 |
| test/episodes                  | 4290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0191    |
| test/info_shaping_reward_mean  | -0.0603    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1065202 |
| test/Q_plus_P                  | -1.1065202 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 171600     |
| train/episodes                 | 17160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00643   |
| train/info_shaping_reward_mean | -0.0762    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 686400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 429        |
| stats_o/mean                   | 0.2049469  |
| stats_o/std                    | 0.06795922 |
| test/episodes                  | 4300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0175    |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.901292  |
| test/Q_plus_P                  | -0.901292  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 172000     |
| train/episodes                 | 17200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.68       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00588   |
| train/info_shaping_reward_mean | -0.0626    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 688000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 430         |
| stats_o/mean                   | 0.20495032  |
| stats_o/std                    | 0.06798549  |
| test/episodes                  | 4310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0151     |
| test/info_shaping_reward_mean  | -0.0554     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.98606145 |
| test/Q_plus_P                  | -0.98606145 |
| test/reward_per_eps            | -9          |
| test/steps                     | 172400      |
| train/episodes                 | 17240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00766    |
| train/info_shaping_reward_mean | -0.0788     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 689600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 431         |
| stats_o/mean                   | 0.20495225  |
| stats_o/std                    | 0.068003796 |
| test/episodes                  | 4320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.019      |
| test/info_shaping_reward_mean  | -0.0617     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0054622  |
| test/Q_plus_P                  | -1.0054622  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 172800      |
| train/episodes                 | 17280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00635    |
| train/info_shaping_reward_mean | -0.0749     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 691200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 432        |
| stats_o/mean                   | 0.20496319 |
| stats_o/std                    | 0.06801107 |
| test/episodes                  | 4330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00992   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1228681 |
| test/Q_plus_P                  | -1.1228681 |
| test/reward_per_eps            | -9         |
| test/steps                     | 173200     |
| train/episodes                 | 17320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0062    |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 692800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 433        |
| stats_o/mean                   | 0.20496723 |
| stats_o/std                    | 0.06798045 |
| test/episodes                  | 4340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0129    |
| test/info_shaping_reward_mean  | -0.0621    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0486283 |
| test/Q_plus_P                  | -1.0486283 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 173600     |
| train/episodes                 | 17360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0074    |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 694400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 434         |
| stats_o/mean                   | 0.2049589   |
| stats_o/std                    | 0.06796834  |
| test/episodes                  | 4350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00446    |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.96237737 |
| test/Q_plus_P                  | -0.96237737 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 174000      |
| train/episodes                 | 17400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.578       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00689    |
| train/info_shaping_reward_mean | -0.0723     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 696000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 435        |
| stats_o/mean                   | 0.20496622 |
| stats_o/std                    | 0.06799096 |
| test/episodes                  | 4360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0146    |
| test/info_shaping_reward_mean  | -0.057     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0596672 |
| test/Q_plus_P                  | -1.0596672 |
| test/reward_per_eps            | -9         |
| test/steps                     | 174400     |
| train/episodes                 | 17440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00876   |
| train/info_shaping_reward_mean | -0.0745    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 697600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 436        |
| stats_o/mean                   | 0.20497718 |
| stats_o/std                    | 0.06797128 |
| test/episodes                  | 4370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0123    |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.8809605 |
| test/Q_plus_P                  | -0.8809605 |
| test/reward_per_eps            | -8         |
| test/steps                     | 174800     |
| train/episodes                 | 17480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.007     |
| train/info_shaping_reward_mean | -0.0674    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 699200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 437         |
| stats_o/mean                   | 0.20497824  |
| stats_o/std                    | 0.068003826 |
| test/episodes                  | 4380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0204     |
| test/info_shaping_reward_mean  | -0.0583     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.99872977 |
| test/Q_plus_P                  | -0.99872977 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 175200      |
| train/episodes                 | 17520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.589       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00917    |
| train/info_shaping_reward_mean | -0.0819     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 700800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 438         |
| stats_o/mean                   | 0.20497555  |
| stats_o/std                    | 0.06800233  |
| test/episodes                  | 4390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0138     |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.86694837 |
| test/Q_plus_P                  | -0.86694837 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 175600      |
| train/episodes                 | 17560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00709    |
| train/info_shaping_reward_mean | -0.0761     |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 702400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 439         |
| stats_o/mean                   | 0.20497356  |
| stats_o/std                    | 0.068001166 |
| test/episodes                  | 4400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0016     |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.86356986 |
| test/Q_plus_P                  | -0.86356986 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 176000      |
| train/episodes                 | 17600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00576    |
| train/info_shaping_reward_mean | -0.0779     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 704000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 440        |
| stats_o/mean                   | 0.2049844  |
| stats_o/std                    | 0.06799074 |
| test/episodes                  | 4410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0195    |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9805491 |
| test/Q_plus_P                  | -0.9805491 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 176400     |
| train/episodes                 | 17640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00562   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 705600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 441        |
| stats_o/mean                   | 0.20499463 |
| stats_o/std                    | 0.06800085 |
| test/episodes                  | 4420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0159    |
| test/info_shaping_reward_mean  | -0.058     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.946094  |
| test/Q_plus_P                  | -0.946094  |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 176800     |
| train/episodes                 | 17680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00569   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 707200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 442        |
| stats_o/mean                   | 0.20498595 |
| stats_o/std                    | 0.06798253 |
| test/episodes                  | 4430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00796   |
| test/info_shaping_reward_mean  | -0.0592    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.1510471 |
| test/Q_plus_P                  | -1.1510471 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 177200     |
| train/episodes                 | 17720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00924   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 708800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 443        |
| stats_o/mean                   | 0.20499054 |
| stats_o/std                    | 0.06795752 |
| test/episodes                  | 4440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0118    |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.997607  |
| test/Q_plus_P                  | -0.997607  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 177600     |
| train/episodes                 | 17760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00583   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 710400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 444         |
| stats_o/mean                   | 0.20499332  |
| stats_o/std                    | 0.067966305 |
| test/episodes                  | 4450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0167     |
| test/info_shaping_reward_mean  | -0.059      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0480819  |
| test/Q_plus_P                  | -1.0480819  |
| test/reward_per_eps            | -9          |
| test/steps                     | 178000      |
| train/episodes                 | 17800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00742    |
| train/info_shaping_reward_mean | -0.0838     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 712000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 445        |
| stats_o/mean                   | 0.2049967  |
| stats_o/std                    | 0.06793428 |
| test/episodes                  | 4460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0136    |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0216814 |
| test/Q_plus_P                  | -1.0216814 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 178400     |
| train/episodes                 | 17840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00711   |
| train/info_shaping_reward_mean | -0.0749    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 713600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 446        |
| stats_o/mean                   | 0.2049878  |
| stats_o/std                    | 0.06800977 |
| test/episodes                  | 4470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0169    |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.7973166 |
| test/Q_plus_P                  | -0.7973166 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 178800     |
| train/episodes                 | 17880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00928   |
| train/info_shaping_reward_mean | -0.0788    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 715200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 447         |
| stats_o/mean                   | 0.20499162  |
| stats_o/std                    | 0.06804187  |
| test/episodes                  | 4480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00326    |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.92314285 |
| test/Q_plus_P                  | -0.92314285 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 179200      |
| train/episodes                 | 17920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00538    |
| train/info_shaping_reward_mean | -0.0876     |
| train/info_shaping_reward_min  | -0.304      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 716800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 448         |
| stats_o/mean                   | 0.20499009  |
| stats_o/std                    | 0.0680312   |
| test/episodes                  | 4490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0183     |
| test/info_shaping_reward_mean  | -0.0571     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.99036056 |
| test/Q_plus_P                  | -0.99036056 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 179600      |
| train/episodes                 | 17960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00744    |
| train/info_shaping_reward_mean | -0.0684     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 718400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 449        |
| stats_o/mean                   | 0.20498781 |
| stats_o/std                    | 0.06804846 |
| test/episodes                  | 4500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00164   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8782248 |
| test/Q_plus_P                  | -0.8782248 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 180000     |
| train/episodes                 | 18000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00847   |
| train/info_shaping_reward_mean | -0.0723    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 720000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.20498796 |
| stats_o/std                    | 0.06805749 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0113    |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.956773  |
| test/Q_plus_P                  | -0.956773  |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00616   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 451        |
| stats_o/mean                   | 0.20499204 |
| stats_o/std                    | 0.06803589 |
| test/episodes                  | 4520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0132    |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0970324 |
| test/Q_plus_P                  | -1.0970324 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 180800     |
| train/episodes                 | 18080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00677   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 723200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 452         |
| stats_o/mean                   | 0.20498067  |
| stats_o/std                    | 0.068040274 |
| test/episodes                  | 4530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00322    |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.85062826 |
| test/Q_plus_P                  | -0.85062826 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 181200      |
| train/episodes                 | 18120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00736    |
| train/info_shaping_reward_mean | -0.0772     |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 724800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 453         |
| stats_o/mean                   | 0.20498577  |
| stats_o/std                    | 0.06801872  |
| test/episodes                  | 4540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00132    |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.88245463 |
| test/Q_plus_P                  | -0.88245463 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 181600      |
| train/episodes                 | 18160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00569    |
| train/info_shaping_reward_mean | -0.0659     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 726400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 454        |
| stats_o/mean                   | 0.20499498 |
| stats_o/std                    | 0.06800439 |
| test/episodes                  | 4550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0134    |
| test/info_shaping_reward_mean  | -0.0597    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.090627  |
| test/Q_plus_P                  | -1.090627  |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 182000     |
| train/episodes                 | 18200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00471   |
| train/info_shaping_reward_mean | -0.068     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 728000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 455        |
| stats_o/mean                   | 0.20500283 |
| stats_o/std                    | 0.0680313  |
| test/episodes                  | 4560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00734   |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.019345  |
| test/Q_plus_P                  | -1.019345  |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 182400     |
| train/episodes                 | 18240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00593   |
| train/info_shaping_reward_mean | -0.0708    |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 729600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 456        |
| stats_o/mean                   | 0.20500444 |
| stats_o/std                    | 0.068067   |
| test/episodes                  | 4570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00858   |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -0.9676831 |
| test/Q_plus_P                  | -0.9676831 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 182800     |
| train/episodes                 | 18280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00612   |
| train/info_shaping_reward_mean | -0.0813    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 731200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 457        |
| stats_o/mean                   | 0.2050033  |
| stats_o/std                    | 0.06806281 |
| test/episodes                  | 4580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0063    |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.036655  |
| test/Q_plus_P                  | -1.036655  |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 183200     |
| train/episodes                 | 18320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00646   |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 732800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 458        |
| stats_o/mean                   | 0.20500802 |
| stats_o/std                    | 0.06804332 |
| test/episodes                  | 4590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00928   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0227872 |
| test/Q_plus_P                  | -1.0227872 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 183600     |
| train/episodes                 | 18360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00694   |
| train/info_shaping_reward_mean | -0.0782    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 734400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 459         |
| stats_o/mean                   | 0.20501252  |
| stats_o/std                    | 0.06805654  |
| test/episodes                  | 4600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00838    |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.97744596 |
| test/Q_plus_P                  | -0.97744596 |
| test/reward_per_eps            | -9          |
| test/steps                     | 184000      |
| train/episodes                 | 18400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00555    |
| train/info_shaping_reward_mean | -0.074      |
| train/info_shaping_reward_min  | -0.211      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 736000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 460        |
| stats_o/mean                   | 0.20501907 |
| stats_o/std                    | 0.06804624 |
| test/episodes                  | 4610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0175    |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9623895 |
| test/Q_plus_P                  | -0.9623895 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 184400     |
| train/episodes                 | 18440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00721   |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 737600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 461         |
| stats_o/mean                   | 0.20501271  |
| stats_o/std                    | 0.068016805 |
| test/episodes                  | 4620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0199     |
| test/info_shaping_reward_mean  | -0.0576     |
| test/info_shaping_reward_min   | -0.19       |
| test/Q                         | -1.0406016  |
| test/Q_plus_P                  | -1.0406016  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 184800      |
| train/episodes                 | 18480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0049     |
| train/info_shaping_reward_mean | -0.0677     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 739200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 462        |
| stats_o/mean                   | 0.20500714 |
| stats_o/std                    | 0.06800928 |
| test/episodes                  | 4630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0157    |
| test/info_shaping_reward_mean  | -0.057     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0225306 |
| test/Q_plus_P                  | -1.0225306 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 185200     |
| train/episodes                 | 18520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00749   |
| train/info_shaping_reward_mean | -0.0646    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 740800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 463        |
| stats_o/mean                   | 0.20500226 |
| stats_o/std                    | 0.06799484 |
| test/episodes                  | 4640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00296   |
| test/info_shaping_reward_mean  | -0.0547    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.0101341 |
| test/Q_plus_P                  | -1.0101341 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 185600     |
| train/episodes                 | 18560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00706   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 742400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 464        |
| stats_o/mean                   | 0.20499973 |
| stats_o/std                    | 0.06798088 |
| test/episodes                  | 4650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0149    |
| test/info_shaping_reward_mean  | -0.0599    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0866876 |
| test/Q_plus_P                  | -1.0866876 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 186000     |
| train/episodes                 | 18600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00736   |
| train/info_shaping_reward_mean | -0.0761    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 744000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 465        |
| stats_o/mean                   | 0.20500712 |
| stats_o/std                    | 0.06800332 |
| test/episodes                  | 4660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0177    |
| test/info_shaping_reward_mean  | -0.058     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2079377 |
| test/Q_plus_P                  | -1.2079377 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 186400     |
| train/episodes                 | 18640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00693   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 745600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.20500986  |
| stats_o/std                    | 0.068028495 |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0127     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.968071   |
| test/Q_plus_P                  | -0.968071   |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00822    |
| train/info_shaping_reward_mean | -0.0797     |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 467        |
| stats_o/mean                   | 0.20500661 |
| stats_o/std                    | 0.06799365 |
| test/episodes                  | 4680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0128    |
| test/info_shaping_reward_mean  | -0.0588    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9350704 |
| test/Q_plus_P                  | -0.9350704 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 187200     |
| train/episodes                 | 18720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00847   |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 748800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 468        |
| stats_o/mean                   | 0.2050134  |
| stats_o/std                    | 0.0679814  |
| test/episodes                  | 4690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0161    |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9978865 |
| test/Q_plus_P                  | -0.9978865 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 187600     |
| train/episodes                 | 18760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00663   |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 750400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 469        |
| stats_o/mean                   | 0.20501459 |
| stats_o/std                    | 0.06798564 |
| test/episodes                  | 4700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00572   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.7860916 |
| test/Q_plus_P                  | -0.7860916 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 188000     |
| train/episodes                 | 18800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00683   |
| train/info_shaping_reward_mean | -0.0717    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 752000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.20501523 |
| stats_o/std                    | 0.06794366 |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0171    |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9498139 |
| test/Q_plus_P                  | -0.9498139 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00721   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 471         |
| stats_o/mean                   | 0.20501144  |
| stats_o/std                    | 0.06805113  |
| test/episodes                  | 4720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0181     |
| test/info_shaping_reward_mean  | -0.0579     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.92159355 |
| test/Q_plus_P                  | -0.92159355 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 188800      |
| train/episodes                 | 18880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.544       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00655    |
| train/info_shaping_reward_mean | -0.0966     |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 755200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.20501205 |
| stats_o/std                    | 0.06802383 |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0182    |
| test/info_shaping_reward_mean  | -0.0596    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0411713 |
| test/Q_plus_P                  | -1.0411713 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.658      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.007     |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 473         |
| stats_o/mean                   | 0.20501912  |
| stats_o/std                    | 0.068051115 |
| test/episodes                  | 4740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0123     |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.9706799  |
| test/Q_plus_P                  | -0.9706799  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 189600      |
| train/episodes                 | 18960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.567       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00796    |
| train/info_shaping_reward_mean | -0.0853     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 758400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 474        |
| stats_o/mean                   | 0.20501724 |
| stats_o/std                    | 0.06803516 |
| test/episodes                  | 4750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0116    |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9586081 |
| test/Q_plus_P                  | -0.9586081 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 190000     |
| train/episodes                 | 19000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0054    |
| train/info_shaping_reward_mean | -0.0648    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 760000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 475        |
| stats_o/mean                   | 0.20502424 |
| stats_o/std                    | 0.06802805 |
| test/episodes                  | 4760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00422   |
| test/info_shaping_reward_mean  | -0.0563    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.9126296 |
| test/Q_plus_P                  | -0.9126296 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 190400     |
| train/episodes                 | 19040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00379   |
| train/info_shaping_reward_mean | -0.0693    |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 761600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 476        |
| stats_o/mean                   | 0.20502378 |
| stats_o/std                    | 0.06800873 |
| test/episodes                  | 4770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00709   |
| test/info_shaping_reward_mean  | -0.0602    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1042521 |
| test/Q_plus_P                  | -1.1042521 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 190800     |
| train/episodes                 | 19080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00764   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 763200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.2050303   |
| stats_o/std                    | 0.067982815 |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0137     |
| test/info_shaping_reward_mean  | -0.0559     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.9746306  |
| test/Q_plus_P                  | -0.9746306  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00794    |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 478        |
| stats_o/mean                   | 0.20502709 |
| stats_o/std                    | 0.06797991 |
| test/episodes                  | 4790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0139    |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.95862   |
| test/Q_plus_P                  | -0.95862   |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 191600     |
| train/episodes                 | 19160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.675      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00752   |
| train/info_shaping_reward_mean | -0.0627    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 766400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.20501776  |
| stats_o/std                    | 0.06796104  |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0144     |
| test/info_shaping_reward_mean  | -0.0546     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.94635844 |
| test/Q_plus_P                  | -0.94635844 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00818    |
| train/info_shaping_reward_mean | -0.0664     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 480        |
| stats_o/mean                   | 0.2050305  |
| stats_o/std                    | 0.0679988  |
| test/episodes                  | 4810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00331   |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -0.8498804 |
| test/Q_plus_P                  | -0.8498804 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 192400     |
| train/episodes                 | 19240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00938   |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 769600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 481         |
| stats_o/mean                   | 0.20504428  |
| stats_o/std                    | 0.068013124 |
| test/episodes                  | 4820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0195     |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.9220711  |
| test/Q_plus_P                  | -0.9220711  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 192800      |
| train/episodes                 | 19280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.62        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00883    |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 771200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 482        |
| stats_o/mean                   | 0.20504726 |
| stats_o/std                    | 0.06799757 |
| test/episodes                  | 4830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00621   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8868887 |
| test/Q_plus_P                  | -0.8868887 |
| test/reward_per_eps            | -8         |
| test/steps                     | 193200     |
| train/episodes                 | 19320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00539   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 772800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 483         |
| stats_o/mean                   | 0.205047    |
| stats_o/std                    | 0.067982495 |
| test/episodes                  | 4840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0171     |
| test/info_shaping_reward_mean  | -0.0583     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.9408822  |
| test/Q_plus_P                  | -0.9408822  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 193600      |
| train/episodes                 | 19360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00837    |
| train/info_shaping_reward_mean | -0.0669     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 774400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.20504545  |
| stats_o/std                    | 0.067963965 |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.011      |
| test/info_shaping_reward_mean  | -0.0575     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.8784139  |
| test/Q_plus_P                  | -0.8784139  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00712    |
| train/info_shaping_reward_mean | -0.0642     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.20503937  |
| stats_o/std                    | 0.067994215 |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0231     |
| test/info_shaping_reward_mean  | -0.0577     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.8478627  |
| test/Q_plus_P                  | -0.8478627  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.577       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00703    |
| train/info_shaping_reward_mean | -0.0858     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 486         |
| stats_o/mean                   | 0.2050427   |
| stats_o/std                    | 0.0679851   |
| test/episodes                  | 4870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0213     |
| test/info_shaping_reward_mean  | -0.0593     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -0.98730856 |
| test/Q_plus_P                  | -0.98730856 |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 194800      |
| train/episodes                 | 19480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00656    |
| train/info_shaping_reward_mean | -0.0769     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 779200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 487         |
| stats_o/mean                   | 0.20504153  |
| stats_o/std                    | 0.06795779  |
| test/episodes                  | 4880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00987    |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.92409545 |
| test/Q_plus_P                  | -0.92409545 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 195200      |
| train/episodes                 | 19520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00725    |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 780800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 488         |
| stats_o/mean                   | 0.20503777  |
| stats_o/std                    | 0.06799404  |
| test/episodes                  | 4890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0138     |
| test/info_shaping_reward_mean  | -0.0582     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.86432886 |
| test/Q_plus_P                  | -0.86432886 |
| test/reward_per_eps            | -8          |
| test/steps                     | 195600      |
| train/episodes                 | 19560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00814    |
| train/info_shaping_reward_mean | -0.083      |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 782400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 489        |
| stats_o/mean                   | 0.205038   |
| stats_o/std                    | 0.06795381 |
| test/episodes                  | 4900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0221    |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9275532 |
| test/Q_plus_P                  | -0.9275532 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 196000     |
| train/episodes                 | 19600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.681      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00643   |
| train/info_shaping_reward_mean | -0.0627    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 784000     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 490       |
| stats_o/mean                   | 0.2050433 |
| stats_o/std                    | 0.0679537 |
| test/episodes                  | 4910      |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.775     |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.00477  |
| test/info_shaping_reward_mean  | -0.0575   |
| test/info_shaping_reward_min   | -0.177    |
| test/Q                         | -1.028107 |
| test/Q_plus_P                  | -1.028107 |
| test/reward_per_eps            | -9        |
| test/steps                     | 196400    |
| train/episodes                 | 19640     |
| train/info_is_success_max      | 1         |
| train/info_is_success_mean     | 0.631     |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.00543  |
| train/info_shaping_reward_mean | -0.0651   |
| train/info_shaping_reward_min  | -0.183    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -14.8     |
| train/steps                    | 785600    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 491        |
| stats_o/mean                   | 0.20505014 |
| stats_o/std                    | 0.06796215 |
| test/episodes                  | 4920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00609   |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9154132 |
| test/Q_plus_P                  | -0.9154132 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 196800     |
| train/episodes                 | 19680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00729   |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 787200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 492         |
| stats_o/mean                   | 0.20505229  |
| stats_o/std                    | 0.06793304  |
| test/episodes                  | 4930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0101     |
| test/info_shaping_reward_mean  | -0.0561     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.94145614 |
| test/Q_plus_P                  | -0.94145614 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 197200      |
| train/episodes                 | 19720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00748    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 788800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.20505366 |
| stats_o/std                    | 0.06792452 |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.012     |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.8687376 |
| test/Q_plus_P                  | -0.8687376 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0087    |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 494        |
| stats_o/mean                   | 0.20506871 |
| stats_o/std                    | 0.06793323 |
| test/episodes                  | 4950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.018     |
| test/info_shaping_reward_mean  | -0.0601    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0292454 |
| test/Q_plus_P                  | -1.0292454 |
| test/reward_per_eps            | -9         |
| test/steps                     | 198000     |
| train/episodes                 | 19800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00616   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 792000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 495         |
| stats_o/mean                   | 0.20507565  |
| stats_o/std                    | 0.067909576 |
| test/episodes                  | 4960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0108     |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0119257  |
| test/Q_plus_P                  | -1.0119257  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 198400      |
| train/episodes                 | 19840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00736    |
| train/info_shaping_reward_mean | -0.0631     |
| train/info_shaping_reward_min  | -0.197      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 793600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 496         |
| stats_o/mean                   | 0.20506677  |
| stats_o/std                    | 0.06792727  |
| test/episodes                  | 4970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000818   |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.91104263 |
| test/Q_plus_P                  | -0.91104263 |
| test/reward_per_eps            | -8          |
| test/steps                     | 198800      |
| train/episodes                 | 19880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00791    |
| train/info_shaping_reward_mean | -0.0728     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 795200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 497         |
| stats_o/mean                   | 0.20507178  |
| stats_o/std                    | 0.067922354 |
| test/episodes                  | 4980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00764    |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.9880177  |
| test/Q_plus_P                  | -0.9880177  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 199200      |
| train/episodes                 | 19920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00766    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.219      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 796800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 498        |
| stats_o/mean                   | 0.20507391 |
| stats_o/std                    | 0.06791918 |
| test/episodes                  | 4990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0135    |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -0.9976709 |
| test/Q_plus_P                  | -0.9976709 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 199600     |
| train/episodes                 | 19960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00637   |
| train/info_shaping_reward_mean | -0.0746    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 798400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.20507258  |
| stats_o/std                    | 0.067905486 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0104     |
| test/info_shaping_reward_mean  | -0.0527     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.8440874  |
| test/Q_plus_P                  | -0.8440874  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00623    |
| train/info_shaping_reward_mean | -0.0741     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 500        |
| stats_o/mean                   | 0.20505989 |
| stats_o/std                    | 0.06790681 |
| test/episodes                  | 5010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0157    |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.9593776 |
| test/Q_plus_P                  | -0.9593776 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 200400     |
| train/episodes                 | 20040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00585   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 801600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 501        |
| stats_o/mean                   | 0.20506237 |
| stats_o/std                    | 0.0678851  |
| test/episodes                  | 5020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00734   |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8928616 |
| test/Q_plus_P                  | -0.8928616 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 200800     |
| train/episodes                 | 20080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00675   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 803200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.20506763  |
| stats_o/std                    | 0.067929365 |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0114     |
| test/info_shaping_reward_mean  | -0.061      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2238811  |
| test/Q_plus_P                  | -1.2238811  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.589       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00625    |
| train/info_shaping_reward_mean | -0.0782     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 503        |
| stats_o/mean                   | 0.20506369 |
| stats_o/std                    | 0.06789941 |
| test/episodes                  | 5040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0182    |
| test/info_shaping_reward_mean  | -0.0606    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1087999 |
| test/Q_plus_P                  | -1.1087999 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 201600     |
| train/episodes                 | 20160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.652      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00755   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 806400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 504        |
| stats_o/mean                   | 0.20506309 |
| stats_o/std                    | 0.06789278 |
| test/episodes                  | 5050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0196    |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9694765 |
| test/Q_plus_P                  | -0.9694765 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 202000     |
| train/episodes                 | 20200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00768   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 808000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 505         |
| stats_o/mean                   | 0.20505637  |
| stats_o/std                    | 0.067892306 |
| test/episodes                  | 5060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0114     |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.92971694 |
| test/Q_plus_P                  | -0.92971694 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 202400      |
| train/episodes                 | 20240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0092     |
| train/info_shaping_reward_mean | -0.0691     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 809600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 506        |
| stats_o/mean                   | 0.20505013 |
| stats_o/std                    | 0.06793755 |
| test/episodes                  | 5070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.014     |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.9862701 |
| test/Q_plus_P                  | -0.9862701 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 202800     |
| train/episodes                 | 20280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.553      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00668   |
| train/info_shaping_reward_mean | -0.0758    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 811200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 507         |
| stats_o/mean                   | 0.20505339  |
| stats_o/std                    | 0.06793815  |
| test/episodes                  | 5080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0164     |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.97741246 |
| test/Q_plus_P                  | -0.97741246 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 203200      |
| train/episodes                 | 20320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00795    |
| train/info_shaping_reward_mean | -0.0688     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 812800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 508        |
| stats_o/mean                   | 0.20505264 |
| stats_o/std                    | 0.06794575 |
| test/episodes                  | 5090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0184    |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.1403848 |
| test/Q_plus_P                  | -1.1403848 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 203600     |
| train/episodes                 | 20360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00661   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 814400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 509         |
| stats_o/mean                   | 0.20505224  |
| stats_o/std                    | 0.067981236 |
| test/episodes                  | 5100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00572    |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.91556925 |
| test/Q_plus_P                  | -0.91556925 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 204000      |
| train/episodes                 | 20400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00974    |
| train/info_shaping_reward_mean | -0.0874     |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 816000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 510        |
| stats_o/mean                   | 0.20505483 |
| stats_o/std                    | 0.06801478 |
| test/episodes                  | 5110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00506   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8779054 |
| test/Q_plus_P                  | -0.8779054 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 204400     |
| train/episodes                 | 20440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00877   |
| train/info_shaping_reward_mean | -0.0779    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 817600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 511        |
| stats_o/mean                   | 0.20505382 |
| stats_o/std                    | 0.06804912 |
| test/episodes                  | 5120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00867   |
| test/info_shaping_reward_mean  | -0.0574    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0802675 |
| test/Q_plus_P                  | -1.0802675 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 204800     |
| train/episodes                 | 20480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.666      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00754   |
| train/info_shaping_reward_mean | -0.0647    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 819200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 512         |
| stats_o/mean                   | 0.20505345  |
| stats_o/std                    | 0.068020284 |
| test/episodes                  | 5130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.023      |
| test/info_shaping_reward_mean  | -0.0604     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.9221365  |
| test/Q_plus_P                  | -0.9221365  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 205200      |
| train/episodes                 | 20520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00907    |
| train/info_shaping_reward_mean | -0.0673     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 820800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 513         |
| stats_o/mean                   | 0.20505121  |
| stats_o/std                    | 0.068027444 |
| test/episodes                  | 5140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0226     |
| test/info_shaping_reward_mean  | -0.0989     |
| test/info_shaping_reward_min   | -0.539      |
| test/Q                         | -4.1265407  |
| test/Q_plus_P                  | -4.1265407  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 205600      |
| train/episodes                 | 20560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00573    |
| train/info_shaping_reward_mean | -0.0744     |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 822400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 514        |
| stats_o/mean                   | 0.20504482 |
| stats_o/std                    | 0.06803268 |
| test/episodes                  | 5150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0182    |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.8774997 |
| test/Q_plus_P                  | -0.8774997 |
| test/reward_per_eps            | -8         |
| test/steps                     | 206000     |
| train/episodes                 | 20600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00608   |
| train/info_shaping_reward_mean | -0.0795    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 824000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 515        |
| stats_o/mean                   | 0.2050469  |
| stats_o/std                    | 0.06809003 |
| test/episodes                  | 5160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0168    |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.9977586 |
| test/Q_plus_P                  | -0.9977586 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 206400     |
| train/episodes                 | 20640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0085    |
| train/info_shaping_reward_mean | -0.0793    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 825600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 516        |
| stats_o/mean                   | 0.20504947 |
| stats_o/std                    | 0.06812706 |
| test/episodes                  | 5170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0146    |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9332566 |
| test/Q_plus_P                  | -0.9332566 |
| test/reward_per_eps            | -8         |
| test/steps                     | 206800     |
| train/episodes                 | 20680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00719   |
| train/info_shaping_reward_mean | -0.0823    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 827200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 517        |
| stats_o/mean                   | 0.20504402 |
| stats_o/std                    | 0.06814536 |
| test/episodes                  | 5180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00677   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.8391456 |
| test/Q_plus_P                  | -0.8391456 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 207200     |
| train/episodes                 | 20720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00832   |
| train/info_shaping_reward_mean | -0.0764    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 828800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 518         |
| stats_o/mean                   | 0.20505363  |
| stats_o/std                    | 0.068191774 |
| test/episodes                  | 5190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.016      |
| test/info_shaping_reward_mean  | -0.0598     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1432763  |
| test/Q_plus_P                  | -1.1432763  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 207600      |
| train/episodes                 | 20760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.602       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00765    |
| train/info_shaping_reward_mean | -0.0798     |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 830400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 519        |
| stats_o/mean                   | 0.20506123 |
| stats_o/std                    | 0.06824278 |
| test/episodes                  | 5200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.014     |
| test/info_shaping_reward_mean  | -0.0581    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0513939 |
| test/Q_plus_P                  | -1.0513939 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 208000     |
| train/episodes                 | 20800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00737   |
| train/info_shaping_reward_mean | -0.0758    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 832000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.20506266  |
| stats_o/std                    | 0.068306305 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0125     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.89083844 |
| test/Q_plus_P                  | -0.89083844 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00638    |
| train/info_shaping_reward_mean | -0.0787     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 521        |
| stats_o/mean                   | 0.2050573  |
| stats_o/std                    | 0.06832231 |
| test/episodes                  | 5220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0169    |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9943215 |
| test/Q_plus_P                  | -0.9943215 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 208800     |
| train/episodes                 | 20880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00886   |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 835200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 522        |
| stats_o/mean                   | 0.2050626  |
| stats_o/std                    | 0.06830012 |
| test/episodes                  | 5230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0168    |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.959776  |
| test/Q_plus_P                  | -0.959776  |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 209200     |
| train/episodes                 | 20920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00568   |
| train/info_shaping_reward_mean | -0.0666    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 836800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 523        |
| stats_o/mean                   | 0.2050649  |
| stats_o/std                    | 0.06830734 |
| test/episodes                  | 5240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0166    |
| test/info_shaping_reward_mean  | -0.0584    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9818124 |
| test/Q_plus_P                  | -0.9818124 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 209600     |
| train/episodes                 | 20960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00672   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 838400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 524        |
| stats_o/mean                   | 0.20506632 |
| stats_o/std                    | 0.06835373 |
| test/episodes                  | 5250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00798   |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9288805 |
| test/Q_plus_P                  | -0.9288805 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 210000     |
| train/episodes                 | 21000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00839   |
| train/info_shaping_reward_mean | -0.0878    |
| train/info_shaping_reward_min  | -0.287     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 840000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 525        |
| stats_o/mean                   | 0.20507596 |
| stats_o/std                    | 0.06835429 |
| test/episodes                  | 5260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0231    |
| test/info_shaping_reward_mean  | -0.0565    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8256648 |
| test/Q_plus_P                  | -0.8256648 |
| test/reward_per_eps            | -8         |
| test/steps                     | 210400     |
| train/episodes                 | 21040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00807   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 841600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.20506983  |
| stats_o/std                    | 0.06834951  |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0131     |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.96385825 |
| test/Q_plus_P                  | -0.96385825 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00745    |
| train/info_shaping_reward_mean | -0.068      |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 527         |
| stats_o/mean                   | 0.20506716  |
| stats_o/std                    | 0.06835845  |
| test/episodes                  | 5280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0104     |
| test/info_shaping_reward_mean  | -0.0556     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.94315076 |
| test/Q_plus_P                  | -0.94315076 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 211200      |
| train/episodes                 | 21120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.62        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00742    |
| train/info_shaping_reward_mean | -0.0688     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 844800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 528         |
| stats_o/mean                   | 0.20507349  |
| stats_o/std                    | 0.06836475  |
| test/episodes                  | 5290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0167     |
| test/info_shaping_reward_mean  | -0.0578     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.91710824 |
| test/Q_plus_P                  | -0.91710824 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 211600      |
| train/episodes                 | 21160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00865    |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 846400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 529        |
| stats_o/mean                   | 0.20507269 |
| stats_o/std                    | 0.06836011 |
| test/episodes                  | 5300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0172    |
| test/info_shaping_reward_mean  | -0.0652    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2378159 |
| test/Q_plus_P                  | -1.2378159 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 212000     |
| train/episodes                 | 21200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00723   |
| train/info_shaping_reward_mean | -0.0662    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 848000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 530        |
| stats_o/mean                   | 0.20506532 |
| stats_o/std                    | 0.06833493 |
| test/episodes                  | 5310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0171    |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0490102 |
| test/Q_plus_P                  | -1.0490102 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 212400     |
| train/episodes                 | 21240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00677   |
| train/info_shaping_reward_mean | -0.0633    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 849600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 531        |
| stats_o/mean                   | 0.20507388 |
| stats_o/std                    | 0.0683539  |
| test/episodes                  | 5320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0205    |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9177037 |
| test/Q_plus_P                  | -0.9177037 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 212800     |
| train/episodes                 | 21280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0052    |
| train/info_shaping_reward_mean | -0.0739    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 851200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 532        |
| stats_o/mean                   | 0.20506947 |
| stats_o/std                    | 0.06833572 |
| test/episodes                  | 5330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0199    |
| test/info_shaping_reward_mean  | -0.0575    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9960861 |
| test/Q_plus_P                  | -0.9960861 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 213200     |
| train/episodes                 | 21320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00595   |
| train/info_shaping_reward_mean | -0.0722    |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 852800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.20506738  |
| stats_o/std                    | 0.06835473  |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00821    |
| test/info_shaping_reward_mean  | -0.0563     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.96624804 |
| test/Q_plus_P                  | -0.96624804 |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00992    |
| train/info_shaping_reward_mean | -0.0769     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 534         |
| stats_o/mean                   | 0.20507127  |
| stats_o/std                    | 0.068372466 |
| test/episodes                  | 5350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0173     |
| test/info_shaping_reward_mean  | -0.0596     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1878548  |
| test/Q_plus_P                  | -1.1878548  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 214000      |
| train/episodes                 | 21400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.585       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00819    |
| train/info_shaping_reward_mean | -0.0762     |
| train/info_shaping_reward_min  | -0.206      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 856000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 535        |
| stats_o/mean                   | 0.20507179 |
| stats_o/std                    | 0.06837144 |
| test/episodes                  | 5360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0168    |
| test/info_shaping_reward_mean  | -0.0593    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0063764 |
| test/Q_plus_P                  | -1.0063764 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 214400     |
| train/episodes                 | 21440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00898   |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 857600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 536         |
| stats_o/mean                   | 0.20507368  |
| stats_o/std                    | 0.068398125 |
| test/episodes                  | 5370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00851    |
| test/info_shaping_reward_mean  | -0.0598     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.0361998  |
| test/Q_plus_P                  | -1.0361998  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 214800      |
| train/episodes                 | 21480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00828    |
| train/info_shaping_reward_mean | -0.0757     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 859200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 537         |
| stats_o/mean                   | 0.20507334  |
| stats_o/std                    | 0.06838218  |
| test/episodes                  | 5380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0126     |
| test/info_shaping_reward_mean  | -0.0556     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.88900805 |
| test/Q_plus_P                  | -0.88900805 |
| test/reward_per_eps            | -8          |
| test/steps                     | 215200      |
| train/episodes                 | 21520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00715    |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 860800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 538         |
| stats_o/mean                   | 0.2050812   |
| stats_o/std                    | 0.068371095 |
| test/episodes                  | 5390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0121     |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.85201836 |
| test/Q_plus_P                  | -0.85201836 |
| test/reward_per_eps            | -8          |
| test/steps                     | 215600      |
| train/episodes                 | 21560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00616    |
| train/info_shaping_reward_mean | -0.068      |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 862400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 539         |
| stats_o/mean                   | 0.20507805  |
| stats_o/std                    | 0.0683452   |
| test/episodes                  | 5400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.014      |
| test/info_shaping_reward_mean  | -0.0533     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.84133965 |
| test/Q_plus_P                  | -0.84133965 |
| test/reward_per_eps            | -8          |
| test/steps                     | 216000      |
| train/episodes                 | 21600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00712    |
| train/info_shaping_reward_mean | -0.0663     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 864000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 540        |
| stats_o/mean                   | 0.20507818 |
| stats_o/std                    | 0.06832826 |
| test/episodes                  | 5410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.016     |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.0623568 |
| test/Q_plus_P                  | -1.0623568 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 216400     |
| train/episodes                 | 21640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00636   |
| train/info_shaping_reward_mean | -0.065     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 865600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.2050797   |
| stats_o/std                    | 0.068306014 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00689    |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.97435266 |
| test/Q_plus_P                  | -0.97435266 |
| test/reward_per_eps            | -9          |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00773    |
| train/info_shaping_reward_mean | -0.0671     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 542         |
| stats_o/mean                   | 0.20508684  |
| stats_o/std                    | 0.068306066 |
| test/episodes                  | 5430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0187     |
| test/info_shaping_reward_mean  | -0.055      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.8737591  |
| test/Q_plus_P                  | -0.8737591  |
| test/reward_per_eps            | -8          |
| test/steps                     | 217200      |
| train/episodes                 | 21720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00767    |
| train/info_shaping_reward_mean | -0.0661     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 868800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 543         |
| stats_o/mean                   | 0.20508283  |
| stats_o/std                    | 0.068364345 |
| test/episodes                  | 5440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0216     |
| test/info_shaping_reward_mean  | -0.0571     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0566552  |
| test/Q_plus_P                  | -1.0566552  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 217600      |
| train/episodes                 | 21760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00737    |
| train/info_shaping_reward_mean | -0.0732     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 870400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.20507875  |
| stats_o/std                    | 0.06833921  |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0203     |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.83320445 |
| test/Q_plus_P                  | -0.83320445 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00548    |
| train/info_shaping_reward_mean | -0.0679     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 545        |
| stats_o/mean                   | 0.20508064 |
| stats_o/std                    | 0.06834207 |
| test/episodes                  | 5460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0255    |
| test/info_shaping_reward_mean  | -0.0596    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8678812 |
| test/Q_plus_P                  | -0.8678812 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 218400     |
| train/episodes                 | 21840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00708   |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 873600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 546        |
| stats_o/mean                   | 0.20507391 |
| stats_o/std                    | 0.06836868 |
| test/episodes                  | 5470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0271    |
| test/info_shaping_reward_mean  | -0.0614    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0480255 |
| test/Q_plus_P                  | -1.0480255 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 218800     |
| train/episodes                 | 21880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00916   |
| train/info_shaping_reward_mean | -0.0752    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 875200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 547        |
| stats_o/mean                   | 0.20506568 |
| stats_o/std                    | 0.06835593 |
| test/episodes                  | 5480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0236    |
| test/info_shaping_reward_mean  | -0.0656    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.0084713 |
| test/Q_plus_P                  | -1.0084713 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 219200     |
| train/episodes                 | 21920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00791   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 876800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 548         |
| stats_o/mean                   | 0.2050656   |
| stats_o/std                    | 0.06838338  |
| test/episodes                  | 5490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0166     |
| test/info_shaping_reward_mean  | -0.0569     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.86359286 |
| test/Q_plus_P                  | -0.86359286 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 219600      |
| train/episodes                 | 21960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00698    |
| train/info_shaping_reward_mean | -0.0812     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 878400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.20506503  |
| stats_o/std                    | 0.068372674 |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0203     |
| test/info_shaping_reward_mean  | -0.0578     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.98928    |
| test/Q_plus_P                  | -0.98928    |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00761    |
| train/info_shaping_reward_mean | -0.0704     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 550        |
| stats_o/mean                   | 0.20505883 |
| stats_o/std                    | 0.06840175 |
| test/episodes                  | 5510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0166    |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.8579474 |
| test/Q_plus_P                  | -0.8579474 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 220400     |
| train/episodes                 | 22040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.55       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00749   |
| train/info_shaping_reward_mean | -0.0876    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 881600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 551         |
| stats_o/mean                   | 0.20505975  |
| stats_o/std                    | 0.06838176  |
| test/episodes                  | 5520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0149     |
| test/info_shaping_reward_mean  | -0.0585     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -0.97386384 |
| test/Q_plus_P                  | -0.97386384 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 220800      |
| train/episodes                 | 22080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00518    |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 883200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 552         |
| stats_o/mean                   | 0.20505638  |
| stats_o/std                    | 0.068370774 |
| test/episodes                  | 5530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0226     |
| test/info_shaping_reward_mean  | -0.0582     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.8339558  |
| test/Q_plus_P                  | -0.8339558  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 221200      |
| train/episodes                 | 22120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0102     |
| train/info_shaping_reward_mean | -0.0695     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 884800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 553        |
| stats_o/mean                   | 0.20506434 |
| stats_o/std                    | 0.06840832 |
| test/episodes                  | 5540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.022     |
| test/info_shaping_reward_mean  | -0.06      |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.054235  |
| test/Q_plus_P                  | -1.054235  |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 221600     |
| train/episodes                 | 22160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00667   |
| train/info_shaping_reward_mean | -0.0774    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 886400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 554        |
| stats_o/mean                   | 0.20505832 |
| stats_o/std                    | 0.06842094 |
| test/episodes                  | 5550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.012     |
| test/info_shaping_reward_mean  | -0.0557    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9782251 |
| test/Q_plus_P                  | -0.9782251 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 222000     |
| train/episodes                 | 22200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00756   |
| train/info_shaping_reward_mean | -0.0768    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 888000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 555        |
| stats_o/mean                   | 0.20506124 |
| stats_o/std                    | 0.06841528 |
| test/episodes                  | 5560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0107    |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.9230201 |
| test/Q_plus_P                  | -0.9230201 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 222400     |
| train/episodes                 | 22240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00855   |
| train/info_shaping_reward_mean | -0.07      |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 889600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 556         |
| stats_o/mean                   | 0.20506915  |
| stats_o/std                    | 0.068461135 |
| test/episodes                  | 5570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0199     |
| test/info_shaping_reward_mean  | -0.0602     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -0.99723184 |
| test/Q_plus_P                  | -0.99723184 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 222800      |
| train/episodes                 | 22280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.583       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00796    |
| train/info_shaping_reward_mean | -0.0835     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 891200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 557        |
| stats_o/mean                   | 0.20507139 |
| stats_o/std                    | 0.06846376 |
| test/episodes                  | 5580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0144    |
| test/info_shaping_reward_mean  | -0.0624    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0376768 |
| test/Q_plus_P                  | -1.0376768 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 223200     |
| train/episodes                 | 22320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.63       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0109    |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 892800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 558        |
| stats_o/mean                   | 0.20506933 |
| stats_o/std                    | 0.06844991 |
| test/episodes                  | 5590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00577   |
| test/info_shaping_reward_mean  | -0.062     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0803579 |
| test/Q_plus_P                  | -1.0803579 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 223600     |
| train/episodes                 | 22360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00765   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 894400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 559        |
| stats_o/mean                   | 0.20507286 |
| stats_o/std                    | 0.06843075 |
| test/episodes                  | 5600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.017     |
| test/info_shaping_reward_mean  | -0.0598    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.9833137 |
| test/Q_plus_P                  | -0.9833137 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 224000     |
| train/episodes                 | 22400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0073    |
| train/info_shaping_reward_mean | -0.0669    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 896000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 560        |
| stats_o/mean                   | 0.20507273 |
| stats_o/std                    | 0.06843947 |
| test/episodes                  | 5610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0242    |
| test/info_shaping_reward_mean  | -0.0973    |
| test/info_shaping_reward_min   | -0.591     |
| test/Q                         | -3.5631192 |
| test/Q_plus_P                  | -3.5631192 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 224400     |
| train/episodes                 | 22440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.679      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00572   |
| train/info_shaping_reward_mean | -0.0629    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 897600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 561         |
| stats_o/mean                   | 0.20507154  |
| stats_o/std                    | 0.06845377  |
| test/episodes                  | 5620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0172     |
| test/info_shaping_reward_mean  | -0.0579     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -0.86218625 |
| test/Q_plus_P                  | -0.86218625 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 224800      |
| train/episodes                 | 22480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00886    |
| train/info_shaping_reward_mean | -0.0763     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 899200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 562        |
| stats_o/mean                   | 0.20507261 |
| stats_o/std                    | 0.06846594 |
| test/episodes                  | 5630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0106    |
| test/info_shaping_reward_mean  | -0.0602    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0024326 |
| test/Q_plus_P                  | -1.0024326 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 225200     |
| train/episodes                 | 22520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00928   |
| train/info_shaping_reward_mean | -0.0744    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 900800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.20507042  |
| stats_o/std                    | 0.06844757  |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0251     |
| test/info_shaping_reward_mean  | -0.061      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -0.98867846 |
| test/Q_plus_P                  | -0.98867846 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00613    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 564        |
| stats_o/mean                   | 0.20507024 |
| stats_o/std                    | 0.06844575 |
| test/episodes                  | 5650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0213    |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -0.8165447 |
| test/Q_plus_P                  | -0.8165447 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 226000     |
| train/episodes                 | 22600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0109    |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 904000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 565        |
| stats_o/mean                   | 0.20506844 |
| stats_o/std                    | 0.06846872 |
| test/episodes                  | 5660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.018     |
| test/info_shaping_reward_mean  | -0.0588    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.9212319 |
| test/Q_plus_P                  | -0.9212319 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 226400     |
| train/episodes                 | 22640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00928   |
| train/info_shaping_reward_mean | -0.0837    |
| train/info_shaping_reward_min  | -0.282     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 905600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.2050764   |
| stats_o/std                    | 0.06846145  |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0141     |
| test/info_shaping_reward_mean  | -0.0586     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.86946654 |
| test/Q_plus_P                  | -0.86946654 |
| test/reward_per_eps            | -8          |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00769    |
| train/info_shaping_reward_mean | -0.0665     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 567        |
| stats_o/mean                   | 0.20507519 |
| stats_o/std                    | 0.06847129 |
| test/episodes                  | 5680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00307   |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8457318 |
| test/Q_plus_P                  | -0.8457318 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 227200     |
| train/episodes                 | 22720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00859   |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 908800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 568        |
| stats_o/mean                   | 0.20506608 |
| stats_o/std                    | 0.06846393 |
| test/episodes                  | 5690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0211    |
| test/info_shaping_reward_mean  | -0.0602    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0181936 |
| test/Q_plus_P                  | -1.0181936 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 227600     |
| train/episodes                 | 22760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00779   |
| train/info_shaping_reward_mean | -0.0641    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 910400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 569        |
| stats_o/mean                   | 0.20505689 |
| stats_o/std                    | 0.06846962 |
| test/episodes                  | 5700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0223    |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.887713  |
| test/Q_plus_P                  | -0.887713  |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 228000     |
| train/episodes                 | 22800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00753   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 912000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.20505579  |
| stats_o/std                    | 0.068436295 |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0223     |
| test/info_shaping_reward_mean  | -0.062      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.82059264 |
| test/Q_plus_P                  | -0.82059264 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00422    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 571        |
| stats_o/mean                   | 0.20505883 |
| stats_o/std                    | 0.06842876 |
| test/episodes                  | 5720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00802   |
| test/info_shaping_reward_mean  | -0.0615    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.037359  |
| test/Q_plus_P                  | -1.037359  |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 228800     |
| train/episodes                 | 22880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00973   |
| train/info_shaping_reward_mean | -0.0715    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 915200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.20506087  |
| stats_o/std                    | 0.068429686 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0238     |
| test/info_shaping_reward_mean  | -0.0602     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.95180094 |
| test/Q_plus_P                  | -0.95180094 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00589    |
| train/info_shaping_reward_mean | -0.0798     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 573        |
| stats_o/mean                   | 0.205057   |
| stats_o/std                    | 0.06843935 |
| test/episodes                  | 5740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.022     |
| test/info_shaping_reward_mean  | -0.0633    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0656006 |
| test/Q_plus_P                  | -1.0656006 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 229600     |
| train/episodes                 | 22960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00914   |
| train/info_shaping_reward_mean | -0.0722    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 918400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.20506056  |
| stats_o/std                    | 0.068449065 |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0205     |
| test/info_shaping_reward_mean  | -0.0603     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.0310361  |
| test/Q_plus_P                  | -1.0310361  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00866    |
| train/info_shaping_reward_mean | -0.0769     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.20506053  |
| stats_o/std                    | 0.068459526 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0142     |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.85811543 |
| test/Q_plus_P                  | -0.85811543 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00867    |
| train/info_shaping_reward_mean | -0.0666     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 576        |
| stats_o/mean                   | 0.20506182 |
| stats_o/std                    | 0.06848055 |
| test/episodes                  | 5770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0177    |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.8820763 |
| test/Q_plus_P                  | -0.8820763 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 230800     |
| train/episodes                 | 23080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00652   |
| train/info_shaping_reward_mean | -0.0752    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 923200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 577        |
| stats_o/mean                   | 0.20506378 |
| stats_o/std                    | 0.06846434 |
| test/episodes                  | 5780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0167    |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9666175 |
| test/Q_plus_P                  | -0.9666175 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 231200     |
| train/episodes                 | 23120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00701   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 924800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.20506285 |
| stats_o/std                    | 0.06850173 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0257    |
| test/info_shaping_reward_mean  | -0.0595    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0888366 |
| test/Q_plus_P                  | -1.0888366 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00518   |
| train/info_shaping_reward_mean | -0.0826    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 579        |
| stats_o/mean                   | 0.20506164 |
| stats_o/std                    | 0.0685112  |
| test/episodes                  | 5800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0264    |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.8672641 |
| test/Q_plus_P                  | -0.8672641 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 232000     |
| train/episodes                 | 23200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.63       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00794   |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 928000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 580         |
| stats_o/mean                   | 0.20505077  |
| stats_o/std                    | 0.0685193   |
| test/episodes                  | 5810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.025      |
| test/info_shaping_reward_mean  | -0.0583     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.96416664 |
| test/Q_plus_P                  | -0.96416664 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 232400      |
| train/episodes                 | 23240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.59        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0104     |
| train/info_shaping_reward_mean | -0.0797     |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 929600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 581        |
| stats_o/mean                   | 0.20505524 |
| stats_o/std                    | 0.06851234 |
| test/episodes                  | 5820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0228    |
| test/info_shaping_reward_mean  | -0.0584    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9185913 |
| test/Q_plus_P                  | -0.9185913 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 232800     |
| train/episodes                 | 23280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00872   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 931200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 582        |
| stats_o/mean                   | 0.20505504 |
| stats_o/std                    | 0.06850547 |
| test/episodes                  | 5830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0193    |
| test/info_shaping_reward_mean  | -0.0606    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0261482 |
| test/Q_plus_P                  | -1.0261482 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 233200     |
| train/episodes                 | 23320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00958   |
| train/info_shaping_reward_mean | -0.068     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 932800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 583        |
| stats_o/mean                   | 0.20505027 |
| stats_o/std                    | 0.06852005 |
| test/episodes                  | 5840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0133    |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.9065012 |
| test/Q_plus_P                  | -0.9065012 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 233600     |
| train/episodes                 | 23360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0084    |
| train/info_shaping_reward_mean | -0.0755    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 934400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 584        |
| stats_o/mean                   | 0.20505606 |
| stats_o/std                    | 0.06856411 |
| test/episodes                  | 5850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0226    |
| test/info_shaping_reward_mean  | -0.0571    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -0.924186  |
| test/Q_plus_P                  | -0.924186  |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 234000     |
| train/episodes                 | 23400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00828   |
| train/info_shaping_reward_mean | -0.089     |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 936000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 585         |
| stats_o/mean                   | 0.20505571  |
| stats_o/std                    | 0.06856027  |
| test/episodes                  | 5860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0245     |
| test/info_shaping_reward_mean  | -0.0577     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -0.89043075 |
| test/Q_plus_P                  | -0.89043075 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 234400      |
| train/episodes                 | 23440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00534    |
| train/info_shaping_reward_mean | -0.0679     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 937600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.20504913  |
| stats_o/std                    | 0.068579815 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0211     |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.93133247 |
| test/Q_plus_P                  | -0.93133247 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00643    |
| train/info_shaping_reward_mean | -0.0691     |
| train/info_shaping_reward_min  | -0.201      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 587        |
| stats_o/mean                   | 0.20504434 |
| stats_o/std                    | 0.06858636 |
| test/episodes                  | 5880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00622   |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.925551  |
| test/Q_plus_P                  | -0.925551  |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 235200     |
| train/episodes                 | 23520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00844   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 940800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 588         |
| stats_o/mean                   | 0.2050434   |
| stats_o/std                    | 0.06858588  |
| test/episodes                  | 5890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0173     |
| test/info_shaping_reward_mean  | -0.0581     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.97561187 |
| test/Q_plus_P                  | -0.97561187 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 235600      |
| train/episodes                 | 23560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00809    |
| train/info_shaping_reward_mean | -0.0702     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 942400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 589        |
| stats_o/mean                   | 0.20504972 |
| stats_o/std                    | 0.06859155 |
| test/episodes                  | 5900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0253    |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9764476 |
| test/Q_plus_P                  | -0.9764476 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 236000     |
| train/episodes                 | 23600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00731   |
| train/info_shaping_reward_mean | -0.0788    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 944000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.20504399  |
| stats_o/std                    | 0.068571895 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0184     |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.876302   |
| test/Q_plus_P                  | -0.876302   |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0079     |
| train/info_shaping_reward_mean | -0.0701     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 591        |
| stats_o/mean                   | 0.20504262 |
| stats_o/std                    | 0.06858896 |
| test/episodes                  | 5920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.812      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0113    |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.8842508 |
| test/Q_plus_P                  | -0.8842508 |
| test/reward_per_eps            | -7.5       |
| test/steps                     | 236800     |
| train/episodes                 | 23680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00775   |
| train/info_shaping_reward_mean | -0.0785    |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 947200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 592        |
| stats_o/mean                   | 0.20503607 |
| stats_o/std                    | 0.06859605 |
| test/episodes                  | 5930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0227    |
| test/info_shaping_reward_mean  | -0.0616    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0646183 |
| test/Q_plus_P                  | -1.0646183 |
| test/reward_per_eps            | -9         |
| test/steps                     | 237200     |
| train/episodes                 | 23720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00866   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 948800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 593        |
| stats_o/mean                   | 0.20504707 |
| stats_o/std                    | 0.06861907 |
| test/episodes                  | 5940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0229    |
| test/info_shaping_reward_mean  | -0.0601    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0403434 |
| test/Q_plus_P                  | -1.0403434 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 237600     |
| train/episodes                 | 23760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00902   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 950400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 594        |
| stats_o/mean                   | 0.20504254 |
| stats_o/std                    | 0.06865931 |
| test/episodes                  | 5950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0156    |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.8996742 |
| test/Q_plus_P                  | -0.8996742 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 238000     |
| train/episodes                 | 23800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00783   |
| train/info_shaping_reward_mean | -0.0842    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 952000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 595        |
| stats_o/mean                   | 0.20503968 |
| stats_o/std                    | 0.0686856  |
| test/episodes                  | 5960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000876  |
| test/info_shaping_reward_mean  | -0.061     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0042026 |
| test/Q_plus_P                  | -1.0042026 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 238400     |
| train/episodes                 | 23840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00755   |
| train/info_shaping_reward_mean | -0.0739    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 953600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 596        |
| stats_o/mean                   | 0.20502841 |
| stats_o/std                    | 0.06869035 |
| test/episodes                  | 5970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0196    |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.926179  |
| test/Q_plus_P                  | -0.926179  |
| test/reward_per_eps            | -8         |
| test/steps                     | 238800     |
| train/episodes                 | 23880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00842   |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 955200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 597         |
| stats_o/mean                   | 0.20503472  |
| stats_o/std                    | 0.068676606 |
| test/episodes                  | 5980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0258     |
| test/info_shaping_reward_mean  | -0.0603     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0027034  |
| test/Q_plus_P                  | -1.0027034  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 239200      |
| train/episodes                 | 23920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00673    |
| train/info_shaping_reward_mean | -0.0656     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 956800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 598        |
| stats_o/mean                   | 0.20503043 |
| stats_o/std                    | 0.06870625 |
| test/episodes                  | 5990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0106    |
| test/info_shaping_reward_mean  | -0.063     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0012184 |
| test/Q_plus_P                  | -1.0012184 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 239600     |
| train/episodes                 | 23960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00743   |
| train/info_shaping_reward_mean | -0.0845    |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 958400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.205033   |
| stats_o/std                    | 0.06869148 |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.025     |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.8382955 |
| test/Q_plus_P                  | -0.8382955 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.67       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00792   |
| train/info_shaping_reward_mean | -0.0653    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
