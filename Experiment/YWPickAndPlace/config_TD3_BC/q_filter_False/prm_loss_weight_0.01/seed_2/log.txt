Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPickAndPlace/config_TD3_BC/q_filter_False/prm_loss_weight_0.01/seed_2
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.20285064  |
| stats_o/std                    | 0.090917856 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.188       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00488    |
| test/info_shaping_reward_mean  | -0.13       |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.5149692  |
| test/Q_plus_P                  | -1.5149692  |
| test/reward_per_eps            | -32.5       |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00125     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.113      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.52       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 1          |
| stats_o/mean                   | 0.20412719 |
| stats_o/std                    | 0.11284943 |
| test/episodes                  | 20         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.142      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00562   |
| test/info_shaping_reward_mean  | -0.147     |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -1.7564496 |
| test/Q_plus_P                  | -1.7564496 |
| test/reward_per_eps            | -34.3      |
| test/steps                     | 800        |
| train/episodes                 | 80         |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0858    |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.528     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 3200       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 2          |
| stats_o/mean                   | 0.20225342 |
| stats_o/std                    | 0.10960308 |
| test/episodes                  | 30         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.14       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0122    |
| test/info_shaping_reward_mean  | -0.157     |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -2.0866828 |
| test/Q_plus_P                  | -2.0866828 |
| test/reward_per_eps            | -34.4      |
| test/steps                     | 1200       |
| train/episodes                 | 120        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 4800       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.20249403 |
| stats_o/std                    | 0.10797038 |
| test/episodes                  | 40         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.318      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00307   |
| test/info_shaping_reward_mean  | -0.146     |
| test/info_shaping_reward_min   | -0.561     |
| test/Q                         | -2.1540465 |
| test/Q_plus_P                  | -2.1540465 |
| test/reward_per_eps            | -27.3      |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0106     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 4           |
| stats_o/mean                   | 0.20299844  |
| stats_o/std                    | 0.103536084 |
| test/episodes                  | 50          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.12        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0139     |
| test/info_shaping_reward_mean  | -0.167      |
| test/info_shaping_reward_min   | -0.521      |
| test/Q                         | -2.7852764  |
| test/Q_plus_P                  | -2.7852764  |
| test/reward_per_eps            | -35.2       |
| test/steps                     | 2000        |
| train/episodes                 | 200         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.185      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 8000        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 5          |
| stats_o/mean                   | 0.20318004 |
| stats_o/std                    | 0.10169145 |
| test/episodes                  | 60         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0475     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0264    |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -3.357146  |
| test/Q_plus_P                  | -3.357146  |
| test/reward_per_eps            | -38.1      |
| test/steps                     | 2400       |
| train/episodes                 | 240        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 9600       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 6           |
| stats_o/mean                   | 0.20335495  |
| stats_o/std                    | 0.097557686 |
| test/episodes                  | 70          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.09        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00859    |
| test/info_shaping_reward_mean  | -0.15       |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -3.571309   |
| test/Q_plus_P                  | -3.571309   |
| test/reward_per_eps            | -36.4       |
| test/steps                     | 2800        |
| train/episodes                 | 280         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.208      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 11200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 7           |
| stats_o/mean                   | 0.20366773  |
| stats_o/std                    | 0.095248595 |
| test/episodes                  | 80          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0325      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0253     |
| test/info_shaping_reward_mean  | -0.162      |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -4.111718   |
| test/Q_plus_P                  | -4.111718   |
| test/reward_per_eps            | -38.7       |
| test/steps                     | 3200        |
| train/episodes                 | 320         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.184      |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 12800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 8           |
| stats_o/mean                   | 0.20307896  |
| stats_o/std                    | 0.093505085 |
| test/episodes                  | 90          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.131      |
| test/info_shaping_reward_mean  | -0.168      |
| test/info_shaping_reward_min   | -0.19       |
| test/Q                         | -4.6196685  |
| test/Q_plus_P                  | -4.6196685  |
| test/reward_per_eps            | -40         |
| test/steps                     | 3600        |
| train/episodes                 | 360         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.18       |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 14400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 9          |
| stats_o/mean                   | 0.2032477  |
| stats_o/std                    | 0.0908773  |
| test/episodes                  | 100        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.145      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00378   |
| test/info_shaping_reward_mean  | -0.15      |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -4.4682746 |
| test/Q_plus_P                  | -4.4682746 |
| test/reward_per_eps            | -34.2      |
| test/steps                     | 4000       |
| train/episodes                 | 400        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 16000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 10         |
| stats_o/mean                   | 0.20290743 |
| stats_o/std                    | 0.0889163  |
| test/episodes                  | 110        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.045      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0204    |
| test/info_shaping_reward_mean  | -0.161     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -5.1460743 |
| test/Q_plus_P                  | -5.1460743 |
| test/reward_per_eps            | -38.2      |
| test/steps                     | 4400       |
| train/episodes                 | 440        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 17600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 11         |
| stats_o/mean                   | 0.20332669 |
| stats_o/std                    | 0.0868016  |
| test/episodes                  | 120        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.055      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00234   |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -5.629801  |
| test/Q_plus_P                  | -5.629801  |
| test/reward_per_eps            | -37.8      |
| test/steps                     | 4800       |
| train/episodes                 | 480        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 19200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.20302083  |
| stats_o/std                    | 0.085012704 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.11        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0112     |
| test/info_shaping_reward_mean  | -0.154      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -5.8371997  |
| test/Q_plus_P                  | -5.8371997  |
| test/reward_per_eps            | -35.6       |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 13         |
| stats_o/mean                   | 0.20317985 |
| stats_o/std                    | 0.08338505 |
| test/episodes                  | 140        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.145     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -6.591099  |
| test/Q_plus_P                  | -6.591099  |
| test/reward_per_eps            | -40        |
| test/steps                     | 5600       |
| train/episodes                 | 560        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00937    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 22400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 14          |
| stats_o/mean                   | 0.20293973  |
| stats_o/std                    | 0.081747025 |
| test/episodes                  | 150         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -7.0302534  |
| test/Q_plus_P                  | -7.0302534  |
| test/reward_per_eps            | -40         |
| test/steps                     | 6000        |
| train/episodes                 | 600         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 24000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 15         |
| stats_o/mean                   | 0.20271055 |
| stats_o/std                    | 0.08026747 |
| test/episodes                  | 160        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.06       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00753   |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -7.1705165 |
| test/Q_plus_P                  | -7.1705165 |
| test/reward_per_eps            | -37.6      |
| test/steps                     | 6400       |
| train/episodes                 | 640        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 25600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 16          |
| stats_o/mean                   | 0.20282738  |
| stats_o/std                    | 0.079060696 |
| test/episodes                  | 170         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.129      |
| test/info_shaping_reward_mean  | -0.171      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -7.779859   |
| test/Q_plus_P                  | -7.779859   |
| test/reward_per_eps            | -40         |
| test/steps                     | 6800        |
| train/episodes                 | 680         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 27200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 17         |
| stats_o/mean                   | 0.20284681 |
| stats_o/std                    | 0.07789616 |
| test/episodes                  | 180        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -8.093605  |
| test/Q_plus_P                  | -8.093605  |
| test/reward_per_eps            | -40        |
| test/steps                     | 7200       |
| train/episodes                 | 720        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 28800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 18         |
| stats_o/mean                   | 0.20312236 |
| stats_o/std                    | 0.07698273 |
| test/episodes                  | 190        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.065      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0238    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -8.187179  |
| test/Q_plus_P                  | -8.187179  |
| test/reward_per_eps            | -37.4      |
| test/steps                     | 7600       |
| train/episodes                 | 760        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 30400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.20321664 |
| stats_o/std                    | 0.07608567 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -8.924199  |
| test/Q_plus_P                  | -8.924199  |
| test/reward_per_eps            | -40        |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 20         |
| stats_o/mean                   | 0.20333834 |
| stats_o/std                    | 0.07522224 |
| test/episodes                  | 210        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.122      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.009     |
| test/info_shaping_reward_mean  | -0.149     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -8.559227  |
| test/Q_plus_P                  | -8.559227  |
| test/reward_per_eps            | -35.1      |
| test/steps                     | 8400       |
| train/episodes                 | 840        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 33600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 21         |
| stats_o/mean                   | 0.20328952 |
| stats_o/std                    | 0.0744653  |
| test/episodes                  | 220        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.548676  |
| test/Q_plus_P                  | -9.548676  |
| test/reward_per_eps            | -40        |
| test/steps                     | 8800       |
| train/episodes                 | 880        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 35200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 22         |
| stats_o/mean                   | 0.20327784 |
| stats_o/std                    | 0.07385341 |
| test/episodes                  | 230        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.116     |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.907785  |
| test/Q_plus_P                  | -9.907785  |
| test/reward_per_eps            | -40        |
| test/steps                     | 9200       |
| train/episodes                 | 920        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 36800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 23          |
| stats_o/mean                   | 0.20325495  |
| stats_o/std                    | 0.073276356 |
| test/episodes                  | 240         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -10.176344  |
| test/Q_plus_P                  | -10.176344  |
| test/reward_per_eps            | -40         |
| test/steps                     | 9600        |
| train/episodes                 | 960         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 38400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 24         |
| stats_o/mean                   | 0.2032568  |
| stats_o/std                    | 0.07296124 |
| test/episodes                  | 250        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00839   |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.787479  |
| test/Q_plus_P                  | -9.787479  |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 10000      |
| train/episodes                 | 1000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 40000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 25         |
| stats_o/mean                   | 0.20329395 |
| stats_o/std                    | 0.07238304 |
| test/episodes                  | 260        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.117      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00225   |
| test/info_shaping_reward_mean  | -0.154     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.889863  |
| test/Q_plus_P                  | -9.889863  |
| test/reward_per_eps            | -35.3      |
| test/steps                     | 10400      |
| train/episodes                 | 1040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 41600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 26         |
| stats_o/mean                   | 0.20336132 |
| stats_o/std                    | 0.07182728 |
| test/episodes                  | 270        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0525     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0134    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -10.680463 |
| test/Q_plus_P                  | -10.680463 |
| test/reward_per_eps            | -37.9      |
| test/steps                     | 10800      |
| train/episodes                 | 1080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 43200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 27         |
| stats_o/mean                   | 0.20343244 |
| stats_o/std                    | 0.07140693 |
| test/episodes                  | 280        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -11.497712 |
| test/Q_plus_P                  | -11.497712 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11200      |
| train/episodes                 | 1120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 44800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.20339303 |
| stats_o/std                    | 0.07110815 |
| test/episodes                  | 290        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -11.797276 |
| test/Q_plus_P                  | -11.797276 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.20333126 |
| stats_o/std                    | 0.07067492 |
| test/episodes                  | 300        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0103    |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -11.334909 |
| test/Q_plus_P                  | -11.334909 |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 30         |
| stats_o/mean                   | 0.20325783 |
| stats_o/std                    | 0.07061954 |
| test/episodes                  | 310        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -12.379742 |
| test/Q_plus_P                  | -12.379742 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12400      |
| train/episodes                 | 1240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 49600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 31          |
| stats_o/mean                   | 0.20324674  |
| stats_o/std                    | 0.070185244 |
| test/episodes                  | 320         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.103      |
| test/info_shaping_reward_mean  | -0.169      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -12.86769   |
| test/Q_plus_P                  | -12.86769   |
| test/reward_per_eps            | -40         |
| test/steps                     | 12800       |
| train/episodes                 | 1280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 51200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 32         |
| stats_o/mean                   | 0.20318243 |
| stats_o/std                    | 0.06974829 |
| test/episodes                  | 330        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -13.136513 |
| test/Q_plus_P                  | -13.136513 |
| test/reward_per_eps            | -40        |
| test/steps                     | 13200      |
| train/episodes                 | 1320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 52800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 33          |
| stats_o/mean                   | 0.20316666  |
| stats_o/std                    | 0.069340885 |
| test/episodes                  | 340         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.055       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00387    |
| test/info_shaping_reward_mean  | -0.157      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -12.71463   |
| test/Q_plus_P                  | -12.71463   |
| test/reward_per_eps            | -37.8       |
| test/steps                     | 13600       |
| train/episodes                 | 1360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 54400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 34         |
| stats_o/mean                   | 0.20318025 |
| stats_o/std                    | 0.06922373 |
| test/episodes                  | 350        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -13.77444  |
| test/Q_plus_P                  | -13.77444  |
| test/reward_per_eps            | -40        |
| test/steps                     | 14000      |
| train/episodes                 | 1400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 56000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.203201   |
| stats_o/std                    | 0.06893502 |
| test/episodes                  | 360        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.131     |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -14.045385 |
| test/Q_plus_P                  | -14.045385 |
| test/reward_per_eps            | -40        |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 36         |
| stats_o/mean                   | 0.20316887 |
| stats_o/std                    | 0.06860412 |
| test/episodes                  | 370        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.117      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00746   |
| test/info_shaping_reward_mean  | -0.154     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -12.641885 |
| test/Q_plus_P                  | -12.641885 |
| test/reward_per_eps            | -35.3      |
| test/steps                     | 14800      |
| train/episodes                 | 1480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 59200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.20316     |
| stats_o/std                    | 0.068329714 |
| test/episodes                  | 380         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.133      |
| test/info_shaping_reward_mean  | -0.169      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -14.608389  |
| test/Q_plus_P                  | -14.608389  |
| test/reward_per_eps            | -40         |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 38          |
| stats_o/mean                   | 0.2031654   |
| stats_o/std                    | 0.06810569  |
| test/episodes                  | 390         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.104      |
| test/info_shaping_reward_mean  | -0.168      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -14.4746475 |
| test/Q_plus_P                  | -14.4746475 |
| test/reward_per_eps            | -40         |
| test/steps                     | 15600       |
| train/episodes                 | 1560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 62400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 39         |
| stats_o/mean                   | 0.20324539 |
| stats_o/std                    | 0.06794024 |
| test/episodes                  | 400        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0153    |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -13.955641 |
| test/Q_plus_P                  | -13.955641 |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 16000      |
| train/episodes                 | 1600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 64000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 40         |
| stats_o/mean                   | 0.20332783 |
| stats_o/std                    | 0.06777929 |
| test/episodes                  | 410        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.075      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00598   |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -13.396316 |
| test/Q_plus_P                  | -13.396316 |
| test/reward_per_eps            | -37        |
| test/steps                     | 16400      |
| train/episodes                 | 1640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 65600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 41         |
| stats_o/mean                   | 0.2033325  |
| stats_o/std                    | 0.06767128 |
| test/episodes                  | 420        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.196     |
| test/info_shaping_reward_min   | -0.91      |
| test/Q                         | -14.299704 |
| test/Q_plus_P                  | -14.299704 |
| test/reward_per_eps            | -40        |
| test/steps                     | 16800      |
| train/episodes                 | 1680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 67200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 42         |
| stats_o/mean                   | 0.20327485 |
| stats_o/std                    | 0.06787774 |
| test/episodes                  | 430        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.065      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0158    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -14.970048 |
| test/Q_plus_P                  | -14.970048 |
| test/reward_per_eps            | -37.4      |
| test/steps                     | 17200      |
| train/episodes                 | 1720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.205     |
| train/info_shaping_reward_min  | -0.4       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 68800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 43         |
| stats_o/mean                   | 0.20324974 |
| stats_o/std                    | 0.06806089 |
| test/episodes                  | 440        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.188     |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -14.185303 |
| test/Q_plus_P                  | -14.185303 |
| test/reward_per_eps            | -40        |
| test/steps                     | 17600      |
| train/episodes                 | 1760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 70400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 44         |
| stats_o/mean                   | 0.20334238 |
| stats_o/std                    | 0.06846846 |
| test/episodes                  | 450        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.118     |
| test/info_shaping_reward_mean  | -0.202     |
| test/info_shaping_reward_min   | -0.544     |
| test/Q                         | -14.881196 |
| test/Q_plus_P                  | -14.881196 |
| test/reward_per_eps            | -40        |
| test/steps                     | 18000      |
| train/episodes                 | 1800       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00688    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.197     |
| train/info_shaping_reward_min  | -0.371     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 72000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 45         |
| stats_o/mean                   | 0.2034117  |
| stats_o/std                    | 0.06941779 |
| test/episodes                  | 460        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.202     |
| test/info_shaping_reward_min   | -0.502     |
| test/Q                         | -14.164619 |
| test/Q_plus_P                  | -14.164619 |
| test/reward_per_eps            | -40        |
| test/steps                     | 18400      |
| train/episodes                 | 1840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.207     |
| train/info_shaping_reward_min  | -0.426     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 73600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 46         |
| stats_o/mean                   | 0.20333093 |
| stats_o/std                    | 0.07011801 |
| test/episodes                  | 470        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0536    |
| test/info_shaping_reward_mean  | -0.215     |
| test/info_shaping_reward_min   | -0.348     |
| test/Q                         | -11.997437 |
| test/Q_plus_P                  | -11.997437 |
| test/reward_per_eps            | -40        |
| test/steps                     | 18800      |
| train/episodes                 | 1880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.213     |
| train/info_shaping_reward_min  | -0.39      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 75200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 47         |
| stats_o/mean                   | 0.20329967 |
| stats_o/std                    | 0.07098636 |
| test/episodes                  | 480        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.035      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.031     |
| test/info_shaping_reward_mean  | -0.198     |
| test/info_shaping_reward_min   | -0.356     |
| test/Q                         | -12.590664 |
| test/Q_plus_P                  | -12.590664 |
| test/reward_per_eps            | -38.6      |
| test/steps                     | 19200      |
| train/episodes                 | 1920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.203     |
| train/info_shaping_reward_min  | -0.441     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 76800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 48         |
| stats_o/mean                   | 0.20346525 |
| stats_o/std                    | 0.07281351 |
| test/episodes                  | 490        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.675     |
| test/Q                         | -12.627528 |
| test/Q_plus_P                  | -12.627528 |
| test/reward_per_eps            | -40        |
| test/steps                     | 19600      |
| train/episodes                 | 1960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.228     |
| train/info_shaping_reward_min  | -0.405     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 78400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 49         |
| stats_o/mean                   | 0.20355783 |
| stats_o/std                    | 0.07477123 |
| test/episodes                  | 500        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.183     |
| test/info_shaping_reward_min   | -0.227     |
| test/Q                         | -12.890554 |
| test/Q_plus_P                  | -12.890554 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20000      |
| train/episodes                 | 2000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.462     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 80000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.20364691 |
| stats_o/std                    | 0.07548251 |
| test/episodes                  | 510        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0325     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.038     |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -0.569     |
| test/Q                         | -12.544735 |
| test/Q_plus_P                  | -12.544735 |
| test/reward_per_eps            | -38.7      |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.2037271  |
| stats_o/std                    | 0.07576248 |
| test/episodes                  | 520        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0876    |
| test/info_shaping_reward_mean  | -0.274     |
| test/info_shaping_reward_min   | -0.852     |
| test/Q                         | -13.86417  |
| test/Q_plus_P                  | -13.86417  |
| test/reward_per_eps            | -40        |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00125    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.194     |
| train/info_shaping_reward_min  | -0.443     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 52          |
| stats_o/mean                   | 0.20384713  |
| stats_o/std                    | 0.076662876 |
| test/episodes                  | 530         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.177      |
| test/info_shaping_reward_min   | -0.213      |
| test/Q                         | -11.845337  |
| test/Q_plus_P                  | -11.845337  |
| test/reward_per_eps            | -40         |
| test/steps                     | 21200       |
| train/episodes                 | 2120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.425      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 84800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 53         |
| stats_o/mean                   | 0.20394464 |
| stats_o/std                    | 0.07713495 |
| test/episodes                  | 540        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.206     |
| test/info_shaping_reward_min   | -0.595     |
| test/Q                         | -10.799993 |
| test/Q_plus_P                  | -10.799993 |
| test/reward_per_eps            | -40        |
| test/steps                     | 21600      |
| train/episodes                 | 2160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 86400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 54         |
| stats_o/mean                   | 0.20404777 |
| stats_o/std                    | 0.07747086 |
| test/episodes                  | 550        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.106     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.211     |
| test/Q                         | -12.191303 |
| test/Q_plus_P                  | -12.191303 |
| test/reward_per_eps            | -40        |
| test/steps                     | 22000      |
| train/episodes                 | 2200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.295     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 88000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 55         |
| stats_o/mean                   | 0.20405413 |
| stats_o/std                    | 0.0783044  |
| test/episodes                  | 560        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.18      |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -12.604513 |
| test/Q_plus_P                  | -12.604513 |
| test/reward_per_eps            | -40        |
| test/steps                     | 22400      |
| train/episodes                 | 2240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.272     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 89600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 56          |
| stats_o/mean                   | 0.20407794  |
| stats_o/std                    | 0.078129336 |
| test/episodes                  | 570         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.14       |
| test/info_shaping_reward_mean  | -0.172      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -12.679436  |
| test/Q_plus_P                  | -12.679436  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22800       |
| train/episodes                 | 2280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 91200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 57         |
| stats_o/mean                   | 0.20409073 |
| stats_o/std                    | 0.07804024 |
| test/episodes                  | 580        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0709    |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -12.879636 |
| test/Q_plus_P                  | -12.879636 |
| test/reward_per_eps            | -40        |
| test/steps                     | 23200      |
| train/episodes                 | 2320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 92800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 58         |
| stats_o/mean                   | 0.20418942 |
| stats_o/std                    | 0.07798564 |
| test/episodes                  | 590        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.04       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0158    |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -12.394096 |
| test/Q_plus_P                  | -12.394096 |
| test/reward_per_eps            | -38.4      |
| test/steps                     | 23600      |
| train/episodes                 | 2360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.275     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 94400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 59          |
| stats_o/mean                   | 0.20422709  |
| stats_o/std                    | 0.078039356 |
| test/episodes                  | 600         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.138      |
| test/info_shaping_reward_mean  | -0.171      |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -13.04886   |
| test/Q_plus_P                  | -13.04886   |
| test/reward_per_eps            | -40         |
| test/steps                     | 24000       |
| train/episodes                 | 2400        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.00562     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.116      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.287      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.8       |
| train/steps                    | 96000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 60         |
| stats_o/mean                   | 0.20414114 |
| stats_o/std                    | 0.07799489 |
| test/episodes                  | 610        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.055      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0273    |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -13.835978 |
| test/Q_plus_P                  | -13.835978 |
| test/reward_per_eps            | -37.8      |
| test/steps                     | 24400      |
| train/episodes                 | 2440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.275     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 97600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 61          |
| stats_o/mean                   | 0.20427948  |
| stats_o/std                    | 0.077995576 |
| test/episodes                  | 620         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.132      |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -13.391706  |
| test/Q_plus_P                  | -13.391706  |
| test/reward_per_eps            | -40         |
| test/steps                     | 24800       |
| train/episodes                 | 2480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.19       |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 99200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 62          |
| stats_o/mean                   | 0.20431843  |
| stats_o/std                    | 0.077879444 |
| test/episodes                  | 630         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.134      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.208      |
| test/Q                         | -14.193872  |
| test/Q_plus_P                  | -14.193872  |
| test/reward_per_eps            | -40         |
| test/steps                     | 25200       |
| train/episodes                 | 2520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.193      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 100800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 63         |
| stats_o/mean                   | 0.20433746 |
| stats_o/std                    | 0.07771501 |
| test/episodes                  | 640        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00691   |
| test/info_shaping_reward_mean  | -0.19      |
| test/info_shaping_reward_min   | -0.624     |
| test/Q                         | -13.455261 |
| test/Q_plus_P                  | -13.455261 |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 25600      |
| train/episodes                 | 2560       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0106     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 102400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.20434053 |
| stats_o/std                    | 0.07759533 |
| test/episodes                  | 650        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -14.814101 |
| test/Q_plus_P                  | -14.814101 |
| test/reward_per_eps            | -40        |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.277     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 65         |
| stats_o/mean                   | 0.20436725 |
| stats_o/std                    | 0.07733176 |
| test/episodes                  | 660        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0575     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00751   |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.331     |
| test/Q                         | -13.327801 |
| test/Q_plus_P                  | -13.327801 |
| test/reward_per_eps            | -37.7      |
| test/steps                     | 26400      |
| train/episodes                 | 2640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.29      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 105600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 66         |
| stats_o/mean                   | 0.20438509 |
| stats_o/std                    | 0.07710579 |
| test/episodes                  | 670        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.195     |
| test/info_shaping_reward_min   | -0.388     |
| test/Q                         | -14.201585 |
| test/Q_plus_P                  | -14.201585 |
| test/reward_per_eps            | -40        |
| test/steps                     | 26800      |
| train/episodes                 | 2680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.276     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 107200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 67          |
| stats_o/mean                   | 0.204384    |
| stats_o/std                    | 0.076794766 |
| test/episodes                  | 680         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.186      |
| test/info_shaping_reward_min   | -0.452      |
| test/Q                         | -13.619468  |
| test/Q_plus_P                  | -13.619468  |
| test/reward_per_eps            | -40         |
| test/steps                     | 27200       |
| train/episodes                 | 2720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 108800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 68         |
| stats_o/mean                   | 0.204375   |
| stats_o/std                    | 0.07678999 |
| test/episodes                  | 690        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.126     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -14.797766 |
| test/Q_plus_P                  | -14.797766 |
| test/reward_per_eps            | -40        |
| test/steps                     | 27600      |
| train/episodes                 | 2760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.197     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 110400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 69         |
| stats_o/mean                   | 0.20434624 |
| stats_o/std                    | 0.07647387 |
| test/episodes                  | 700        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0075     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0316    |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -15.069306 |
| test/Q_plus_P                  | -15.069306 |
| test/reward_per_eps            | -39.7      |
| test/steps                     | 28000      |
| train/episodes                 | 2800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 112000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 70          |
| stats_o/mean                   | 0.20431349  |
| stats_o/std                    | 0.076324016 |
| test/episodes                  | 710         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.065       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0211     |
| test/info_shaping_reward_mean  | -0.163      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -15.059126  |
| test/Q_plus_P                  | -15.059126  |
| test/reward_per_eps            | -37.4       |
| test/steps                     | 28400       |
| train/episodes                 | 2840        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00125     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 113600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 71         |
| stats_o/mean                   | 0.20427965 |
| stats_o/std                    | 0.07609275 |
| test/episodes                  | 720        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.147      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0101    |
| test/info_shaping_reward_mean  | -0.148     |
| test/info_shaping_reward_min   | -0.215     |
| test/Q                         | -11.511515 |
| test/Q_plus_P                  | -11.511515 |
| test/reward_per_eps            | -34.1      |
| test/steps                     | 28800      |
| train/episodes                 | 2880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 115200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 72         |
| stats_o/mean                   | 0.20424248 |
| stats_o/std                    | 0.07587669 |
| test/episodes                  | 730        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0575     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00519   |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -14.194109 |
| test/Q_plus_P                  | -14.194109 |
| test/reward_per_eps            | -37.7      |
| test/steps                     | 29200      |
| train/episodes                 | 2920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 116800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 73          |
| stats_o/mean                   | 0.20427664  |
| stats_o/std                    | 0.075585835 |
| test/episodes                  | 740         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.025       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0329     |
| test/info_shaping_reward_mean  | -0.168      |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -15.752661  |
| test/Q_plus_P                  | -15.752661  |
| test/reward_per_eps            | -39         |
| test/steps                     | 29600       |
| train/episodes                 | 2960        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00437     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.8       |
| train/steps                    | 118400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 74         |
| stats_o/mean                   | 0.20430619 |
| stats_o/std                    | 0.07535436 |
| test/episodes                  | 750        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0875     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0124    |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -13.113247 |
| test/Q_plus_P                  | -13.113247 |
| test/reward_per_eps            | -36.5      |
| test/steps                     | 30000      |
| train/episodes                 | 3000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 120000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 75          |
| stats_o/mean                   | 0.20428276  |
| stats_o/std                    | 0.075115755 |
| test/episodes                  | 760         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0975      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00528    |
| test/info_shaping_reward_mean  | -0.153      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -13.460415  |
| test/Q_plus_P                  | -13.460415  |
| test/reward_per_eps            | -36.1       |
| test/steps                     | 30400       |
| train/episodes                 | 3040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.211      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 121600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 76         |
| stats_o/mean                   | 0.20425403 |
| stats_o/std                    | 0.0749551  |
| test/episodes                  | 770        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0725     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0249    |
| test/info_shaping_reward_mean  | -0.161     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -14.09713  |
| test/Q_plus_P                  | -14.09713  |
| test/reward_per_eps            | -37.1      |
| test/steps                     | 30800      |
| train/episodes                 | 3080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 123200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 77          |
| stats_o/mean                   | 0.20415783  |
| stats_o/std                    | 0.074789315 |
| test/episodes                  | 780         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.065       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0284     |
| test/info_shaping_reward_mean  | -0.156      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -15.714068  |
| test/Q_plus_P                  | -15.714068  |
| test/reward_per_eps            | -37.4       |
| test/steps                     | 31200       |
| train/episodes                 | 3120        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00187     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 124800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 78         |
| stats_o/mean                   | 0.20415409 |
| stats_o/std                    | 0.07461897 |
| test/episodes                  | 790        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.08       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0136    |
| test/info_shaping_reward_mean  | -0.158     |
| test/info_shaping_reward_min   | -0.406     |
| test/Q                         | -15.67562  |
| test/Q_plus_P                  | -15.67562  |
| test/reward_per_eps            | -36.8      |
| test/steps                     | 31600      |
| train/episodes                 | 3160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 126400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 79         |
| stats_o/mean                   | 0.20415401 |
| stats_o/std                    | 0.07444634 |
| test/episodes                  | 800        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0948    |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -16.779743 |
| test/Q_plus_P                  | -16.779743 |
| test/reward_per_eps            | -40        |
| test/steps                     | 32000      |
| train/episodes                 | 3200       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0112     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.108     |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 128000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 80         |
| stats_o/mean                   | 0.20409536 |
| stats_o/std                    | 0.07431312 |
| test/episodes                  | 810        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.05       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0215    |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -14.939655 |
| test/Q_plus_P                  | -14.939655 |
| test/reward_per_eps            | -38        |
| test/steps                     | 32400      |
| train/episodes                 | 3240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 129600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 81         |
| stats_o/mean                   | 0.20412473 |
| stats_o/std                    | 0.07425427 |
| test/episodes                  | 820        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0375     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0085    |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -14.337241 |
| test/Q_plus_P                  | -14.337241 |
| test/reward_per_eps            | -38.5      |
| test/steps                     | 32800      |
| train/episodes                 | 3280       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0125     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0899    |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 131200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 82         |
| stats_o/mean                   | 0.20417634 |
| stats_o/std                    | 0.07445695 |
| test/episodes                  | 830        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.135      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0227    |
| test/info_shaping_reward_mean  | -0.142     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -13.631741 |
| test/Q_plus_P                  | -13.631741 |
| test/reward_per_eps            | -34.6      |
| test/steps                     | 33200      |
| train/episodes                 | 3320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.12      |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 132800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 83         |
| stats_o/mean                   | 0.2041651  |
| stats_o/std                    | 0.07438868 |
| test/episodes                  | 840        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.035      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0378    |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.209     |
| test/Q                         | -16.890875 |
| test/Q_plus_P                  | -16.890875 |
| test/reward_per_eps            | -38.6      |
| test/steps                     | 33600      |
| train/episodes                 | 3360       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0212     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0947    |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 134400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 84         |
| stats_o/mean                   | 0.20413063 |
| stats_o/std                    | 0.074335   |
| test/episodes                  | 850        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.075      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0157    |
| test/info_shaping_reward_mean  | -0.15      |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -16.109068 |
| test/Q_plus_P                  | -16.109068 |
| test/reward_per_eps            | -37        |
| test/steps                     | 34000      |
| train/episodes                 | 3400       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0356     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.1       |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 136000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 85         |
| stats_o/mean                   | 0.20413657 |
| stats_o/std                    | 0.07435132 |
| test/episodes                  | 860        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.107      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00524   |
| test/info_shaping_reward_mean  | -0.149     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -15.426443 |
| test/Q_plus_P                  | -15.426443 |
| test/reward_per_eps            | -35.7      |
| test/steps                     | 34400      |
| train/episodes                 | 3440       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.035      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0557    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 137600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 86         |
| stats_o/mean                   | 0.20418768 |
| stats_o/std                    | 0.07427831 |
| test/episodes                  | 870        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0975     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0055    |
| test/info_shaping_reward_mean  | -0.146     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -14.403757 |
| test/Q_plus_P                  | -14.403757 |
| test/reward_per_eps            | -36.1      |
| test/steps                     | 34800      |
| train/episodes                 | 3480       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0494     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0559    |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38        |
| train/steps                    | 139200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 87          |
| stats_o/mean                   | 0.20421605  |
| stats_o/std                    | 0.074367106 |
| test/episodes                  | 880         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.02        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0291     |
| test/info_shaping_reward_mean  | -0.172      |
| test/info_shaping_reward_min   | -0.425      |
| test/Q                         | -16.921082  |
| test/Q_plus_P                  | -16.921082  |
| test/reward_per_eps            | -39.2       |
| test/steps                     | 35200       |
| train/episodes                 | 3520        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0375      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.052      |
| train/info_shaping_reward_mean | -0.167      |
| train/info_shaping_reward_min  | -0.285      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.5       |
| train/steps                    | 140800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 88          |
| stats_o/mean                   | 0.20426278  |
| stats_o/std                    | 0.074421205 |
| test/episodes                  | 890         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.18        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0188     |
| test/info_shaping_reward_mean  | -0.143      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -14.528993  |
| test/Q_plus_P                  | -14.528993  |
| test/reward_per_eps            | -32.8       |
| test/steps                     | 35600       |
| train/episodes                 | 3560        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0363      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0681     |
| train/info_shaping_reward_mean | -0.169      |
| train/info_shaping_reward_min  | -0.298      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.5       |
| train/steps                    | 142400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 89          |
| stats_o/mean                   | 0.20428194  |
| stats_o/std                    | 0.074273564 |
| test/episodes                  | 900         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.215       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00216    |
| test/info_shaping_reward_mean  | -0.134      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -13.122076  |
| test/Q_plus_P                  | -13.122076  |
| test/reward_per_eps            | -31.4       |
| test/steps                     | 36000       |
| train/episodes                 | 3600        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.0394      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0508     |
| train/info_shaping_reward_mean | -0.155      |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.4       |
| train/steps                    | 144000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 90         |
| stats_o/mean                   | 0.20428827 |
| stats_o/std                    | 0.07423745 |
| test/episodes                  | 910        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.292      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00447   |
| test/info_shaping_reward_mean  | -0.11      |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -10.090915 |
| test/Q_plus_P                  | -10.090915 |
| test/reward_per_eps            | -28.3      |
| test/steps                     | 36400      |
| train/episodes                 | 3640       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0656     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0453    |
| train/info_shaping_reward_mean | -0.158     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.4      |
| train/steps                    | 145600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 91         |
| stats_o/mean                   | 0.20427337 |
| stats_o/std                    | 0.07448528 |
| test/episodes                  | 920        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.245      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0136    |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -12.510699 |
| test/Q_plus_P                  | -12.510699 |
| test/reward_per_eps            | -30.2      |
| test/steps                     | 36800      |
| train/episodes                 | 3680       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0781     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0421    |
| train/info_shaping_reward_mean | -0.153     |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.9      |
| train/steps                    | 147200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 92         |
| stats_o/mean                   | 0.20430118 |
| stats_o/std                    | 0.07447548 |
| test/episodes                  | 930        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.19       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00855   |
| test/info_shaping_reward_mean  | -0.131     |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -12.922005 |
| test/Q_plus_P                  | -12.922005 |
| test/reward_per_eps            | -32.4      |
| test/steps                     | 37200      |
| train/episodes                 | 3720       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0844     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0353    |
| train/info_shaping_reward_mean | -0.154     |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.6      |
| train/steps                    | 148800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 93         |
| stats_o/mean                   | 0.20435576 |
| stats_o/std                    | 0.07436338 |
| test/episodes                  | 940        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.502      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0102    |
| test/info_shaping_reward_mean  | -0.0906    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -7.7585735 |
| test/Q_plus_P                  | -7.7585735 |
| test/reward_per_eps            | -19.9      |
| test/steps                     | 37600      |
| train/episodes                 | 3760       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.11       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0392    |
| train/info_shaping_reward_mean | -0.15      |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.6      |
| train/steps                    | 150400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 94         |
| stats_o/mean                   | 0.20435853 |
| stats_o/std                    | 0.07467907 |
| test/episodes                  | 950        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.27       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00255   |
| test/info_shaping_reward_mean  | -0.111     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -10.781939 |
| test/Q_plus_P                  | -10.781939 |
| test/reward_per_eps            | -29.2      |
| test/steps                     | 38000      |
| train/episodes                 | 3800       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0981     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0273    |
| train/info_shaping_reward_mean | -0.156     |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.1      |
| train/steps                    | 152000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 95         |
| stats_o/mean                   | 0.20437787 |
| stats_o/std                    | 0.07456784 |
| test/episodes                  | 960        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.315      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0037    |
| test/info_shaping_reward_mean  | -0.12      |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -11.055755 |
| test/Q_plus_P                  | -11.055755 |
| test/reward_per_eps            | -27.4      |
| test/steps                     | 38400      |
| train/episodes                 | 3840       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.156      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0263    |
| train/info_shaping_reward_mean | -0.134     |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.8      |
| train/steps                    | 153600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 96          |
| stats_o/mean                   | 0.20439127  |
| stats_o/std                    | 0.074493624 |
| test/episodes                  | 970         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.305       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00861    |
| test/info_shaping_reward_mean  | -0.12       |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -11.732877  |
| test/Q_plus_P                  | -11.732877  |
| test/reward_per_eps            | -27.8       |
| test/steps                     | 38800       |
| train/episodes                 | 3880        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.186       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0156     |
| train/info_shaping_reward_mean | -0.128      |
| train/info_shaping_reward_min  | -0.225      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.6       |
| train/steps                    | 155200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 97          |
| stats_o/mean                   | 0.20442978  |
| stats_o/std                    | 0.074517325 |
| test/episodes                  | 980         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.383       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00152    |
| test/info_shaping_reward_mean  | -0.105      |
| test/info_shaping_reward_min   | -0.203      |
| test/Q                         | -9.3021755  |
| test/Q_plus_P                  | -9.3021755  |
| test/reward_per_eps            | -24.7       |
| test/steps                     | 39200       |
| train/episodes                 | 3920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.202       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0148     |
| train/info_shaping_reward_mean | -0.13       |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.9       |
| train/steps                    | 156800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 98          |
| stats_o/mean                   | 0.20445843  |
| stats_o/std                    | 0.074719615 |
| test/episodes                  | 990         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.632       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00253    |
| test/info_shaping_reward_mean  | -0.0661     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -4.9765105  |
| test/Q_plus_P                  | -4.9765105  |
| test/reward_per_eps            | -14.7       |
| test/steps                     | 39600       |
| train/episodes                 | 3960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.22        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.011      |
| train/info_shaping_reward_mean | -0.13       |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.2       |
| train/steps                    | 158400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.20442551 |
| stats_o/std                    | 0.07497177 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.492      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00174   |
| test/info_shaping_reward_mean  | -0.0941    |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -9.598058  |
| test/Q_plus_P                  | -9.598058  |
| test/reward_per_eps            | -20.3      |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.25       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0189    |
| train/info_shaping_reward_mean | -0.133     |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30        |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 100        |
| stats_o/mean                   | 0.20443684 |
| stats_o/std                    | 0.07501282 |
| test/episodes                  | 1010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.605      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00102   |
| test/info_shaping_reward_mean  | -0.0737    |
| test/info_shaping_reward_min   | -0.224     |
| test/Q                         | -6.358307  |
| test/Q_plus_P                  | -6.358307  |
| test/reward_per_eps            | -15.8      |
| test/steps                     | 40400      |
| train/episodes                 | 4040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.244      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0109    |
| train/info_shaping_reward_mean | -0.13      |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.2      |
| train/steps                    | 161600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.20446451  |
| stats_o/std                    | 0.075255625 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.53        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00491    |
| test/info_shaping_reward_mean  | -0.0774     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -5.2832646  |
| test/Q_plus_P                  | -5.2832646  |
| test/reward_per_eps            | -18.8       |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.232       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0117     |
| train/info_shaping_reward_mean | -0.123      |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.7       |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 102        |
| stats_o/mean                   | 0.20447345 |
| stats_o/std                    | 0.07518138 |
| test/episodes                  | 1030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.54       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00215   |
| test/info_shaping_reward_mean  | -0.074     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -5.1756964 |
| test/Q_plus_P                  | -5.1756964 |
| test/reward_per_eps            | -18.4      |
| test/steps                     | 41200      |
| train/episodes                 | 4120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.312      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00763   |
| train/info_shaping_reward_mean | -0.0996    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.5      |
| train/steps                    | 164800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 103        |
| stats_o/mean                   | 0.20449904 |
| stats_o/std                    | 0.0751617  |
| test/episodes                  | 1040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.505      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0043    |
| test/info_shaping_reward_mean  | -0.0829    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -6.456386  |
| test/Q_plus_P                  | -6.456386  |
| test/reward_per_eps            | -19.8      |
| test/steps                     | 41600      |
| train/episodes                 | 4160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.298      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0102    |
| train/info_shaping_reward_mean | -0.113     |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.1      |
| train/steps                    | 166400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 104         |
| stats_o/mean                   | 0.20449415  |
| stats_o/std                    | 0.075029224 |
| test/episodes                  | 1050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.647       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00345    |
| test/info_shaping_reward_mean  | -0.0651     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -4.1653695  |
| test/Q_plus_P                  | -4.1653695  |
| test/reward_per_eps            | -14.1       |
| test/steps                     | 42000       |
| train/episodes                 | 4200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.372       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00657    |
| train/info_shaping_reward_mean | -0.0959     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.1       |
| train/steps                    | 168000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 105        |
| stats_o/mean                   | 0.20449856 |
| stats_o/std                    | 0.07489564 |
| test/episodes                  | 1060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00227   |
| test/info_shaping_reward_mean  | -0.0567    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -3.3837519 |
| test/Q_plus_P                  | -3.3837519 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 42400      |
| train/episodes                 | 4240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.351      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00892   |
| train/info_shaping_reward_mean | -0.0988    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26        |
| train/steps                    | 169600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 106        |
| stats_o/mean                   | 0.20449121 |
| stats_o/std                    | 0.07485313 |
| test/episodes                  | 1070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00109   |
| test/info_shaping_reward_mean  | -0.0601    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.8677719 |
| test/Q_plus_P                  | -3.8677719 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 42800      |
| train/episodes                 | 4280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.425      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00612   |
| train/info_shaping_reward_mean | -0.0949    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23        |
| train/steps                    | 171200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 107        |
| stats_o/mean                   | 0.20447008 |
| stats_o/std                    | 0.07475547 |
| test/episodes                  | 1080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00149   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.7133512 |
| test/Q_plus_P                  | -2.7133512 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 43200      |
| train/episodes                 | 4320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.376      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00662   |
| train/info_shaping_reward_mean | -0.0955    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.9      |
| train/steps                    | 172800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 108         |
| stats_o/mean                   | 0.20443006  |
| stats_o/std                    | 0.074759364 |
| test/episodes                  | 1090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00288    |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -2.8409944  |
| test/Q_plus_P                  | -2.8409944  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 43600       |
| train/episodes                 | 4360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.4         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00485    |
| train/info_shaping_reward_mean | -0.0999     |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24         |
| train/steps                    | 174400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 109         |
| stats_o/mean                   | 0.20442474  |
| stats_o/std                    | 0.074603364 |
| test/episodes                  | 1100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.627       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.07       |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -5.2400026  |
| test/Q_plus_P                  | -5.2400026  |
| test/reward_per_eps            | -14.9       |
| test/steps                     | 44000       |
| train/episodes                 | 4400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.438       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00436    |
| train/info_shaping_reward_mean | -0.0902     |
| train/info_shaping_reward_min  | -0.199      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.5       |
| train/steps                    | 176000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 110         |
| stats_o/mean                   | 0.20441878  |
| stats_o/std                    | 0.074521005 |
| test/episodes                  | 1110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00272    |
| test/info_shaping_reward_mean  | -0.0538     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -2.6938345  |
| test/Q_plus_P                  | -2.6938345  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 44400       |
| train/episodes                 | 4440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.469       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00437    |
| train/info_shaping_reward_mean | -0.0939     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.2       |
| train/steps                    | 177600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 111        |
| stats_o/mean                   | 0.20438029 |
| stats_o/std                    | 0.07447053 |
| test/episodes                  | 1120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00122   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -2.8985293 |
| test/Q_plus_P                  | -2.8985293 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 44800      |
| train/episodes                 | 4480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.41       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00615   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.6      |
| train/steps                    | 179200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 112        |
| stats_o/mean                   | 0.20440336 |
| stats_o/std                    | 0.07436816 |
| test/episodes                  | 1130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00124   |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.7872872 |
| test/Q_plus_P                  | -2.7872872 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 45200      |
| train/episodes                 | 4520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.426      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00667   |
| train/info_shaping_reward_mean | -0.0902    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.9      |
| train/steps                    | 180800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 113        |
| stats_o/mean                   | 0.20437708 |
| stats_o/std                    | 0.0743371  |
| test/episodes                  | 1140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00279   |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.6457145 |
| test/Q_plus_P                  | -2.6457145 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 45600      |
| train/episodes                 | 4560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.445      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00609   |
| train/info_shaping_reward_mean | -0.0906    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.2      |
| train/steps                    | 182400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 114        |
| stats_o/mean                   | 0.20436162 |
| stats_o/std                    | 0.07427392 |
| test/episodes                  | 1150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0018    |
| test/info_shaping_reward_mean  | -0.0541    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.6265812 |
| test/Q_plus_P                  | -2.6265812 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 46000      |
| train/episodes                 | 4600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.435      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00671   |
| train/info_shaping_reward_mean | -0.0895    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.6      |
| train/steps                    | 184000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 115        |
| stats_o/mean                   | 0.20437416 |
| stats_o/std                    | 0.07436195 |
| test/episodes                  | 1160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.552      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00184   |
| test/info_shaping_reward_mean  | -0.0809    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -6.264916  |
| test/Q_plus_P                  | -6.264916  |
| test/reward_per_eps            | -17.9      |
| test/steps                     | 46400      |
| train/episodes                 | 4640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.447      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0068    |
| train/info_shaping_reward_mean | -0.0917    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.1      |
| train/steps                    | 185600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 116        |
| stats_o/mean                   | 0.20440415 |
| stats_o/std                    | 0.07424852 |
| test/episodes                  | 1170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00245   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.2563236 |
| test/Q_plus_P                  | -2.2563236 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 46800      |
| train/episodes                 | 4680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.468      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0053    |
| train/info_shaping_reward_mean | -0.0824    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.3      |
| train/steps                    | 187200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 117         |
| stats_o/mean                   | 0.20439672  |
| stats_o/std                    | 0.074119985 |
| test/episodes                  | 1180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00288    |
| test/info_shaping_reward_mean  | -0.0559     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -2.3054724  |
| test/Q_plus_P                  | -2.3054724  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 47200       |
| train/episodes                 | 4720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.473       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00559    |
| train/info_shaping_reward_mean | -0.0817     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.1       |
| train/steps                    | 188800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 118        |
| stats_o/mean                   | 0.20441677 |
| stats_o/std                    | 0.07410572 |
| test/episodes                  | 1190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000547  |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.2129014 |
| test/Q_plus_P                  | -2.2129014 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 47600      |
| train/episodes                 | 4760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.398      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00618   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.1      |
| train/steps                    | 190400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 119        |
| stats_o/mean                   | 0.2044305  |
| stats_o/std                    | 0.07397012 |
| test/episodes                  | 1200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00283   |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -2.1579418 |
| test/Q_plus_P                  | -2.1579418 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 48000      |
| train/episodes                 | 4800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.477      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00649   |
| train/info_shaping_reward_mean | -0.0848    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 192000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 120        |
| stats_o/mean                   | 0.20444602 |
| stats_o/std                    | 0.07378132 |
| test/episodes                  | 1210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00239   |
| test/info_shaping_reward_mean  | -0.0567    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.3362646 |
| test/Q_plus_P                  | -2.3362646 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 48400      |
| train/episodes                 | 4840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.528      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00557   |
| train/info_shaping_reward_mean | -0.078     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 193600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.20447767 |
| stats_o/std                    | 0.07360497 |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00201   |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.8382777 |
| test/Q_plus_P                  | -1.8382777 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00479   |
| train/info_shaping_reward_mean | -0.0793    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 122         |
| stats_o/mean                   | 0.20445244  |
| stats_o/std                    | 0.073562406 |
| test/episodes                  | 1230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00457    |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.9894891  |
| test/Q_plus_P                  | -1.9894891  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 49200       |
| train/episodes                 | 4920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.415       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00587    |
| train/info_shaping_reward_mean | -0.089      |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.4       |
| train/steps                    | 196800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 123        |
| stats_o/mean                   | 0.2044512  |
| stats_o/std                    | 0.07341909 |
| test/episodes                  | 1240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00238   |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -3.387715  |
| test/Q_plus_P                  | -3.387715  |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 49600      |
| train/episodes                 | 4960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.516      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00467   |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 198400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 124        |
| stats_o/mean                   | 0.20447028 |
| stats_o/std                    | 0.07336338 |
| test/episodes                  | 1250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000914  |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.9850944 |
| test/Q_plus_P                  | -1.9850944 |
| test/reward_per_eps            | -11        |
| test/steps                     | 50000      |
| train/episodes                 | 5000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.465      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0888    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 200000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 125         |
| stats_o/mean                   | 0.20450315  |
| stats_o/std                    | 0.073270544 |
| test/episodes                  | 1260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00192    |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -2.0146463  |
| test/Q_plus_P                  | -2.0146463  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 50400       |
| train/episodes                 | 5040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.464       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00518    |
| train/info_shaping_reward_mean | -0.0877     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.4       |
| train/steps                    | 201600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 126        |
| stats_o/mean                   | 0.20450039 |
| stats_o/std                    | 0.07318532 |
| test/episodes                  | 1270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000646  |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.6045763 |
| test/Q_plus_P                  | -2.6045763 |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 50800      |
| train/episodes                 | 5080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.493      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00631   |
| train/info_shaping_reward_mean | -0.0833    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.3      |
| train/steps                    | 203200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 127         |
| stats_o/mean                   | 0.20448282  |
| stats_o/std                    | 0.073041074 |
| test/episodes                  | 1280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00146    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.7661557  |
| test/Q_plus_P                  | -1.7661557  |
| test/reward_per_eps            | -11         |
| test/steps                     | 51200       |
| train/episodes                 | 5120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.561       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0044     |
| train/info_shaping_reward_mean | -0.0735     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 204800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.20449646 |
| stats_o/std                    | 0.07296535 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00205   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -1.8453199 |
| test/Q_plus_P                  | -1.8453199 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.438      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00449   |
| train/info_shaping_reward_mean | -0.0912    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.5      |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 129        |
| stats_o/mean                   | 0.20449205 |
| stats_o/std                    | 0.07282709 |
| test/episodes                  | 1300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000912  |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.8855847 |
| test/Q_plus_P                  | -1.8855847 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 52000      |
| train/episodes                 | 5200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0779    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 208000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 130         |
| stats_o/mean                   | 0.20452313  |
| stats_o/std                    | 0.072892964 |
| test/episodes                  | 1310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00283    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.5280452  |
| test/Q_plus_P                  | -1.5280452  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 52400       |
| train/episodes                 | 5240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.474       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.0929     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21         |
| train/steps                    | 209600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 131        |
| stats_o/mean                   | 0.204545   |
| stats_o/std                    | 0.07274663 |
| test/episodes                  | 1320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.555796  |
| test/Q_plus_P                  | -1.555796  |
| test/reward_per_eps            | -11        |
| test/steps                     | 52800      |
| train/episodes                 | 5280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00424   |
| train/info_shaping_reward_mean | -0.078     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 211200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 132        |
| stats_o/mean                   | 0.20455447 |
| stats_o/std                    | 0.07259431 |
| test/episodes                  | 1330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00101   |
| test/info_shaping_reward_mean  | -0.0405    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2513543 |
| test/Q_plus_P                  | -1.2513543 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 53200      |
| train/episodes                 | 5320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00506   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 212800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 133        |
| stats_o/mean                   | 0.2045857  |
| stats_o/std                    | 0.07249799 |
| test/episodes                  | 1340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.0451    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.4235173 |
| test/Q_plus_P                  | -1.4235173 |
| test/reward_per_eps            | -10        |
| test/steps                     | 53600      |
| train/episodes                 | 5360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.0832    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 214400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 134        |
| stats_o/mean                   | 0.20461248 |
| stats_o/std                    | 0.07245079 |
| test/episodes                  | 1350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00304   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.5390604 |
| test/Q_plus_P                  | -1.5390604 |
| test/reward_per_eps            | -11        |
| test/steps                     | 54000      |
| train/episodes                 | 5400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.48       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00657   |
| train/info_shaping_reward_mean | -0.092     |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 216000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 135        |
| stats_o/mean                   | 0.20460735 |
| stats_o/std                    | 0.07245303 |
| test/episodes                  | 1360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0012    |
| test/info_shaping_reward_mean  | -0.0472    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.6246574 |
| test/Q_plus_P                  | -1.6246574 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 54400      |
| train/episodes                 | 5440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.466      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0894    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 217600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 136        |
| stats_o/mean                   | 0.20462167 |
| stats_o/std                    | 0.07235504 |
| test/episodes                  | 1370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.8217142 |
| test/Q_plus_P                  | -1.8217142 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 54800      |
| train/episodes                 | 5480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00404   |
| train/info_shaping_reward_mean | -0.0768    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 219200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 137        |
| stats_o/mean                   | 0.20461233 |
| stats_o/std                    | 0.0724041  |
| test/episodes                  | 1380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00188   |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.5295883 |
| test/Q_plus_P                  | -1.5295883 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 55200      |
| train/episodes                 | 5520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.522      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00448   |
| train/info_shaping_reward_mean | -0.0841    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 220800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.20461415 |
| stats_o/std                    | 0.07229745 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.5400816 |
| test/Q_plus_P                  | -1.5400816 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.499      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00448   |
| train/info_shaping_reward_mean | -0.0829    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20        |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.2046249  |
| stats_o/std                    | 0.07215353 |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00128   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.4106716 |
| test/Q_plus_P                  | -1.4106716 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.0784    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 140         |
| stats_o/mean                   | 0.2046421   |
| stats_o/std                    | 0.071983255 |
| test/episodes                  | 1410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5865867  |
| test/Q_plus_P                  | -1.5865867  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 56400       |
| train/episodes                 | 5640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00414    |
| train/info_shaping_reward_mean | -0.0753     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 225600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.20465094 |
| stats_o/std                    | 0.07185898 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00178   |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.495502  |
| test/Q_plus_P                  | -1.495502  |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.582      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00385   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 142         |
| stats_o/mean                   | 0.20463243  |
| stats_o/std                    | 0.071725704 |
| test/episodes                  | 1430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0462     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.4623936  |
| test/Q_plus_P                  | -1.4623936  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 57200       |
| train/episodes                 | 5720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.582       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00558    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 228800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 143        |
| stats_o/mean                   | 0.20463175 |
| stats_o/std                    | 0.07158627 |
| test/episodes                  | 1440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00329   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3485222 |
| test/Q_plus_P                  | -1.3485222 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 57600      |
| train/episodes                 | 5760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00301   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 230400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 144        |
| stats_o/mean                   | 0.20462829 |
| stats_o/std                    | 0.07144609 |
| test/episodes                  | 1450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00204   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3924748 |
| test/Q_plus_P                  | -1.3924748 |
| test/reward_per_eps            | -10        |
| test/steps                     | 58000      |
| train/episodes                 | 5800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00362   |
| train/info_shaping_reward_mean | -0.0757    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 232000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 145        |
| stats_o/mean                   | 0.20465061 |
| stats_o/std                    | 0.07150067 |
| test/episodes                  | 1460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00256   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.6090661 |
| test/Q_plus_P                  | -1.6090661 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 58400      |
| train/episodes                 | 5840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00536   |
| train/info_shaping_reward_mean | -0.0802    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 233600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 146        |
| stats_o/mean                   | 0.20467451 |
| stats_o/std                    | 0.07145109 |
| test/episodes                  | 1470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00326   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4188696 |
| test/Q_plus_P                  | -1.4188696 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 58800      |
| train/episodes                 | 5880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00468   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 235200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 147        |
| stats_o/mean                   | 0.20466766 |
| stats_o/std                    | 0.07137528 |
| test/episodes                  | 1480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0033    |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4403222 |
| test/Q_plus_P                  | -1.4403222 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 59200      |
| train/episodes                 | 5920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.528      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00412   |
| train/info_shaping_reward_mean | -0.0775    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 236800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 148         |
| stats_o/mean                   | 0.20469943  |
| stats_o/std                    | 0.071323425 |
| test/episodes                  | 1490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000604   |
| test/info_shaping_reward_mean  | -0.0475     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3798876  |
| test/Q_plus_P                  | -1.3798876  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 59600       |
| train/episodes                 | 5960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.511       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00506    |
| train/info_shaping_reward_mean | -0.0814     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.6       |
| train/steps                    | 238400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 149        |
| stats_o/mean                   | 0.20472512 |
| stats_o/std                    | 0.07120204 |
| test/episodes                  | 1500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00235   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2676854 |
| test/Q_plus_P                  | -1.2676854 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 60000      |
| train/episodes                 | 6000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00569   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 240000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 150        |
| stats_o/mean                   | 0.20473303 |
| stats_o/std                    | 0.07108441 |
| test/episodes                  | 1510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00262   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3904812 |
| test/Q_plus_P                  | -1.3904812 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 60400      |
| train/episodes                 | 6040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.53       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00454   |
| train/info_shaping_reward_mean | -0.0762    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 241600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 151        |
| stats_o/mean                   | 0.20475578 |
| stats_o/std                    | 0.07098362 |
| test/episodes                  | 1520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00306   |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4829739 |
| test/Q_plus_P                  | -1.4829739 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 60800      |
| train/episodes                 | 6080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00435   |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 243200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 152        |
| stats_o/mean                   | 0.20474114 |
| stats_o/std                    | 0.07093017 |
| test/episodes                  | 1530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00417   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.41645   |
| test/Q_plus_P                  | -1.41645   |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 61200      |
| train/episodes                 | 6120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.516      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00473   |
| train/info_shaping_reward_mean | -0.0827    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 244800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 153        |
| stats_o/mean                   | 0.20477265 |
| stats_o/std                    | 0.07098889 |
| test/episodes                  | 1540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00806   |
| test/info_shaping_reward_mean  | -0.0584    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.6981968 |
| test/Q_plus_P                  | -1.6981968 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 61600      |
| train/episodes                 | 6160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00441   |
| train/info_shaping_reward_mean | -0.0813    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 246400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.20481074 |
| stats_o/std                    | 0.07118662 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00472   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3013492 |
| test/Q_plus_P                  | -1.3013492 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.469      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00662   |
| train/info_shaping_reward_mean | -0.0975    |
| train/info_shaping_reward_min  | -0.287     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.20480669 |
| stats_o/std                    | 0.07112035 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00622   |
| test/info_shaping_reward_mean  | -0.0557    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6404933 |
| test/Q_plus_P                  | -1.6404933 |
| test/reward_per_eps            | -11        |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.498      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00503   |
| train/info_shaping_reward_mean | -0.0839    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 156        |
| stats_o/mean                   | 0.20480546 |
| stats_o/std                    | 0.07101497 |
| test/episodes                  | 1570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00636   |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5914632 |
| test/Q_plus_P                  | -1.5914632 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 62800      |
| train/episodes                 | 6280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.509      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00562   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 251200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 157         |
| stats_o/mean                   | 0.20481388  |
| stats_o/std                    | 0.070988454 |
| test/episodes                  | 1580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00173    |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.417997   |
| test/Q_plus_P                  | -1.417997   |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 63200       |
| train/episodes                 | 6320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.526       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00649    |
| train/info_shaping_reward_mean | -0.0832     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19         |
| train/steps                    | 252800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 158        |
| stats_o/mean                   | 0.20480742 |
| stats_o/std                    | 0.07089733 |
| test/episodes                  | 1590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00676   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4484084 |
| test/Q_plus_P                  | -1.4484084 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 63600      |
| train/episodes                 | 6360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.543      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00562   |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 254400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 159        |
| stats_o/mean                   | 0.20481035 |
| stats_o/std                    | 0.07080905 |
| test/episodes                  | 1600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00677   |
| test/info_shaping_reward_mean  | -0.0531    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3776573 |
| test/Q_plus_P                  | -1.3776573 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 64000      |
| train/episodes                 | 6400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.52       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00493   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 256000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 160         |
| stats_o/mean                   | 0.20480533  |
| stats_o/std                    | 0.070740506 |
| test/episodes                  | 1610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00701    |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.3869982  |
| test/Q_plus_P                  | -1.3869982  |
| test/reward_per_eps            | -10         |
| test/steps                     | 64400       |
| train/episodes                 | 6440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.555       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00688    |
| train/info_shaping_reward_mean | -0.0754     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 257600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.20480226  |
| stats_o/std                    | 0.070667855 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0118     |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3543183  |
| test/Q_plus_P                  | -1.3543183  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.558       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00578    |
| train/info_shaping_reward_mean | -0.0784     |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 162        |
| stats_o/mean                   | 0.20480475 |
| stats_o/std                    | 0.07052315 |
| test/episodes                  | 1630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00503   |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5044262 |
| test/Q_plus_P                  | -1.5044262 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 65200      |
| train/episodes                 | 6520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00584   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 260800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 163        |
| stats_o/mean                   | 0.20479046 |
| stats_o/std                    | 0.07038913 |
| test/episodes                  | 1640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0144    |
| test/info_shaping_reward_mean  | -0.0574    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3164873 |
| test/Q_plus_P                  | -1.3164873 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 65600      |
| train/episodes                 | 6560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.572      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0042    |
| train/info_shaping_reward_mean | -0.0749    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 262400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 164        |
| stats_o/mean                   | 0.20479712 |
| stats_o/std                    | 0.07049336 |
| test/episodes                  | 1650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00201   |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3601959 |
| test/Q_plus_P                  | -1.3601959 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 66000      |
| train/episodes                 | 6600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.515      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 264000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 165        |
| stats_o/mean                   | 0.20483291 |
| stats_o/std                    | 0.07047008 |
| test/episodes                  | 1660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00725   |
| test/info_shaping_reward_mean  | -0.0648    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.2669594 |
| test/Q_plus_P                  | -2.2669594 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 66400      |
| train/episodes                 | 6640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.567      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00372   |
| train/info_shaping_reward_mean | -0.0796    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 265600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 166        |
| stats_o/mean                   | 0.2048384  |
| stats_o/std                    | 0.07044078 |
| test/episodes                  | 1670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00603   |
| test/info_shaping_reward_mean  | -0.0601    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4104887 |
| test/Q_plus_P                  | -1.4104887 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 66800      |
| train/episodes                 | 6680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.522      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00613   |
| train/info_shaping_reward_mean | -0.0829    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 267200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.20483595 |
| stats_o/std                    | 0.07036092 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0047    |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4954617 |
| test/Q_plus_P                  | -1.4954617 |
| test/reward_per_eps            | -11        |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00576   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 168        |
| stats_o/mean                   | 0.20482716 |
| stats_o/std                    | 0.07027856 |
| test/episodes                  | 1690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00477   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3122455 |
| test/Q_plus_P                  | -1.3122455 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 67600      |
| train/episodes                 | 6760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.527      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.0823    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 270400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.2048214  |
| stats_o/std                    | 0.07014548 |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00478   |
| test/info_shaping_reward_mean  | -0.0595    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5747606 |
| test/Q_plus_P                  | -1.5747606 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00463   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 170        |
| stats_o/mean                   | 0.20481463 |
| stats_o/std                    | 0.07000534 |
| test/episodes                  | 1710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00418   |
| test/info_shaping_reward_mean  | -0.0571    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4341975 |
| test/Q_plus_P                  | -1.4341975 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 68400      |
| train/episodes                 | 6840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00535   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 273600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 171        |
| stats_o/mean                   | 0.20479919 |
| stats_o/std                    | 0.06991011 |
| test/episodes                  | 1720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00524   |
| test/info_shaping_reward_mean  | -0.0593    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4283533 |
| test/Q_plus_P                  | -1.4283533 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 68800      |
| train/episodes                 | 6880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0768    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 275200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 172         |
| stats_o/mean                   | 0.2047997   |
| stats_o/std                    | 0.069942996 |
| test/episodes                  | 1730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00593    |
| test/info_shaping_reward_mean  | -0.058      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.3385651  |
| test/Q_plus_P                  | -1.3385651  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 69200       |
| train/episodes                 | 6920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00458    |
| train/info_shaping_reward_mean | -0.0782     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 276800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 173         |
| stats_o/mean                   | 0.20481874  |
| stats_o/std                    | 0.069862396 |
| test/episodes                  | 1740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00384    |
| test/info_shaping_reward_mean  | -0.0582     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.4472113  |
| test/Q_plus_P                  | -1.4472113  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 69600       |
| train/episodes                 | 6960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00413    |
| train/info_shaping_reward_mean | -0.0696     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 278400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 174         |
| stats_o/mean                   | 0.20482644  |
| stats_o/std                    | 0.069782265 |
| test/episodes                  | 1750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00636    |
| test/info_shaping_reward_mean  | -0.0576     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3363     |
| test/Q_plus_P                  | -1.3363     |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 70000       |
| train/episodes                 | 7000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.526       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00434    |
| train/info_shaping_reward_mean | -0.0754     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 280000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 175        |
| stats_o/mean                   | 0.20483217 |
| stats_o/std                    | 0.06968744 |
| test/episodes                  | 1760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0112    |
| test/info_shaping_reward_mean  | -0.0611    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.392637  |
| test/Q_plus_P                  | -1.392637  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 70400      |
| train/episodes                 | 7040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00481   |
| train/info_shaping_reward_mean | -0.0732    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 281600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 176        |
| stats_o/mean                   | 0.20482534 |
| stats_o/std                    | 0.06966226 |
| test/episodes                  | 1770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0102    |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.208467  |
| test/Q_plus_P                  | -1.208467  |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 70800      |
| train/episodes                 | 7080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00397   |
| train/info_shaping_reward_mean | -0.0738    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 283200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.2048333   |
| stats_o/std                    | 0.069544055 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00784    |
| test/info_shaping_reward_mean  | -0.0606     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.4135889  |
| test/Q_plus_P                  | -1.4135889  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.57        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00395    |
| train/info_shaping_reward_mean | -0.0715     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.20483868 |
| stats_o/std                    | 0.06945424 |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00443   |
| test/info_shaping_reward_mean  | -0.0569    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.3616862 |
| test/Q_plus_P                  | -1.3616862 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00526   |
| train/info_shaping_reward_mean | -0.0756    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 179         |
| stats_o/mean                   | 0.20484093  |
| stats_o/std                    | 0.069374606 |
| test/episodes                  | 1800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00466    |
| test/info_shaping_reward_mean  | -0.0578     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3907214  |
| test/Q_plus_P                  | -1.3907214  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 72000       |
| train/episodes                 | 7200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00623    |
| train/info_shaping_reward_mean | -0.0708     |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 288000      |
------------------------------------------------
Saving latest policy.
---------------------------------------------
| epoch                          | 180      |
| stats_o/mean                   | 0.20484  |
| stats_o/std                    | 0.069308 |
| test/episodes                  | 1810     |
| test/info_is_success_max       | 1        |
| test/info_is_success_mean      | 0.78     |
| test/info_is_success_min       | 0        |
| test/info_shaping_reward_max   | -0.00569 |
| test/info_shaping_reward_mean  | -0.054   |
| test/info_shaping_reward_min   | -0.175   |
| test/Q                         | -1.23918 |
| test/Q_plus_P                  | -1.23918 |
| test/reward_per_eps            | -8.8     |
| test/steps                     | 72400    |
| train/episodes                 | 7240     |
| train/info_is_success_max      | 1        |
| train/info_is_success_mean     | 0.593    |
| train/info_is_success_min      | 0        |
| train/info_shaping_reward_max  | -0.00463 |
| train/info_shaping_reward_mean | -0.0811  |
| train/info_shaping_reward_min  | -0.255   |
| train/Q                        | nan      |
| train/Q_plus_P                 | nan      |
| train/reward_per_eps           | -16.3    |
| train/steps                    | 289600   |
---------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 181        |
| stats_o/mean                   | 0.20485298 |
| stats_o/std                    | 0.06933095 |
| test/episodes                  | 1820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00735   |
| test/info_shaping_reward_mean  | -0.0642    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.5392414 |
| test/Q_plus_P                  | -1.5392414 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 72800      |
| train/episodes                 | 7280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00628   |
| train/info_shaping_reward_mean | -0.0831    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 291200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 182        |
| stats_o/mean                   | 0.20484732 |
| stats_o/std                    | 0.06921117 |
| test/episodes                  | 1830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0113    |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3892202 |
| test/Q_plus_P                  | -1.3892202 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 73200      |
| train/episodes                 | 7320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00468   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 292800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 183        |
| stats_o/mean                   | 0.20484094 |
| stats_o/std                    | 0.06913242 |
| test/episodes                  | 1840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0112    |
| test/info_shaping_reward_mean  | -0.0574    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3131938 |
| test/Q_plus_P                  | -1.3131938 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 73600      |
| train/episodes                 | 7360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00487   |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 294400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 184         |
| stats_o/mean                   | 0.20483206  |
| stats_o/std                    | 0.069012955 |
| test/episodes                  | 1850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0123     |
| test/info_shaping_reward_mean  | -0.0651     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.5234756  |
| test/Q_plus_P                  | -1.5234756  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 74000       |
| train/episodes                 | 7400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.576       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00556    |
| train/info_shaping_reward_mean | -0.0708     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 296000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 185         |
| stats_o/mean                   | 0.20484078  |
| stats_o/std                    | 0.069039166 |
| test/episodes                  | 1860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00904    |
| test/info_shaping_reward_mean  | -0.0648     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.410724   |
| test/Q_plus_P                  | -1.410724   |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 74400       |
| train/episodes                 | 7440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.508       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00567    |
| train/info_shaping_reward_mean | -0.0831     |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.7       |
| train/steps                    | 297600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 186        |
| stats_o/mean                   | 0.20484215 |
| stats_o/std                    | 0.06898923 |
| test/episodes                  | 1870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0131    |
| test/info_shaping_reward_mean  | -0.0593    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2069598 |
| test/Q_plus_P                  | -1.2069598 |
| test/reward_per_eps            | -9         |
| test/steps                     | 74800      |
| train/episodes                 | 7480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00565   |
| train/info_shaping_reward_mean | -0.073     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 299200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.20483692 |
| stats_o/std                    | 0.06889071 |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00639   |
| test/info_shaping_reward_mean  | -0.0598    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3317931 |
| test/Q_plus_P                  | -1.3317931 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00552   |
| train/info_shaping_reward_mean | -0.0869    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 188        |
| stats_o/mean                   | 0.20483366 |
| stats_o/std                    | 0.06888769 |
| test/episodes                  | 1890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00521   |
| test/info_shaping_reward_mean  | -0.061     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5047797 |
| test/Q_plus_P                  | -1.5047797 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 75600      |
| train/episodes                 | 7560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00523   |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 302400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.2048386   |
| stats_o/std                    | 0.068788655 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.672       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00848    |
| test/info_shaping_reward_mean  | -0.0675     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.4521396  |
| test/Q_plus_P                  | -2.4521396  |
| test/reward_per_eps            | -13.1       |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00407    |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 190        |
| stats_o/mean                   | 0.20483288 |
| stats_o/std                    | 0.06878811 |
| test/episodes                  | 1910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.013     |
| test/info_shaping_reward_mean  | -0.063     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3354982 |
| test/Q_plus_P                  | -1.3354982 |
| test/reward_per_eps            | -10        |
| test/steps                     | 76400      |
| train/episodes                 | 7640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00383   |
| train/info_shaping_reward_mean | -0.0846    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 305600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 191        |
| stats_o/mean                   | 0.20483959 |
| stats_o/std                    | 0.0687315  |
| test/episodes                  | 1920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.011     |
| test/info_shaping_reward_mean  | -0.0589    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1409299 |
| test/Q_plus_P                  | -1.1409299 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 76800      |
| train/episodes                 | 7680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00663   |
| train/info_shaping_reward_mean | -0.0748    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 307200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 192        |
| stats_o/mean                   | 0.20484026 |
| stats_o/std                    | 0.06866909 |
| test/episodes                  | 1930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0153    |
| test/info_shaping_reward_mean  | -0.0578    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1230298 |
| test/Q_plus_P                  | -1.1230298 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 77200      |
| train/episodes                 | 7720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 308800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 193        |
| stats_o/mean                   | 0.20484708 |
| stats_o/std                    | 0.06868479 |
| test/episodes                  | 1940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.014     |
| test/info_shaping_reward_mean  | -0.063     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2204293 |
| test/Q_plus_P                  | -1.2204293 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 77600      |
| train/episodes                 | 7760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.563      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00371   |
| train/info_shaping_reward_mean | -0.0847    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 310400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 194         |
| stats_o/mean                   | 0.20485757  |
| stats_o/std                    | 0.068593115 |
| test/episodes                  | 1950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0123     |
| test/info_shaping_reward_mean  | -0.0601     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.36466    |
| test/Q_plus_P                  | -1.36466    |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 78000       |
| train/episodes                 | 7800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.527       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00538    |
| train/info_shaping_reward_mean | -0.0754     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 312000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 195        |
| stats_o/mean                   | 0.20486332 |
| stats_o/std                    | 0.06852448 |
| test/episodes                  | 1960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.014     |
| test/info_shaping_reward_mean  | -0.0565    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2485566 |
| test/Q_plus_P                  | -1.2485566 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 78400      |
| train/episodes                 | 7840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.558      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00552   |
| train/info_shaping_reward_mean | -0.0739    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 313600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 196        |
| stats_o/mean                   | 0.20484449 |
| stats_o/std                    | 0.06850317 |
| test/episodes                  | 1970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00758   |
| test/info_shaping_reward_mean  | -0.0625    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.241338  |
| test/Q_plus_P                  | -1.241338  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 78800      |
| train/episodes                 | 7880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.005     |
| train/info_shaping_reward_mean | -0.082     |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 315200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 197        |
| stats_o/mean                   | 0.20485045 |
| stats_o/std                    | 0.06841441 |
| test/episodes                  | 1980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0117    |
| test/info_shaping_reward_mean  | -0.0612    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3752273 |
| test/Q_plus_P                  | -1.3752273 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 79200      |
| train/episodes                 | 7920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00529   |
| train/info_shaping_reward_mean | -0.074     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 316800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 198        |
| stats_o/mean                   | 0.20484006 |
| stats_o/std                    | 0.06832061 |
| test/episodes                  | 1990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0103    |
| test/info_shaping_reward_mean  | -0.0582    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2343148 |
| test/Q_plus_P                  | -1.2343148 |
| test/reward_per_eps            | -9         |
| test/steps                     | 79600      |
| train/episodes                 | 7960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00491   |
| train/info_shaping_reward_mean | -0.0653    |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 318400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.20483713 |
| stats_o/std                    | 0.06825358 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00951   |
| test/info_shaping_reward_mean  | -0.0608    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3878254 |
| test/Q_plus_P                  | -1.3878254 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00453   |
| train/info_shaping_reward_mean | -0.0746    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 200        |
| stats_o/mean                   | 0.20484214 |
| stats_o/std                    | 0.06818745 |
| test/episodes                  | 2010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00766   |
| test/info_shaping_reward_mean  | -0.0601    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2584033 |
| test/Q_plus_P                  | -1.2584033 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 80400      |
| train/episodes                 | 8040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 321600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 201        |
| stats_o/mean                   | 0.20485987 |
| stats_o/std                    | 0.06817687 |
| test/episodes                  | 2020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00927   |
| test/info_shaping_reward_mean  | -0.0614    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4424577 |
| test/Q_plus_P                  | -1.4424577 |
| test/reward_per_eps            | -10        |
| test/steps                     | 80800      |
| train/episodes                 | 8080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00464   |
| train/info_shaping_reward_mean | -0.0781    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 323200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 202        |
| stats_o/mean                   | 0.2048538  |
| stats_o/std                    | 0.06809614 |
| test/episodes                  | 2030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0102    |
| test/info_shaping_reward_mean  | -0.0617    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3522367 |
| test/Q_plus_P                  | -1.3522367 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 81200      |
| train/episodes                 | 8120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00504   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 324800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 203        |
| stats_o/mean                   | 0.20486021 |
| stats_o/std                    | 0.06806307 |
| test/episodes                  | 2040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0161    |
| test/info_shaping_reward_mean  | -0.0565    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2649919 |
| test/Q_plus_P                  | -1.2649919 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 81600      |
| train/episodes                 | 8160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.575      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00317   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 326400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 204        |
| stats_o/mean                   | 0.20487688 |
| stats_o/std                    | 0.06800177 |
| test/episodes                  | 2050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0143    |
| test/info_shaping_reward_mean  | -0.062     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4298925 |
| test/Q_plus_P                  | -1.4298925 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 82000      |
| train/episodes                 | 8200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0061    |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 328000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 205        |
| stats_o/mean                   | 0.20486769 |
| stats_o/std                    | 0.06794404 |
| test/episodes                  | 2060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00703   |
| test/info_shaping_reward_mean  | -0.0629    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4561934 |
| test/Q_plus_P                  | -1.4561934 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 82400      |
| train/episodes                 | 8240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00401   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 329600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 206        |
| stats_o/mean                   | 0.20485559 |
| stats_o/std                    | 0.06785032 |
| test/episodes                  | 2070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00692   |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2741499 |
| test/Q_plus_P                  | -1.2741499 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 82800      |
| train/episodes                 | 8280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00415   |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 331200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.20484658 |
| stats_o/std                    | 0.06781823 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0138    |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1088661 |
| test/Q_plus_P                  | -1.1088661 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00437   |
| train/info_shaping_reward_mean | -0.0693    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 208        |
| stats_o/mean                   | 0.20485325 |
| stats_o/std                    | 0.06783967 |
| test/episodes                  | 2090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0141    |
| test/info_shaping_reward_mean  | -0.0602    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2372278 |
| test/Q_plus_P                  | -1.2372278 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 83600      |
| train/episodes                 | 8360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.518      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00593   |
| train/info_shaping_reward_mean | -0.0908    |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 334400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 209         |
| stats_o/mean                   | 0.20484836  |
| stats_o/std                    | 0.067806765 |
| test/episodes                  | 2100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0125     |
| test/info_shaping_reward_mean  | -0.0558     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.176556   |
| test/Q_plus_P                  | -1.176556   |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 84000       |
| train/episodes                 | 8400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00463    |
| train/info_shaping_reward_mean | -0.0689     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 336000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 210        |
| stats_o/mean                   | 0.20484646 |
| stats_o/std                    | 0.06774419 |
| test/episodes                  | 2110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0163    |
| test/info_shaping_reward_mean  | -0.0622    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.3217534 |
| test/Q_plus_P                  | -1.3217534 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 84400      |
| train/episodes                 | 8440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00572   |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 337600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 211        |
| stats_o/mean                   | 0.20485196 |
| stats_o/std                    | 0.06768128 |
| test/episodes                  | 2120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0143    |
| test/info_shaping_reward_mean  | -0.0571    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2165958 |
| test/Q_plus_P                  | -1.2165958 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 84800      |
| train/episodes                 | 8480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00445   |
| train/info_shaping_reward_mean | -0.0684    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 339200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 212         |
| stats_o/mean                   | 0.20485741  |
| stats_o/std                    | 0.067612216 |
| test/episodes                  | 2130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0129     |
| test/info_shaping_reward_mean  | -0.0612     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.299908   |
| test/Q_plus_P                  | -1.299908   |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 85200       |
| train/episodes                 | 8520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.567       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00445    |
| train/info_shaping_reward_mean | -0.0727     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 340800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 213         |
| stats_o/mean                   | 0.20487937  |
| stats_o/std                    | 0.067640334 |
| test/episodes                  | 2140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0127     |
| test/info_shaping_reward_mean  | -0.0589     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.377292   |
| test/Q_plus_P                  | -1.377292   |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 85600       |
| train/episodes                 | 8560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.524       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.005      |
| train/info_shaping_reward_mean | -0.0922     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19         |
| train/steps                    | 342400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 214        |
| stats_o/mean                   | 0.20487882 |
| stats_o/std                    | 0.06778699 |
| test/episodes                  | 2150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00956   |
| test/info_shaping_reward_mean  | -0.0568    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2069916 |
| test/Q_plus_P                  | -1.2069916 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 86000      |
| train/episodes                 | 8600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.503      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00586   |
| train/info_shaping_reward_mean | -0.0871    |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 344000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 215        |
| stats_o/mean                   | 0.2048807  |
| stats_o/std                    | 0.06776167 |
| test/episodes                  | 2160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0116    |
| test/info_shaping_reward_mean  | -0.0636    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3066863 |
| test/Q_plus_P                  | -1.3066863 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 86400      |
| train/episodes                 | 8640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00768   |
| train/info_shaping_reward_mean | -0.0747    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 345600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 216        |
| stats_o/mean                   | 0.20487717 |
| stats_o/std                    | 0.06768197 |
| test/episodes                  | 2170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00945   |
| test/info_shaping_reward_mean  | -0.0604    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3326766 |
| test/Q_plus_P                  | -1.3326766 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 86800      |
| train/episodes                 | 8680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00447   |
| train/info_shaping_reward_mean | -0.066     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 347200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 217        |
| stats_o/mean                   | 0.20488648 |
| stats_o/std                    | 0.0676421  |
| test/episodes                  | 2180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.011     |
| test/info_shaping_reward_mean  | -0.0619    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2529155 |
| test/Q_plus_P                  | -1.2529155 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 87200      |
| train/episodes                 | 8720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.0723    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 348800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 218        |
| stats_o/mean                   | 0.20489073 |
| stats_o/std                    | 0.06755553 |
| test/episodes                  | 2190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0159    |
| test/info_shaping_reward_mean  | -0.0606    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2610115 |
| test/Q_plus_P                  | -1.2610115 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 87600      |
| train/episodes                 | 8760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00479   |
| train/info_shaping_reward_mean | -0.0688    |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 350400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.20487678  |
| stats_o/std                    | 0.067528464 |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0139     |
| test/info_shaping_reward_mean  | -0.062      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2675117  |
| test/Q_plus_P                  | -1.2675117  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00517    |
| train/info_shaping_reward_mean | -0.0762     |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 220         |
| stats_o/mean                   | 0.20487358  |
| stats_o/std                    | 0.067511395 |
| test/episodes                  | 2210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00594    |
| test/info_shaping_reward_mean  | -0.0573     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.104585   |
| test/Q_plus_P                  | -1.104585   |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 88400       |
| train/episodes                 | 8840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.561       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00542    |
| train/info_shaping_reward_mean | -0.0807     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 353600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 221        |
| stats_o/mean                   | 0.20486966 |
| stats_o/std                    | 0.06740456 |
| test/episodes                  | 2220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.018     |
| test/info_shaping_reward_mean  | -0.0591    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2824827 |
| test/Q_plus_P                  | -1.2824827 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 88800      |
| train/episodes                 | 8880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00462   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 355200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 222         |
| stats_o/mean                   | 0.20486265  |
| stats_o/std                    | 0.067402475 |
| test/episodes                  | 2230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0142     |
| test/info_shaping_reward_mean  | -0.0593     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.3005579  |
| test/Q_plus_P                  | -1.3005579  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 89200       |
| train/episodes                 | 8920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.572       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00482    |
| train/info_shaping_reward_mean | -0.0744     |
| train/info_shaping_reward_min  | -0.219      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 356800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 223        |
| stats_o/mean                   | 0.20485601 |
| stats_o/std                    | 0.06748386 |
| test/episodes                  | 2240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0127    |
| test/info_shaping_reward_mean  | -0.0655    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.8926132 |
| test/Q_plus_P                  | -2.8926132 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 89600      |
| train/episodes                 | 8960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00654   |
| train/info_shaping_reward_mean | -0.0928    |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 358400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.20485142 |
| stats_o/std                    | 0.06761213 |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0102    |
| test/info_shaping_reward_mean  | -0.0615    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.3474092 |
| test/Q_plus_P                  | -1.3474092 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.57       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00524   |
| train/info_shaping_reward_mean | -0.085     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 225        |
| stats_o/mean                   | 0.204854   |
| stats_o/std                    | 0.06755271 |
| test/episodes                  | 2260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00647   |
| test/info_shaping_reward_mean  | -0.0625    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.302423  |
| test/Q_plus_P                  | -1.302423  |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 90400      |
| train/episodes                 | 9040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00512   |
| train/info_shaping_reward_mean | -0.0754    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 361600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 226        |
| stats_o/mean                   | 0.20483738 |
| stats_o/std                    | 0.06761202 |
| test/episodes                  | 2270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00421   |
| test/info_shaping_reward_mean  | -0.0586    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1738528 |
| test/Q_plus_P                  | -1.1738528 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 90800      |
| train/episodes                 | 9080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.541      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00607   |
| train/info_shaping_reward_mean | -0.0745    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 363200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 227        |
| stats_o/mean                   | 0.20483482 |
| stats_o/std                    | 0.06760911 |
| test/episodes                  | 2280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.016     |
| test/info_shaping_reward_mean  | -0.0628    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2522522 |
| test/Q_plus_P                  | -1.2522522 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 91200      |
| train/episodes                 | 9120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.528      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00472   |
| train/info_shaping_reward_mean | -0.0803    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 364800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 228        |
| stats_o/mean                   | 0.20483467 |
| stats_o/std                    | 0.0675103  |
| test/episodes                  | 2290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00257   |
| test/info_shaping_reward_mean  | -0.0575    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2150507 |
| test/Q_plus_P                  | -1.2150507 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 91600      |
| train/episodes                 | 9160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00579   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 366400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 229         |
| stats_o/mean                   | 0.2048283   |
| stats_o/std                    | 0.067495726 |
| test/episodes                  | 2300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00898    |
| test/info_shaping_reward_mean  | -0.0592     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1042209  |
| test/Q_plus_P                  | -1.1042209  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 92000       |
| train/episodes                 | 9200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.602       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00473    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 368000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 230        |
| stats_o/mean                   | 0.2048211  |
| stats_o/std                    | 0.06750784 |
| test/episodes                  | 2310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00804   |
| test/info_shaping_reward_mean  | -0.0585    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1013924 |
| test/Q_plus_P                  | -1.1013924 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 92400      |
| train/episodes                 | 9240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.528      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00556   |
| train/info_shaping_reward_mean | -0.0808    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 369600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 231        |
| stats_o/mean                   | 0.20480843 |
| stats_o/std                    | 0.06745236 |
| test/episodes                  | 2320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0135    |
| test/info_shaping_reward_mean  | -0.0568    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0919181 |
| test/Q_plus_P                  | -1.0919181 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 92800      |
| train/episodes                 | 9280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00593   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 371200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 232        |
| stats_o/mean                   | 0.2048098  |
| stats_o/std                    | 0.06736462 |
| test/episodes                  | 2330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0121    |
| test/info_shaping_reward_mean  | -0.0626    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.2963204 |
| test/Q_plus_P                  | -1.2963204 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 93200      |
| train/episodes                 | 9320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00452   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 372800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 233        |
| stats_o/mean                   | 0.20481534 |
| stats_o/std                    | 0.06733913 |
| test/episodes                  | 2340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0116    |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0857097 |
| test/Q_plus_P                  | -1.0857097 |
| test/reward_per_eps            | -9         |
| test/steps                     | 93600      |
| train/episodes                 | 9360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00486   |
| train/info_shaping_reward_mean | -0.0753    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 374400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.20481035 |
| stats_o/std                    | 0.06725179 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00985   |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3164415 |
| test/Q_plus_P                  | -1.3164415 |
| test/reward_per_eps            | -10        |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00528   |
| train/info_shaping_reward_mean | -0.0656    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.20480695  |
| stats_o/std                    | 0.067271054 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0116     |
| test/info_shaping_reward_mean  | -0.0587     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.3540562  |
| test/Q_plus_P                  | -1.3540562  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.551       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0052     |
| train/info_shaping_reward_mean | -0.0789     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 236        |
| stats_o/mean                   | 0.20481558 |
| stats_o/std                    | 0.06726644 |
| test/episodes                  | 2370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00399   |
| test/info_shaping_reward_mean  | -0.0593    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2743593 |
| test/Q_plus_P                  | -1.2743593 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 94800      |
| train/episodes                 | 9480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00602   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 379200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 237        |
| stats_o/mean                   | 0.20482427 |
| stats_o/std                    | 0.06727124 |
| test/episodes                  | 2380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00361   |
| test/info_shaping_reward_mean  | -0.0564    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1870449 |
| test/Q_plus_P                  | -1.1870449 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 95200      |
| train/episodes                 | 9520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.005     |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 380800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 238         |
| stats_o/mean                   | 0.20482269  |
| stats_o/std                    | 0.067226194 |
| test/episodes                  | 2390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0178     |
| test/info_shaping_reward_mean  | -0.0585     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.2132188  |
| test/Q_plus_P                  | -1.2132188  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 95600       |
| train/episodes                 | 9560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.539       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00575    |
| train/info_shaping_reward_mean | -0.0765     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 382400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 239        |
| stats_o/mean                   | 0.20482653 |
| stats_o/std                    | 0.06715853 |
| test/episodes                  | 2400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0118    |
| test/info_shaping_reward_mean  | -0.0571    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2786921 |
| test/Q_plus_P                  | -1.2786921 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 96000      |
| train/episodes                 | 9600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00544   |
| train/info_shaping_reward_mean | -0.0683    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 384000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 240        |
| stats_o/mean                   | 0.20483029 |
| stats_o/std                    | 0.06706803 |
| test/episodes                  | 2410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00624   |
| test/info_shaping_reward_mean  | -0.0596    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2390758 |
| test/Q_plus_P                  | -1.2390758 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 96400      |
| train/episodes                 | 9640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00432   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 385600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 241        |
| stats_o/mean                   | 0.20482317 |
| stats_o/std                    | 0.06701848 |
| test/episodes                  | 2420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00826   |
| test/info_shaping_reward_mean  | -0.0642    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4886796 |
| test/Q_plus_P                  | -1.4886796 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 96800      |
| train/episodes                 | 9680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00629   |
| train/info_shaping_reward_mean | -0.07      |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 387200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 242        |
| stats_o/mean                   | 0.2048256  |
| stats_o/std                    | 0.06697319 |
| test/episodes                  | 2430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0124    |
| test/info_shaping_reward_mean  | -0.0604    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.255525  |
| test/Q_plus_P                  | -1.255525  |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 97200      |
| train/episodes                 | 9720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00481   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 388800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 243         |
| stats_o/mean                   | 0.20482497  |
| stats_o/std                    | 0.066946775 |
| test/episodes                  | 2440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00658    |
| test/info_shaping_reward_mean  | -0.0559     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.13487    |
| test/Q_plus_P                  | -1.13487    |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 97600       |
| train/episodes                 | 9760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.592       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00577    |
| train/info_shaping_reward_mean | -0.0692     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 390400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.20482928 |
| stats_o/std                    | 0.06693836 |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00621   |
| test/info_shaping_reward_mean  | -0.0584    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0935359 |
| test/Q_plus_P                  | -1.0935359 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00575   |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 245        |
| stats_o/mean                   | 0.20482814 |
| stats_o/std                    | 0.0668805  |
| test/episodes                  | 2460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00803   |
| test/info_shaping_reward_mean  | -0.0567    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1251206 |
| test/Q_plus_P                  | -1.1251206 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 98400      |
| train/episodes                 | 9840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00465   |
| train/info_shaping_reward_mean | -0.0639    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 393600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 246        |
| stats_o/mean                   | 0.2048293  |
| stats_o/std                    | 0.06680428 |
| test/episodes                  | 2470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00935   |
| test/info_shaping_reward_mean  | -0.0564    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2101657 |
| test/Q_plus_P                  | -1.2101657 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 98800      |
| train/episodes                 | 9880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00546   |
| train/info_shaping_reward_mean | -0.0654    |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 395200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 247        |
| stats_o/mean                   | 0.20483728 |
| stats_o/std                    | 0.06675246 |
| test/episodes                  | 2480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00564   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1418359 |
| test/Q_plus_P                  | -1.1418359 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 99200      |
| train/episodes                 | 9920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00641   |
| train/info_shaping_reward_mean | -0.078     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 396800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.20484    |
| stats_o/std                    | 0.06671375 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00811   |
| test/info_shaping_reward_mean  | -0.0587    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3547586 |
| test/Q_plus_P                  | -1.3547586 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00586   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 249        |
| stats_o/mean                   | 0.20483045 |
| stats_o/std                    | 0.06666282 |
| test/episodes                  | 2500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.012     |
| test/info_shaping_reward_mean  | -0.0594    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.318475  |
| test/Q_plus_P                  | -1.318475  |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 100000     |
| train/episodes                 | 10000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00384   |
| train/info_shaping_reward_mean | -0.0749    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 400000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 250        |
| stats_o/mean                   | 0.20483029 |
| stats_o/std                    | 0.06659858 |
| test/episodes                  | 2510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00404   |
| test/info_shaping_reward_mean  | -0.0592    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4655945 |
| test/Q_plus_P                  | -1.4655945 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 100400     |
| train/episodes                 | 10040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.595      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0058    |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 401600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 251        |
| stats_o/mean                   | 0.2048438  |
| stats_o/std                    | 0.06660401 |
| test/episodes                  | 2520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00682   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1911134 |
| test/Q_plus_P                  | -1.1911134 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 100800     |
| train/episodes                 | 10080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00464   |
| train/info_shaping_reward_mean | -0.0794    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 403200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 252        |
| stats_o/mean                   | 0.20484494 |
| stats_o/std                    | 0.06658083 |
| test/episodes                  | 2530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00976   |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.119589  |
| test/Q_plus_P                  | -1.119589  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 101200     |
| train/episodes                 | 10120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.587      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00316   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 404800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.2048493   |
| stats_o/std                    | 0.066572525 |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00709    |
| test/info_shaping_reward_mean  | -0.0568     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.160253   |
| test/Q_plus_P                  | -1.160253   |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.573       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00475    |
| train/info_shaping_reward_mean | -0.0772     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 254        |
| stats_o/mean                   | 0.20485072 |
| stats_o/std                    | 0.06650377 |
| test/episodes                  | 2550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00842   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0546901 |
| test/Q_plus_P                  | -1.0546901 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 102000     |
| train/episodes                 | 10200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00561   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 408000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 255         |
| stats_o/mean                   | 0.20486458  |
| stats_o/std                    | 0.066526115 |
| test/episodes                  | 2560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0109     |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2285774  |
| test/Q_plus_P                  | -1.2285774  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 102400      |
| train/episodes                 | 10240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.536       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00345    |
| train/info_shaping_reward_mean | -0.0765     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 409600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 256         |
| stats_o/mean                   | 0.20484695  |
| stats_o/std                    | 0.066588335 |
| test/episodes                  | 2570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0072     |
| test/info_shaping_reward_mean  | -0.0556     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2185311  |
| test/Q_plus_P                  | -1.2185311  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 102800      |
| train/episodes                 | 10280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00447    |
| train/info_shaping_reward_mean | -0.0863     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 411200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 257        |
| stats_o/mean                   | 0.2048542  |
| stats_o/std                    | 0.06660573 |
| test/episodes                  | 2580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00902   |
| test/info_shaping_reward_mean  | -0.0568    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0340275 |
| test/Q_plus_P                  | -1.0340275 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 103200     |
| train/episodes                 | 10320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00565   |
| train/info_shaping_reward_mean | -0.0845    |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 412800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 258        |
| stats_o/mean                   | 0.20485947 |
| stats_o/std                    | 0.06654638 |
| test/episodes                  | 2590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.014     |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0748638 |
| test/Q_plus_P                  | -1.0748638 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 103600     |
| train/episodes                 | 10360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00583   |
| train/info_shaping_reward_mean | -0.0693    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 414400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 259        |
| stats_o/mean                   | 0.20485394 |
| stats_o/std                    | 0.06652972 |
| test/episodes                  | 2600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0164    |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0054982 |
| test/Q_plus_P                  | -1.0054982 |
| test/reward_per_eps            | -8         |
| test/steps                     | 104000     |
| train/episodes                 | 10400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00416   |
| train/info_shaping_reward_mean | -0.0788    |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 416000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 260        |
| stats_o/mean                   | 0.20485733 |
| stats_o/std                    | 0.06650677 |
| test/episodes                  | 2610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00567   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1278957 |
| test/Q_plus_P                  | -1.1278957 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 104400     |
| train/episodes                 | 10440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00462   |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 417600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 261        |
| stats_o/mean                   | 0.2048419  |
| stats_o/std                    | 0.06650518 |
| test/episodes                  | 2620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00942   |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2302773 |
| test/Q_plus_P                  | -1.2302773 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 104800     |
| train/episodes                 | 10480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00376   |
| train/info_shaping_reward_mean | -0.0724    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 419200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 262        |
| stats_o/mean                   | 0.20485386 |
| stats_o/std                    | 0.06645914 |
| test/episodes                  | 2630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00361   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.082335  |
| test/Q_plus_P                  | -1.082335  |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 105200     |
| train/episodes                 | 10520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00368   |
| train/info_shaping_reward_mean | -0.0674    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 420800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 263        |
| stats_o/mean                   | 0.20485754 |
| stats_o/std                    | 0.06639722 |
| test/episodes                  | 2640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00892   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0452815 |
| test/Q_plus_P                  | -1.0452815 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 105600     |
| train/episodes                 | 10560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00488   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 422400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 264        |
| stats_o/mean                   | 0.20485196 |
| stats_o/std                    | 0.06634672 |
| test/episodes                  | 2650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00623   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1611762 |
| test/Q_plus_P                  | -1.1611762 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 106000     |
| train/episodes                 | 10600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00577   |
| train/info_shaping_reward_mean | -0.0715    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 424000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 265        |
| stats_o/mean                   | 0.20484439 |
| stats_o/std                    | 0.06628077 |
| test/episodes                  | 2660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00445   |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.144934  |
| test/Q_plus_P                  | -1.144934  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 106400     |
| train/episodes                 | 10640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.59       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00548   |
| train/info_shaping_reward_mean | -0.0713    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 425600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.20485409  |
| stats_o/std                    | 0.066227905 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00491    |
| test/info_shaping_reward_mean  | -0.0598     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.249597   |
| test/Q_plus_P                  | -1.249597   |
| test/reward_per_eps            | -10         |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00436    |
| train/info_shaping_reward_mean | -0.0635     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 267         |
| stats_o/mean                   | 0.20484829  |
| stats_o/std                    | 0.066238366 |
| test/episodes                  | 2680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0168     |
| test/info_shaping_reward_mean  | -0.0571     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1701102  |
| test/Q_plus_P                  | -1.1701102  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 107200      |
| train/episodes                 | 10720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00507    |
| train/info_shaping_reward_mean | -0.0789     |
| train/info_shaping_reward_min  | -0.211      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 428800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.2048463   |
| stats_o/std                    | 0.066263594 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00552    |
| test/info_shaping_reward_mean  | -0.0556     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1986252  |
| test/Q_plus_P                  | -1.1986252  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00454    |
| train/info_shaping_reward_mean | -0.0727     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 269        |
| stats_o/mean                   | 0.20483421 |
| stats_o/std                    | 0.0662052  |
| test/episodes                  | 2700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0147    |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1362524 |
| test/Q_plus_P                  | -1.1362524 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 108000     |
| train/episodes                 | 10800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00529   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 432000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.2048279   |
| stats_o/std                    | 0.066185094 |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00937    |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.129802   |
| test/Q_plus_P                  | -1.129802   |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.56        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00595    |
| train/info_shaping_reward_mean | -0.0758     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 271        |
| stats_o/mean                   | 0.20481169 |
| stats_o/std                    | 0.06615812 |
| test/episodes                  | 2720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00326   |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2248533 |
| test/Q_plus_P                  | -1.2248533 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 108800     |
| train/episodes                 | 10880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.005     |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 435200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.20481734  |
| stats_o/std                    | 0.066098936 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0104     |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1577567  |
| test/Q_plus_P                  | -1.1577567  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00562    |
| train/info_shaping_reward_mean | -0.0711     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 273         |
| stats_o/mean                   | 0.20481518  |
| stats_o/std                    | 0.066020705 |
| test/episodes                  | 2740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00309    |
| test/info_shaping_reward_mean  | -0.0605     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.3623662  |
| test/Q_plus_P                  | -1.3623662  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 109600      |
| train/episodes                 | 10960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00438    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 438400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 274        |
| stats_o/mean                   | 0.20481423 |
| stats_o/std                    | 0.06602259 |
| test/episodes                  | 2750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0101    |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1632874 |
| test/Q_plus_P                  | -1.1632874 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 110000     |
| train/episodes                 | 11000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00454   |
| train/info_shaping_reward_mean | -0.0851    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 440000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 275        |
| stats_o/mean                   | 0.2048124  |
| stats_o/std                    | 0.06602879 |
| test/episodes                  | 2760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0183    |
| test/info_shaping_reward_mean  | -0.0547    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.110525  |
| test/Q_plus_P                  | -1.110525  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 110400     |
| train/episodes                 | 11040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00513   |
| train/info_shaping_reward_mean | -0.0755    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 441600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 276        |
| stats_o/mean                   | 0.20481573 |
| stats_o/std                    | 0.06598525 |
| test/episodes                  | 2770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0134    |
| test/info_shaping_reward_mean  | -0.0623    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.397037  |
| test/Q_plus_P                  | -1.397037  |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 110800     |
| train/episodes                 | 11080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0044    |
| train/info_shaping_reward_mean | -0.073     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 443200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 277        |
| stats_o/mean                   | 0.20481333 |
| stats_o/std                    | 0.06594792 |
| test/episodes                  | 2780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.013     |
| test/info_shaping_reward_mean  | -0.0605    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3500993 |
| test/Q_plus_P                  | -1.3500993 |
| test/reward_per_eps            | -10        |
| test/steps                     | 111200     |
| train/episodes                 | 11120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00423   |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 444800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 278        |
| stats_o/mean                   | 0.20481186 |
| stats_o/std                    | 0.06593338 |
| test/episodes                  | 2790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00411   |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1192529 |
| test/Q_plus_P                  | -1.1192529 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 111600     |
| train/episodes                 | 11160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00583   |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 446400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 279        |
| stats_o/mean                   | 0.20480669 |
| stats_o/std                    | 0.06587835 |
| test/episodes                  | 2800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0178    |
| test/info_shaping_reward_mean  | -0.0574    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1302431 |
| test/Q_plus_P                  | -1.1302431 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 112000     |
| train/episodes                 | 11200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 448000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 280        |
| stats_o/mean                   | 0.20480642 |
| stats_o/std                    | 0.06583626 |
| test/episodes                  | 2810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00864   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0610088 |
| test/Q_plus_P                  | -1.0610088 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 112400     |
| train/episodes                 | 11240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00372   |
| train/info_shaping_reward_mean | -0.0652    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 449600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.20480141 |
| stats_o/std                    | 0.06579553 |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00919   |
| test/info_shaping_reward_mean  | -0.0565    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3263102 |
| test/Q_plus_P                  | -1.3263102 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00518   |
| train/info_shaping_reward_mean | -0.0755    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.2048017  |
| stats_o/std                    | 0.06573768 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00688   |
| test/info_shaping_reward_mean  | -0.0563    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.3008043 |
| test/Q_plus_P                  | -1.3008043 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00398   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.20479752 |
| stats_o/std                    | 0.06574382 |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.675      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0131    |
| test/info_shaping_reward_mean  | -0.0728    |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -2.8794155 |
| test/Q_plus_P                  | -2.8794155 |
| test/reward_per_eps            | -13        |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.563      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00523   |
| train/info_shaping_reward_mean | -0.0746    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 284        |
| stats_o/mean                   | 0.20479433 |
| stats_o/std                    | 0.06568369 |
| test/episodes                  | 2850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0117    |
| test/info_shaping_reward_mean  | -0.0632    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3934582 |
| test/Q_plus_P                  | -1.3934582 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 114000     |
| train/episodes                 | 11400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0055    |
| train/info_shaping_reward_mean | -0.0719    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 456000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 285        |
| stats_o/mean                   | 0.20479344 |
| stats_o/std                    | 0.06561307 |
| test/episodes                  | 2860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0117    |
| test/info_shaping_reward_mean  | -0.0581    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.206979  |
| test/Q_plus_P                  | -1.206979  |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 114400     |
| train/episodes                 | 11440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.652      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00452   |
| train/info_shaping_reward_mean | -0.0647    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 457600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 286        |
| stats_o/mean                   | 0.20479792 |
| stats_o/std                    | 0.06556344 |
| test/episodes                  | 2870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00674   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1111943 |
| test/Q_plus_P                  | -1.1111943 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 114800     |
| train/episodes                 | 11480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00555   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 459200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.20479207 |
| stats_o/std                    | 0.06551593 |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00288   |
| test/info_shaping_reward_mean  | -0.0611    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1478362 |
| test/Q_plus_P                  | -1.1478362 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.657      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00526   |
| train/info_shaping_reward_mean | -0.0644    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 288         |
| stats_o/mean                   | 0.20480321  |
| stats_o/std                    | 0.065536164 |
| test/episodes                  | 2890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0216     |
| test/info_shaping_reward_mean  | -0.0639     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2695785  |
| test/Q_plus_P                  | -1.2695785  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 115600      |
| train/episodes                 | 11560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00476    |
| train/info_shaping_reward_mean | -0.0729     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 462400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 289        |
| stats_o/mean                   | 0.2048019  |
| stats_o/std                    | 0.06549606 |
| test/episodes                  | 2900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00674   |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0510263 |
| test/Q_plus_P                  | -1.0510263 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 116000     |
| train/episodes                 | 11600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00437   |
| train/info_shaping_reward_mean | -0.0658    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 464000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 290        |
| stats_o/mean                   | 0.20480871 |
| stats_o/std                    | 0.06546687 |
| test/episodes                  | 2910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0121    |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0495682 |
| test/Q_plus_P                  | -1.0495682 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 116400     |
| train/episodes                 | 11640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00406   |
| train/info_shaping_reward_mean | -0.0688    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 465600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 291        |
| stats_o/mean                   | 0.20480388 |
| stats_o/std                    | 0.06541781 |
| test/episodes                  | 2920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0169    |
| test/info_shaping_reward_mean  | -0.0601    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.198276  |
| test/Q_plus_P                  | -1.198276  |
| test/reward_per_eps            | -9         |
| test/steps                     | 116800     |
| train/episodes                 | 11680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 467200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 292         |
| stats_o/mean                   | 0.2048056   |
| stats_o/std                    | 0.065355614 |
| test/episodes                  | 2930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0056     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.1663308  |
| test/Q_plus_P                  | -1.1663308  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 117200      |
| train/episodes                 | 11720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.62        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00684    |
| train/info_shaping_reward_mean | -0.0676     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 468800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 293        |
| stats_o/mean                   | 0.20480181 |
| stats_o/std                    | 0.06531178 |
| test/episodes                  | 2940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0128    |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2010753 |
| test/Q_plus_P                  | -1.2010753 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 117600     |
| train/episodes                 | 11760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00593   |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 470400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 294        |
| stats_o/mean                   | 0.2047837  |
| stats_o/std                    | 0.06529999 |
| test/episodes                  | 2950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0148    |
| test/info_shaping_reward_mean  | -0.057     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1664742 |
| test/Q_plus_P                  | -1.1664742 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 118000     |
| train/episodes                 | 11800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.0664    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 472000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 295         |
| stats_o/mean                   | 0.2047923   |
| stats_o/std                    | 0.065297715 |
| test/episodes                  | 2960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00541    |
| test/info_shaping_reward_mean  | -0.0527     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.2010988  |
| test/Q_plus_P                  | -1.2010988  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 118400      |
| train/episodes                 | 11840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00414    |
| train/info_shaping_reward_mean | -0.0798     |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 473600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 296         |
| stats_o/mean                   | 0.20479093  |
| stats_o/std                    | 0.065316856 |
| test/episodes                  | 2970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00368    |
| test/info_shaping_reward_mean  | -0.0546     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.1231362  |
| test/Q_plus_P                  | -1.1231362  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 118800      |
| train/episodes                 | 11880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00596    |
| train/info_shaping_reward_mean | -0.0835     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 475200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 297        |
| stats_o/mean                   | 0.20478734 |
| stats_o/std                    | 0.06530712 |
| test/episodes                  | 2980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0122    |
| test/info_shaping_reward_mean  | -0.0611    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.3011783 |
| test/Q_plus_P                  | -1.3011783 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 119200     |
| train/episodes                 | 11920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00512   |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 476800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 298        |
| stats_o/mean                   | 0.20478964 |
| stats_o/std                    | 0.0652477  |
| test/episodes                  | 2990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00121   |
| test/info_shaping_reward_mean  | -0.0581    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1113179 |
| test/Q_plus_P                  | -1.1113179 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 119600     |
| train/episodes                 | 11960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00522   |
| train/info_shaping_reward_mean | -0.0627    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 478400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 299        |
| stats_o/mean                   | 0.2047855  |
| stats_o/std                    | 0.06521883 |
| test/episodes                  | 3000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0153    |
| test/info_shaping_reward_mean  | -0.0586    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2131114 |
| test/Q_plus_P                  | -1.2131114 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 120000     |
| train/episodes                 | 12000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00344   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 480000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.20479392  |
| stats_o/std                    | 0.065176226 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0184     |
| test/info_shaping_reward_mean  | -0.0612     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.2215766  |
| test/Q_plus_P                  | -1.2215766  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00605    |
| train/info_shaping_reward_mean | -0.0716     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.20479517 |
| stats_o/std                    | 0.06517774 |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0146    |
| test/info_shaping_reward_mean  | -0.0574    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1296768 |
| test/Q_plus_P                  | -1.1296768 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.575      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00721   |
| train/info_shaping_reward_mean | -0.0745    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 302        |
| stats_o/mean                   | 0.20479167 |
| stats_o/std                    | 0.06513486 |
| test/episodes                  | 3030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0181    |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.265957  |
| test/Q_plus_P                  | -1.265957  |
| test/reward_per_eps            | -9         |
| test/steps                     | 121200     |
| train/episodes                 | 12120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00436   |
| train/info_shaping_reward_mean | -0.0775    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 484800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 303        |
| stats_o/mean                   | 0.2047896  |
| stats_o/std                    | 0.06509339 |
| test/episodes                  | 3040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0119    |
| test/info_shaping_reward_mean  | -0.058     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.0733948 |
| test/Q_plus_P                  | -1.0733948 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 121600     |
| train/episodes                 | 12160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00567   |
| train/info_shaping_reward_mean | -0.0654    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 486400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 304        |
| stats_o/mean                   | 0.20478678 |
| stats_o/std                    | 0.06504971 |
| test/episodes                  | 3050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0108    |
| test/info_shaping_reward_mean  | -0.0606    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.251695  |
| test/Q_plus_P                  | -1.251695  |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 122000     |
| train/episodes                 | 12200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00403   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 488000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.20478642 |
| stats_o/std                    | 0.06504346 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0128    |
| test/info_shaping_reward_mean  | -0.0549    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0217047 |
| test/Q_plus_P                  | -1.0217047 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00553   |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 306        |
| stats_o/mean                   | 0.20478682 |
| stats_o/std                    | 0.06498884 |
| test/episodes                  | 3070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00629   |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2974237 |
| test/Q_plus_P                  | -1.2974237 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 122800     |
| train/episodes                 | 12280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00557   |
| train/info_shaping_reward_mean | -0.0655    |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 491200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.20478596 |
| stats_o/std                    | 0.06496699 |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0141    |
| test/info_shaping_reward_mean  | -0.0575    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.1542445 |
| test/Q_plus_P                  | -1.1542445 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00374   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 308         |
| stats_o/mean                   | 0.20478469  |
| stats_o/std                    | 0.064975224 |
| test/episodes                  | 3090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0174     |
| test/info_shaping_reward_mean  | -0.057      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.1322653  |
| test/Q_plus_P                  | -1.1322653  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 123600      |
| train/episodes                 | 12360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00635    |
| train/info_shaping_reward_mean | -0.0735     |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 494400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 309        |
| stats_o/mean                   | 0.20479184 |
| stats_o/std                    | 0.06497173 |
| test/episodes                  | 3100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00749   |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1710836 |
| test/Q_plus_P                  | -1.1710836 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 124000     |
| train/episodes                 | 12400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.645      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.0632    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 496000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.2047966   |
| stats_o/std                    | 0.064908445 |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00847    |
| test/info_shaping_reward_mean  | -0.0586     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.3325124  |
| test/Q_plus_P                  | -1.3325124  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00392    |
| train/info_shaping_reward_mean | -0.0682     |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.20479168 |
| stats_o/std                    | 0.06485178 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00933   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2299972 |
| test/Q_plus_P                  | -1.2299972 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.65       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00484   |
| train/info_shaping_reward_mean | -0.0651    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 312        |
| stats_o/mean                   | 0.20478737 |
| stats_o/std                    | 0.06492203 |
| test/episodes                  | 3130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00818   |
| test/info_shaping_reward_mean  | -0.0577    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0929502 |
| test/Q_plus_P                  | -1.0929502 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 125200     |
| train/episodes                 | 12520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00702   |
| train/info_shaping_reward_mean | -0.0815    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 500800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 313        |
| stats_o/mean                   | 0.20479007 |
| stats_o/std                    | 0.06485506 |
| test/episodes                  | 3140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.015     |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1199591 |
| test/Q_plus_P                  | -1.1199591 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 125600     |
| train/episodes                 | 12560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.655      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00568   |
| train/info_shaping_reward_mean | -0.0648    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 502400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 314        |
| stats_o/mean                   | 0.20479019 |
| stats_o/std                    | 0.06481904 |
| test/episodes                  | 3150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0148    |
| test/info_shaping_reward_mean  | -0.0656    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4958167 |
| test/Q_plus_P                  | -1.4958167 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 126000     |
| train/episodes                 | 12600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.657      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00691   |
| train/info_shaping_reward_mean | -0.0647    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 504000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 315        |
| stats_o/mean                   | 0.20478693 |
| stats_o/std                    | 0.06478915 |
| test/episodes                  | 3160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0104    |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1823754 |
| test/Q_plus_P                  | -1.1823754 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 126400     |
| train/episodes                 | 12640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.63       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00511   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 505600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.2047897   |
| stats_o/std                    | 0.064740665 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.014      |
| test/info_shaping_reward_mean  | -0.0563     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1519666  |
| test/Q_plus_P                  | -1.1519666  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.005      |
| train/info_shaping_reward_mean | -0.0663     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 317         |
| stats_o/mean                   | 0.20478752  |
| stats_o/std                    | 0.064741254 |
| test/episodes                  | 3180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0133     |
| test/info_shaping_reward_mean  | -0.0572     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1064804  |
| test/Q_plus_P                  | -1.1064804  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 127200      |
| train/episodes                 | 12720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0057     |
| train/info_shaping_reward_mean | -0.0742     |
| train/info_shaping_reward_min  | -0.198      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 508800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 318        |
| stats_o/mean                   | 0.20479347 |
| stats_o/std                    | 0.0647299  |
| test/episodes                  | 3190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0126    |
| test/info_shaping_reward_mean  | -0.0645    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2298069 |
| test/Q_plus_P                  | -1.2298069 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 127600     |
| train/episodes                 | 12760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00706   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 510400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 319        |
| stats_o/mean                   | 0.20480852 |
| stats_o/std                    | 0.06479669 |
| test/episodes                  | 3200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0167    |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0637525 |
| test/Q_plus_P                  | -1.0637525 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 128000     |
| train/episodes                 | 12800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.555      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00596   |
| train/info_shaping_reward_mean | -0.0782    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 512000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 320        |
| stats_o/mean                   | 0.20480938 |
| stats_o/std                    | 0.06474399 |
| test/episodes                  | 3210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00809   |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0210578 |
| test/Q_plus_P                  | -1.0210578 |
| test/reward_per_eps            | -8         |
| test/steps                     | 128400     |
| train/episodes                 | 12840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.67       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.0628    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 513600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 321        |
| stats_o/mean                   | 0.20482276 |
| stats_o/std                    | 0.06474485 |
| test/episodes                  | 3220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0117    |
| test/info_shaping_reward_mean  | -0.0682    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.8156576 |
| test/Q_plus_P                  | -1.8156576 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 128800     |
| train/episodes                 | 12880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00325   |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 515200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 322        |
| stats_o/mean                   | 0.20482393 |
| stats_o/std                    | 0.06474369 |
| test/episodes                  | 3230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00649   |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1208633 |
| test/Q_plus_P                  | -1.1208633 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 129200     |
| train/episodes                 | 12920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00641   |
| train/info_shaping_reward_mean | -0.0739    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 516800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 323        |
| stats_o/mean                   | 0.20482507 |
| stats_o/std                    | 0.06475572 |
| test/episodes                  | 3240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0213    |
| test/info_shaping_reward_mean  | -0.0609    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0796105 |
| test/Q_plus_P                  | -1.0796105 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 129600     |
| train/episodes                 | 12960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.555      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00598   |
| train/info_shaping_reward_mean | -0.0708    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 518400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 324        |
| stats_o/mean                   | 0.20480898 |
| stats_o/std                    | 0.06477902 |
| test/episodes                  | 3250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00608   |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.037354  |
| test/Q_plus_P                  | -1.037354  |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 130000     |
| train/episodes                 | 13000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0051    |
| train/info_shaping_reward_mean | -0.0727    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 520000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 325        |
| stats_o/mean                   | 0.204816   |
| stats_o/std                    | 0.06478241 |
| test/episodes                  | 3260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0124    |
| test/info_shaping_reward_mean  | -0.0603    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.247521  |
| test/Q_plus_P                  | -1.247521  |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 130400     |
| train/episodes                 | 13040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00577   |
| train/info_shaping_reward_mean | -0.0761    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 521600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 326        |
| stats_o/mean                   | 0.20482379 |
| stats_o/std                    | 0.0647993  |
| test/episodes                  | 3270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0115    |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1593462 |
| test/Q_plus_P                  | -1.1593462 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 130800     |
| train/episodes                 | 13080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00513   |
| train/info_shaping_reward_mean | -0.0723    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 523200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 327        |
| stats_o/mean                   | 0.20482446 |
| stats_o/std                    | 0.06480924 |
| test/episodes                  | 3280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00525   |
| test/info_shaping_reward_mean  | -0.0591    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3061612 |
| test/Q_plus_P                  | -1.3061612 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 131200     |
| train/episodes                 | 13120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00583   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 524800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 328        |
| stats_o/mean                   | 0.20483056 |
| stats_o/std                    | 0.06483375 |
| test/episodes                  | 3290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00306   |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0548866 |
| test/Q_plus_P                  | -1.0548866 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 131600     |
| train/episodes                 | 13160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00494   |
| train/info_shaping_reward_mean | -0.0825    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 526400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 329        |
| stats_o/mean                   | 0.20483312 |
| stats_o/std                    | 0.06481737 |
| test/episodes                  | 3300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0165    |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1324221 |
| test/Q_plus_P                  | -1.1324221 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 132000     |
| train/episodes                 | 13200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00514   |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 528000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 330        |
| stats_o/mean                   | 0.2048447  |
| stats_o/std                    | 0.06478076 |
| test/episodes                  | 3310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00812   |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0451915 |
| test/Q_plus_P                  | -1.0451915 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 132400     |
| train/episodes                 | 13240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00733   |
| train/info_shaping_reward_mean | -0.0681    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 529600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 331        |
| stats_o/mean                   | 0.20484892 |
| stats_o/std                    | 0.06473498 |
| test/episodes                  | 3320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00557   |
| test/info_shaping_reward_mean  | -0.0547    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0174268 |
| test/Q_plus_P                  | -1.0174268 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 132800     |
| train/episodes                 | 13280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00673   |
| train/info_shaping_reward_mean | -0.0732    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 531200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 332        |
| stats_o/mean                   | 0.20485263 |
| stats_o/std                    | 0.06469288 |
| test/episodes                  | 3330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0139    |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0762705 |
| test/Q_plus_P                  | -1.0762705 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 133200     |
| train/episodes                 | 13320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00699   |
| train/info_shaping_reward_mean | -0.068     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 532800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 333         |
| stats_o/mean                   | 0.20485497  |
| stats_o/std                    | 0.064690016 |
| test/episodes                  | 3340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00504    |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1154065  |
| test/Q_plus_P                  | -1.1154065  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 133600      |
| train/episodes                 | 13360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00762    |
| train/info_shaping_reward_mean | -0.0807     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 534400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 334        |
| stats_o/mean                   | 0.2048414  |
| stats_o/std                    | 0.06466711 |
| test/episodes                  | 3350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.012     |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1854241 |
| test/Q_plus_P                  | -1.1854241 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 134000     |
| train/episodes                 | 13400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00509   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 536000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.20484523 |
| stats_o/std                    | 0.06469012 |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.014     |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1992283 |
| test/Q_plus_P                  | -1.1992283 |
| test/reward_per_eps            | -9         |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.587      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00737   |
| train/info_shaping_reward_mean | -0.0845    |
| train/info_shaping_reward_min  | -0.29      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.20484291  |
| stats_o/std                    | 0.064679936 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00989    |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.0228753  |
| test/Q_plus_P                  | -1.0228753  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00592    |
| train/info_shaping_reward_mean | -0.0705     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 337        |
| stats_o/mean                   | 0.20484495 |
| stats_o/std                    | 0.06465082 |
| test/episodes                  | 3380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00403   |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1350317 |
| test/Q_plus_P                  | -1.1350317 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 135200     |
| train/episodes                 | 13520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00665   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 540800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.20484602 |
| stats_o/std                    | 0.06461486 |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00618   |
| test/info_shaping_reward_mean  | -0.0581    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2426372 |
| test/Q_plus_P                  | -1.2426372 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00571   |
| train/info_shaping_reward_mean | -0.0667    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.20485216  |
| stats_o/std                    | 0.064627916 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0166     |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.073559   |
| test/Q_plus_P                  | -1.073559   |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00842    |
| train/info_shaping_reward_mean | -0.0768     |
| train/info_shaping_reward_min  | -0.201      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 340         |
| stats_o/mean                   | 0.20486546  |
| stats_o/std                    | 0.064647436 |
| test/episodes                  | 3410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0136     |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.2900652  |
| test/Q_plus_P                  | -1.2900652  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 136400      |
| train/episodes                 | 13640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.574       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00662    |
| train/info_shaping_reward_mean | -0.0716     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 545600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 341        |
| stats_o/mean                   | 0.20486069 |
| stats_o/std                    | 0.06460958 |
| test/episodes                  | 3420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.613      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00173   |
| test/info_shaping_reward_mean  | -0.113     |
| test/info_shaping_reward_min   | -0.531     |
| test/Q                         | -5.9873815 |
| test/Q_plus_P                  | -5.9873815 |
| test/reward_per_eps            | -15.5      |
| test/steps                     | 136800     |
| train/episodes                 | 13680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00594   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 547200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 342         |
| stats_o/mean                   | 0.20485835  |
| stats_o/std                    | 0.064579085 |
| test/episodes                  | 3430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.01       |
| test/info_shaping_reward_mean  | -0.0553     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2281446  |
| test/Q_plus_P                  | -1.2281446  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 137200      |
| train/episodes                 | 13720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00492    |
| train/info_shaping_reward_mean | -0.0756     |
| train/info_shaping_reward_min  | -0.206      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 548800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 343        |
| stats_o/mean                   | 0.20485413 |
| stats_o/std                    | 0.06453941 |
| test/episodes                  | 3440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0109    |
| test/info_shaping_reward_mean  | -0.0843    |
| test/info_shaping_reward_min   | -0.522     |
| test/Q                         | -3.4196131 |
| test/Q_plus_P                  | -3.4196131 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 137600     |
| train/episodes                 | 13760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00494   |
| train/info_shaping_reward_mean | -0.0634    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 550400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 344        |
| stats_o/mean                   | 0.2048543  |
| stats_o/std                    | 0.06460435 |
| test/episodes                  | 3450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0131    |
| test/info_shaping_reward_mean  | -0.0549    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0888267 |
| test/Q_plus_P                  | -1.0888267 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 138000     |
| train/episodes                 | 13800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00814   |
| train/info_shaping_reward_mean | -0.0853    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 552000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.20485267  |
| stats_o/std                    | 0.064557254 |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00923    |
| test/info_shaping_reward_mean  | -0.0563     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.3200693  |
| test/Q_plus_P                  | -1.3200693  |
| test/reward_per_eps            | -10         |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00576    |
| train/info_shaping_reward_mean | -0.0666     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 346         |
| stats_o/mean                   | 0.20486706  |
| stats_o/std                    | 0.064615496 |
| test/episodes                  | 3470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.012      |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.1244763  |
| test/Q_plus_P                  | -1.1244763  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 138800      |
| train/episodes                 | 13880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.572       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00478    |
| train/info_shaping_reward_mean | -0.0795     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 555200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 347        |
| stats_o/mean                   | 0.20486571 |
| stats_o/std                    | 0.06462193 |
| test/episodes                  | 3480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00895   |
| test/info_shaping_reward_mean  | -0.0531    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9640152 |
| test/Q_plus_P                  | -0.9640152 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 139200     |
| train/episodes                 | 13920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00755   |
| train/info_shaping_reward_mean | -0.0793    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 556800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.20486335  |
| stats_o/std                    | 0.064669855 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0182     |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.2091932  |
| test/Q_plus_P                  | -1.2091932  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00799    |
| train/info_shaping_reward_mean | -0.0871     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 349         |
| stats_o/mean                   | 0.20487519  |
| stats_o/std                    | 0.064702414 |
| test/episodes                  | 3500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0123     |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1139067  |
| test/Q_plus_P                  | -1.1139067  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 140000      |
| train/episodes                 | 14000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00675    |
| train/info_shaping_reward_mean | -0.0862     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 560000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 350         |
| stats_o/mean                   | 0.20487215  |
| stats_o/std                    | 0.064698584 |
| test/episodes                  | 3510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0109     |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0474093  |
| test/Q_plus_P                  | -1.0474093  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 140400      |
| train/episodes                 | 14040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00739    |
| train/info_shaping_reward_mean | -0.0783     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 561600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 351        |
| stats_o/mean                   | 0.20488364 |
| stats_o/std                    | 0.06466051 |
| test/episodes                  | 3520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0089    |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1843218 |
| test/Q_plus_P                  | -1.1843218 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 140800     |
| train/episodes                 | 14080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00714   |
| train/info_shaping_reward_mean | -0.0684    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 563200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 352         |
| stats_o/mean                   | 0.20488411  |
| stats_o/std                    | 0.06460477  |
| test/episodes                  | 3530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0128     |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.92725545 |
| test/Q_plus_P                  | -0.92725545 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 141200      |
| train/episodes                 | 14120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00592    |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 564800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 353        |
| stats_o/mean                   | 0.20487316 |
| stats_o/std                    | 0.06466974 |
| test/episodes                  | 3540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0141    |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0497473 |
| test/Q_plus_P                  | -1.0497473 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 141600     |
| train/episodes                 | 14160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.501      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00819   |
| train/info_shaping_reward_mean | -0.0865    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 566400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 354        |
| stats_o/mean                   | 0.20487267 |
| stats_o/std                    | 0.06471875 |
| test/episodes                  | 3550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00838   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0946105 |
| test/Q_plus_P                  | -1.0946105 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 142000     |
| train/episodes                 | 14200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00583   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 568000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 355        |
| stats_o/mean                   | 0.20487636 |
| stats_o/std                    | 0.06468996 |
| test/episodes                  | 3560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00846   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0794415 |
| test/Q_plus_P                  | -1.0794415 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 142400     |
| train/episodes                 | 14240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0063    |
| train/info_shaping_reward_mean | -0.0652    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 569600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 356        |
| stats_o/mean                   | 0.2048793  |
| stats_o/std                    | 0.06467218 |
| test/episodes                  | 3570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00533   |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0793078 |
| test/Q_plus_P                  | -1.0793078 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 142800     |
| train/episodes                 | 14280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00722   |
| train/info_shaping_reward_mean | -0.072     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 571200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 357         |
| stats_o/mean                   | 0.20488349  |
| stats_o/std                    | 0.064645335 |
| test/episodes                  | 3580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0107     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1682425  |
| test/Q_plus_P                  | -1.1682425  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 143200      |
| train/episodes                 | 14320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0054     |
| train/info_shaping_reward_mean | -0.072      |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 572800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 358        |
| stats_o/mean                   | 0.2048914  |
| stats_o/std                    | 0.06468591 |
| test/episodes                  | 3590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.014     |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.700561  |
| test/Q_plus_P                  | -1.700561  |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 143600     |
| train/episodes                 | 14360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00496   |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 574400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.204878    |
| stats_o/std                    | 0.064653456 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0217     |
| test/info_shaping_reward_mean  | -0.0572     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1462742  |
| test/Q_plus_P                  | -1.1462742  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00556    |
| train/info_shaping_reward_mean | -0.0716     |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 360        |
| stats_o/mean                   | 0.20487681 |
| stats_o/std                    | 0.06463486 |
| test/episodes                  | 3610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0125    |
| test/info_shaping_reward_mean  | -0.0574    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1757057 |
| test/Q_plus_P                  | -1.1757057 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 144400     |
| train/episodes                 | 14440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00641   |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 577600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 361        |
| stats_o/mean                   | 0.20488207 |
| stats_o/std                    | 0.06461447 |
| test/episodes                  | 3620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0122    |
| test/info_shaping_reward_mean  | -0.0565    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0665443 |
| test/Q_plus_P                  | -1.0665443 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 144800     |
| train/episodes                 | 14480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00708   |
| train/info_shaping_reward_mean | -0.0767    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 579200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 362        |
| stats_o/mean                   | 0.20487463 |
| stats_o/std                    | 0.06463614 |
| test/episodes                  | 3630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0176    |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9960419 |
| test/Q_plus_P                  | -0.9960419 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 145200     |
| train/episodes                 | 14520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00769   |
| train/info_shaping_reward_mean | -0.0814    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 580800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.204882    |
| stats_o/std                    | 0.064607844 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0106     |
| test/info_shaping_reward_mean  | -0.0564     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1875304  |
| test/Q_plus_P                  | -1.1875304  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00591    |
| train/info_shaping_reward_mean | -0.0678     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 364        |
| stats_o/mean                   | 0.2048851  |
| stats_o/std                    | 0.06462028 |
| test/episodes                  | 3650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0119    |
| test/info_shaping_reward_mean  | -0.0596    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3498596 |
| test/Q_plus_P                  | -1.3498596 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 146000     |
| train/episodes                 | 14600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0056    |
| train/info_shaping_reward_mean | -0.0787    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 584000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 365        |
| stats_o/mean                   | 0.20488098 |
| stats_o/std                    | 0.06458369 |
| test/episodes                  | 3660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00613   |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0137458 |
| test/Q_plus_P                  | -1.0137458 |
| test/reward_per_eps            | -8         |
| test/steps                     | 146400     |
| train/episodes                 | 14640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00826   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 585600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 366        |
| stats_o/mean                   | 0.20488127 |
| stats_o/std                    | 0.06455625 |
| test/episodes                  | 3670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0137    |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.1978312 |
| test/Q_plus_P                  | -1.1978312 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 146800     |
| train/episodes                 | 14680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00516   |
| train/info_shaping_reward_mean | -0.0638    |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 587200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.20488092  |
| stats_o/std                    | 0.064503744 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0009     |
| test/info_shaping_reward_mean  | -0.0569     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.3931998  |
| test/Q_plus_P                  | -1.3931998  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00827    |
| train/info_shaping_reward_mean | -0.0671     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 368        |
| stats_o/mean                   | 0.2048736  |
| stats_o/std                    | 0.0645568  |
| test/episodes                  | 3690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0134    |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1552973 |
| test/Q_plus_P                  | -1.1552973 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 147600     |
| train/episodes                 | 14760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00537   |
| train/info_shaping_reward_mean | -0.0761    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 590400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 369        |
| stats_o/mean                   | 0.20487893 |
| stats_o/std                    | 0.06456482 |
| test/episodes                  | 3700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0119    |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1290218 |
| test/Q_plus_P                  | -1.1290218 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 148000     |
| train/episodes                 | 14800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00617   |
| train/info_shaping_reward_mean | -0.0717    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 592000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 370        |
| stats_o/mean                   | 0.20487998 |
| stats_o/std                    | 0.06458396 |
| test/episodes                  | 3710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0199    |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2025197 |
| test/Q_plus_P                  | -1.2025197 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 148400     |
| train/episodes                 | 14840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00566   |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 593600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 371         |
| stats_o/mean                   | 0.2048788   |
| stats_o/std                    | 0.064594686 |
| test/episodes                  | 3720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0187     |
| test/info_shaping_reward_mean  | -0.0556     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1555327  |
| test/Q_plus_P                  | -1.1555327  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 148800      |
| train/episodes                 | 14880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.6         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00584    |
| train/info_shaping_reward_mean | -0.0714     |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 595200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 372        |
| stats_o/mean                   | 0.2048697  |
| stats_o/std                    | 0.06458812 |
| test/episodes                  | 3730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00291   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0982155 |
| test/Q_plus_P                  | -1.0982155 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 149200     |
| train/episodes                 | 14920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00887   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 596800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 373         |
| stats_o/mean                   | 0.20486975  |
| stats_o/std                    | 0.064563826 |
| test/episodes                  | 3740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0144     |
| test/info_shaping_reward_mean  | -0.0582     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1529452  |
| test/Q_plus_P                  | -1.1529452  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 149600      |
| train/episodes                 | 14960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00696    |
| train/info_shaping_reward_mean | -0.0709     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 598400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.20486298 |
| stats_o/std                    | 0.06453895 |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0141    |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1715763 |
| test/Q_plus_P                  | -1.1715763 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00757   |
| train/info_shaping_reward_mean | -0.0681    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 375        |
| stats_o/mean                   | 0.20486568 |
| stats_o/std                    | 0.06453368 |
| test/episodes                  | 3760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0172    |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0993278 |
| test/Q_plus_P                  | -1.0993278 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 150400     |
| train/episodes                 | 15040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00782   |
| train/info_shaping_reward_mean | -0.0769    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 601600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.20486382 |
| stats_o/std                    | 0.0644922  |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0082    |
| test/info_shaping_reward_mean  | -0.0563    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1278212 |
| test/Q_plus_P                  | -1.1278212 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00548   |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.20487225  |
| stats_o/std                    | 0.064463794 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0161     |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0545735  |
| test/Q_plus_P                  | -1.0545735  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.574       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00958    |
| train/info_shaping_reward_mean | -0.0714     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 378        |
| stats_o/mean                   | 0.20487955 |
| stats_o/std                    | 0.06443227 |
| test/episodes                  | 3790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00237   |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.195365  |
| test/Q_plus_P                  | -1.195365  |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 151600     |
| train/episodes                 | 15160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 606400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 379         |
| stats_o/mean                   | 0.20489337  |
| stats_o/std                    | 0.06440248  |
| test/episodes                  | 3800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.01       |
| test/info_shaping_reward_mean  | -0.0514     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.95060396 |
| test/Q_plus_P                  | -0.95060396 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 152000      |
| train/episodes                 | 15200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.589       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00584    |
| train/info_shaping_reward_mean | -0.0709     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 608000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.20487724  |
| stats_o/std                    | 0.064449035 |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0135     |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1482055  |
| test/Q_plus_P                  | -1.1482055  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00867    |
| train/info_shaping_reward_mean | -0.0788     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 381        |
| stats_o/mean                   | 0.2048786  |
| stats_o/std                    | 0.06441325 |
| test/episodes                  | 3820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0135    |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9829951 |
| test/Q_plus_P                  | -0.9829951 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 152800     |
| train/episodes                 | 15280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.647      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00716   |
| train/info_shaping_reward_mean | -0.0652    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 611200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.20487644 |
| stats_o/std                    | 0.06438816 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0117    |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9326907 |
| test/Q_plus_P                  | -0.9326907 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00697   |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 383        |
| stats_o/mean                   | 0.20488118 |
| stats_o/std                    | 0.06443388 |
| test/episodes                  | 3840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0135    |
| test/info_shaping_reward_mean  | -0.0585    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2878381 |
| test/Q_plus_P                  | -1.2878381 |
| test/reward_per_eps            | -10        |
| test/steps                     | 153600     |
| train/episodes                 | 15360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00773   |
| train/info_shaping_reward_mean | -0.0884    |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 614400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 384        |
| stats_o/mean                   | 0.20488085 |
| stats_o/std                    | 0.06443134 |
| test/episodes                  | 3850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0101    |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0060052 |
| test/Q_plus_P                  | -1.0060052 |
| test/reward_per_eps            | -8         |
| test/steps                     | 154000     |
| train/episodes                 | 15400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.63       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00695   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 616000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 385         |
| stats_o/mean                   | 0.2048867   |
| stats_o/std                    | 0.064403854 |
| test/episodes                  | 3860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0182     |
| test/info_shaping_reward_mean  | -0.0574     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1708717  |
| test/Q_plus_P                  | -1.1708717  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 154400      |
| train/episodes                 | 15440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00543    |
| train/info_shaping_reward_mean | -0.0697     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 617600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 386         |
| stats_o/mean                   | 0.20489632  |
| stats_o/std                    | 0.064419515 |
| test/episodes                  | 3870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0121     |
| test/info_shaping_reward_mean  | -0.0554     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1058525  |
| test/Q_plus_P                  | -1.1058525  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 154800      |
| train/episodes                 | 15480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00595    |
| train/info_shaping_reward_mean | -0.076      |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 619200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 387        |
| stats_o/mean                   | 0.20490324 |
| stats_o/std                    | 0.06444235 |
| test/episodes                  | 3880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00718   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.258885  |
| test/Q_plus_P                  | -1.258885  |
| test/reward_per_eps            | -9         |
| test/steps                     | 155200     |
| train/episodes                 | 15520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00596   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 620800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 388        |
| stats_o/mean                   | 0.20490246 |
| stats_o/std                    | 0.06442057 |
| test/episodes                  | 3890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0127    |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0490319 |
| test/Q_plus_P                  | -1.0490319 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 155600     |
| train/episodes                 | 15560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00491   |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 622400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 389         |
| stats_o/mean                   | 0.20490885  |
| stats_o/std                    | 0.064416006 |
| test/episodes                  | 3900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00935    |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1210033  |
| test/Q_plus_P                  | -1.1210033  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 156000      |
| train/episodes                 | 15600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00585    |
| train/info_shaping_reward_mean | -0.0703     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 624000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 390         |
| stats_o/mean                   | 0.20491001  |
| stats_o/std                    | 0.064433224 |
| test/episodes                  | 3910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00228    |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2371361  |
| test/Q_plus_P                  | -1.2371361  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 156400      |
| train/episodes                 | 15640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.572       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00743    |
| train/info_shaping_reward_mean | -0.0773     |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 625600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 391        |
| stats_o/mean                   | 0.20491366 |
| stats_o/std                    | 0.06440959 |
| test/episodes                  | 3920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0107    |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.185946  |
| test/Q_plus_P                  | -1.185946  |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 156800     |
| train/episodes                 | 15680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00775   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 627200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 392        |
| stats_o/mean                   | 0.20492083 |
| stats_o/std                    | 0.06441202 |
| test/episodes                  | 3930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0107    |
| test/info_shaping_reward_mean  | -0.0515    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0213077 |
| test/Q_plus_P                  | -1.0213077 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 157200     |
| train/episodes                 | 15720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00538   |
| train/info_shaping_reward_mean | -0.0749    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 628800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 393        |
| stats_o/mean                   | 0.20490846 |
| stats_o/std                    | 0.06441954 |
| test/episodes                  | 3940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0153    |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0567272 |
| test/Q_plus_P                  | -1.0567272 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 157600     |
| train/episodes                 | 15760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00667   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 630400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 394        |
| stats_o/mean                   | 0.2049086  |
| stats_o/std                    | 0.06439271 |
| test/episodes                  | 3950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.01      |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1338677 |
| test/Q_plus_P                  | -1.1338677 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 158000     |
| train/episodes                 | 15800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00482   |
| train/info_shaping_reward_mean | -0.0762    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 632000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 395         |
| stats_o/mean                   | 0.20490928  |
| stats_o/std                    | 0.064347856 |
| test/episodes                  | 3960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00413    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0524251  |
| test/Q_plus_P                  | -1.0524251  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 158400      |
| train/episodes                 | 15840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00587    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 633600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 396         |
| stats_o/mean                   | 0.20490429  |
| stats_o/std                    | 0.064331576 |
| test/episodes                  | 3970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.01       |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2751198  |
| test/Q_plus_P                  | -1.2751198  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 158800      |
| train/episodes                 | 15880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00508    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 635200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.20490505  |
| stats_o/std                    | 0.064287804 |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00232    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.068526   |
| test/Q_plus_P                  | -1.068526   |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00739    |
| train/info_shaping_reward_mean | -0.0667     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 398        |
| stats_o/mean                   | 0.20490754 |
| stats_o/std                    | 0.06424248 |
| test/episodes                  | 3990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0025    |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.080707  |
| test/Q_plus_P                  | -1.080707  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 159600     |
| train/episodes                 | 15960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00589   |
| train/info_shaping_reward_mean | -0.066     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 638400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 399         |
| stats_o/mean                   | 0.20492058  |
| stats_o/std                    | 0.064280726 |
| test/episodes                  | 4000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00901    |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1923946  |
| test/Q_plus_P                  | -1.1923946  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 160000      |
| train/episodes                 | 16000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00576    |
| train/info_shaping_reward_mean | -0.0787     |
| train/info_shaping_reward_min  | -0.219      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 640000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 400        |
| stats_o/mean                   | 0.20492512 |
| stats_o/std                    | 0.0642584  |
| test/episodes                  | 4010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0109    |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0026013 |
| test/Q_plus_P                  | -1.0026013 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 160400     |
| train/episodes                 | 16040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00698   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 641600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 401         |
| stats_o/mean                   | 0.20492777  |
| stats_o/std                    | 0.06420891  |
| test/episodes                  | 4020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00896    |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.94072914 |
| test/Q_plus_P                  | -0.94072914 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 160800      |
| train/episodes                 | 16080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00428    |
| train/info_shaping_reward_mean | -0.0656     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 643200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.20492008 |
| stats_o/std                    | 0.06419729 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0077    |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1323112 |
| test/Q_plus_P                  | -1.1323112 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00499   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 403        |
| stats_o/mean                   | 0.20491368 |
| stats_o/std                    | 0.06426511 |
| test/episodes                  | 4040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0117    |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.164816  |
| test/Q_plus_P                  | -1.164816  |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 161600     |
| train/episodes                 | 16160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.0812    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 646400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.20491084  |
| stats_o/std                    | 0.064237654 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00418    |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1528753  |
| test/Q_plus_P                  | -1.1528753  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0069     |
| train/info_shaping_reward_mean | -0.0689     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 405        |
| stats_o/mean                   | 0.20491156 |
| stats_o/std                    | 0.06424089 |
| test/episodes                  | 4060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00384   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3551538 |
| test/Q_plus_P                  | -1.3551538 |
| test/reward_per_eps            | -10        |
| test/steps                     | 162400     |
| train/episodes                 | 16240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.007     |
| train/info_shaping_reward_mean | -0.0784    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 649600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 406        |
| stats_o/mean                   | 0.20491913 |
| stats_o/std                    | 0.06423716 |
| test/episodes                  | 4070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00789   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2856703 |
| test/Q_plus_P                  | -1.2856703 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 162800     |
| train/episodes                 | 16280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0061    |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 651200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.2049184   |
| stats_o/std                    | 0.064228386 |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00566    |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1950663  |
| test/Q_plus_P                  | -1.1950663  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.595       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00449    |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 408         |
| stats_o/mean                   | 0.20491795  |
| stats_o/std                    | 0.064269975 |
| test/episodes                  | 4090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0043     |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1406232  |
| test/Q_plus_P                  | -1.1406232  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 163600      |
| train/episodes                 | 16360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.57        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00689    |
| train/info_shaping_reward_mean | -0.0805     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 654400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 409        |
| stats_o/mean                   | 0.2049094  |
| stats_o/std                    | 0.06427727 |
| test/episodes                  | 4100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00946   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9882547 |
| test/Q_plus_P                  | -0.9882547 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 164000     |
| train/episodes                 | 16400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00708   |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 656000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 410        |
| stats_o/mean                   | 0.20491578 |
| stats_o/std                    | 0.06428738 |
| test/episodes                  | 4110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00622   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1424717 |
| test/Q_plus_P                  | -1.1424717 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 164400     |
| train/episodes                 | 16440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00713   |
| train/info_shaping_reward_mean | -0.0739    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 657600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 411        |
| stats_o/mean                   | 0.20491554 |
| stats_o/std                    | 0.06425999 |
| test/episodes                  | 4120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00282   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0822475 |
| test/Q_plus_P                  | -1.0822475 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 164800     |
| train/episodes                 | 16480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00504   |
| train/info_shaping_reward_mean | -0.0619    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 659200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 412        |
| stats_o/mean                   | 0.20491487 |
| stats_o/std                    | 0.06421937 |
| test/episodes                  | 4130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00999   |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0545208 |
| test/Q_plus_P                  | -1.0545208 |
| test/reward_per_eps            | -8         |
| test/steps                     | 165200     |
| train/episodes                 | 16520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00671   |
| train/info_shaping_reward_mean | -0.0634    |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 660800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 413        |
| stats_o/mean                   | 0.20491125 |
| stats_o/std                    | 0.06421853 |
| test/episodes                  | 4140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00818   |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1069664 |
| test/Q_plus_P                  | -1.1069664 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 165600     |
| train/episodes                 | 16560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00717   |
| train/info_shaping_reward_mean | -0.0749    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 662400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 414        |
| stats_o/mean                   | 0.20491259 |
| stats_o/std                    | 0.06418855 |
| test/episodes                  | 4150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0127    |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.056498  |
| test/Q_plus_P                  | -1.056498  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 166000     |
| train/episodes                 | 16600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0075    |
| train/info_shaping_reward_mean | -0.0696    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 664000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 415        |
| stats_o/mean                   | 0.20491366 |
| stats_o/std                    | 0.06416839 |
| test/episodes                  | 4160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00443   |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2179277 |
| test/Q_plus_P                  | -1.2179277 |
| test/reward_per_eps            | -9         |
| test/steps                     | 166400     |
| train/episodes                 | 16640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00537   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 665600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 416         |
| stats_o/mean                   | 0.20491417  |
| stats_o/std                    | 0.064162895 |
| test/episodes                  | 4170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0067     |
| test/info_shaping_reward_mean  | -0.0475     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0547671  |
| test/Q_plus_P                  | -1.0547671  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 166800      |
| train/episodes                 | 16680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.59        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00691    |
| train/info_shaping_reward_mean | -0.0708     |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 667200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 417        |
| stats_o/mean                   | 0.2049213  |
| stats_o/std                    | 0.06419841 |
| test/episodes                  | 4180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0045    |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0529416 |
| test/Q_plus_P                  | -1.0529416 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 167200     |
| train/episodes                 | 16720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00773   |
| train/info_shaping_reward_mean | -0.0868    |
| train/info_shaping_reward_min  | -0.304     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 668800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.20492034  |
| stats_o/std                    | 0.064167745 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00761    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.9675701  |
| test/Q_plus_P                  | -0.9675701  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00651    |
| train/info_shaping_reward_mean | -0.069      |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 419        |
| stats_o/mean                   | 0.20492208 |
| stats_o/std                    | 0.06413736 |
| test/episodes                  | 4200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00544   |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.1162802 |
| test/Q_plus_P                  | -1.1162802 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 168000     |
| train/episodes                 | 16800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00608   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 672000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 420        |
| stats_o/mean                   | 0.2049282  |
| stats_o/std                    | 0.06412659 |
| test/episodes                  | 4210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.011     |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0759645 |
| test/Q_plus_P                  | -1.0759645 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 168400     |
| train/episodes                 | 16840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00523   |
| train/info_shaping_reward_mean | -0.0674    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 673600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 421        |
| stats_o/mean                   | 0.20493597 |
| stats_o/std                    | 0.0641862  |
| test/episodes                  | 4220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00271   |
| test/info_shaping_reward_mean  | -0.0565    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3601936 |
| test/Q_plus_P                  | -1.3601936 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 168800     |
| train/episodes                 | 16880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00564   |
| train/info_shaping_reward_mean | -0.0835    |
| train/info_shaping_reward_min  | -0.296     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 675200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.2049382   |
| stats_o/std                    | 0.064186245 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00411    |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2152407  |
| test/Q_plus_P                  | -1.2152407  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.582       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00529    |
| train/info_shaping_reward_mean | -0.0764     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 423        |
| stats_o/mean                   | 0.20495054 |
| stats_o/std                    | 0.06419854 |
| test/episodes                  | 4240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00392   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.221173  |
| test/Q_plus_P                  | -1.221173  |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 169600     |
| train/episodes                 | 16960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0066    |
| train/info_shaping_reward_mean | -0.0787    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 678400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 424        |
| stats_o/mean                   | 0.20494202 |
| stats_o/std                    | 0.06422823 |
| test/episodes                  | 4250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00345   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1073182 |
| test/Q_plus_P                  | -1.1073182 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 170000     |
| train/episodes                 | 17000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.582      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00509   |
| train/info_shaping_reward_mean | -0.0756    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 680000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 425        |
| stats_o/mean                   | 0.20494586 |
| stats_o/std                    | 0.06424094 |
| test/episodes                  | 4260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1128948 |
| test/Q_plus_P                  | -1.1128948 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 170400     |
| train/episodes                 | 17040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.59       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00713   |
| train/info_shaping_reward_mean | -0.0759    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 681600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 426        |
| stats_o/mean                   | 0.20494452 |
| stats_o/std                    | 0.0642052  |
| test/episodes                  | 4270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000696  |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2373658 |
| test/Q_plus_P                  | -1.2373658 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 170800     |
| train/episodes                 | 17080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.645      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0064    |
| train/info_shaping_reward_mean | -0.065     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 683200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 427        |
| stats_o/mean                   | 0.20494032 |
| stats_o/std                    | 0.06416776 |
| test/episodes                  | 4280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0032    |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2354991 |
| test/Q_plus_P                  | -1.2354991 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 171200     |
| train/episodes                 | 17120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00744   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 684800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.204937    |
| stats_o/std                    | 0.064166486 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00146    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.2277596  |
| test/Q_plus_P                  | -1.2277596  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00715    |
| train/info_shaping_reward_mean | -0.0672     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 429        |
| stats_o/mean                   | 0.20493656 |
| stats_o/std                    | 0.06416005 |
| test/episodes                  | 4300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00882   |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.1835704 |
| test/Q_plus_P                  | -1.1835704 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 172000     |
| train/episodes                 | 17200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00633   |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 688000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 430         |
| stats_o/mean                   | 0.20493746  |
| stats_o/std                    | 0.064132616 |
| test/episodes                  | 4310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2526765  |
| test/Q_plus_P                  | -1.2526765  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 172400      |
| train/episodes                 | 17240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0095     |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 689600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 431        |
| stats_o/mean                   | 0.20494644 |
| stats_o/std                    | 0.06423591 |
| test/episodes                  | 4320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00849   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2457969 |
| test/Q_plus_P                  | -1.2457969 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 172800     |
| train/episodes                 | 17280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.547      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00986   |
| train/info_shaping_reward_mean | -0.0892    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 691200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 432        |
| stats_o/mean                   | 0.20494364 |
| stats_o/std                    | 0.06420777 |
| test/episodes                  | 4330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00347   |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2598057 |
| test/Q_plus_P                  | -1.2598057 |
| test/reward_per_eps            | -9         |
| test/steps                     | 173200     |
| train/episodes                 | 17320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00574   |
| train/info_shaping_reward_mean | -0.0665    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 692800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 433        |
| stats_o/mean                   | 0.20494121 |
| stats_o/std                    | 0.06417766 |
| test/episodes                  | 4340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000199  |
| test/info_shaping_reward_mean  | -0.0437    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1281242 |
| test/Q_plus_P                  | -1.1281242 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 173600     |
| train/episodes                 | 17360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00772   |
| train/info_shaping_reward_mean | -0.065     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 694400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 434         |
| stats_o/mean                   | 0.20493673  |
| stats_o/std                    | 0.064165816 |
| test/episodes                  | 4350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00483    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.0875632  |
| test/Q_plus_P                  | -1.0875632  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 174000      |
| train/episodes                 | 17400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.605       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00708    |
| train/info_shaping_reward_mean | -0.0791     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 696000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 435        |
| stats_o/mean                   | 0.20494421 |
| stats_o/std                    | 0.06420891 |
| test/episodes                  | 4360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0032    |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1972932 |
| test/Q_plus_P                  | -1.1972932 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 174400     |
| train/episodes                 | 17440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00785   |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 697600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 436        |
| stats_o/mean                   | 0.20494844 |
| stats_o/std                    | 0.06421122 |
| test/episodes                  | 4370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00261   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.1547463 |
| test/Q_plus_P                  | -1.1547463 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 174800     |
| train/episodes                 | 17480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00825   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 699200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 437        |
| stats_o/mean                   | 0.20494278 |
| stats_o/std                    | 0.06419554 |
| test/episodes                  | 4380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00305   |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.0718807 |
| test/Q_plus_P                  | -1.0718807 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 175200     |
| train/episodes                 | 17520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00798   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 700800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 438        |
| stats_o/mean                   | 0.20495115 |
| stats_o/std                    | 0.06420968 |
| test/episodes                  | 4390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000795  |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1995888 |
| test/Q_plus_P                  | -1.1995888 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 175600     |
| train/episodes                 | 17560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00708   |
| train/info_shaping_reward_mean | -0.0814    |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 702400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 439        |
| stats_o/mean                   | 0.2049549  |
| stats_o/std                    | 0.06425541 |
| test/episodes                  | 4400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00584   |
| test/info_shaping_reward_mean  | -0.0571    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -1.6053712 |
| test/Q_plus_P                  | -1.6053712 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 176000     |
| train/episodes                 | 17600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00772   |
| train/info_shaping_reward_mean | -0.0787    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 704000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 440        |
| stats_o/mean                   | 0.20496595 |
| stats_o/std                    | 0.06424492 |
| test/episodes                  | 4410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00188   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1896715 |
| test/Q_plus_P                  | -1.1896715 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 176400     |
| train/episodes                 | 17640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00731   |
| train/info_shaping_reward_mean | -0.0767    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 705600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.20497166  |
| stats_o/std                    | 0.064229205 |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00281    |
| test/info_shaping_reward_mean  | -0.0488     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0816041  |
| test/Q_plus_P                  | -1.0816041  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.577       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0067     |
| train/info_shaping_reward_mean | -0.071      |
| train/info_shaping_reward_min  | -0.197      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 442        |
| stats_o/mean                   | 0.20496611 |
| stats_o/std                    | 0.06422528 |
| test/episodes                  | 4430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00534   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0870684 |
| test/Q_plus_P                  | -1.0870684 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 177200     |
| train/episodes                 | 17720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00685   |
| train/info_shaping_reward_mean | -0.074     |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 708800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.2049656   |
| stats_o/std                    | 0.064205915 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00263    |
| test/info_shaping_reward_mean  | -0.0462     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0712947  |
| test/Q_plus_P                  | -1.0712947  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.62        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00772    |
| train/info_shaping_reward_mean | -0.0666     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 444        |
| stats_o/mean                   | 0.20496245 |
| stats_o/std                    | 0.06416717 |
| test/episodes                  | 4450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.01      |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0854297 |
| test/Q_plus_P                  | -1.0854297 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 178000     |
| train/episodes                 | 17800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00681   |
| train/info_shaping_reward_mean | -0.0644    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 712000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.20497228  |
| stats_o/std                    | 0.064160824 |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00735    |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1033063  |
| test/Q_plus_P                  | -1.1033063  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00602    |
| train/info_shaping_reward_mean | -0.0704     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 446         |
| stats_o/mean                   | 0.20496878  |
| stats_o/std                    | 0.064170435 |
| test/episodes                  | 4470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00698    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1140859  |
| test/Q_plus_P                  | -1.1140859  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 178800      |
| train/episodes                 | 17880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00634    |
| train/info_shaping_reward_mean | -0.0772     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 715200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 447        |
| stats_o/mean                   | 0.20497477 |
| stats_o/std                    | 0.06417687 |
| test/episodes                  | 4480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00309   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2735122 |
| test/Q_plus_P                  | -1.2735122 |
| test/reward_per_eps            | -9         |
| test/steps                     | 179200     |
| train/episodes                 | 17920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00884   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 716800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 448        |
| stats_o/mean                   | 0.20497721 |
| stats_o/std                    | 0.06414806 |
| test/episodes                  | 4490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00457   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0505173 |
| test/Q_plus_P                  | -1.0505173 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 179600     |
| train/episodes                 | 17960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.66       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00601   |
| train/info_shaping_reward_mean | -0.0636    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 718400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 449        |
| stats_o/mean                   | 0.20497027 |
| stats_o/std                    | 0.06412735 |
| test/episodes                  | 4500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00298   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1353184 |
| test/Q_plus_P                  | -1.1353184 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 180000     |
| train/episodes                 | 18000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00754   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 720000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.20496683 |
| stats_o/std                    | 0.06412307 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00137   |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2132959 |
| test/Q_plus_P                  | -1.2132959 |
| test/reward_per_eps            | -9         |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00763   |
| train/info_shaping_reward_mean | -0.0724    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 451        |
| stats_o/mean                   | 0.2049697  |
| stats_o/std                    | 0.0640929  |
| test/episodes                  | 4520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00274   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2195321 |
| test/Q_plus_P                  | -1.2195321 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 180800     |
| train/episodes                 | 18080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00662   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 723200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 452         |
| stats_o/mean                   | 0.2049667   |
| stats_o/std                    | 0.064097315 |
| test/episodes                  | 4530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00259    |
| test/info_shaping_reward_mean  | -0.0488     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0921266  |
| test/Q_plus_P                  | -1.0921266  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 181200      |
| train/episodes                 | 18120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00774    |
| train/info_shaping_reward_mean | -0.0661     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 724800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 453        |
| stats_o/mean                   | 0.20496966 |
| stats_o/std                    | 0.0640887  |
| test/episodes                  | 4540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00351   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.2212594 |
| test/Q_plus_P                  | -1.2212594 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 181600     |
| train/episodes                 | 18160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00535   |
| train/info_shaping_reward_mean | -0.0653    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 726400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 454        |
| stats_o/mean                   | 0.20497307 |
| stats_o/std                    | 0.06415021 |
| test/episodes                  | 4550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00555   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1450198 |
| test/Q_plus_P                  | -1.1450198 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 182000     |
| train/episodes                 | 18200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0079    |
| train/info_shaping_reward_mean | -0.0801    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 728000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.20497334  |
| stats_o/std                    | 0.064104706 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00461    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.038424   |
| test/Q_plus_P                  | -1.038424   |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00631    |
| train/info_shaping_reward_mean | -0.0639     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 456         |
| stats_o/mean                   | 0.2049789   |
| stats_o/std                    | 0.064084165 |
| test/episodes                  | 4570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00547    |
| test/info_shaping_reward_mean  | -0.046      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0569026  |
| test/Q_plus_P                  | -1.0569026  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 182800      |
| train/episodes                 | 18280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00753    |
| train/info_shaping_reward_mean | -0.0677     |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 731200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 457        |
| stats_o/mean                   | 0.20498344 |
| stats_o/std                    | 0.0640548  |
| test/episodes                  | 4580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00132   |
| test/info_shaping_reward_mean  | -0.0448    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9740585 |
| test/Q_plus_P                  | -0.9740585 |
| test/reward_per_eps            | -8         |
| test/steps                     | 183200     |
| train/episodes                 | 18320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.667      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00756   |
| train/info_shaping_reward_mean | -0.0616    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 732800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 458        |
| stats_o/mean                   | 0.20498602 |
| stats_o/std                    | 0.06408626 |
| test/episodes                  | 4590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.003     |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1387155 |
| test/Q_plus_P                  | -1.1387155 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 183600     |
| train/episodes                 | 18360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00899   |
| train/info_shaping_reward_mean | -0.0844    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 734400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 459        |
| stats_o/mean                   | 0.20498662 |
| stats_o/std                    | 0.06405297 |
| test/episodes                  | 4600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00665   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0610353 |
| test/Q_plus_P                  | -1.0610353 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 184000     |
| train/episodes                 | 18400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00513   |
| train/info_shaping_reward_mean | -0.0667    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 736000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 460        |
| stats_o/mean                   | 0.20498353 |
| stats_o/std                    | 0.06401816 |
| test/episodes                  | 4610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00623   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3222451 |
| test/Q_plus_P                  | -1.3222451 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 184400     |
| train/episodes                 | 18440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.663      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0067    |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 737600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 461         |
| stats_o/mean                   | 0.20499024  |
| stats_o/std                    | 0.064019665 |
| test/episodes                  | 4620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00545    |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.22068    |
| test/Q_plus_P                  | -1.22068    |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 184800      |
| train/episodes                 | 18480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00806    |
| train/info_shaping_reward_mean | -0.0749     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 739200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 462        |
| stats_o/mean                   | 0.20498863 |
| stats_o/std                    | 0.06401764 |
| test/episodes                  | 4630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00537   |
| test/info_shaping_reward_mean  | -0.0447    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9118219 |
| test/Q_plus_P                  | -0.9118219 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 185200     |
| train/episodes                 | 18520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.653      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00771   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 740800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 463        |
| stats_o/mean                   | 0.20497869 |
| stats_o/std                    | 0.06400626 |
| test/episodes                  | 4640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00567   |
| test/info_shaping_reward_mean  | -0.0463    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.1763444 |
| test/Q_plus_P                  | -1.1763444 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 185600     |
| train/episodes                 | 18560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00718   |
| train/info_shaping_reward_mean | -0.0724    |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 742400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 464        |
| stats_o/mean                   | 0.20497043 |
| stats_o/std                    | 0.06398518 |
| test/episodes                  | 4650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0116    |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0035758 |
| test/Q_plus_P                  | -1.0035758 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 186000     |
| train/episodes                 | 18600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00688   |
| train/info_shaping_reward_mean | -0.0666    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 744000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 465         |
| stats_o/mean                   | 0.20497946  |
| stats_o/std                    | 0.063974805 |
| test/episodes                  | 4660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00594    |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1837107  |
| test/Q_plus_P                  | -1.1837107  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 186400      |
| train/episodes                 | 18640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00919    |
| train/info_shaping_reward_mean | -0.0734     |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 745600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.20498173  |
| stats_o/std                    | 0.063974574 |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00606    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1074202  |
| test/Q_plus_P                  | -1.1074202  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00637    |
| train/info_shaping_reward_mean | -0.0704     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 467        |
| stats_o/mean                   | 0.20499143 |
| stats_o/std                    | 0.0639873  |
| test/episodes                  | 4680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00239   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1038673 |
| test/Q_plus_P                  | -1.1038673 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 187200     |
| train/episodes                 | 18720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00652   |
| train/info_shaping_reward_mean | -0.0717    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 748800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 468         |
| stats_o/mean                   | 0.20499395  |
| stats_o/std                    | 0.064009756 |
| test/episodes                  | 4690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0051     |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.0859264  |
| test/Q_plus_P                  | -1.0859264  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 187600      |
| train/episodes                 | 18760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.556       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00897    |
| train/info_shaping_reward_mean | -0.0761     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 750400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 469        |
| stats_o/mean                   | 0.20499927 |
| stats_o/std                    | 0.06397733 |
| test/episodes                  | 4700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00565   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1025003 |
| test/Q_plus_P                  | -1.1025003 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 188000     |
| train/episodes                 | 18800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00648   |
| train/info_shaping_reward_mean | -0.0634    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 752000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.2050038  |
| stats_o/std                    | 0.06396074 |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00725   |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3702168 |
| test/Q_plus_P                  | -1.3702168 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00619   |
| train/info_shaping_reward_mean | -0.0683    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.20500618 |
| stats_o/std                    | 0.06392517 |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00261   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1799856 |
| test/Q_plus_P                  | -1.1799856 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0075    |
| train/info_shaping_reward_mean | -0.0666    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.20500414 |
| stats_o/std                    | 0.06391797 |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00744   |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2074766 |
| test/Q_plus_P                  | -1.2074766 |
| test/reward_per_eps            | -9         |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00849   |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 473         |
| stats_o/mean                   | 0.20500775  |
| stats_o/std                    | 0.063955076 |
| test/episodes                  | 4740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00729    |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.274169   |
| test/Q_plus_P                  | -1.274169   |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 189600      |
| train/episodes                 | 18960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.56        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00785    |
| train/info_shaping_reward_mean | -0.0779     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 758400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 474         |
| stats_o/mean                   | 0.2050019   |
| stats_o/std                    | 0.063955896 |
| test/episodes                  | 4750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00863    |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1974941  |
| test/Q_plus_P                  | -1.1974941  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 190000      |
| train/episodes                 | 19000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.574       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00803    |
| train/info_shaping_reward_mean | -0.0766     |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 760000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 475        |
| stats_o/mean                   | 0.20500349 |
| stats_o/std                    | 0.0639404  |
| test/episodes                  | 4760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00274   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1058172 |
| test/Q_plus_P                  | -1.1058172 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 190400     |
| train/episodes                 | 19040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00875   |
| train/info_shaping_reward_mean | -0.0708    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 761600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.20500574  |
| stats_o/std                    | 0.063904665 |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00744    |
| test/info_shaping_reward_mean  | -0.0538     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3546139  |
| test/Q_plus_P                  | -1.3546139  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00686    |
| train/info_shaping_reward_mean | -0.07       |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.2050014   |
| stats_o/std                    | 0.063896924 |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00903    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.2117468  |
| test/Q_plus_P                  | -1.2117468  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00602    |
| train/info_shaping_reward_mean | -0.0694     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 478        |
| stats_o/mean                   | 0.20500061 |
| stats_o/std                    | 0.06387915 |
| test/episodes                  | 4790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00733   |
| test/info_shaping_reward_mean  | -0.05      |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1559684 |
| test/Q_plus_P                  | -1.1559684 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 191600     |
| train/episodes                 | 19160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00905   |
| train/info_shaping_reward_mean | -0.0757    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 766400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 479        |
| stats_o/mean                   | 0.20500052 |
| stats_o/std                    | 0.06386026 |
| test/episodes                  | 4800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00829   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.065938  |
| test/Q_plus_P                  | -1.065938  |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 192000     |
| train/episodes                 | 19200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0072    |
| train/info_shaping_reward_mean | -0.0774    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 768000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 480         |
| stats_o/mean                   | 0.20500536  |
| stats_o/std                    | 0.063836224 |
| test/episodes                  | 4810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00334    |
| test/info_shaping_reward_mean  | -0.0559     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.3665793  |
| test/Q_plus_P                  | -1.3665793  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 192400      |
| train/episodes                 | 19240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00499    |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 769600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 481         |
| stats_o/mean                   | 0.20500794  |
| stats_o/std                    | 0.063809074 |
| test/episodes                  | 4820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00143    |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.2997279  |
| test/Q_plus_P                  | -1.2997279  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 192800      |
| train/episodes                 | 19280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00811    |
| train/info_shaping_reward_mean | -0.0683     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 771200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 482        |
| stats_o/mean                   | 0.20500597 |
| stats_o/std                    | 0.06377408 |
| test/episodes                  | 4830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00821   |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2731978 |
| test/Q_plus_P                  | -1.2731978 |
| test/reward_per_eps            | -9         |
| test/steps                     | 193200     |
| train/episodes                 | 19320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.655      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00885   |
| train/info_shaping_reward_mean | -0.0639    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 772800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 483         |
| stats_o/mean                   | 0.20499933  |
| stats_o/std                    | 0.063773915 |
| test/episodes                  | 4840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00642    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0650635  |
| test/Q_plus_P                  | -1.0650635  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 193600      |
| train/episodes                 | 19360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.605       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0085     |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 774400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 484        |
| stats_o/mean                   | 0.20500332 |
| stats_o/std                    | 0.06376552 |
| test/episodes                  | 4850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1258277 |
| test/Q_plus_P                  | -1.1258277 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 194000     |
| train/episodes                 | 19400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00727   |
| train/info_shaping_reward_mean | -0.0658    |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 776000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 485        |
| stats_o/mean                   | 0.20500658 |
| stats_o/std                    | 0.0637497  |
| test/episodes                  | 4860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00836   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1896538 |
| test/Q_plus_P                  | -1.1896538 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 194400     |
| train/episodes                 | 19440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00911   |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 777600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 486        |
| stats_o/mean                   | 0.2050205  |
| stats_o/std                    | 0.06380279 |
| test/episodes                  | 4870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.006     |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2012993 |
| test/Q_plus_P                  | -1.2012993 |
| test/reward_per_eps            | -9         |
| test/steps                     | 194800     |
| train/episodes                 | 19480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00718   |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 779200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 487        |
| stats_o/mean                   | 0.20502146 |
| stats_o/std                    | 0.06378769 |
| test/episodes                  | 4880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00751   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.173388  |
| test/Q_plus_P                  | -1.173388  |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 195200     |
| train/episodes                 | 19520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00631   |
| train/info_shaping_reward_mean | -0.0639    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 780800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 488        |
| stats_o/mean                   | 0.20502034 |
| stats_o/std                    | 0.06380663 |
| test/episodes                  | 4890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00595   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0680933 |
| test/Q_plus_P                  | -1.0680933 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 195600     |
| train/episodes                 | 19560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00787   |
| train/info_shaping_reward_mean | -0.0792    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 782400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 489         |
| stats_o/mean                   | 0.20501755  |
| stats_o/std                    | 0.063818336 |
| test/episodes                  | 4900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00202    |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2307965  |
| test/Q_plus_P                  | -1.2307965  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 196000      |
| train/episodes                 | 19600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00802    |
| train/info_shaping_reward_mean | -0.0714     |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 784000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 490        |
| stats_o/mean                   | 0.20502505 |
| stats_o/std                    | 0.06381611 |
| test/episodes                  | 4910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00852   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2311811 |
| test/Q_plus_P                  | -1.2311811 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 196400     |
| train/episodes                 | 19640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.635      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00861   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 785600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 491        |
| stats_o/mean                   | 0.20502588 |
| stats_o/std                    | 0.0637786  |
| test/episodes                  | 4920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00856   |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.187268  |
| test/Q_plus_P                  | -1.187268  |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 196800     |
| train/episodes                 | 19680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00803   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 787200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 492        |
| stats_o/mean                   | 0.2050337  |
| stats_o/std                    | 0.06376439 |
| test/episodes                  | 4930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00334   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0320902 |
| test/Q_plus_P                  | -1.0320902 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 197200     |
| train/episodes                 | 19720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0083    |
| train/info_shaping_reward_mean | -0.0734    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 788800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.20504038 |
| stats_o/std                    | 0.06376936 |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00276   |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2562981 |
| test/Q_plus_P                  | -1.2562981 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00714   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 494        |
| stats_o/mean                   | 0.20504093 |
| stats_o/std                    | 0.0637474  |
| test/episodes                  | 4950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00145   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0714945 |
| test/Q_plus_P                  | -1.0714945 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 198000     |
| train/episodes                 | 19800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.662      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00866   |
| train/info_shaping_reward_mean | -0.0644    |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 792000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 495         |
| stats_o/mean                   | 0.20503724  |
| stats_o/std                    | 0.063841335 |
| test/episodes                  | 4960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00801    |
| test/info_shaping_reward_mean  | -0.0472     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1378754  |
| test/Q_plus_P                  | -1.1378754  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 198400      |
| train/episodes                 | 19840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00959    |
| train/info_shaping_reward_mean | -0.0861     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 793600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 496        |
| stats_o/mean                   | 0.20503907 |
| stats_o/std                    | 0.06381115 |
| test/episodes                  | 4970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00729   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1054088 |
| test/Q_plus_P                  | -1.1054088 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 198800     |
| train/episodes                 | 19880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.669      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00514   |
| train/info_shaping_reward_mean | -0.0622    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 795200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 497        |
| stats_o/mean                   | 0.20503984 |
| stats_o/std                    | 0.06378592 |
| test/episodes                  | 4980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00715   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1605835 |
| test/Q_plus_P                  | -1.1605835 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 199200     |
| train/episodes                 | 19920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00826   |
| train/info_shaping_reward_mean | -0.0654    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 796800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 498        |
| stats_o/mean                   | 0.20503694 |
| stats_o/std                    | 0.06378369 |
| test/episodes                  | 4990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00614   |
| test/info_shaping_reward_mean  | -0.0448    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.108674  |
| test/Q_plus_P                  | -1.108674  |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 199600     |
| train/episodes                 | 19960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00801   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 798400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 499        |
| stats_o/mean                   | 0.20503478 |
| stats_o/std                    | 0.06377453 |
| test/episodes                  | 5000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00993   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1648583 |
| test/Q_plus_P                  | -1.1648583 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 200000     |
| train/episodes                 | 20000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0079    |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 800000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 500         |
| stats_o/mean                   | 0.2050299   |
| stats_o/std                    | 0.063771494 |
| test/episodes                  | 5010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00727    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1406745  |
| test/Q_plus_P                  | -1.1406745  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 200400      |
| train/episodes                 | 20040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0102     |
| train/info_shaping_reward_mean | -0.0703     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 801600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 501        |
| stats_o/mean                   | 0.2050331  |
| stats_o/std                    | 0.06380212 |
| test/episodes                  | 5020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00979   |
| test/info_shaping_reward_mean  | -0.0456    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9885458 |
| test/Q_plus_P                  | -0.9885458 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 200800     |
| train/episodes                 | 20080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0062    |
| train/info_shaping_reward_mean | -0.0834    |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 803200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 502        |
| stats_o/mean                   | 0.20502676 |
| stats_o/std                    | 0.06380519 |
| test/episodes                  | 5030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0051    |
| test/info_shaping_reward_mean  | -0.0831    |
| test/info_shaping_reward_min   | -0.519     |
| test/Q                         | -3.8368745 |
| test/Q_plus_P                  | -3.8368745 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 201200     |
| train/episodes                 | 20120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00827   |
| train/info_shaping_reward_mean | -0.0683    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 804800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 503        |
| stats_o/mean                   | 0.20503265 |
| stats_o/std                    | 0.06378809 |
| test/episodes                  | 5040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00406   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.114486  |
| test/Q_plus_P                  | -1.114486  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 201600     |
| train/episodes                 | 20160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.635      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00633   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 806400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 504        |
| stats_o/mean                   | 0.20503825 |
| stats_o/std                    | 0.06377736 |
| test/episodes                  | 5050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.812      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00507   |
| test/info_shaping_reward_mean  | -0.0427    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8677885 |
| test/Q_plus_P                  | -0.8677885 |
| test/reward_per_eps            | -7.5       |
| test/steps                     | 202000     |
| train/episodes                 | 20200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00938   |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 808000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 505         |
| stats_o/mean                   | 0.20503512  |
| stats_o/std                    | 0.063748434 |
| test/episodes                  | 5060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0047     |
| test/info_shaping_reward_mean  | -0.0582     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.4681076  |
| test/Q_plus_P                  | -1.4681076  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 202400      |
| train/episodes                 | 20240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00673    |
| train/info_shaping_reward_mean | -0.0672     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 809600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 506        |
| stats_o/mean                   | 0.20503925 |
| stats_o/std                    | 0.06374957 |
| test/episodes                  | 5070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00299   |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1393652 |
| test/Q_plus_P                  | -1.1393652 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 202800     |
| train/episodes                 | 20280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.653      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00726   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 811200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 507         |
| stats_o/mean                   | 0.20504351  |
| stats_o/std                    | 0.063725844 |
| test/episodes                  | 5080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00831    |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1901295  |
| test/Q_plus_P                  | -1.1901295  |
| test/reward_per_eps            | -9          |
| test/steps                     | 203200      |
| train/episodes                 | 20320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0069     |
| train/info_shaping_reward_mean | -0.0649     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 812800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 508        |
| stats_o/mean                   | 0.20504098 |
| stats_o/std                    | 0.06371531 |
| test/episodes                  | 5090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1747825 |
| test/Q_plus_P                  | -1.1747825 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 203600     |
| train/episodes                 | 20360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00813   |
| train/info_shaping_reward_mean | -0.0683    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 814400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 509        |
| stats_o/mean                   | 0.20504141 |
| stats_o/std                    | 0.06368533 |
| test/episodes                  | 5100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0115    |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1996698 |
| test/Q_plus_P                  | -1.1996698 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 204000     |
| train/episodes                 | 20400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00805   |
| train/info_shaping_reward_mean | -0.0684    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 816000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 510        |
| stats_o/mean                   | 0.20504099 |
| stats_o/std                    | 0.06367972 |
| test/episodes                  | 5110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00332   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2177919 |
| test/Q_plus_P                  | -1.2177919 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 204400     |
| train/episodes                 | 20440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0089    |
| train/info_shaping_reward_mean | -0.0717    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 817600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 511        |
| stats_o/mean                   | 0.20504232 |
| stats_o/std                    | 0.06366972 |
| test/episodes                  | 5120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00452   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.121855  |
| test/Q_plus_P                  | -1.121855  |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 204800     |
| train/episodes                 | 20480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00927   |
| train/info_shaping_reward_mean | -0.0722    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 819200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 512        |
| stats_o/mean                   | 0.2050393  |
| stats_o/std                    | 0.06363324 |
| test/episodes                  | 5130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00943   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1714432 |
| test/Q_plus_P                  | -1.1714432 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 205200     |
| train/episodes                 | 20520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.661      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0078    |
| train/info_shaping_reward_mean | -0.0651    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 820800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 513        |
| stats_o/mean                   | 0.205042   |
| stats_o/std                    | 0.06367218 |
| test/episodes                  | 5140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00385   |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0192909 |
| test/Q_plus_P                  | -1.0192909 |
| test/reward_per_eps            | -8         |
| test/steps                     | 205600     |
| train/episodes                 | 20560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00549   |
| train/info_shaping_reward_mean | -0.0772    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 822400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 514         |
| stats_o/mean                   | 0.20504823  |
| stats_o/std                    | 0.06368076  |
| test/episodes                  | 5150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00655    |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.99828637 |
| test/Q_plus_P                  | -0.99828637 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 206000      |
| train/episodes                 | 20600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00957    |
| train/info_shaping_reward_mean | -0.0734     |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 824000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 515        |
| stats_o/mean                   | 0.20505318 |
| stats_o/std                    | 0.06364662 |
| test/episodes                  | 5160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0118    |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1515731 |
| test/Q_plus_P                  | -1.1515731 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 206400     |
| train/episodes                 | 20640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0116    |
| train/info_shaping_reward_mean | -0.0688    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 825600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 516        |
| stats_o/mean                   | 0.20505266 |
| stats_o/std                    | 0.06365498 |
| test/episodes                  | 5170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00857   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1141628 |
| test/Q_plus_P                  | -1.1141628 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 206800     |
| train/episodes                 | 20680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00784   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 827200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 517         |
| stats_o/mean                   | 0.20506068  |
| stats_o/std                    | 0.063669026 |
| test/episodes                  | 5180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0116     |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.3516431  |
| test/Q_plus_P                  | -1.3516431  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 207200      |
| train/episodes                 | 20720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00688    |
| train/info_shaping_reward_mean | -0.0724     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 828800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 518        |
| stats_o/mean                   | 0.20506322 |
| stats_o/std                    | 0.06367978 |
| test/episodes                  | 5190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0119    |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1206231 |
| test/Q_plus_P                  | -1.1206231 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 207600     |
| train/episodes                 | 20760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00693   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 830400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 519         |
| stats_o/mean                   | 0.20506327  |
| stats_o/std                    | 0.063711725 |
| test/episodes                  | 5200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0124     |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1164368  |
| test/Q_plus_P                  | -1.1164368  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 208000      |
| train/episodes                 | 20800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00829    |
| train/info_shaping_reward_mean | -0.0762     |
| train/info_shaping_reward_min  | -0.21       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 832000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 520        |
| stats_o/mean                   | 0.20506746 |
| stats_o/std                    | 0.06369491 |
| test/episodes                  | 5210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.011     |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2407442 |
| test/Q_plus_P                  | -1.2407442 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 208400     |
| train/episodes                 | 20840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00854   |
| train/info_shaping_reward_mean | -0.0641    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 833600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 521        |
| stats_o/mean                   | 0.20506826 |
| stats_o/std                    | 0.06368075 |
| test/episodes                  | 5220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0105    |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0791584 |
| test/Q_plus_P                  | -1.0791584 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 208800     |
| train/episodes                 | 20880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.676      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00536   |
| train/info_shaping_reward_mean | -0.0607    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.9      |
| train/steps                    | 835200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 522        |
| stats_o/mean                   | 0.20506588 |
| stats_o/std                    | 0.0636641  |
| test/episodes                  | 5230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0111    |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0430589 |
| test/Q_plus_P                  | -1.0430589 |
| test/reward_per_eps            | -8         |
| test/steps                     | 209200     |
| train/episodes                 | 20920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.645      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00688   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 836800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 523         |
| stats_o/mean                   | 0.20506302  |
| stats_o/std                    | 0.063641936 |
| test/episodes                  | 5240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00505    |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0250881  |
| test/Q_plus_P                  | -1.0250881  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 209600      |
| train/episodes                 | 20960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0071     |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 838400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 524        |
| stats_o/mean                   | 0.20507252 |
| stats_o/std                    | 0.06363854 |
| test/episodes                  | 5250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0116    |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1940371 |
| test/Q_plus_P                  | -1.1940371 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 210000     |
| train/episodes                 | 21000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0112    |
| train/info_shaping_reward_mean | -0.0717    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 840000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 525        |
| stats_o/mean                   | 0.20508072 |
| stats_o/std                    | 0.06366495 |
| test/episodes                  | 5260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00545   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0100415 |
| test/Q_plus_P                  | -1.0100415 |
| test/reward_per_eps            | -8         |
| test/steps                     | 210400     |
| train/episodes                 | 21040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0116    |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 841600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.20508131  |
| stats_o/std                    | 0.063718244 |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00675    |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.1178368  |
| test/Q_plus_P                  | -1.1178368  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.552       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00804    |
| train/info_shaping_reward_mean | -0.0873     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 527        |
| stats_o/mean                   | 0.20508833 |
| stats_o/std                    | 0.06372233 |
| test/episodes                  | 5280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00271   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1701454 |
| test/Q_plus_P                  | -1.1701454 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 211200     |
| train/episodes                 | 21120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00694   |
| train/info_shaping_reward_mean | -0.0674    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 844800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 528         |
| stats_o/mean                   | 0.20509271  |
| stats_o/std                    | 0.063747704 |
| test/episodes                  | 5290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0036     |
| test/info_shaping_reward_mean  | -0.0496     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1912476  |
| test/Q_plus_P                  | -1.1912476  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 211600      |
| train/episodes                 | 21160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00756    |
| train/info_shaping_reward_mean | -0.0749     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 846400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 529        |
| stats_o/mean                   | 0.2050895  |
| stats_o/std                    | 0.06372913 |
| test/episodes                  | 5300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0108    |
| test/info_shaping_reward_mean  | -0.05      |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.124829  |
| test/Q_plus_P                  | -1.124829  |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 212000     |
| train/episodes                 | 21200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00844   |
| train/info_shaping_reward_mean | -0.0684    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 848000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 530        |
| stats_o/mean                   | 0.20509107 |
| stats_o/std                    | 0.06373965 |
| test/episodes                  | 5310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00508   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0353436 |
| test/Q_plus_P                  | -1.0353436 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 212400     |
| train/episodes                 | 21240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0105    |
| train/info_shaping_reward_mean | -0.0723    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 849600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 531        |
| stats_o/mean                   | 0.20510331 |
| stats_o/std                    | 0.06377283 |
| test/episodes                  | 5320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0107    |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1026454 |
| test/Q_plus_P                  | -1.1026454 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 212800     |
| train/episodes                 | 21280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00772   |
| train/info_shaping_reward_mean | -0.08      |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 851200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 532        |
| stats_o/mean                   | 0.20511042 |
| stats_o/std                    | 0.06376479 |
| test/episodes                  | 5330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.012     |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3467858 |
| test/Q_plus_P                  | -1.3467858 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 213200     |
| train/episodes                 | 21320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00821   |
| train/info_shaping_reward_mean | -0.0744    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 852800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 533        |
| stats_o/mean                   | 0.20511597 |
| stats_o/std                    | 0.06376235 |
| test/episodes                  | 5340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00747   |
| test/info_shaping_reward_mean  | -0.0515    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1769121 |
| test/Q_plus_P                  | -1.1769121 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 213600     |
| train/episodes                 | 21360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00723   |
| train/info_shaping_reward_mean | -0.0696    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 854400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 534         |
| stats_o/mean                   | 0.20512083  |
| stats_o/std                    | 0.063746855 |
| test/episodes                  | 5350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00461    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1069376  |
| test/Q_plus_P                  | -1.1069376  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 214000      |
| train/episodes                 | 21400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00943    |
| train/info_shaping_reward_mean | -0.0701     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 856000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 535        |
| stats_o/mean                   | 0.20511667 |
| stats_o/std                    | 0.06372461 |
| test/episodes                  | 5360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0121    |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1644771 |
| test/Q_plus_P                  | -1.1644771 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 214400     |
| train/episodes                 | 21440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00918   |
| train/info_shaping_reward_mean | -0.0643    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 857600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 536        |
| stats_o/mean                   | 0.20511128 |
| stats_o/std                    | 0.06370846 |
| test/episodes                  | 5370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00886   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.093446  |
| test/Q_plus_P                  | -1.093446  |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 214800     |
| train/episodes                 | 21480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00906   |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 859200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 537        |
| stats_o/mean                   | 0.20511082 |
| stats_o/std                    | 0.06370026 |
| test/episodes                  | 5380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0142    |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1837078 |
| test/Q_plus_P                  | -1.1837078 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 215200     |
| train/episodes                 | 21520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00926   |
| train/info_shaping_reward_mean | -0.0638    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 860800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 538        |
| stats_o/mean                   | 0.20511007 |
| stats_o/std                    | 0.06371505 |
| test/episodes                  | 5390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0122    |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1965396 |
| test/Q_plus_P                  | -1.1965396 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 215600     |
| train/episodes                 | 21560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00834   |
| train/info_shaping_reward_mean | -0.0723    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 862400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 539         |
| stats_o/mean                   | 0.20510441  |
| stats_o/std                    | 0.063718066 |
| test/episodes                  | 5400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0109     |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2349079  |
| test/Q_plus_P                  | -1.2349079  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 216000      |
| train/episodes                 | 21600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00828    |
| train/info_shaping_reward_mean | -0.073      |
| train/info_shaping_reward_min  | -0.226      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 864000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 540         |
| stats_o/mean                   | 0.20510992  |
| stats_o/std                    | 0.063728094 |
| test/episodes                  | 5410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00695    |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2133555  |
| test/Q_plus_P                  | -1.2133555  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 216400      |
| train/episodes                 | 21640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0105     |
| train/info_shaping_reward_mean | -0.0794     |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 865600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.20511149  |
| stats_o/std                    | 0.063705824 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00685    |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1072327  |
| test/Q_plus_P                  | -1.1072327  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00939    |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 542        |
| stats_o/mean                   | 0.20511067 |
| stats_o/std                    | 0.06369634 |
| test/episodes                  | 5430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00192   |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2986252 |
| test/Q_plus_P                  | -1.2986252 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 217200     |
| train/episodes                 | 21720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00908   |
| train/info_shaping_reward_mean | -0.078     |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 868800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 543        |
| stats_o/mean                   | 0.20512041 |
| stats_o/std                    | 0.06374727 |
| test/episodes                  | 5440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0127    |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1375585 |
| test/Q_plus_P                  | -1.1375585 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 217600     |
| train/episodes                 | 21760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0077    |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 870400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 544        |
| stats_o/mean                   | 0.20512234 |
| stats_o/std                    | 0.06375325 |
| test/episodes                  | 5450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00532   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0652846 |
| test/Q_plus_P                  | -1.0652846 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 218000     |
| train/episodes                 | 21800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.64       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00877   |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.275     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 872000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 545         |
| stats_o/mean                   | 0.20512098  |
| stats_o/std                    | 0.063730225 |
| test/episodes                  | 5460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00262    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.9997461  |
| test/Q_plus_P                  | -0.9997461  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 218400      |
| train/episodes                 | 21840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0109     |
| train/info_shaping_reward_mean | -0.0694     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 873600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 546        |
| stats_o/mean                   | 0.20512068 |
| stats_o/std                    | 0.06369634 |
| test/episodes                  | 5470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00484   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1572447 |
| test/Q_plus_P                  | -1.1572447 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 218800     |
| train/episodes                 | 21880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00795   |
| train/info_shaping_reward_mean | -0.0666    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 875200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 547         |
| stats_o/mean                   | 0.20512542  |
| stats_o/std                    | 0.06367869  |
| test/episodes                  | 5480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00861    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.94964635 |
| test/Q_plus_P                  | -0.94964635 |
| test/reward_per_eps            | -8          |
| test/steps                     | 219200      |
| train/episodes                 | 21920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00798    |
| train/info_shaping_reward_mean | -0.0671     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 876800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 548        |
| stats_o/mean                   | 0.20512988 |
| stats_o/std                    | 0.06367782 |
| test/episodes                  | 5490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00208   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0902982 |
| test/Q_plus_P                  | -1.0902982 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 219600     |
| train/episodes                 | 21960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00978   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 878400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.20512429  |
| stats_o/std                    | 0.06366549  |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0158     |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.97614694 |
| test/Q_plus_P                  | -0.97614694 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00802    |
| train/info_shaping_reward_mean | -0.0676     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 550        |
| stats_o/mean                   | 0.20512532 |
| stats_o/std                    | 0.06364176 |
| test/episodes                  | 5510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00481   |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0745636 |
| test/Q_plus_P                  | -1.0745636 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 220400     |
| train/episodes                 | 22040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00683   |
| train/info_shaping_reward_mean | -0.0641    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 881600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 551        |
| stats_o/mean                   | 0.20512997 |
| stats_o/std                    | 0.0636417  |
| test/episodes                  | 5520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00968   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0727525 |
| test/Q_plus_P                  | -1.0727525 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 220800     |
| train/episodes                 | 22080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0071    |
| train/info_shaping_reward_mean | -0.0662    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 883200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 552         |
| stats_o/mean                   | 0.2051342   |
| stats_o/std                    | 0.063632034 |
| test/episodes                  | 5530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00747    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.2940989  |
| test/Q_plus_P                  | -1.2940989  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 221200      |
| train/episodes                 | 22120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00527    |
| train/info_shaping_reward_mean | -0.0659     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 884800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 553        |
| stats_o/mean                   | 0.20513628 |
| stats_o/std                    | 0.06368    |
| test/episodes                  | 5540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00561   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.076761  |
| test/Q_plus_P                  | -1.076761  |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 221600     |
| train/episodes                 | 22160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00868   |
| train/info_shaping_reward_mean | -0.0847    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 886400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 554        |
| stats_o/mean                   | 0.20513645 |
| stats_o/std                    | 0.06366627 |
| test/episodes                  | 5550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0094    |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.155483  |
| test/Q_plus_P                  | -1.155483  |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 222000     |
| train/episodes                 | 22200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00874   |
| train/info_shaping_reward_mean | -0.072     |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 888000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 555        |
| stats_o/mean                   | 0.20513432 |
| stats_o/std                    | 0.06365731 |
| test/episodes                  | 5560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00884   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0694827 |
| test/Q_plus_P                  | -1.0694827 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 222400     |
| train/episodes                 | 22240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00876   |
| train/info_shaping_reward_mean | -0.087     |
| train/info_shaping_reward_min  | -0.278     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 889600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 556        |
| stats_o/mean                   | 0.20513377 |
| stats_o/std                    | 0.06371524 |
| test/episodes                  | 5570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00651   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0318003 |
| test/Q_plus_P                  | -1.0318003 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 222800     |
| train/episodes                 | 22280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00739   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 891200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 557        |
| stats_o/mean                   | 0.20513536 |
| stats_o/std                    | 0.06372344 |
| test/episodes                  | 5580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0125    |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.254575  |
| test/Q_plus_P                  | -1.254575  |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 223200     |
| train/episodes                 | 22320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00677   |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 892800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 558        |
| stats_o/mean                   | 0.20514141 |
| stats_o/std                    | 0.06372427 |
| test/episodes                  | 5590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0102    |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1523757 |
| test/Q_plus_P                  | -1.1523757 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 223600     |
| train/episodes                 | 22360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00955   |
| train/info_shaping_reward_mean | -0.0723    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 894400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 559        |
| stats_o/mean                   | 0.205144   |
| stats_o/std                    | 0.06370445 |
| test/episodes                  | 5600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0157    |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1617852 |
| test/Q_plus_P                  | -1.1617852 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 224000     |
| train/episodes                 | 22400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0111    |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 896000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 560        |
| stats_o/mean                   | 0.2051412  |
| stats_o/std                    | 0.06371204 |
| test/episodes                  | 5610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00725   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1001843 |
| test/Q_plus_P                  | -1.1001843 |
| test/reward_per_eps            | -8         |
| test/steps                     | 224400     |
| train/episodes                 | 22440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00714   |
| train/info_shaping_reward_mean | -0.073     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 897600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 561         |
| stats_o/mean                   | 0.20514475  |
| stats_o/std                    | 0.06371625  |
| test/episodes                  | 5620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00855    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.98982865 |
| test/Q_plus_P                  | -0.98982865 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 224800      |
| train/episodes                 | 22480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0084     |
| train/info_shaping_reward_mean | -0.082      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 899200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 562        |
| stats_o/mean                   | 0.20514442 |
| stats_o/std                    | 0.06371502 |
| test/episodes                  | 5630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0108    |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1521455 |
| test/Q_plus_P                  | -1.1521455 |
| test/reward_per_eps            | -9         |
| test/steps                     | 225200     |
| train/episodes                 | 22520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00904   |
| train/info_shaping_reward_mean | -0.0648    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 900800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 563        |
| stats_o/mean                   | 0.20514238 |
| stats_o/std                    | 0.06370257 |
| test/episodes                  | 5640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000585  |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.071987  |
| test/Q_plus_P                  | -1.071987  |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 225600     |
| train/episodes                 | 22560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00695   |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 902400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 564        |
| stats_o/mean                   | 0.20514774 |
| stats_o/std                    | 0.06369596 |
| test/episodes                  | 5650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00617   |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.113398  |
| test/Q_plus_P                  | -1.113398  |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 226000     |
| train/episodes                 | 22600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00874   |
| train/info_shaping_reward_mean | -0.0691    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 904000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 565        |
| stats_o/mean                   | 0.20515226 |
| stats_o/std                    | 0.06368041 |
| test/episodes                  | 5660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00744   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0543652 |
| test/Q_plus_P                  | -1.0543652 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 226400     |
| train/episodes                 | 22640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00889   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 905600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 566        |
| stats_o/mean                   | 0.20515318 |
| stats_o/std                    | 0.06365898 |
| test/episodes                  | 5670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0104    |
| test/info_shaping_reward_mean  | -0.05      |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1718212 |
| test/Q_plus_P                  | -1.1718212 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 226800     |
| train/episodes                 | 22680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00664   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 907200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 567        |
| stats_o/mean                   | 0.20515637 |
| stats_o/std                    | 0.06372805 |
| test/episodes                  | 5680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00263   |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -1.3031802 |
| test/Q_plus_P                  | -1.3031802 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 227200     |
| train/episodes                 | 22720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.565      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00834   |
| train/info_shaping_reward_mean | -0.0897    |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 908800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.20515743  |
| stats_o/std                    | 0.063703425 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0154     |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1812446  |
| test/Q_plus_P                  | -1.1812446  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00864    |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.20515394  |
| stats_o/std                    | 0.063759424 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0095     |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0919745  |
| test/Q_plus_P                  | -1.0919745  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00823    |
| train/info_shaping_reward_mean | -0.0703     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.20515564  |
| stats_o/std                    | 0.063782044 |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0154     |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1549296  |
| test/Q_plus_P                  | -1.1549296  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.6         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0101     |
| train/info_shaping_reward_mean | -0.0747     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 571        |
| stats_o/mean                   | 0.20515864 |
| stats_o/std                    | 0.06377613 |
| test/episodes                  | 5720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00201   |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.1654359 |
| test/Q_plus_P                  | -1.1654359 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 228800     |
| train/episodes                 | 22880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00681   |
| train/info_shaping_reward_mean | -0.0686    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 915200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.20516111  |
| stats_o/std                    | 0.063759156 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00611    |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.257387   |
| test/Q_plus_P                  | -1.257387   |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00523    |
| train/info_shaping_reward_mean | -0.0676     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 573         |
| stats_o/mean                   | 0.2051641   |
| stats_o/std                    | 0.063756034 |
| test/episodes                  | 5740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0103     |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0003972  |
| test/Q_plus_P                  | -1.0003972  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 229600      |
| train/episodes                 | 22960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0081     |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 918400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 574        |
| stats_o/mean                   | 0.20516747 |
| stats_o/std                    | 0.06374886 |
| test/episodes                  | 5750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00726   |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0805404 |
| test/Q_plus_P                  | -1.0805404 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 230000     |
| train/episodes                 | 23000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00816   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 920000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 575        |
| stats_o/mean                   | 0.20517273 |
| stats_o/std                    | 0.0637437  |
| test/episodes                  | 5760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00639   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0904479 |
| test/Q_plus_P                  | -1.0904479 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 230400     |
| train/episodes                 | 23040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00674   |
| train/info_shaping_reward_mean | -0.0673    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 921600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 576        |
| stats_o/mean                   | 0.20517467 |
| stats_o/std                    | 0.06374468 |
| test/episodes                  | 5770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0102    |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1576613 |
| test/Q_plus_P                  | -1.1576613 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 230800     |
| train/episodes                 | 23080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00763   |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 923200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 577        |
| stats_o/mean                   | 0.20517923 |
| stats_o/std                    | 0.06373169 |
| test/episodes                  | 5780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00712   |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1657336 |
| test/Q_plus_P                  | -1.1657336 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 231200     |
| train/episodes                 | 23120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00773   |
| train/info_shaping_reward_mean | -0.0726    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 924800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.2051799  |
| stats_o/std                    | 0.06374018 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00563   |
| test/info_shaping_reward_mean  | -0.0872    |
| test/info_shaping_reward_min   | -0.503     |
| test/Q                         | -4.2243342 |
| test/Q_plus_P                  | -4.2243342 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00681   |
| train/info_shaping_reward_mean | -0.0669    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 579        |
| stats_o/mean                   | 0.20518516 |
| stats_o/std                    | 0.06378924 |
| test/episodes                  | 5800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00985   |
| test/info_shaping_reward_mean  | -0.0568    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3376825 |
| test/Q_plus_P                  | -1.3376825 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 232000     |
| train/episodes                 | 23200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00941   |
| train/info_shaping_reward_mean | -0.0746    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 928000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 580        |
| stats_o/mean                   | 0.20518759 |
| stats_o/std                    | 0.06378088 |
| test/episodes                  | 5810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00425   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2368538 |
| test/Q_plus_P                  | -1.2368538 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 232400     |
| train/episodes                 | 23240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0085    |
| train/info_shaping_reward_mean | -0.0683    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 929600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 581        |
| stats_o/mean                   | 0.20518893 |
| stats_o/std                    | 0.06375802 |
| test/episodes                  | 5820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00294   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0034515 |
| test/Q_plus_P                  | -1.0034515 |
| test/reward_per_eps            | -8         |
| test/steps                     | 232800     |
| train/episodes                 | 23280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0108    |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 931200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 582        |
| stats_o/mean                   | 0.20518948 |
| stats_o/std                    | 0.06373348 |
| test/episodes                  | 5830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0069    |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.068472  |
| test/Q_plus_P                  | -1.068472  |
| test/reward_per_eps            | -8         |
| test/steps                     | 233200     |
| train/episodes                 | 23320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0077    |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 932800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 583         |
| stats_o/mean                   | 0.2051903   |
| stats_o/std                    | 0.063724145 |
| test/episodes                  | 5840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00421    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1451508  |
| test/Q_plus_P                  | -1.1451508  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 233600      |
| train/episodes                 | 23360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0103     |
| train/info_shaping_reward_mean | -0.0694     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 934400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 584        |
| stats_o/mean                   | 0.20519163 |
| stats_o/std                    | 0.06370066 |
| test/episodes                  | 5850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00698   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1698036 |
| test/Q_plus_P                  | -1.1698036 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 234000     |
| train/episodes                 | 23400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.666      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00806   |
| train/info_shaping_reward_mean | -0.0632    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 936000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 585        |
| stats_o/mean                   | 0.2051888  |
| stats_o/std                    | 0.06368279 |
| test/episodes                  | 5860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00479   |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0527416 |
| test/Q_plus_P                  | -1.0527416 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 234400     |
| train/episodes                 | 23440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00699   |
| train/info_shaping_reward_mean | -0.0655    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 937600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.2051828   |
| stats_o/std                    | 0.063693434 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00343    |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1652093  |
| test/Q_plus_P                  | -1.1652093  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00833    |
| train/info_shaping_reward_mean | -0.0682     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 587         |
| stats_o/mean                   | 0.20518786  |
| stats_o/std                    | 0.063720345 |
| test/episodes                  | 5880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0135     |
| test/info_shaping_reward_mean  | -0.0538     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1062676  |
| test/Q_plus_P                  | -1.1062676  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 235200      |
| train/episodes                 | 23520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00679    |
| train/info_shaping_reward_mean | -0.0773     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 940800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 588        |
| stats_o/mean                   | 0.20519231 |
| stats_o/std                    | 0.06374027 |
| test/episodes                  | 5890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0147    |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1466011 |
| test/Q_plus_P                  | -1.1466011 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 235600     |
| train/episodes                 | 23560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00565   |
| train/info_shaping_reward_mean | -0.0791    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 942400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 589         |
| stats_o/mean                   | 0.20519494  |
| stats_o/std                    | 0.063736364 |
| test/episodes                  | 5900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0092     |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.956592   |
| test/Q_plus_P                  | -0.956592   |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 236000      |
| train/episodes                 | 23600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00738    |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 944000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.2051934   |
| stats_o/std                    | 0.063722186 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00342    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1629459  |
| test/Q_plus_P                  | -1.1629459  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00836    |
| train/info_shaping_reward_mean | -0.0643     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 591        |
| stats_o/mean                   | 0.20519517 |
| stats_o/std                    | 0.06374022 |
| test/episodes                  | 5920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0108    |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3223343 |
| test/Q_plus_P                  | -1.3223343 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 236800     |
| train/episodes                 | 23680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00635   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 947200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 592         |
| stats_o/mean                   | 0.20520641  |
| stats_o/std                    | 0.063774355 |
| test/episodes                  | 5930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0057     |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1310048  |
| test/Q_plus_P                  | -1.1310048  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 237200      |
| train/episodes                 | 23720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.605       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00817    |
| train/info_shaping_reward_mean | -0.0762     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 948800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 593        |
| stats_o/mean                   | 0.20520696 |
| stats_o/std                    | 0.06379981 |
| test/episodes                  | 5940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00937   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0894817 |
| test/Q_plus_P                  | -1.0894817 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 237600     |
| train/episodes                 | 23760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00864   |
| train/info_shaping_reward_mean | -0.0812    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 950400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 594        |
| stats_o/mean                   | 0.20520972 |
| stats_o/std                    | 0.0637714  |
| test/episodes                  | 5950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00653   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1145941 |
| test/Q_plus_P                  | -1.1145941 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 238000     |
| train/episodes                 | 23800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0063    |
| train/info_shaping_reward_mean | -0.0663    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 952000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 595        |
| stats_o/mean                   | 0.20521216 |
| stats_o/std                    | 0.06376212 |
| test/episodes                  | 5960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0148    |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0739262 |
| test/Q_plus_P                  | -1.0739262 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 238400     |
| train/episodes                 | 23840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.67       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00463   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 953600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 596         |
| stats_o/mean                   | 0.20520924  |
| stats_o/std                    | 0.063777044 |
| test/episodes                  | 5970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00882    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1045785  |
| test/Q_plus_P                  | -1.1045785  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 238800      |
| train/episodes                 | 23880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00543    |
| train/info_shaping_reward_mean | -0.0748     |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 955200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 597        |
| stats_o/mean                   | 0.20521471 |
| stats_o/std                    | 0.06379734 |
| test/episodes                  | 5980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00674   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0915556 |
| test/Q_plus_P                  | -1.0915556 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 239200     |
| train/episodes                 | 23920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00893   |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 956800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.20522058  |
| stats_o/std                    | 0.063842185 |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00324    |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.94955224 |
| test/Q_plus_P                  | -0.94955224 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.526       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0103     |
| train/info_shaping_reward_mean | -0.088      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19         |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.20522895 |
| stats_o/std                    | 0.06384583 |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00497   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0959004 |
| test/Q_plus_P                  | -1.0959004 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0116    |
| train/info_shaping_reward_mean | -0.0719    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
