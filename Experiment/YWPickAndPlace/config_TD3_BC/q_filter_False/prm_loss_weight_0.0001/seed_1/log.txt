Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPickAndPlace/config_TD3_BC/q_filter_False/prm_loss_weight_0.0001/seed_1
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.19995424  |
| stats_o/std                    | 0.101063184 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.215       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00677    |
| test/info_shaping_reward_mean  | -0.126      |
| test/info_shaping_reward_min   | -0.387      |
| test/Q                         | -1.0189661  |
| test/Q_plus_P                  | -1.0189661  |
| test/reward_per_eps            | -31.4       |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00562     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.215      |
| train/info_shaping_reward_min  | -0.407      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.8       |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.20087333  |
| stats_o/std                    | 0.113266096 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.165       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0022     |
| test/info_shaping_reward_mean  | -0.139      |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.4507034  |
| test/Q_plus_P                  | -1.4507034  |
| test/reward_per_eps            | -33.4       |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0112      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.113      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.526      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 2          |
| stats_o/mean                   | 0.19771452 |
| stats_o/std                    | 0.12087764 |
| test/episodes                  | 30         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.228      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00492   |
| test/info_shaping_reward_mean  | -0.134     |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -1.6966785 |
| test/Q_plus_P                  | -1.6966785 |
| test/reward_per_eps            | -30.9      |
| test/steps                     | 1200       |
| train/episodes                 | 120        |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0231     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0901    |
| train/info_shaping_reward_mean | -0.211     |
| train/info_shaping_reward_min  | -0.448     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 4800       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.19962877 |
| stats_o/std                    | 0.12385052 |
| test/episodes                  | 40         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.223      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00293   |
| test/info_shaping_reward_mean  | -0.124     |
| test/info_shaping_reward_min   | -0.223     |
| test/Q                         | -2.0402887 |
| test/Q_plus_P                  | -2.0402887 |
| test/reward_per_eps            | -31.1      |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0106     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.578     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 4          |
| stats_o/mean                   | 0.19879486 |
| stats_o/std                    | 0.12810583 |
| test/episodes                  | 50         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.297      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000536  |
| test/info_shaping_reward_mean  | -0.111     |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -2.2295065 |
| test/Q_plus_P                  | -2.2295065 |
| test/reward_per_eps            | -28.1      |
| test/steps                     | 2000       |
| train/episodes                 | 200        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00125    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.122     |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.494     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 8000       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 5           |
| stats_o/mean                   | 0.19911629  |
| stats_o/std                    | 0.124232225 |
| test/episodes                  | 60          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.292       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00287    |
| test/info_shaping_reward_mean  | -0.108      |
| test/info_shaping_reward_min   | -0.194      |
| test/Q                         | -2.4591923  |
| test/Q_plus_P                  | -2.4591923  |
| test/reward_per_eps            | -28.3       |
| test/steps                     | 2400        |
| train/episodes                 | 240         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0263      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.211      |
| train/info_shaping_reward_min  | -0.387      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39         |
| train/steps                    | 9600        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 6          |
| stats_o/mean                   | 0.200037   |
| stats_o/std                    | 0.12474608 |
| test/episodes                  | 70         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.328      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0037    |
| test/info_shaping_reward_mean  | -0.109     |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -2.672363  |
| test/Q_plus_P                  | -2.672363  |
| test/reward_per_eps            | -26.9      |
| test/steps                     | 2800       |
| train/episodes                 | 280        |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0425     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0932    |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.423     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.3      |
| train/steps                    | 11200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 7           |
| stats_o/mean                   | 0.1994819   |
| stats_o/std                    | 0.123700075 |
| test/episodes                  | 80          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.182       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00716    |
| test/info_shaping_reward_mean  | -0.131      |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -3.4812622  |
| test/Q_plus_P                  | -3.4812622  |
| test/reward_per_eps            | -32.7       |
| test/steps                     | 3200        |
| train/episodes                 | 320         |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0512      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.101      |
| train/info_shaping_reward_mean | -0.201      |
| train/info_shaping_reward_min  | -0.442      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38         |
| train/steps                    | 12800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 8          |
| stats_o/mean                   | 0.19960946 |
| stats_o/std                    | 0.12399349 |
| test/episodes                  | 90         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.375      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00303   |
| test/info_shaping_reward_mean  | -0.103     |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -3.3676848 |
| test/Q_plus_P                  | -3.3676848 |
| test/reward_per_eps            | -25        |
| test/steps                     | 3600       |
| train/episodes                 | 360        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.005      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.458     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 14400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 9          |
| stats_o/mean                   | 0.19984543 |
| stats_o/std                    | 0.12333146 |
| test/episodes                  | 100        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.168      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00625   |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -4.0663495 |
| test/Q_plus_P                  | -4.0663495 |
| test/reward_per_eps            | -33.3      |
| test/steps                     | 4000       |
| train/episodes                 | 400        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.209     |
| train/info_shaping_reward_min  | -0.417     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 16000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 10         |
| stats_o/mean                   | 0.19945575 |
| stats_o/std                    | 0.12308242 |
| test/episodes                  | 110        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.268      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00098   |
| test/info_shaping_reward_mean  | -0.114     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -3.9557726 |
| test/Q_plus_P                  | -3.9557726 |
| test/reward_per_eps            | -29.3      |
| test/steps                     | 4400       |
| train/episodes                 | 440        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00313    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.423     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 17600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 11          |
| stats_o/mean                   | 0.19971848  |
| stats_o/std                    | 0.122463316 |
| test/episodes                  | 120         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.458       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000539   |
| test/info_shaping_reward_mean  | -0.107      |
| test/info_shaping_reward_min   | -0.361      |
| test/Q                         | -3.7199852  |
| test/Q_plus_P                  | -3.7199852  |
| test/reward_per_eps            | -21.7       |
| test/steps                     | 4800        |
| train/episodes                 | 480         |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0231      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.105      |
| train/info_shaping_reward_mean | -0.204      |
| train/info_shaping_reward_min  | -0.418      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.1       |
| train/steps                    | 19200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.2000648   |
| stats_o/std                    | 0.122121595 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.302       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00523    |
| test/info_shaping_reward_mean  | -0.109      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -4.271012   |
| test/Q_plus_P                  | -4.271012   |
| test/reward_per_eps            | -27.9       |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00813     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.195      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 13          |
| stats_o/mean                   | 0.20006973  |
| stats_o/std                    | 0.122092284 |
| test/episodes                  | 140         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.295       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00423    |
| test/info_shaping_reward_mean  | -0.118      |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -4.7537065  |
| test/Q_plus_P                  | -4.7537065  |
| test/reward_per_eps            | -28.2       |
| test/steps                     | 5600        |
| train/episodes                 | 560         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0163      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.105      |
| train/info_shaping_reward_mean | -0.198      |
| train/info_shaping_reward_min  | -0.438      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 22400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 14          |
| stats_o/mean                   | 0.20036104  |
| stats_o/std                    | 0.123011544 |
| test/episodes                  | 150         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.383       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.109      |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -4.427756   |
| test/Q_plus_P                  | -4.427756   |
| test/reward_per_eps            | -24.7       |
| test/steps                     | 6000        |
| train/episodes                 | 600         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.015       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.115      |
| train/info_shaping_reward_mean | -0.193      |
| train/info_shaping_reward_min  | -0.376      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 24000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 15         |
| stats_o/mean                   | 0.20062342 |
| stats_o/std                    | 0.12277498 |
| test/episodes                  | 160        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.41       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00194   |
| test/info_shaping_reward_mean  | -0.129     |
| test/info_shaping_reward_min   | -0.587     |
| test/Q                         | -4.433992  |
| test/Q_plus_P                  | -4.433992  |
| test/reward_per_eps            | -23.6      |
| test/steps                     | 6400       |
| train/episodes                 | 640        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00813    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.373     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 25600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 16         |
| stats_o/mean                   | 0.20042847 |
| stats_o/std                    | 0.12285056 |
| test/episodes                  | 170        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.212      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00332   |
| test/info_shaping_reward_mean  | -0.134     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -5.993406  |
| test/Q_plus_P                  | -5.993406  |
| test/reward_per_eps            | -31.5      |
| test/steps                     | 6800       |
| train/episodes                 | 680        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0294     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.109     |
| train/info_shaping_reward_mean | -0.201     |
| train/info_shaping_reward_min  | -0.401     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 27200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 17         |
| stats_o/mean                   | 0.20062758 |
| stats_o/std                    | 0.12309245 |
| test/episodes                  | 180        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.33       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00449   |
| test/info_shaping_reward_mean  | -0.101     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -5.0027843 |
| test/Q_plus_P                  | -5.0027843 |
| test/reward_per_eps            | -26.8      |
| test/steps                     | 7200       |
| train/episodes                 | 720        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0138     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.546     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 28800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 18          |
| stats_o/mean                   | 0.20007133  |
| stats_o/std                    | 0.123611994 |
| test/episodes                  | 190         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.41        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00222    |
| test/info_shaping_reward_mean  | -0.0988     |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -4.9606133  |
| test/Q_plus_P                  | -4.9606133  |
| test/reward_per_eps            | -23.6       |
| test/steps                     | 7600        |
| train/episodes                 | 760         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0075      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.456      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 30400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.19980513 |
| stats_o/std                    | 0.12358237 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.287      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00365   |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.533     |
| test/Q                         | -5.850874  |
| test/Q_plus_P                  | -5.850874  |
| test/reward_per_eps            | -28.5      |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0163     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.112     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.45      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 20         |
| stats_o/mean                   | 0.19961247 |
| stats_o/std                    | 0.12415665 |
| test/episodes                  | 210        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.403      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00133   |
| test/info_shaping_reward_mean  | -0.0911    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -5.2451563 |
| test/Q_plus_P                  | -5.2451563 |
| test/reward_per_eps            | -23.9      |
| test/steps                     | 8400       |
| train/episodes                 | 840        |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0412     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0959    |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.548     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 33600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 21         |
| stats_o/mean                   | 0.19970807 |
| stats_o/std                    | 0.12428723 |
| test/episodes                  | 220        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.383      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0126    |
| test/info_shaping_reward_mean  | -0.101     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -5.5459814 |
| test/Q_plus_P                  | -5.5459814 |
| test/reward_per_eps            | -24.7      |
| test/steps                     | 8800       |
| train/episodes                 | 880        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.114     |
| train/info_shaping_reward_mean | -0.203     |
| train/info_shaping_reward_min  | -0.382     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 35200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 22         |
| stats_o/mean                   | 0.19962916 |
| stats_o/std                    | 0.12400904 |
| test/episodes                  | 230        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.388      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00276   |
| test/info_shaping_reward_mean  | -0.0984    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -5.8952293 |
| test/Q_plus_P                  | -5.8952293 |
| test/reward_per_eps            | -24.5      |
| test/steps                     | 9200       |
| train/episodes                 | 920        |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0312     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0814    |
| train/info_shaping_reward_mean | -0.212     |
| train/info_shaping_reward_min  | -0.419     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 36800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 23          |
| stats_o/mean                   | 0.1996977   |
| stats_o/std                    | 0.124427244 |
| test/episodes                  | 240         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.395       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00342    |
| test/info_shaping_reward_mean  | -0.0985     |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -5.6593227  |
| test/Q_plus_P                  | -5.6593227  |
| test/reward_per_eps            | -24.2       |
| test/steps                     | 9600        |
| train/episodes                 | 960         |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0263      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.104      |
| train/info_shaping_reward_mean | -0.21       |
| train/info_shaping_reward_min  | -0.451      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39         |
| train/steps                    | 38400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.19976066  |
| stats_o/std                    | 0.124990985 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.388       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000527   |
| test/info_shaping_reward_mean  | -0.103      |
| test/info_shaping_reward_min   | -0.199      |
| test/Q                         | -6.1781287  |
| test/Q_plus_P                  | -6.1781287  |
| test/reward_per_eps            | -24.5       |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0244      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0997     |
| train/info_shaping_reward_mean | -0.185      |
| train/info_shaping_reward_min  | -0.301      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39         |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 25         |
| stats_o/mean                   | 0.1999536  |
| stats_o/std                    | 0.12456619 |
| test/episodes                  | 260        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.282      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00422   |
| test/info_shaping_reward_mean  | -0.123     |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -7.417056  |
| test/Q_plus_P                  | -7.417056  |
| test/reward_per_eps            | -28.7      |
| test/steps                     | 10400      |
| train/episodes                 | 1040       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0194     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.194     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 41600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 26         |
| stats_o/mean                   | 0.19981888 |
| stats_o/std                    | 0.12404725 |
| test/episodes                  | 270        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.245      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00533   |
| test/info_shaping_reward_mean  | -0.121     |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -7.9527698 |
| test/Q_plus_P                  | -7.9527698 |
| test/reward_per_eps            | -30.2      |
| test/steps                     | 10800      |
| train/episodes                 | 1080       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.02       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.106     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.296     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 43200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 27         |
| stats_o/mean                   | 0.19970289 |
| stats_o/std                    | 0.12430666 |
| test/episodes                  | 280        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.235      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00488   |
| test/info_shaping_reward_mean  | -0.116     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -8.108367  |
| test/Q_plus_P                  | -8.108367  |
| test/reward_per_eps            | -30.6      |
| test/steps                     | 11200      |
| train/episodes                 | 1120       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0194     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.101     |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.486     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 44800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.20013525 |
| stats_o/std                    | 0.1240384  |
| test/episodes                  | 290        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.23       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00623   |
| test/info_shaping_reward_mean  | -0.127     |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -8.974341  |
| test/Q_plus_P                  | -8.974341  |
| test/reward_per_eps            | -30.8      |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00313    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.115     |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.292     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.20026827 |
| stats_o/std                    | 0.12409618 |
| test/episodes                  | 300        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.393      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00493   |
| test/info_shaping_reward_mean  | -0.1       |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -6.721255  |
| test/Q_plus_P                  | -6.721255  |
| test/reward_per_eps            | -24.3      |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00562    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.472     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 30         |
| stats_o/mean                   | 0.20036344 |
| stats_o/std                    | 0.12484457 |
| test/episodes                  | 310        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.365      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00479   |
| test/info_shaping_reward_mean  | -0.112     |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -7.0684686 |
| test/Q_plus_P                  | -7.0684686 |
| test/reward_per_eps            | -25.4      |
| test/steps                     | 12400      |
| train/episodes                 | 1240       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0187     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.109     |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.548     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 49600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 31          |
| stats_o/mean                   | 0.20044634  |
| stats_o/std                    | 0.124661066 |
| test/episodes                  | 320         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.482       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00143    |
| test/info_shaping_reward_mean  | -0.0876     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -5.3631086  |
| test/Q_plus_P                  | -5.3631086  |
| test/reward_per_eps            | -20.7       |
| test/steps                     | 12800       |
| train/episodes                 | 1280        |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0344      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.094      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.538      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.6       |
| train/steps                    | 51200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 32         |
| stats_o/mean                   | 0.20057474 |
| stats_o/std                    | 0.12533289 |
| test/episodes                  | 330        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.362      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00563   |
| test/info_shaping_reward_mean  | -0.106     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -7.345383  |
| test/Q_plus_P                  | -7.345383  |
| test/reward_per_eps            | -25.5      |
| test/steps                     | 13200      |
| train/episodes                 | 1320       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0556     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0766    |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.317     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 52800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 33         |
| stats_o/mean                   | 0.20076603 |
| stats_o/std                    | 0.12548752 |
| test/episodes                  | 340        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.38       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00216   |
| test/info_shaping_reward_mean  | -0.126     |
| test/info_shaping_reward_min   | -0.543     |
| test/Q                         | -7.236849  |
| test/Q_plus_P                  | -7.236849  |
| test/reward_per_eps            | -24.8      |
| test/steps                     | 13600      |
| train/episodes                 | 1360       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0275     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0991    |
| train/info_shaping_reward_mean | -0.211     |
| train/info_shaping_reward_min  | -0.44      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 54400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 34         |
| stats_o/mean                   | 0.20093684 |
| stats_o/std                    | 0.12507015 |
| test/episodes                  | 350        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.33       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00346   |
| test/info_shaping_reward_mean  | -0.105     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -7.64323   |
| test/Q_plus_P                  | -7.64323   |
| test/reward_per_eps            | -26.8      |
| test/steps                     | 14000      |
| train/episodes                 | 1400       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0194     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0973    |
| train/info_shaping_reward_mean | -0.205     |
| train/info_shaping_reward_min  | -0.4       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 56000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 35          |
| stats_o/mean                   | 0.20091826  |
| stats_o/std                    | 0.124990575 |
| test/episodes                  | 360         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.372       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00413    |
| test/info_shaping_reward_mean  | -0.106      |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -8.427134   |
| test/Q_plus_P                  | -8.427134   |
| test/reward_per_eps            | -25.1       |
| test/steps                     | 14400       |
| train/episodes                 | 1440        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0138      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.113      |
| train/info_shaping_reward_mean | -0.185      |
| train/info_shaping_reward_min  | -0.283      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 57600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 36         |
| stats_o/mean                   | 0.20095083 |
| stats_o/std                    | 0.125491   |
| test/episodes                  | 370        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.318      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00539   |
| test/info_shaping_reward_mean  | -0.116     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -8.871611  |
| test/Q_plus_P                  | -8.871611  |
| test/reward_per_eps            | -27.3      |
| test/steps                     | 14800      |
| train/episodes                 | 1480       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0138     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.118     |
| train/info_shaping_reward_mean | -0.196     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 59200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 37         |
| stats_o/mean                   | 0.20104368 |
| stats_o/std                    | 0.12607923 |
| test/episodes                  | 380        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.27       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0116    |
| test/info_shaping_reward_mean  | -0.118     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -9.579245  |
| test/Q_plus_P                  | -9.579245  |
| test/reward_per_eps            | -29.2      |
| test/steps                     | 15200      |
| train/episodes                 | 1520       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0331     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0805    |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.613     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 60800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 38         |
| stats_o/mean                   | 0.20100988 |
| stats_o/std                    | 0.12587889 |
| test/episodes                  | 390        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.278      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00479   |
| test/info_shaping_reward_mean  | -0.122     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -10.311089 |
| test/Q_plus_P                  | -10.311089 |
| test/reward_per_eps            | -28.9      |
| test/steps                     | 15600      |
| train/episodes                 | 1560       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0281     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.094     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 62400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 39         |
| stats_o/mean                   | 0.20084268 |
| stats_o/std                    | 0.12636857 |
| test/episodes                  | 400        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.372      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00135   |
| test/info_shaping_reward_mean  | -0.11      |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -7.593208  |
| test/Q_plus_P                  | -7.593208  |
| test/reward_per_eps            | -25.1      |
| test/steps                     | 16000      |
| train/episodes                 | 1600       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0362     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0826    |
| train/info_shaping_reward_mean | -0.212     |
| train/info_shaping_reward_min  | -0.443     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 64000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 40         |
| stats_o/mean                   | 0.2008853  |
| stats_o/std                    | 0.12610556 |
| test/episodes                  | 410        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.38       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00294   |
| test/info_shaping_reward_mean  | -0.107     |
| test/info_shaping_reward_min   | -0.211     |
| test/Q                         | -8.18892   |
| test/Q_plus_P                  | -8.18892   |
| test/reward_per_eps            | -24.8      |
| test/steps                     | 16400      |
| train/episodes                 | 1640       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0344     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0872    |
| train/info_shaping_reward_mean | -0.205     |
| train/info_shaping_reward_min  | -0.434     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 65600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.20088194  |
| stats_o/std                    | 0.12587707  |
| test/episodes                  | 420         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.24        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00812    |
| test/info_shaping_reward_mean  | -0.124      |
| test/info_shaping_reward_min   | -0.191      |
| test/Q                         | -10.8010645 |
| test/Q_plus_P                  | -10.8010645 |
| test/reward_per_eps            | -30.4       |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0312      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0921     |
| train/info_shaping_reward_mean | -0.189      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.8       |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 42         |
| stats_o/mean                   | 0.20079392 |
| stats_o/std                    | 0.12592748 |
| test/episodes                  | 430        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.395      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00439   |
| test/info_shaping_reward_mean  | -0.11      |
| test/info_shaping_reward_min   | -0.33      |
| test/Q                         | -7.8548374 |
| test/Q_plus_P                  | -7.8548374 |
| test/reward_per_eps            | -24.2      |
| test/steps                     | 17200      |
| train/episodes                 | 1720       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0238     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.104     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.437     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 68800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 43         |
| stats_o/mean                   | 0.2008985  |
| stats_o/std                    | 0.12595437 |
| test/episodes                  | 440        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.37       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00484   |
| test/info_shaping_reward_mean  | -0.107     |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -9.011805  |
| test/Q_plus_P                  | -9.011805  |
| test/reward_per_eps            | -25.2      |
| test/steps                     | 17600      |
| train/episodes                 | 1760       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.115     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.537     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 70400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 44         |
| stats_o/mean                   | 0.20117196 |
| stats_o/std                    | 0.12583162 |
| test/episodes                  | 450        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.355      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00319   |
| test/info_shaping_reward_mean  | -0.112     |
| test/info_shaping_reward_min   | -0.328     |
| test/Q                         | -8.207554  |
| test/Q_plus_P                  | -8.207554  |
| test/reward_per_eps            | -25.8      |
| test/steps                     | 18000      |
| train/episodes                 | 1800       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0213     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0983    |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.416     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 72000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 45         |
| stats_o/mean                   | 0.20128155 |
| stats_o/std                    | 0.12612732 |
| test/episodes                  | 460        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.432      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00917   |
| test/info_shaping_reward_mean  | -0.0941    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -7.5670595 |
| test/Q_plus_P                  | -7.5670595 |
| test/reward_per_eps            | -22.7      |
| test/steps                     | 18400      |
| train/episodes                 | 1840       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0506     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0912    |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.524     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38        |
| train/steps                    | 73600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 46         |
| stats_o/mean                   | 0.20135832 |
| stats_o/std                    | 0.12620209 |
| test/episodes                  | 470        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.38       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00746   |
| test/info_shaping_reward_mean  | -0.105     |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -8.023647  |
| test/Q_plus_P                  | -8.023647  |
| test/reward_per_eps            | -24.8      |
| test/steps                     | 18800      |
| train/episodes                 | 1880       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0306     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0641    |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 75200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 47         |
| stats_o/mean                   | 0.20133257 |
| stats_o/std                    | 0.12613408 |
| test/episodes                  | 480        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.403      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00377   |
| test/info_shaping_reward_mean  | -0.0956    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -7.248897  |
| test/Q_plus_P                  | -7.248897  |
| test/reward_per_eps            | -23.9      |
| test/steps                     | 19200      |
| train/episodes                 | 1920       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0437     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0757    |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.406     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.2      |
| train/steps                    | 76800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 48         |
| stats_o/mean                   | 0.20150132 |
| stats_o/std                    | 0.12683205 |
| test/episodes                  | 490        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.427      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00254   |
| test/info_shaping_reward_mean  | -0.0948    |
| test/info_shaping_reward_min   | -0.226     |
| test/Q                         | -7.299883  |
| test/Q_plus_P                  | -7.299883  |
| test/reward_per_eps            | -22.9      |
| test/steps                     | 19600      |
| train/episodes                 | 1960       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.035      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0751    |
| train/info_shaping_reward_mean | -0.222     |
| train/info_shaping_reward_min  | -0.467     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 78400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 49         |
| stats_o/mean                   | 0.2014876  |
| stats_o/std                    | 0.1270226  |
| test/episodes                  | 500        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.297      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00305   |
| test/info_shaping_reward_mean  | -0.128     |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -10.796001 |
| test/Q_plus_P                  | -10.796001 |
| test/reward_per_eps            | -28.1      |
| test/steps                     | 20000      |
| train/episodes                 | 2000       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0263     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0913    |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.459     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 80000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.20151235 |
| stats_o/std                    | 0.12706722 |
| test/episodes                  | 510        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0825     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0111    |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -12.565797 |
| test/Q_plus_P                  | -12.565797 |
| test/reward_per_eps            | -36.7      |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0369     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0802    |
| train/info_shaping_reward_mean | -0.199     |
| train/info_shaping_reward_min  | -0.45      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.20161794 |
| stats_o/std                    | 0.12765065 |
| test/episodes                  | 520        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.188      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00925   |
| test/info_shaping_reward_mean  | -0.133     |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -9.864347  |
| test/Q_plus_P                  | -9.864347  |
| test/reward_per_eps            | -32.5      |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0369     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0725    |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.564     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.20172635 |
| stats_o/std                    | 0.12815474 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.403      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00987   |
| test/info_shaping_reward_mean  | -0.109     |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -7.229028  |
| test/Q_plus_P                  | -7.229028  |
| test/reward_per_eps            | -23.9      |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00313    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0922    |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.534     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 53         |
| stats_o/mean                   | 0.20175608 |
| stats_o/std                    | 0.12857324 |
| test/episodes                  | 540        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.28       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00308   |
| test/info_shaping_reward_mean  | -0.112     |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -8.629434  |
| test/Q_plus_P                  | -8.629434  |
| test/reward_per_eps            | -28.8      |
| test/steps                     | 21600      |
| train/episodes                 | 2160       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0481     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0456    |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.375     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.1      |
| train/steps                    | 86400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 54         |
| stats_o/mean                   | 0.20177959 |
| stats_o/std                    | 0.1287159  |
| test/episodes                  | 550        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.152      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00374   |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -11.561799 |
| test/Q_plus_P                  | -11.561799 |
| test/reward_per_eps            | -33.9      |
| test/steps                     | 22000      |
| train/episodes                 | 2200       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0338     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0577    |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.377     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 88000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 55         |
| stats_o/mean                   | 0.20175156 |
| stats_o/std                    | 0.12871689 |
| test/episodes                  | 560        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.133      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00758   |
| test/info_shaping_reward_mean  | -0.141     |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -10.085189 |
| test/Q_plus_P                  | -10.085189 |
| test/reward_per_eps            | -34.7      |
| test/steps                     | 22400      |
| train/episodes                 | 2240       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0194     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0612    |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.365     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 89600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 56         |
| stats_o/mean                   | 0.20173427 |
| stats_o/std                    | 0.12953793 |
| test/episodes                  | 570        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.128      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0165    |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.508     |
| test/Q                         | -11.278804 |
| test/Q_plus_P                  | -11.278804 |
| test/reward_per_eps            | -34.9      |
| test/steps                     | 22800      |
| train/episodes                 | 2280       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.04       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0601    |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.376     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 91200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 57         |
| stats_o/mean                   | 0.20176643 |
| stats_o/std                    | 0.12975936 |
| test/episodes                  | 580        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.278      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00319   |
| test/info_shaping_reward_mean  | -0.124     |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -8.977709  |
| test/Q_plus_P                  | -8.977709  |
| test/reward_per_eps            | -28.9      |
| test/steps                     | 23200      |
| train/episodes                 | 2320       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0381     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0567    |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.37      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 92800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 58         |
| stats_o/mean                   | 0.20185831 |
| stats_o/std                    | 0.12962334 |
| test/episodes                  | 590        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.147      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00848   |
| test/info_shaping_reward_mean  | -0.127     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -9.319745  |
| test/Q_plus_P                  | -9.319745  |
| test/reward_per_eps            | -34.1      |
| test/steps                     | 23600      |
| train/episodes                 | 2360       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.06       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0505    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.6      |
| train/steps                    | 94400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 59         |
| stats_o/mean                   | 0.20181885 |
| stats_o/std                    | 0.1296805  |
| test/episodes                  | 600        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.117      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.019     |
| test/info_shaping_reward_mean  | -0.151     |
| test/info_shaping_reward_min   | -0.575     |
| test/Q                         | -10.940407 |
| test/Q_plus_P                  | -10.940407 |
| test/reward_per_eps            | -35.3      |
| test/steps                     | 24000      |
| train/episodes                 | 2400       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0119     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0682    |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 96000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 60         |
| stats_o/mean                   | 0.20196077 |
| stats_o/std                    | 0.12945631 |
| test/episodes                  | 610        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.233      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00987   |
| test/info_shaping_reward_mean  | -0.14      |
| test/info_shaping_reward_min   | -0.606     |
| test/Q                         | -10.049071 |
| test/Q_plus_P                  | -10.049071 |
| test/reward_per_eps            | -30.7      |
| test/steps                     | 24400      |
| train/episodes                 | 2440       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0369     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.048     |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 97600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 61         |
| stats_o/mean                   | 0.2020241  |
| stats_o/std                    | 0.12909451 |
| test/episodes                  | 620        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.223      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000912  |
| test/info_shaping_reward_mean  | -0.123     |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -9.306114  |
| test/Q_plus_P                  | -9.306114  |
| test/reward_per_eps            | -31.1      |
| test/steps                     | 24800      |
| train/episodes                 | 2480       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0619     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0402    |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.38      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 99200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 62         |
| stats_o/mean                   | 0.20205143 |
| stats_o/std                    | 0.12882036 |
| test/episodes                  | 630        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.263      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00244   |
| test/info_shaping_reward_mean  | -0.118     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -8.971314  |
| test/Q_plus_P                  | -8.971314  |
| test/reward_per_eps            | -29.5      |
| test/steps                     | 25200      |
| train/episodes                 | 2520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.106      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0182    |
| train/info_shaping_reward_mean | -0.16      |
| train/info_shaping_reward_min  | -0.36      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.8      |
| train/steps                    | 100800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 63         |
| stats_o/mean                   | 0.20211786 |
| stats_o/std                    | 0.12884866 |
| test/episodes                  | 640        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.403      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00646   |
| test/info_shaping_reward_mean  | -0.0952    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -6.812704  |
| test/Q_plus_P                  | -6.812704  |
| test/reward_per_eps            | -23.9      |
| test/steps                     | 25600      |
| train/episodes                 | 2560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.0844     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0182    |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.393     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.6      |
| train/steps                    | 102400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.2021632  |
| stats_o/std                    | 0.12886256 |
| test/episodes                  | 650        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.417      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00389   |
| test/info_shaping_reward_mean  | -0.0976    |
| test/info_shaping_reward_min   | -0.226     |
| test/Q                         | -7.229735  |
| test/Q_plus_P                  | -7.229735  |
| test/reward_per_eps            | -23.3      |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.0975     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0267    |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.379     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.1      |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 65         |
| stats_o/mean                   | 0.2020511  |
| stats_o/std                    | 0.12847158 |
| test/episodes                  | 660        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.425      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00326   |
| test/info_shaping_reward_mean  | -0.0874    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -7.541001  |
| test/Q_plus_P                  | -7.541001  |
| test/reward_per_eps            | -23        |
| test/steps                     | 26400      |
| train/episodes                 | 2640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.2        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0118    |
| train/info_shaping_reward_mean | -0.129     |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32        |
| train/steps                    | 105600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 66         |
| stats_o/mean                   | 0.2019703  |
| stats_o/std                    | 0.1284501  |
| test/episodes                  | 670        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.54       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00549   |
| test/info_shaping_reward_mean  | -0.0836    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -6.1818256 |
| test/Q_plus_P                  | -6.1818256 |
| test/reward_per_eps            | -18.4      |
| test/steps                     | 26800      |
| train/episodes                 | 2680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.13       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0158    |
| train/info_shaping_reward_mean | -0.154     |
| train/info_shaping_reward_min  | -0.384     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.8      |
| train/steps                    | 107200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 67         |
| stats_o/mean                   | 0.20193896 |
| stats_o/std                    | 0.12816434 |
| test/episodes                  | 680        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.613      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00244   |
| test/info_shaping_reward_mean  | -0.0717    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -4.7254877 |
| test/Q_plus_P                  | -4.7254877 |
| test/reward_per_eps            | -15.5      |
| test/steps                     | 27200      |
| train/episodes                 | 2720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.177      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0117    |
| train/info_shaping_reward_mean | -0.147     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.9      |
| train/steps                    | 108800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 68         |
| stats_o/mean                   | 0.20191684 |
| stats_o/std                    | 0.1279388  |
| test/episodes                  | 690        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.605      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00411   |
| test/info_shaping_reward_mean  | -0.0758    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -5.281289  |
| test/Q_plus_P                  | -5.281289  |
| test/reward_per_eps            | -15.8      |
| test/steps                     | 27600      |
| train/episodes                 | 2760       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.149      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.02      |
| train/info_shaping_reward_mean | -0.153     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34        |
| train/steps                    | 110400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 69         |
| stats_o/mean                   | 0.20200647 |
| stats_o/std                    | 0.1275946  |
| test/episodes                  | 700        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.552      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00425   |
| test/info_shaping_reward_mean  | -0.0794    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -4.813341  |
| test/Q_plus_P                  | -4.813341  |
| test/reward_per_eps            | -17.9      |
| test/steps                     | 28000      |
| train/episodes                 | 2800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.171      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0102    |
| train/info_shaping_reward_mean | -0.133     |
| train/info_shaping_reward_min  | -0.301     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.1      |
| train/steps                    | 112000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.20195822 |
| stats_o/std                    | 0.12723222 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.56       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00455   |
| test/info_shaping_reward_mean  | -0.0778    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -4.5860095 |
| test/Q_plus_P                  | -4.5860095 |
| test/reward_per_eps            | -17.6      |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.223      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0105    |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.1      |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 71         |
| stats_o/mean                   | 0.20188366 |
| stats_o/std                    | 0.1268952  |
| test/episodes                  | 720        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.515      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00341   |
| test/info_shaping_reward_mean  | -0.0866    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -5.7272506 |
| test/Q_plus_P                  | -5.7272506 |
| test/reward_per_eps            | -19.4      |
| test/steps                     | 28800      |
| train/episodes                 | 2880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.208      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.016     |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.7      |
| train/steps                    | 115200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 72         |
| stats_o/mean                   | 0.20185684 |
| stats_o/std                    | 0.12650357 |
| test/episodes                  | 730        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.58       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00302   |
| test/info_shaping_reward_mean  | -0.0769    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -4.4072943 |
| test/Q_plus_P                  | -4.4072943 |
| test/reward_per_eps            | -16.8      |
| test/steps                     | 29200      |
| train/episodes                 | 2920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.228      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0118    |
| train/info_shaping_reward_mean | -0.123     |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.9      |
| train/steps                    | 116800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.2018214  |
| stats_o/std                    | 0.12606053 |
| test/episodes                  | 740        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.495      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00451   |
| test/info_shaping_reward_mean  | -0.0835    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -4.754416  |
| test/Q_plus_P                  | -4.754416  |
| test/reward_per_eps            | -20.2      |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.252      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0138    |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.9      |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 74         |
| stats_o/mean                   | 0.20175986 |
| stats_o/std                    | 0.12555583 |
| test/episodes                  | 750        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.595      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00118   |
| test/info_shaping_reward_mean  | -0.0717    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -4.141229  |
| test/Q_plus_P                  | -4.141229  |
| test/reward_per_eps            | -16.2      |
| test/steps                     | 30000      |
| train/episodes                 | 3000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.269      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00787   |
| train/info_shaping_reward_mean | -0.114     |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.2      |
| train/steps                    | 120000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 75         |
| stats_o/mean                   | 0.20173146 |
| stats_o/std                    | 0.12524033 |
| test/episodes                  | 760        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.527      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0012    |
| test/info_shaping_reward_mean  | -0.0839    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -5.039265  |
| test/Q_plus_P                  | -5.039265  |
| test/reward_per_eps            | -18.9      |
| test/steps                     | 30400      |
| train/episodes                 | 3040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.206      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0149    |
| train/info_shaping_reward_mean | -0.118     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.8      |
| train/steps                    | 121600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 76         |
| stats_o/mean                   | 0.20160046 |
| stats_o/std                    | 0.12492879 |
| test/episodes                  | 770        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.497      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00365   |
| test/info_shaping_reward_mean  | -0.0859    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -5.151716  |
| test/Q_plus_P                  | -5.151716  |
| test/reward_per_eps            | -20.1      |
| test/steps                     | 30800      |
| train/episodes                 | 3080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.247      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.013     |
| train/info_shaping_reward_mean | -0.127     |
| train/info_shaping_reward_min  | -0.288     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.1      |
| train/steps                    | 123200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 77         |
| stats_o/mean                   | 0.20157863 |
| stats_o/std                    | 0.12473752 |
| test/episodes                  | 780        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.585      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0752    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -4.202347  |
| test/Q_plus_P                  | -4.202347  |
| test/reward_per_eps            | -16.6      |
| test/steps                     | 31200      |
| train/episodes                 | 3120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.257      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0101    |
| train/info_shaping_reward_mean | -0.131     |
| train/info_shaping_reward_min  | -0.302     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.7      |
| train/steps                    | 124800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 78         |
| stats_o/mean                   | 0.2015866  |
| stats_o/std                    | 0.12427176 |
| test/episodes                  | 790        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.562      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00321   |
| test/info_shaping_reward_mean  | -0.08      |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -4.3902545 |
| test/Q_plus_P                  | -4.3902545 |
| test/reward_per_eps            | -17.5      |
| test/steps                     | 31600      |
| train/episodes                 | 3160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.266      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0122    |
| train/info_shaping_reward_mean | -0.114     |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.4      |
| train/steps                    | 126400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 79          |
| stats_o/mean                   | 0.20159695  |
| stats_o/std                    | 0.123833485 |
| test/episodes                  | 800         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.63        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00112    |
| test/info_shaping_reward_mean  | -0.0678     |
| test/info_shaping_reward_min   | -0.213      |
| test/Q                         | -3.700141   |
| test/Q_plus_P                  | -3.700141   |
| test/reward_per_eps            | -14.8       |
| test/steps                     | 32000       |
| train/episodes                 | 3200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.299       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0069     |
| train/info_shaping_reward_mean | -0.101      |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.1       |
| train/steps                    | 128000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 80         |
| stats_o/mean                   | 0.20165421 |
| stats_o/std                    | 0.12347521 |
| test/episodes                  | 810        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000857  |
| test/info_shaping_reward_mean  | -0.0685    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -3.8890817 |
| test/Q_plus_P                  | -3.8890817 |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 32400      |
| train/episodes                 | 3240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.255      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0102    |
| train/info_shaping_reward_mean | -0.121     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.8      |
| train/steps                    | 129600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 81         |
| stats_o/mean                   | 0.20158014 |
| stats_o/std                    | 0.12308407 |
| test/episodes                  | 820        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.637      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00163   |
| test/info_shaping_reward_mean  | -0.0665    |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -3.4847715 |
| test/Q_plus_P                  | -3.4847715 |
| test/reward_per_eps            | -14.5      |
| test/steps                     | 32800      |
| train/episodes                 | 3280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.265      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0105    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.4      |
| train/steps                    | 131200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 82         |
| stats_o/mean                   | 0.20150006 |
| stats_o/std                    | 0.12273348 |
| test/episodes                  | 830        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.615      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00162   |
| test/info_shaping_reward_mean  | -0.0704    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -3.7090926 |
| test/Q_plus_P                  | -3.7090926 |
| test/reward_per_eps            | -15.4      |
| test/steps                     | 33200      |
| train/episodes                 | 3320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.339      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0116    |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.4      |
| train/steps                    | 132800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 83         |
| stats_o/mean                   | 0.20147948 |
| stats_o/std                    | 0.12234371 |
| test/episodes                  | 840        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.65       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00489   |
| test/info_shaping_reward_mean  | -0.0644    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -3.01998   |
| test/Q_plus_P                  | -3.01998   |
| test/reward_per_eps            | -14        |
| test/steps                     | 33600      |
| train/episodes                 | 3360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.329      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00951   |
| train/info_shaping_reward_mean | -0.112     |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.8      |
| train/steps                    | 134400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 84         |
| stats_o/mean                   | 0.20147966 |
| stats_o/std                    | 0.1219347  |
| test/episodes                  | 850        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.0708    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -3.599327  |
| test/Q_plus_P                  | -3.599327  |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 34000      |
| train/episodes                 | 3400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.297      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00559   |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.1      |
| train/steps                    | 136000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 85         |
| stats_o/mean                   | 0.20147036 |
| stats_o/std                    | 0.12157    |
| test/episodes                  | 860        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.645      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00074   |
| test/info_shaping_reward_mean  | -0.0666    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -3.4905186 |
| test/Q_plus_P                  | -3.4905186 |
| test/reward_per_eps            | -14.2      |
| test/steps                     | 34400      |
| train/episodes                 | 3440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.313      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00906   |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.5      |
| train/steps                    | 137600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 86         |
| stats_o/mean                   | 0.20152111 |
| stats_o/std                    | 0.12112067 |
| test/episodes                  | 870        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.655      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00193   |
| test/info_shaping_reward_mean  | -0.0633    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -3.0042658 |
| test/Q_plus_P                  | -3.0042658 |
| test/reward_per_eps            | -13.8      |
| test/steps                     | 34800      |
| train/episodes                 | 3480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.321      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00694   |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.1      |
| train/steps                    | 139200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 87          |
| stats_o/mean                   | 0.20151092  |
| stats_o/std                    | 0.120792694 |
| test/episodes                  | 880         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.62        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00312    |
| test/info_shaping_reward_mean  | -0.0699     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -3.2766483  |
| test/Q_plus_P                  | -3.2766483  |
| test/reward_per_eps            | -15.2       |
| test/steps                     | 35200       |
| train/episodes                 | 3520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.39        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.007      |
| train/info_shaping_reward_mean | -0.12       |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.4       |
| train/steps                    | 140800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 88         |
| stats_o/mean                   | 0.20150416 |
| stats_o/std                    | 0.12047362 |
| test/episodes                  | 890        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0695    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.3942518 |
| test/Q_plus_P                  | -3.3942518 |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 35600      |
| train/episodes                 | 3560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.387      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00592   |
| train/info_shaping_reward_mean | -0.096     |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.5      |
| train/steps                    | 142400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 89         |
| stats_o/mean                   | 0.20143698 |
| stats_o/std                    | 0.12020844 |
| test/episodes                  | 900        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.625      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00245   |
| test/info_shaping_reward_mean  | -0.0673    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -3.3407278 |
| test/Q_plus_P                  | -3.3407278 |
| test/reward_per_eps            | -15        |
| test/steps                     | 36000      |
| train/episodes                 | 3600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.375      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00495   |
| train/info_shaping_reward_mean | -0.0994    |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25        |
| train/steps                    | 144000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 90         |
| stats_o/mean                   | 0.20139702 |
| stats_o/std                    | 0.11973688 |
| test/episodes                  | 910        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.585      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000349  |
| test/info_shaping_reward_mean  | -0.0671    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.518914  |
| test/Q_plus_P                  | -3.518914  |
| test/reward_per_eps            | -16.6      |
| test/steps                     | 36400      |
| train/episodes                 | 3640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.416      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00694   |
| train/info_shaping_reward_mean | -0.0883    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.4      |
| train/steps                    | 145600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 91         |
| stats_o/mean                   | 0.20140313 |
| stats_o/std                    | 0.11945941 |
| test/episodes                  | 920        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.657      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0018    |
| test/info_shaping_reward_mean  | -0.0651    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -3.2049954 |
| test/Q_plus_P                  | -3.2049954 |
| test/reward_per_eps            | -13.7      |
| test/steps                     | 36800      |
| train/episodes                 | 3680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.367      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00674   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.3      |
| train/steps                    | 147200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 92         |
| stats_o/mean                   | 0.20136683 |
| stats_o/std                    | 0.11905026 |
| test/episodes                  | 930        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.655      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00473   |
| test/info_shaping_reward_mean  | -0.0645    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.806172  |
| test/Q_plus_P                  | -2.806172  |
| test/reward_per_eps            | -13.8      |
| test/steps                     | 37200      |
| train/episodes                 | 3720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.35       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00774   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26        |
| train/steps                    | 148800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.20137754  |
| stats_o/std                    | 0.118748054 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00309    |
| test/info_shaping_reward_mean  | -0.0611     |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -2.7112353  |
| test/Q_plus_P                  | -2.7112353  |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.385       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00791    |
| train/info_shaping_reward_mean | -0.104      |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.6       |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 94         |
| stats_o/mean                   | 0.20135666 |
| stats_o/std                    | 0.11841638 |
| test/episodes                  | 950        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.652      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.0641    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -2.667852  |
| test/Q_plus_P                  | -2.667852  |
| test/reward_per_eps            | -13.9      |
| test/steps                     | 38000      |
| train/episodes                 | 3800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.421      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00462   |
| train/info_shaping_reward_mean | -0.0995    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.1      |
| train/steps                    | 152000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 95         |
| stats_o/mean                   | 0.20130599 |
| stats_o/std                    | 0.11832013 |
| test/episodes                  | 960        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.655      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00142   |
| test/info_shaping_reward_mean  | -0.0641    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.7282472 |
| test/Q_plus_P                  | -2.7282472 |
| test/reward_per_eps            | -13.8      |
| test/steps                     | 38400      |
| train/episodes                 | 3840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.331      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00778   |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.366     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.8      |
| train/steps                    | 153600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 96         |
| stats_o/mean                   | 0.20125137 |
| stats_o/std                    | 0.11803441 |
| test/episodes                  | 970        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.623      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000972  |
| test/info_shaping_reward_mean  | -0.0714    |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -4.3985243 |
| test/Q_plus_P                  | -4.3985243 |
| test/reward_per_eps            | -15.1      |
| test/steps                     | 38800      |
| train/episodes                 | 3880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.394      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0049    |
| train/info_shaping_reward_mean | -0.0973    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.2      |
| train/steps                    | 155200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 97         |
| stats_o/mean                   | 0.2012385  |
| stats_o/std                    | 0.11756859 |
| test/episodes                  | 980        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.655      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000738  |
| test/info_shaping_reward_mean  | -0.0638    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.8332615 |
| test/Q_plus_P                  | -2.8332615 |
| test/reward_per_eps            | -13.8      |
| test/steps                     | 39200      |
| train/episodes                 | 3920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.42       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00496   |
| train/info_shaping_reward_mean | -0.0892    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.2      |
| train/steps                    | 156800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 98         |
| stats_o/mean                   | 0.20124196 |
| stats_o/std                    | 0.117324   |
| test/episodes                  | 990        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.642      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.0673    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -2.9108372 |
| test/Q_plus_P                  | -2.9108372 |
| test/reward_per_eps            | -14.3      |
| test/steps                     | 39600      |
| train/episodes                 | 3960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.369      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00762   |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.289     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.2      |
| train/steps                    | 158400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.20125397 |
| stats_o/std                    | 0.11686859 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00303   |
| test/info_shaping_reward_mean  | -0.0584    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -2.516349  |
| test/Q_plus_P                  | -2.516349  |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.499      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00465   |
| train/info_shaping_reward_mean | -0.0875    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20        |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 100         |
| stats_o/mean                   | 0.20121908  |
| stats_o/std                    | 0.116637304 |
| test/episodes                  | 1010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.667       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00139    |
| test/info_shaping_reward_mean  | -0.063      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -2.52818    |
| test/Q_plus_P                  | -2.52818    |
| test/reward_per_eps            | -13.3       |
| test/steps                     | 40400       |
| train/episodes                 | 4040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.475       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00551    |
| train/info_shaping_reward_mean | -0.0894     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21         |
| train/steps                    | 161600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 101        |
| stats_o/mean                   | 0.20124462 |
| stats_o/std                    | 0.11635041 |
| test/episodes                  | 1020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.665      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00282   |
| test/info_shaping_reward_mean  | -0.0645    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.9069293 |
| test/Q_plus_P                  | -2.9069293 |
| test/reward_per_eps            | -13.4      |
| test/steps                     | 40800      |
| train/episodes                 | 4080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.441      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00714   |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.4      |
| train/steps                    | 163200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 102        |
| stats_o/mean                   | 0.20123027 |
| stats_o/std                    | 0.11599407 |
| test/episodes                  | 1030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.63       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00197   |
| test/info_shaping_reward_mean  | -0.0682    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.7959282 |
| test/Q_plus_P                  | -2.7959282 |
| test/reward_per_eps            | -14.8      |
| test/steps                     | 41200      |
| train/episodes                 | 4120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.427      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00539   |
| train/info_shaping_reward_mean | -0.0876    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.9      |
| train/steps                    | 164800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 103        |
| stats_o/mean                   | 0.20122112 |
| stats_o/std                    | 0.11576132 |
| test/episodes                  | 1040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00139   |
| test/info_shaping_reward_mean  | -0.0591    |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -2.3358674 |
| test/Q_plus_P                  | -2.3358674 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 41600      |
| train/episodes                 | 4160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.401      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00506   |
| train/info_shaping_reward_mean | -0.0892    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24        |
| train/steps                    | 166400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 104        |
| stats_o/mean                   | 0.2012053  |
| stats_o/std                    | 0.11566658 |
| test/episodes                  | 1050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.573      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0027    |
| test/info_shaping_reward_mean  | -0.079     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -3.402386  |
| test/Q_plus_P                  | -3.402386  |
| test/reward_per_eps            | -17.1      |
| test/steps                     | 42000      |
| train/episodes                 | 4200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.267      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0113    |
| train/info_shaping_reward_mean | -0.127     |
| train/info_shaping_reward_min  | -0.302     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.3      |
| train/steps                    | 168000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 105         |
| stats_o/mean                   | 0.20122142  |
| stats_o/std                    | 0.115491726 |
| test/episodes                  | 1060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.59        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00167    |
| test/info_shaping_reward_mean  | -0.0738     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -2.9935043  |
| test/Q_plus_P                  | -2.9935043  |
| test/reward_per_eps            | -16.4       |
| test/steps                     | 42400       |
| train/episodes                 | 4240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.379       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00855    |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.8       |
| train/steps                    | 169600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 106        |
| stats_o/mean                   | 0.20118769 |
| stats_o/std                    | 0.11519268 |
| test/episodes                  | 1070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.61       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00142   |
| test/info_shaping_reward_mean  | -0.0706    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -2.9155836 |
| test/Q_plus_P                  | -2.9155836 |
| test/reward_per_eps            | -15.6      |
| test/steps                     | 42800      |
| train/episodes                 | 4280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.348      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00798   |
| train/info_shaping_reward_mean | -0.0992    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.1      |
| train/steps                    | 171200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 107        |
| stats_o/mean                   | 0.20117931 |
| stats_o/std                    | 0.11516318 |
| test/episodes                  | 1080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.613      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00147   |
| test/info_shaping_reward_mean  | -0.0694    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -3.007706  |
| test/Q_plus_P                  | -3.007706  |
| test/reward_per_eps            | -15.5      |
| test/steps                     | 43200      |
| train/episodes                 | 4320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.326      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0064    |
| train/info_shaping_reward_mean | -0.117     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.9      |
| train/steps                    | 172800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 108        |
| stats_o/mean                   | 0.20114332 |
| stats_o/std                    | 0.11484487 |
| test/episodes                  | 1090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.67       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00163   |
| test/info_shaping_reward_mean  | -0.0623    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.644963  |
| test/Q_plus_P                  | -2.644963  |
| test/reward_per_eps            | -13.2      |
| test/steps                     | 43600      |
| train/episodes                 | 4360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.384      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00582   |
| train/info_shaping_reward_mean | -0.099     |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.6      |
| train/steps                    | 174400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 109        |
| stats_o/mean                   | 0.2010993  |
| stats_o/std                    | 0.11458465 |
| test/episodes                  | 1100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00267   |
| test/info_shaping_reward_mean  | -0.0604    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -2.5755804 |
| test/Q_plus_P                  | -2.5755804 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 44000      |
| train/episodes                 | 4400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.386      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00753   |
| train/info_shaping_reward_mean | -0.0964    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.6      |
| train/steps                    | 176000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 110         |
| stats_o/mean                   | 0.20106295  |
| stats_o/std                    | 0.114345305 |
| test/episodes                  | 1110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.62        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00391    |
| test/info_shaping_reward_mean  | -0.0694     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -2.7397132  |
| test/Q_plus_P                  | -2.7397132  |
| test/reward_per_eps            | -15.2       |
| test/steps                     | 44400       |
| train/episodes                 | 4440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.415       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00509    |
| train/info_shaping_reward_mean | -0.0885     |
| train/info_shaping_reward_min  | -0.194      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.4       |
| train/steps                    | 177600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 111        |
| stats_o/mean                   | 0.20106392 |
| stats_o/std                    | 0.11410236 |
| test/episodes                  | 1120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.625      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00108   |
| test/info_shaping_reward_mean  | -0.0678    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -2.6846883 |
| test/Q_plus_P                  | -2.6846883 |
| test/reward_per_eps            | -15        |
| test/steps                     | 44800      |
| train/episodes                 | 4480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.403      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0067    |
| train/info_shaping_reward_mean | -0.0945    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.9      |
| train/steps                    | 179200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 112         |
| stats_o/mean                   | 0.2010787   |
| stats_o/std                    | 0.113770455 |
| test/episodes                  | 1130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.652       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00127    |
| test/info_shaping_reward_mean  | -0.0639     |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -2.6210546  |
| test/Q_plus_P                  | -2.6210546  |
| test/reward_per_eps            | -13.9       |
| test/steps                     | 45200       |
| train/episodes                 | 4520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.436       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00568    |
| train/info_shaping_reward_mean | -0.0893     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.6       |
| train/steps                    | 180800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 113        |
| stats_o/mean                   | 0.20110846 |
| stats_o/std                    | 0.11351722 |
| test/episodes                  | 1140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00235   |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.0300565 |
| test/Q_plus_P                  | -2.0300565 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 45600      |
| train/episodes                 | 4560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.459      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00329   |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 182400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 114        |
| stats_o/mean                   | 0.20112254 |
| stats_o/std                    | 0.11319821 |
| test/episodes                  | 1150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00181   |
| test/info_shaping_reward_mean  | -0.0578    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.0944102 |
| test/Q_plus_P                  | -2.0944102 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 46000      |
| train/episodes                 | 4600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.432      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0051    |
| train/info_shaping_reward_mean | -0.0939    |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.7      |
| train/steps                    | 184000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 115        |
| stats_o/mean                   | 0.20109564 |
| stats_o/std                    | 0.11292522 |
| test/episodes                  | 1160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.65       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00422   |
| test/info_shaping_reward_mean  | -0.0638    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -2.3655448 |
| test/Q_plus_P                  | -2.3655448 |
| test/reward_per_eps            | -14        |
| test/steps                     | 46400      |
| train/episodes                 | 4640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.426      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00547   |
| train/info_shaping_reward_mean | -0.0953    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23        |
| train/steps                    | 185600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 116        |
| stats_o/mean                   | 0.2011001  |
| stats_o/std                    | 0.11260387 |
| test/episodes                  | 1170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00262   |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.2729738 |
| test/Q_plus_P                  | -2.2729738 |
| test/reward_per_eps            | -12        |
| test/steps                     | 46800      |
| train/episodes                 | 4680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.474      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00585   |
| train/info_shaping_reward_mean | -0.084     |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 187200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 117        |
| stats_o/mean                   | 0.20111223 |
| stats_o/std                    | 0.11240694 |
| test/episodes                  | 1180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00105   |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -2.1019115 |
| test/Q_plus_P                  | -2.1019115 |
| test/reward_per_eps            | -12        |
| test/steps                     | 47200      |
| train/episodes                 | 4720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.451      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00535   |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.9      |
| train/steps                    | 188800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 118        |
| stats_o/mean                   | 0.2011323  |
| stats_o/std                    | 0.11215538 |
| test/episodes                  | 1190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.603      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00289   |
| test/info_shaping_reward_mean  | -0.104     |
| test/info_shaping_reward_min   | -0.647     |
| test/Q                         | -4.38446   |
| test/Q_plus_P                  | -4.38446   |
| test/reward_per_eps            | -15.9      |
| test/steps                     | 47600      |
| train/episodes                 | 4760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.454      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00583   |
| train/info_shaping_reward_mean | -0.0932    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 190400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 119        |
| stats_o/mean                   | 0.2011831  |
| stats_o/std                    | 0.1119928  |
| test/episodes                  | 1200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00125   |
| test/info_shaping_reward_mean  | -0.0668    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.5450053 |
| test/Q_plus_P                  | -2.5450053 |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 48000      |
| train/episodes                 | 4800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.388      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0062    |
| train/info_shaping_reward_mean | -0.112     |
| train/info_shaping_reward_min  | -0.29      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.5      |
| train/steps                    | 192000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 120        |
| stats_o/mean                   | 0.20116186 |
| stats_o/std                    | 0.11169355 |
| test/episodes                  | 1210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.657      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00471   |
| test/info_shaping_reward_mean  | -0.0635    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -2.3836868 |
| test/Q_plus_P                  | -2.3836868 |
| test/reward_per_eps            | -13.7      |
| test/steps                     | 48400      |
| train/episodes                 | 4840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.471      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00405   |
| train/info_shaping_reward_mean | -0.0834    |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 193600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.20115446 |
| stats_o/std                    | 0.11138399 |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.645      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00394   |
| test/info_shaping_reward_mean  | -0.0679    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -2.3665404 |
| test/Q_plus_P                  | -2.3665404 |
| test/reward_per_eps            | -14.2      |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00587   |
| train/info_shaping_reward_mean | -0.0779    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 122        |
| stats_o/mean                   | 0.20114805 |
| stats_o/std                    | 0.11115095 |
| test/episodes                  | 1230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00189   |
| test/info_shaping_reward_mean  | -0.0603    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.0403354 |
| test/Q_plus_P                  | -2.0403354 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 49200      |
| train/episodes                 | 4920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.465      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00591   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 196800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 123        |
| stats_o/mean                   | 0.20112169 |
| stats_o/std                    | 0.11102962 |
| test/episodes                  | 1240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.547      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.12      |
| test/info_shaping_reward_min   | -0.518     |
| test/Q                         | -6.0497727 |
| test/Q_plus_P                  | -6.0497727 |
| test/reward_per_eps            | -18.1      |
| test/steps                     | 49600      |
| train/episodes                 | 4960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.489      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00608   |
| train/info_shaping_reward_mean | -0.0894    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 198400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.20118496  |
| stats_o/std                    | 0.110701606 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.675       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0614     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -2.3014796  |
| test/Q_plus_P                  | -2.3014796  |
| test/reward_per_eps            | -13         |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.473       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00433    |
| train/info_shaping_reward_mean | -0.0835     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.1       |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 125        |
| stats_o/mean                   | 0.20117141 |
| stats_o/std                    | 0.11061412 |
| test/episodes                  | 1260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.665      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00084   |
| test/info_shaping_reward_mean  | -0.0621    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.1679275 |
| test/Q_plus_P                  | -2.1679275 |
| test/reward_per_eps            | -13.4      |
| test/steps                     | 50400      |
| train/episodes                 | 5040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.393      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00519   |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.3      |
| train/steps                    | 201600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 126        |
| stats_o/mean                   | 0.20115721 |
| stats_o/std                    | 0.11038631 |
| test/episodes                  | 1270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.632      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000979  |
| test/info_shaping_reward_mean  | -0.0933    |
| test/info_shaping_reward_min   | -0.556     |
| test/Q                         | -4.0882173 |
| test/Q_plus_P                  | -4.0882173 |
| test/reward_per_eps            | -14.7      |
| test/steps                     | 50800      |
| train/episodes                 | 5080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.472      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00675   |
| train/info_shaping_reward_mean | -0.09      |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 203200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 127        |
| stats_o/mean                   | 0.20113313 |
| stats_o/std                    | 0.11013356 |
| test/episodes                  | 1280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00165   |
| test/info_shaping_reward_mean  | -0.0594    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.2030163 |
| test/Q_plus_P                  | -2.2030163 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 51200      |
| train/episodes                 | 5120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00627   |
| train/info_shaping_reward_mean | -0.0831    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 204800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.20111266 |
| stats_o/std                    | 0.11001858 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00219   |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.7020496 |
| test/Q_plus_P                  | -1.7020496 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.447      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00652   |
| train/info_shaping_reward_mean | -0.0959    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.1      |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 129        |
| stats_o/mean                   | 0.20110254 |
| stats_o/std                    | 0.10997725 |
| test/episodes                  | 1300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00321   |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -1.8548132 |
| test/Q_plus_P                  | -1.8548132 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 52000      |
| train/episodes                 | 5200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.426      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00642   |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.9      |
| train/steps                    | 208000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 130        |
| stats_o/mean                   | 0.20112792 |
| stats_o/std                    | 0.10966764 |
| test/episodes                  | 1310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.657      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.0634    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -2.0665216 |
| test/Q_plus_P                  | -2.0665216 |
| test/reward_per_eps            | -13.7      |
| test/steps                     | 52400      |
| train/episodes                 | 5240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00535   |
| train/info_shaping_reward_mean | -0.0768    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 209600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 131        |
| stats_o/mean                   | 0.20111556 |
| stats_o/std                    | 0.10957105 |
| test/episodes                  | 1320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.642      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00172   |
| test/info_shaping_reward_mean  | -0.0639    |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -2.24776   |
| test/Q_plus_P                  | -2.24776   |
| test/reward_per_eps            | -14.3      |
| test/steps                     | 52800      |
| train/episodes                 | 5280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.488      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00582   |
| train/info_shaping_reward_mean | -0.084     |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.5      |
| train/steps                    | 211200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 132        |
| stats_o/mean                   | 0.20111376 |
| stats_o/std                    | 0.10930471 |
| test/episodes                  | 1330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00418   |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.6948144 |
| test/Q_plus_P                  | -1.6948144 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 53200      |
| train/episodes                 | 5320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.526      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00395   |
| train/info_shaping_reward_mean | -0.0791    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 212800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 133        |
| stats_o/mean                   | 0.20110245 |
| stats_o/std                    | 0.10912308 |
| test/episodes                  | 1340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00246   |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.5610292 |
| test/Q_plus_P                  | -1.5610292 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 53600      |
| train/episodes                 | 5360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.0819    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 214400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 134         |
| stats_o/mean                   | 0.20106494  |
| stats_o/std                    | 0.108932085 |
| test/episodes                  | 1350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00194    |
| test/info_shaping_reward_mean  | -0.0552     |
| test/info_shaping_reward_min   | -0.199      |
| test/Q                         | -1.5911946  |
| test/Q_plus_P                  | -1.5911946  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 54000       |
| train/episodes                 | 5400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.481       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00543    |
| train/info_shaping_reward_mean | -0.0872     |
| train/info_shaping_reward_min  | -0.226      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.8       |
| train/steps                    | 216000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 135        |
| stats_o/mean                   | 0.20105588 |
| stats_o/std                    | 0.10868199 |
| test/episodes                  | 1360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.0662    |
| test/info_shaping_reward_min   | -0.215     |
| test/Q                         | -2.9386404 |
| test/Q_plus_P                  | -2.9386404 |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 54400      |
| train/episodes                 | 5440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.503      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00411   |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 217600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 136        |
| stats_o/mean                   | 0.20110741 |
| stats_o/std                    | 0.10851817 |
| test/episodes                  | 1370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00456   |
| test/info_shaping_reward_mean  | -0.0666    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.325237  |
| test/Q_plus_P                  | -2.325237  |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 54800      |
| train/episodes                 | 5480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.466      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00591   |
| train/info_shaping_reward_mean | -0.0884    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 219200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 137         |
| stats_o/mean                   | 0.20109631  |
| stats_o/std                    | 0.108341254 |
| test/episodes                  | 1380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.657       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00311    |
| test/info_shaping_reward_mean  | -0.0632     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -2.096958   |
| test/Q_plus_P                  | -2.096958   |
| test/reward_per_eps            | -13.7       |
| test/steps                     | 55200       |
| train/episodes                 | 5520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.48        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00462    |
| train/info_shaping_reward_mean | -0.0929     |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.8       |
| train/steps                    | 220800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 138         |
| stats_o/mean                   | 0.20116436  |
| stats_o/std                    | 0.108309574 |
| test/episodes                  | 1390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00242    |
| test/info_shaping_reward_mean  | -0.0574     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.9571311  |
| test/Q_plus_P                  | -1.9571311  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 55600       |
| train/episodes                 | 5560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.398       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00808    |
| train/info_shaping_reward_mean | -0.112      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.1       |
| train/steps                    | 222400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.20119244 |
| stats_o/std                    | 0.10804235 |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0025    |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.7714543 |
| test/Q_plus_P                  | -1.7714543 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.502      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00401   |
| train/info_shaping_reward_mean | -0.0813    |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 140        |
| stats_o/mean                   | 0.20118168 |
| stats_o/std                    | 0.10782963 |
| test/episodes                  | 1410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00182   |
| test/info_shaping_reward_mean  | -0.0582    |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -1.6854491 |
| test/Q_plus_P                  | -1.6854491 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 56400      |
| train/episodes                 | 5640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.489      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00488   |
| train/info_shaping_reward_mean | -0.0848    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 225600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 141         |
| stats_o/mean                   | 0.20116888  |
| stats_o/std                    | 0.107630596 |
| test/episodes                  | 1420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00262    |
| test/info_shaping_reward_mean  | -0.0558     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.8084989  |
| test/Q_plus_P                  | -1.8084989  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 56800       |
| train/episodes                 | 5680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.528       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00504    |
| train/info_shaping_reward_mean | -0.0836     |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 227200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 142         |
| stats_o/mean                   | 0.20116068  |
| stats_o/std                    | 0.107432544 |
| test/episodes                  | 1430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00173    |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.201      |
| test/Q                         | -1.4137733  |
| test/Q_plus_P                  | -1.4137733  |
| test/reward_per_eps            | -11         |
| test/steps                     | 57200       |
| train/episodes                 | 5720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.471       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00532    |
| train/info_shaping_reward_mean | -0.0841     |
| train/info_shaping_reward_min  | -0.201      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.1       |
| train/steps                    | 228800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 143        |
| stats_o/mean                   | 0.20123509 |
| stats_o/std                    | 0.10726016 |
| test/episodes                  | 1440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00311   |
| test/info_shaping_reward_mean  | -0.0578    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.885205  |
| test/Q_plus_P                  | -1.885205  |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 57600      |
| train/episodes                 | 5760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.507      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00554   |
| train/info_shaping_reward_mean | -0.093     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.7      |
| train/steps                    | 230400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.2012081   |
| stats_o/std                    | 0.107131824 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.642       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00183    |
| test/info_shaping_reward_mean  | -0.0662     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -2.0894895  |
| test/Q_plus_P                  | -2.0894895  |
| test/reward_per_eps            | -14.3       |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.427       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00488    |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.9       |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 145        |
| stats_o/mean                   | 0.20118544 |
| stats_o/std                    | 0.10710917 |
| test/episodes                  | 1460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00155   |
| test/info_shaping_reward_mean  | -0.0571    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.8919134 |
| test/Q_plus_P                  | -1.8919134 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 58400      |
| train/episodes                 | 5840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.486      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00509   |
| train/info_shaping_reward_mean | -0.0964    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 233600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 146         |
| stats_o/mean                   | 0.20116113  |
| stats_o/std                    | 0.107012846 |
| test/episodes                  | 1470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00278    |
| test/info_shaping_reward_mean  | -0.0571     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.8934872  |
| test/Q_plus_P                  | -1.8934872  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 58800       |
| train/episodes                 | 5880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.493       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00394    |
| train/info_shaping_reward_mean | -0.0998     |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.3       |
| train/steps                    | 235200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 147        |
| stats_o/mean                   | 0.20116545 |
| stats_o/std                    | 0.10682137 |
| test/episodes                  | 1480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00123   |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.7073658 |
| test/Q_plus_P                  | -1.7073658 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 59200      |
| train/episodes                 | 5920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.487      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00548   |
| train/info_shaping_reward_mean | -0.0879    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.5      |
| train/steps                    | 236800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 148        |
| stats_o/mean                   | 0.20116515 |
| stats_o/std                    | 0.10661251 |
| test/episodes                  | 1490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.63       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00305   |
| test/info_shaping_reward_mean  | -0.0676    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -2.5815063 |
| test/Q_plus_P                  | -2.5815063 |
| test/reward_per_eps            | -14.8      |
| test/steps                     | 59600      |
| train/episodes                 | 5960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.471      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00542   |
| train/info_shaping_reward_mean | -0.0826    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 238400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 149         |
| stats_o/mean                   | 0.20117083  |
| stats_o/std                    | 0.106415056 |
| test/episodes                  | 1500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00116    |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.6895717  |
| test/Q_plus_P                  | -1.6895717  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 60000       |
| train/episodes                 | 6000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.536       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0048     |
| train/info_shaping_reward_mean | -0.081      |
| train/info_shaping_reward_min  | -0.219      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 240000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 150        |
| stats_o/mean                   | 0.2012052  |
| stats_o/std                    | 0.10629004 |
| test/episodes                  | 1510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0014    |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -1.8198802 |
| test/Q_plus_P                  | -1.8198802 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 60400      |
| train/episodes                 | 6040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.445      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.085     |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.2      |
| train/steps                    | 241600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 151        |
| stats_o/mean                   | 0.20119402 |
| stats_o/std                    | 0.10609088 |
| test/episodes                  | 1520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000468  |
| test/info_shaping_reward_mean  | -0.0616    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -1.8730125 |
| test/Q_plus_P                  | -1.8730125 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 60800      |
| train/episodes                 | 6080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.492      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00578   |
| train/info_shaping_reward_mean | -0.093     |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.3      |
| train/steps                    | 243200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 152        |
| stats_o/mean                   | 0.20120734 |
| stats_o/std                    | 0.10598475 |
| test/episodes                  | 1530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.8086625 |
| test/Q_plus_P                  | -1.8086625 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 61200      |
| train/episodes                 | 6120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.449      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00704   |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22        |
| train/steps                    | 244800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 153        |
| stats_o/mean                   | 0.20118059 |
| stats_o/std                    | 0.10589879 |
| test/episodes                  | 1540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00143   |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.5614117 |
| test/Q_plus_P                  | -1.5614117 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 61600      |
| train/episodes                 | 6160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.506      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00489   |
| train/info_shaping_reward_mean | -0.087     |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 246400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.20114811 |
| stats_o/std                    | 0.10570829 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00147   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.6738874 |
| test/Q_plus_P                  | -1.6738874 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00434   |
| train/info_shaping_reward_mean | -0.0762    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.20117179 |
| stats_o/std                    | 0.10551708 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00213   |
| test/info_shaping_reward_mean  | -0.0577    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -1.8710186 |
| test/Q_plus_P                  | -1.8710186 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.517      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 156         |
| stats_o/mean                   | 0.2011688   |
| stats_o/std                    | 0.105386764 |
| test/episodes                  | 1570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000961   |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.6675401  |
| test/Q_plus_P                  | -1.6675401  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 62800       |
| train/episodes                 | 6280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.528       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0037     |
| train/info_shaping_reward_mean | -0.0824     |
| train/info_shaping_reward_min  | -0.219      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 251200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 157        |
| stats_o/mean                   | 0.20117569 |
| stats_o/std                    | 0.10524157 |
| test/episodes                  | 1580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00382   |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6963933 |
| test/Q_plus_P                  | -1.6963933 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 63200      |
| train/episodes                 | 6320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00437   |
| train/info_shaping_reward_mean | -0.0724    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 252800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 158        |
| stats_o/mean                   | 0.20116137 |
| stats_o/std                    | 0.10503537 |
| test/episodes                  | 1590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000439  |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -1.7921199 |
| test/Q_plus_P                  | -1.7921199 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 63600      |
| train/episodes                 | 6360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.526      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0774    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 254400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.20118503  |
| stats_o/std                    | 0.104920626 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00236    |
| test/info_shaping_reward_mean  | -0.0578     |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -1.6998644  |
| test/Q_plus_P                  | -1.6998644  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.529       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00551    |
| train/info_shaping_reward_mean | -0.0907     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.8       |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 160        |
| stats_o/mean                   | 0.20116444 |
| stats_o/std                    | 0.10482318 |
| test/episodes                  | 1610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00453   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -1.2913651 |
| test/Q_plus_P                  | -1.2913651 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 64400      |
| train/episodes                 | 6440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.508      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00565   |
| train/info_shaping_reward_mean | -0.0887    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.7      |
| train/steps                    | 257600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.2012176   |
| stats_o/std                    | 0.104623325 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00226    |
| test/info_shaping_reward_mean  | -0.0594     |
| test/info_shaping_reward_min   | -0.19       |
| test/Q                         | -1.7964888  |
| test/Q_plus_P                  | -1.7964888  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.513       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00516    |
| train/info_shaping_reward_mean | -0.0812     |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.5       |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 162        |
| stats_o/mean                   | 0.20123    |
| stats_o/std                    | 0.10446382 |
| test/episodes                  | 1630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0046    |
| test/info_shaping_reward_mean  | -0.0581    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.8567244 |
| test/Q_plus_P                  | -1.8567244 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 65200      |
| train/episodes                 | 6520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.508      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0827    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.7      |
| train/steps                    | 260800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 163        |
| stats_o/mean                   | 0.20114927 |
| stats_o/std                    | 0.10437982 |
| test/episodes                  | 1640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0605    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.8724381 |
| test/Q_plus_P                  | -1.8724381 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 65600      |
| train/episodes                 | 6560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.0791    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 262400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 164        |
| stats_o/mean                   | 0.2011326  |
| stats_o/std                    | 0.10420666 |
| test/episodes                  | 1650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0019    |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.4826626 |
| test/Q_plus_P                  | -1.4826626 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 66000      |
| train/episodes                 | 6600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00416   |
| train/info_shaping_reward_mean | -0.0717    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 264000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 165        |
| stats_o/mean                   | 0.2011208  |
| stats_o/std                    | 0.10404109 |
| test/episodes                  | 1660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00476   |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.367286  |
| test/Q_plus_P                  | -1.367286  |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 66400      |
| train/episodes                 | 6640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.497      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00545   |
| train/info_shaping_reward_mean | -0.0808    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 265600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 166        |
| stats_o/mean                   | 0.20110802 |
| stats_o/std                    | 0.10383823 |
| test/episodes                  | 1670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0052    |
| test/info_shaping_reward_mean  | -0.0639    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.9850426 |
| test/Q_plus_P                  | -1.9850426 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 66800      |
| train/episodes                 | 6680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00319   |
| train/info_shaping_reward_mean | -0.075     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 267200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.20109999 |
| stats_o/std                    | 0.10371835 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00325   |
| test/info_shaping_reward_mean  | -0.0675    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.2534797 |
| test/Q_plus_P                  | -2.2534797 |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.534      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00437   |
| train/info_shaping_reward_mean | -0.0815    |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 168        |
| stats_o/mean                   | 0.20113581 |
| stats_o/std                    | 0.10365458 |
| test/episodes                  | 1690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.627      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00234   |
| test/info_shaping_reward_mean  | -0.0699    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.2455595 |
| test/Q_plus_P                  | -2.2455595 |
| test/reward_per_eps            | -14.9      |
| test/steps                     | 67600      |
| train/episodes                 | 6760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.457      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00508   |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.7      |
| train/steps                    | 270400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.20115538 |
| stats_o/std                    | 0.10352297 |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00336   |
| test/info_shaping_reward_mean  | -0.06      |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.805494  |
| test/Q_plus_P                  | -1.805494  |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.502      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00494   |
| train/info_shaping_reward_mean | -0.0857    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 170        |
| stats_o/mean                   | 0.20116879 |
| stats_o/std                    | 0.10333606 |
| test/episodes                  | 1710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.657      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00182   |
| test/info_shaping_reward_mean  | -0.0639    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.1279583 |
| test/Q_plus_P                  | -2.1279583 |
| test/reward_per_eps            | -13.7      |
| test/steps                     | 68400      |
| train/episodes                 | 6840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00496   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 273600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 171        |
| stats_o/mean                   | 0.20118544 |
| stats_o/std                    | 0.10318641 |
| test/episodes                  | 1720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00269   |
| test/info_shaping_reward_mean  | -0.0606    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.7494447 |
| test/Q_plus_P                  | -1.7494447 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 68800      |
| train/episodes                 | 6880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00466   |
| train/info_shaping_reward_mean | -0.0862    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 275200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 172        |
| stats_o/mean                   | 0.20120418 |
| stats_o/std                    | 0.10306934 |
| test/episodes                  | 1730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.5017301 |
| test/Q_plus_P                  | -1.5017301 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 69200      |
| train/episodes                 | 6920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00604   |
| train/info_shaping_reward_mean | -0.0837    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 276800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 173         |
| stats_o/mean                   | 0.20123808  |
| stats_o/std                    | 0.102892436 |
| test/episodes                  | 1740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000643   |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.6504277  |
| test/Q_plus_P                  | -1.6504277  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 69600       |
| train/episodes                 | 6960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.511       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00484    |
| train/info_shaping_reward_mean | -0.081      |
| train/info_shaping_reward_min  | -0.194      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.6       |
| train/steps                    | 278400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 174        |
| stats_o/mean                   | 0.2012127  |
| stats_o/std                    | 0.1027661  |
| test/episodes                  | 1750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00382   |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.6723822 |
| test/Q_plus_P                  | -1.6723822 |
| test/reward_per_eps            | -12        |
| test/steps                     | 70000      |
| train/episodes                 | 7000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.53       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00447   |
| train/info_shaping_reward_mean | -0.0881    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 280000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 175         |
| stats_o/mean                   | 0.20121865  |
| stats_o/std                    | 0.102648295 |
| test/episodes                  | 1760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.667       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00252    |
| test/info_shaping_reward_mean  | -0.0623     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -2.0387075  |
| test/Q_plus_P                  | -2.0387075  |
| test/reward_per_eps            | -13.3       |
| test/steps                     | 70400       |
| train/episodes                 | 7040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.516       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00485    |
| train/info_shaping_reward_mean | -0.0798     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.4       |
| train/steps                    | 281600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 176        |
| stats_o/mean                   | 0.20123565 |
| stats_o/std                    | 0.10256121 |
| test/episodes                  | 1770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00116   |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.8135685 |
| test/Q_plus_P                  | -1.8135685 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 70800      |
| train/episodes                 | 7080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.502      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00528   |
| train/info_shaping_reward_mean | -0.0915    |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 283200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.20122793  |
| stats_o/std                    | 0.102391854 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000652   |
| test/info_shaping_reward_mean  | -0.0574     |
| test/info_shaping_reward_min   | -0.199      |
| test/Q                         | -1.6069661  |
| test/Q_plus_P                  | -1.6069661  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.582       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00391    |
| train/info_shaping_reward_mean | -0.0719     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.2012015  |
| stats_o/std                    | 0.10224829 |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00137   |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5094256 |
| test/Q_plus_P                  | -1.5094256 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00516   |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 179        |
| stats_o/mean                   | 0.20120545 |
| stats_o/std                    | 0.10205736 |
| test/episodes                  | 1800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00134   |
| test/info_shaping_reward_mean  | -0.0515    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.514451  |
| test/Q_plus_P                  | -1.514451  |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 72000      |
| train/episodes                 | 7200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00407   |
| train/info_shaping_reward_mean | -0.0688    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 288000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 180        |
| stats_o/mean                   | 0.20121601 |
| stats_o/std                    | 0.10191063 |
| test/episodes                  | 1810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00142   |
| test/info_shaping_reward_mean  | -0.0574    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.7185806 |
| test/Q_plus_P                  | -1.7185806 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 72400      |
| train/episodes                 | 7240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00415   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 289600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 181        |
| stats_o/mean                   | 0.20121263 |
| stats_o/std                    | 0.10175944 |
| test/episodes                  | 1820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000785  |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5300654 |
| test/Q_plus_P                  | -1.5300654 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 72800      |
| train/episodes                 | 7280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00371   |
| train/info_shaping_reward_mean | -0.0797    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 291200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 182         |
| stats_o/mean                   | 0.20123944  |
| stats_o/std                    | 0.101630144 |
| test/episodes                  | 1830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00286    |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.5102407  |
| test/Q_plus_P                  | -1.5102407  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 73200       |
| train/episodes                 | 7320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.547       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00526    |
| train/info_shaping_reward_mean | -0.075      |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 292800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 183         |
| stats_o/mean                   | 0.20128776  |
| stats_o/std                    | 0.101537496 |
| test/episodes                  | 1840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00302    |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.3373201  |
| test/Q_plus_P                  | -1.3373201  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 73600       |
| train/episodes                 | 7360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00429    |
| train/info_shaping_reward_mean | -0.0783     |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 294400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 184        |
| stats_o/mean                   | 0.20128787 |
| stats_o/std                    | 0.1013558  |
| test/episodes                  | 1850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00283   |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.6147221 |
| test/Q_plus_P                  | -1.6147221 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 74000      |
| train/episodes                 | 7400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00434   |
| train/info_shaping_reward_mean | -0.073     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 296000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 185        |
| stats_o/mean                   | 0.20132363 |
| stats_o/std                    | 0.10120929 |
| test/episodes                  | 1860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.7878097 |
| test/Q_plus_P                  | -1.7878097 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 74400      |
| train/episodes                 | 7440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00405   |
| train/info_shaping_reward_mean | -0.0786    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 297600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 186         |
| stats_o/mean                   | 0.20135356  |
| stats_o/std                    | 0.101019554 |
| test/episodes                  | 1870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.688       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00156    |
| test/info_shaping_reward_mean  | -0.058      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.7460397  |
| test/Q_plus_P                  | -1.7460397  |
| test/reward_per_eps            | -12.5       |
| test/steps                     | 74800       |
| train/episodes                 | 7480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0735     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 299200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.20137155 |
| stats_o/std                    | 0.10087695 |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00242   |
| test/info_shaping_reward_mean  | -0.058     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6285231 |
| test/Q_plus_P                  | -1.6285231 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.523      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.0774    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 188         |
| stats_o/mean                   | 0.20135759  |
| stats_o/std                    | 0.100768864 |
| test/episodes                  | 1890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00184    |
| test/info_shaping_reward_mean  | -0.0563     |
| test/info_shaping_reward_min   | -0.194      |
| test/Q                         | -1.7665265  |
| test/Q_plus_P                  | -1.7665265  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 75600       |
| train/episodes                 | 7560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.578       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00269    |
| train/info_shaping_reward_mean | -0.0759     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 302400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.20138684  |
| stats_o/std                    | 0.100632526 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0026     |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.194      |
| test/Q                         | -1.6015713  |
| test/Q_plus_P                  | -1.6015713  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.531       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00464    |
| train/info_shaping_reward_mean | -0.0779     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.8       |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 190        |
| stats_o/mean                   | 0.20139416 |
| stats_o/std                    | 0.10051093 |
| test/episodes                  | 1910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.64       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0051    |
| test/info_shaping_reward_mean  | -0.0674    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.9415473 |
| test/Q_plus_P                  | -1.9415473 |
| test/reward_per_eps            | -14.4      |
| test/steps                     | 76400      |
| train/episodes                 | 7640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00607   |
| train/info_shaping_reward_mean | -0.0809    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 305600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 191        |
| stats_o/mean                   | 0.20140561 |
| stats_o/std                    | 0.10041581 |
| test/episodes                  | 1920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00282   |
| test/info_shaping_reward_mean  | -0.0612    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.7032495 |
| test/Q_plus_P                  | -1.7032495 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 76800      |
| train/episodes                 | 7680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.522      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00529   |
| train/info_shaping_reward_mean | -0.0927    |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 307200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.20142594  |
| stats_o/std                    | 0.100259855 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.693       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00162    |
| test/info_shaping_reward_mean  | -0.0579     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.6842228  |
| test/Q_plus_P                  | -1.6842228  |
| test/reward_per_eps            | -12.3       |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.519       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0046     |
| train/info_shaping_reward_mean | -0.0871     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.2       |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 193        |
| stats_o/mean                   | 0.20143317 |
| stats_o/std                    | 0.10008506 |
| test/episodes                  | 1940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00562   |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.5227733 |
| test/Q_plus_P                  | -1.5227733 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 77600      |
| train/episodes                 | 7760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00388   |
| train/info_shaping_reward_mean | -0.0723    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 310400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 194        |
| stats_o/mean                   | 0.20143586 |
| stats_o/std                    | 0.10001914 |
| test/episodes                  | 1950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00467   |
| test/info_shaping_reward_mean  | -0.058     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.5122511 |
| test/Q_plus_P                  | -1.5122511 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 78000      |
| train/episodes                 | 7800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.502      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0038    |
| train/info_shaping_reward_mean | -0.0846    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 312000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 195        |
| stats_o/mean                   | 0.20144287 |
| stats_o/std                    | 0.10002222 |
| test/episodes                  | 1960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00199   |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.5289447 |
| test/Q_plus_P                  | -1.5289447 |
| test/reward_per_eps            | -11        |
| test/steps                     | 78400      |
| train/episodes                 | 7840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.508      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00434   |
| train/info_shaping_reward_mean | -0.0947    |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.7      |
| train/steps                    | 313600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 196        |
| stats_o/mean                   | 0.20147291 |
| stats_o/std                    | 0.09990158 |
| test/episodes                  | 1970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5753773 |
| test/Q_plus_P                  | -1.5753773 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 78800      |
| train/episodes                 | 7880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00583   |
| train/info_shaping_reward_mean | -0.0777    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 315200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 197        |
| stats_o/mean                   | 0.20148195 |
| stats_o/std                    | 0.09983789 |
| test/episodes                  | 1980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000941  |
| test/info_shaping_reward_mean  | -0.0628    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.8423059 |
| test/Q_plus_P                  | -1.8423059 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 79200      |
| train/episodes                 | 7920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00383   |
| train/info_shaping_reward_mean | -0.0826    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 316800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 198        |
| stats_o/mean                   | 0.20149992 |
| stats_o/std                    | 0.09975009 |
| test/episodes                  | 1990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00305   |
| test/info_shaping_reward_mean  | -0.0585    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5957701 |
| test/Q_plus_P                  | -1.5957701 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 79600      |
| train/episodes                 | 7960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00527   |
| train/info_shaping_reward_mean | -0.0788    |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 318400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.20150718 |
| stats_o/std                    | 0.09959254 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00266   |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2356958 |
| test/Q_plus_P                  | -1.2356958 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00454   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 200        |
| stats_o/mean                   | 0.20153119 |
| stats_o/std                    | 0.09943699 |
| test/episodes                  | 2010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00485   |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4586194 |
| test/Q_plus_P                  | -1.4586194 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 80400      |
| train/episodes                 | 8040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00461   |
| train/info_shaping_reward_mean | -0.0686    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 321600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 201        |
| stats_o/mean                   | 0.20155601 |
| stats_o/std                    | 0.0992825  |
| test/episodes                  | 2020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00228   |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4326092 |
| test/Q_plus_P                  | -1.4326092 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 80800      |
| train/episodes                 | 8080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00417   |
| train/info_shaping_reward_mean | -0.0747    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 323200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 202        |
| stats_o/mean                   | 0.20153797 |
| stats_o/std                    | 0.09916993 |
| test/episodes                  | 2030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00324   |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -1.4830558 |
| test/Q_plus_P                  | -1.4830558 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 81200      |
| train/episodes                 | 8120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00371   |
| train/info_shaping_reward_mean | -0.07      |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 324800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 203        |
| stats_o/mean                   | 0.20153953 |
| stats_o/std                    | 0.09904385 |
| test/episodes                  | 2040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000904  |
| test/info_shaping_reward_mean  | -0.0541    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -1.642448  |
| test/Q_plus_P                  | -1.642448  |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 81600      |
| train/episodes                 | 8160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0031    |
| train/info_shaping_reward_mean | -0.0754    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 326400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 204        |
| stats_o/mean                   | 0.20156178 |
| stats_o/std                    | 0.09887982 |
| test/episodes                  | 2050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00336   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3976513 |
| test/Q_plus_P                  | -1.3976513 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 82000      |
| train/episodes                 | 8200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00433   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 328000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 205        |
| stats_o/mean                   | 0.20155728 |
| stats_o/std                    | 0.09880989 |
| test/episodes                  | 2060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00247   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4208298 |
| test/Q_plus_P                  | -1.4208298 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 82400      |
| train/episodes                 | 8240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00519   |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 329600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 206        |
| stats_o/mean                   | 0.20157166 |
| stats_o/std                    | 0.09865557 |
| test/episodes                  | 2070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00213   |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4853451 |
| test/Q_plus_P                  | -1.4853451 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 82800      |
| train/episodes                 | 8280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00351   |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 331200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.20160227 |
| stats_o/std                    | 0.09858418 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0017    |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.470742  |
| test/Q_plus_P                  | -1.470742  |
| test/reward_per_eps            | -11        |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00463   |
| train/info_shaping_reward_mean | -0.0772    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 208         |
| stats_o/mean                   | 0.20161304  |
| stats_o/std                    | 0.098434284 |
| test/episodes                  | 2090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00197    |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.212      |
| test/Q                         | -1.4065719  |
| test/Q_plus_P                  | -1.4065719  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 83600       |
| train/episodes                 | 8360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00487    |
| train/info_shaping_reward_mean | -0.0712     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 334400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 209         |
| stats_o/mean                   | 0.20160301  |
| stats_o/std                    | 0.098302975 |
| test/episodes                  | 2100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00215    |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.420801   |
| test/Q_plus_P                  | -1.420801   |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 84000       |
| train/episodes                 | 8400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.562       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00419    |
| train/info_shaping_reward_mean | -0.0731     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 336000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 210         |
| stats_o/mean                   | 0.20161937  |
| stats_o/std                    | 0.098153025 |
| test/episodes                  | 2110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0581     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5493857  |
| test/Q_plus_P                  | -1.5493857  |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 84400       |
| train/episodes                 | 8440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00527    |
| train/info_shaping_reward_mean | -0.0698     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 337600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 211        |
| stats_o/mean                   | 0.20163189 |
| stats_o/std                    | 0.09804264 |
| test/episodes                  | 2120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00429   |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2617041 |
| test/Q_plus_P                  | -1.2617041 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 84800      |
| train/episodes                 | 8480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00601   |
| train/info_shaping_reward_mean | -0.0798    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 339200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 212        |
| stats_o/mean                   | 0.20164515 |
| stats_o/std                    | 0.09794754 |
| test/episodes                  | 2130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.642      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00322   |
| test/info_shaping_reward_mean  | -0.0641    |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -2.0323527 |
| test/Q_plus_P                  | -2.0323527 |
| test/reward_per_eps            | -14.3      |
| test/steps                     | 85200      |
| train/episodes                 | 8520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00476   |
| train/info_shaping_reward_mean | -0.0732    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 340800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 213        |
| stats_o/mean                   | 0.20165879 |
| stats_o/std                    | 0.09791149 |
| test/episodes                  | 2140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00211   |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.5006378 |
| test/Q_plus_P                  | -1.5006378 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 85600      |
| train/episodes                 | 8560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00383   |
| train/info_shaping_reward_mean | -0.0897    |
| train/info_shaping_reward_min  | -0.293     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 342400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 214        |
| stats_o/mean                   | 0.20165527 |
| stats_o/std                    | 0.09783762 |
| test/episodes                  | 2150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0019    |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.3390326 |
| test/Q_plus_P                  | -1.3390326 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 86000      |
| train/episodes                 | 8600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00449   |
| train/info_shaping_reward_mean | -0.0843    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 344000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 215        |
| stats_o/mean                   | 0.20164816 |
| stats_o/std                    | 0.09771237 |
| test/episodes                  | 2160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00194   |
| test/info_shaping_reward_mean  | -0.0568    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4785542 |
| test/Q_plus_P                  | -1.4785542 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 86400      |
| train/episodes                 | 8640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00487   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 345600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 216        |
| stats_o/mean                   | 0.20164567 |
| stats_o/std                    | 0.0976103  |
| test/episodes                  | 2170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00276   |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.641747  |
| test/Q_plus_P                  | -1.641747  |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 86800      |
| train/episodes                 | 8680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0665    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 347200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 217        |
| stats_o/mean                   | 0.20165104 |
| stats_o/std                    | 0.09749012 |
| test/episodes                  | 2180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00332   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1163913 |
| test/Q_plus_P                  | -1.1163913 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 87200      |
| train/episodes                 | 8720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00441   |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 348800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 218        |
| stats_o/mean                   | 0.2016554  |
| stats_o/std                    | 0.09737482 |
| test/episodes                  | 2190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00153   |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -1.3512722 |
| test/Q_plus_P                  | -1.3512722 |
| test/reward_per_eps            | -11        |
| test/steps                     | 87600      |
| train/episodes                 | 8760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00511   |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 350400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 219        |
| stats_o/mean                   | 0.20163558 |
| stats_o/std                    | 0.09728078 |
| test/episodes                  | 2200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00335   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.4073888 |
| test/Q_plus_P                  | -1.4073888 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 88000      |
| train/episodes                 | 8800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 352000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 220        |
| stats_o/mean                   | 0.2016292  |
| stats_o/std                    | 0.09716293 |
| test/episodes                  | 2210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00123   |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -1.0543163 |
| test/Q_plus_P                  | -1.0543163 |
| test/reward_per_eps            | -9         |
| test/steps                     | 88400      |
| train/episodes                 | 8840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00513   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 353600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 221        |
| stats_o/mean                   | 0.20164599 |
| stats_o/std                    | 0.09706017 |
| test/episodes                  | 2220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000847  |
| test/info_shaping_reward_mean  | -0.0515    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4079247 |
| test/Q_plus_P                  | -1.4079247 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 88800      |
| train/episodes                 | 8880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.567      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00424   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 355200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 222         |
| stats_o/mean                   | 0.20166174  |
| stats_o/std                    | 0.096912526 |
| test/episodes                  | 2230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00143    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.5089856  |
| test/Q_plus_P                  | -1.5089856  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 89200       |
| train/episodes                 | 8920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.605       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0047     |
| train/info_shaping_reward_mean | -0.0682     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 356800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 223        |
| stats_o/mean                   | 0.20163625 |
| stats_o/std                    | 0.09684607 |
| test/episodes                  | 2240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00121   |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.5134708 |
| test/Q_plus_P                  | -1.5134708 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 89600      |
| train/episodes                 | 8960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00449   |
| train/info_shaping_reward_mean | -0.0809    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 358400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 224         |
| stats_o/mean                   | 0.2016267   |
| stats_o/std                    | 0.096763164 |
| test/episodes                  | 2250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00229    |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.4272449  |
| test/Q_plus_P                  | -1.4272449  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 90000       |
| train/episodes                 | 9000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.605       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.0755     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 360000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 225         |
| stats_o/mean                   | 0.20162274  |
| stats_o/std                    | 0.096661046 |
| test/episodes                  | 2260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00151    |
| test/info_shaping_reward_mean  | -0.0589     |
| test/info_shaping_reward_min   | -0.19       |
| test/Q                         | -2.5675526  |
| test/Q_plus_P                  | -2.5675526  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 90400       |
| train/episodes                 | 9040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00434    |
| train/info_shaping_reward_mean | -0.0674     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 361600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 226        |
| stats_o/mean                   | 0.20162678 |
| stats_o/std                    | 0.09661437 |
| test/episodes                  | 2270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00107   |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3796812 |
| test/Q_plus_P                  | -1.3796812 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 90800      |
| train/episodes                 | 9080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00521   |
| train/info_shaping_reward_mean | -0.0896    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 363200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 227        |
| stats_o/mean                   | 0.2016267  |
| stats_o/std                    | 0.09651746 |
| test/episodes                  | 2280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00227   |
| test/info_shaping_reward_mean  | -0.0607    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -1.7363963 |
| test/Q_plus_P                  | -1.7363963 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 91200      |
| train/episodes                 | 9120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00442   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 364800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 228        |
| stats_o/mean                   | 0.20163268 |
| stats_o/std                    | 0.09639737 |
| test/episodes                  | 2290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00154   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.262918  |
| test/Q_plus_P                  | -1.262918  |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 91600      |
| train/episodes                 | 9160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.565      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00325   |
| train/info_shaping_reward_mean | -0.074     |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 366400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 229         |
| stats_o/mean                   | 0.20163536  |
| stats_o/std                    | 0.0962806   |
| test/episodes                  | 2300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0029     |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.96658736 |
| test/Q_plus_P                  | -0.96658736 |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 92000       |
| train/episodes                 | 9200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.066      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 368000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 230         |
| stats_o/mean                   | 0.20167941  |
| stats_o/std                    | 0.096151724 |
| test/episodes                  | 2310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00408    |
| test/info_shaping_reward_mean  | -0.0565     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.5139575  |
| test/Q_plus_P                  | -1.5139575  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 92400       |
| train/episodes                 | 9240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.004      |
| train/info_shaping_reward_mean | -0.0656     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 369600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 231         |
| stats_o/mean                   | 0.20168881  |
| stats_o/std                    | 0.096035406 |
| test/episodes                  | 2320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00266    |
| test/info_shaping_reward_mean  | -0.0565     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.5880765  |
| test/Q_plus_P                  | -1.5880765  |
| test/reward_per_eps            | -12         |
| test/steps                     | 92800       |
| train/episodes                 | 9280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0041     |
| train/info_shaping_reward_mean | -0.0712     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 371200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 232         |
| stats_o/mean                   | 0.20169161  |
| stats_o/std                    | 0.095906354 |
| test/episodes                  | 2330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00178    |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2375354  |
| test/Q_plus_P                  | -1.2375354  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 93200       |
| train/episodes                 | 9320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00384    |
| train/info_shaping_reward_mean | -0.0687     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 372800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 233        |
| stats_o/mean                   | 0.20169707 |
| stats_o/std                    | 0.0958432  |
| test/episodes                  | 2340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00187   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.3299453 |
| test/Q_plus_P                  | -1.3299453 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 93600      |
| train/episodes                 | 9360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00537   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 374400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.20170724 |
| stats_o/std                    | 0.09575044 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0017    |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4853599 |
| test/Q_plus_P                  | -1.4853599 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00401   |
| train/info_shaping_reward_mean | -0.0679    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.20172496  |
| stats_o/std                    | 0.095647305 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.665       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00373    |
| test/info_shaping_reward_mean  | -0.0703     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -2.8449714  |
| test/Q_plus_P                  | -2.8449714  |
| test/reward_per_eps            | -13.4       |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0044     |
| train/info_shaping_reward_mean | -0.0697     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 236        |
| stats_o/mean                   | 0.20172378 |
| stats_o/std                    | 0.09552204 |
| test/episodes                  | 2370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00233   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1285791 |
| test/Q_plus_P                  | -1.1285791 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 94800      |
| train/episodes                 | 9480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00351   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 379200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 237         |
| stats_o/mean                   | 0.2017209   |
| stats_o/std                    | 0.095406234 |
| test/episodes                  | 2380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00265    |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.4042667  |
| test/Q_plus_P                  | -1.4042667  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 95200       |
| train/episodes                 | 9520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00267    |
| train/info_shaping_reward_mean | -0.0709     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 380800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 238        |
| stats_o/mean                   | 0.20172557 |
| stats_o/std                    | 0.09534792 |
| test/episodes                  | 2390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00427   |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -1.2860988 |
| test/Q_plus_P                  | -1.2860988 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 95600      |
| train/episodes                 | 9560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0038    |
| train/info_shaping_reward_mean | -0.0806    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 382400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 239        |
| stats_o/mean                   | 0.20173529 |
| stats_o/std                    | 0.09532827 |
| test/episodes                  | 2400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00373   |
| test/info_shaping_reward_mean  | -0.061     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5421252 |
| test/Q_plus_P                  | -1.5421252 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 96000      |
| train/episodes                 | 9600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00473   |
| train/info_shaping_reward_mean | -0.075     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 384000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 240        |
| stats_o/mean                   | 0.20175633 |
| stats_o/std                    | 0.09529561 |
| test/episodes                  | 2410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00203   |
| test/info_shaping_reward_mean  | -0.0618    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.7329909 |
| test/Q_plus_P                  | -1.7329909 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 96400      |
| train/episodes                 | 9640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00464   |
| train/info_shaping_reward_mean | -0.0816    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 385600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 241         |
| stats_o/mean                   | 0.20178108  |
| stats_o/std                    | 0.095213324 |
| test/episodes                  | 2420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00278    |
| test/info_shaping_reward_mean  | -0.0564     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.5436076  |
| test/Q_plus_P                  | -1.5436076  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 96800       |
| train/episodes                 | 9680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0738     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 387200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 242         |
| stats_o/mean                   | 0.20177113  |
| stats_o/std                    | 0.095141545 |
| test/episodes                  | 2430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000993   |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.1392319  |
| test/Q_plus_P                  | -1.1392319  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 97200       |
| train/episodes                 | 9720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.566       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00402    |
| train/info_shaping_reward_mean | -0.0782     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 388800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 243        |
| stats_o/mean                   | 0.2017958  |
| stats_o/std                    | 0.09509542 |
| test/episodes                  | 2440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000969  |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3758806 |
| test/Q_plus_P                  | -1.3758806 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 97600      |
| train/episodes                 | 9760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0053    |
| train/info_shaping_reward_mean | -0.0814    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 390400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.20178501 |
| stats_o/std                    | 0.0950425  |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00358   |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0990369 |
| test/Q_plus_P                  | -1.0990369 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00366   |
| train/info_shaping_reward_mean | -0.076     |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 245        |
| stats_o/mean                   | 0.20181082 |
| stats_o/std                    | 0.09496565 |
| test/episodes                  | 2460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00185   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2697594 |
| test/Q_plus_P                  | -1.2697594 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 98400      |
| train/episodes                 | 9840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00462   |
| train/info_shaping_reward_mean | -0.068     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 393600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 246         |
| stats_o/mean                   | 0.2017876   |
| stats_o/std                    | 0.094895564 |
| test/episodes                  | 2470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.677       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00204    |
| test/info_shaping_reward_mean  | -0.0592     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.7191457  |
| test/Q_plus_P                  | -1.7191457  |
| test/reward_per_eps            | -12.9       |
| test/steps                     | 98800       |
| train/episodes                 | 9880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.571       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00442    |
| train/info_shaping_reward_mean | -0.08       |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 395200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 247         |
| stats_o/mean                   | 0.2017948   |
| stats_o/std                    | 0.094845206 |
| test/episodes                  | 2480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00358    |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.2808676  |
| test/Q_plus_P                  | -1.2808676  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 99200       |
| train/episodes                 | 9920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.55        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00514    |
| train/info_shaping_reward_mean | -0.088      |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18         |
| train/steps                    | 396800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.20178603 |
| stats_o/std                    | 0.09473612 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00328   |
| test/info_shaping_reward_mean  | -0.0588    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5831311 |
| test/Q_plus_P                  | -1.5831311 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0052    |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 249         |
| stats_o/mean                   | 0.20180811  |
| stats_o/std                    | 0.094678886 |
| test/episodes                  | 2500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.675       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00236    |
| test/info_shaping_reward_mean  | -0.0627     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.7824289  |
| test/Q_plus_P                  | -1.7824289  |
| test/reward_per_eps            | -13         |
| test/steps                     | 100000      |
| train/episodes                 | 10000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.521       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00413    |
| train/info_shaping_reward_mean | -0.0797     |
| train/info_shaping_reward_min  | -0.202      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 400000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 250        |
| stats_o/mean                   | 0.20181604 |
| stats_o/std                    | 0.09455991 |
| test/episodes                  | 2510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00146   |
| test/info_shaping_reward_mean  | -0.0625    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.7857444 |
| test/Q_plus_P                  | -1.7857444 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 100400     |
| train/episodes                 | 10040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 401600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 251        |
| stats_o/mean                   | 0.20182294 |
| stats_o/std                    | 0.09444037 |
| test/episodes                  | 2520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00272   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2572968 |
| test/Q_plus_P                  | -1.2572968 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 100800     |
| train/episodes                 | 10080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00358   |
| train/info_shaping_reward_mean | -0.0727    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 403200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 252         |
| stats_o/mean                   | 0.20183292  |
| stats_o/std                    | 0.094339505 |
| test/episodes                  | 2530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00291    |
| test/info_shaping_reward_mean  | -0.0552     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.4144647  |
| test/Q_plus_P                  | -1.4144647  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 101200      |
| train/episodes                 | 10120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00348    |
| train/info_shaping_reward_mean | -0.0771     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 404800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 253        |
| stats_o/mean                   | 0.20185053 |
| stats_o/std                    | 0.09427599 |
| test/episodes                  | 2540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000207  |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -1.2749594 |
| test/Q_plus_P                  | -1.2749594 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 101600     |
| train/episodes                 | 10160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00438   |
| train/info_shaping_reward_mean | -0.0777    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 406400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.20186876  |
| stats_o/std                    | 0.094172895 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00289    |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.2635491  |
| test/Q_plus_P                  | -1.2635491  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00411    |
| train/info_shaping_reward_mean | -0.0659     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 255        |
| stats_o/mean                   | 0.20187451 |
| stats_o/std                    | 0.09409366 |
| test/episodes                  | 2560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00256   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1065204 |
| test/Q_plus_P                  | -1.1065204 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 102400     |
| train/episodes                 | 10240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00525   |
| train/info_shaping_reward_mean | -0.0828    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 409600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 256        |
| stats_o/mean                   | 0.20188999 |
| stats_o/std                    | 0.09407524 |
| test/episodes                  | 2570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00121   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.439628  |
| test/Q_plus_P                  | -1.439628  |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 102800     |
| train/episodes                 | 10280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00424   |
| train/info_shaping_reward_mean | -0.0723    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 411200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 257        |
| stats_o/mean                   | 0.20190334 |
| stats_o/std                    | 0.09399026 |
| test/episodes                  | 2580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0025    |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.4115142 |
| test/Q_plus_P                  | -1.4115142 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 103200     |
| train/episodes                 | 10320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00396   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 412800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 258        |
| stats_o/mean                   | 0.20191613 |
| stats_o/std                    | 0.09389117 |
| test/episodes                  | 2590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00272   |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4917579 |
| test/Q_plus_P                  | -1.4917579 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 103600     |
| train/episodes                 | 10360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00371   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 414400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 259        |
| stats_o/mean                   | 0.20193602 |
| stats_o/std                    | 0.0938031  |
| test/episodes                  | 2600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00216   |
| test/info_shaping_reward_mean  | -0.0549    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2973956 |
| test/Q_plus_P                  | -1.2973956 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 104000     |
| train/episodes                 | 10400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00349   |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 416000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 260        |
| stats_o/mean                   | 0.2019318  |
| stats_o/std                    | 0.09372064 |
| test/episodes                  | 2610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00187   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.1391128 |
| test/Q_plus_P                  | -1.1391128 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 104400     |
| train/episodes                 | 10440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00332   |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 417600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 261        |
| stats_o/mean                   | 0.20192945 |
| stats_o/std                    | 0.09362946 |
| test/episodes                  | 2620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00379   |
| test/info_shaping_reward_mean  | -0.0565    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3301436 |
| test/Q_plus_P                  | -1.3301436 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 104800     |
| train/episodes                 | 10480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00476   |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 419200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 262        |
| stats_o/mean                   | 0.20195736 |
| stats_o/std                    | 0.09357244 |
| test/episodes                  | 2630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000725  |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1220932 |
| test/Q_plus_P                  | -1.1220932 |
| test/reward_per_eps            | -10        |
| test/steps                     | 105200     |
| train/episodes                 | 10520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.538      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00482   |
| train/info_shaping_reward_mean | -0.0805    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 420800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.20196185  |
| stats_o/std                    | 0.093455665 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0039     |
| test/info_shaping_reward_mean  | -0.0593     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.4095247  |
| test/Q_plus_P                  | -1.4095247  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00376    |
| train/info_shaping_reward_mean | -0.0705     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 264        |
| stats_o/mean                   | 0.20197488 |
| stats_o/std                    | 0.09336818 |
| test/episodes                  | 2650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00386   |
| test/info_shaping_reward_mean  | -0.0653    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.6966254 |
| test/Q_plus_P                  | -1.6966254 |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 106000     |
| train/episodes                 | 10600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00443   |
| train/info_shaping_reward_mean | -0.0778    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 424000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 265         |
| stats_o/mean                   | 0.20197949  |
| stats_o/std                    | 0.093299694 |
| test/episodes                  | 2660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00489    |
| test/info_shaping_reward_mean  | -0.0538     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.1245468  |
| test/Q_plus_P                  | -1.1245468  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 106400      |
| train/episodes                 | 10640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.582       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00432    |
| train/info_shaping_reward_mean | -0.0719     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 425600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 266        |
| stats_o/mean                   | 0.20198913 |
| stats_o/std                    | 0.09325118 |
| test/episodes                  | 2670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00305   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2282821 |
| test/Q_plus_P                  | -1.2282821 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 106800     |
| train/episodes                 | 10680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00492   |
| train/info_shaping_reward_mean | -0.0851    |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 427200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 267        |
| stats_o/mean                   | 0.20199318 |
| stats_o/std                    | 0.09320906 |
| test/episodes                  | 2680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00156   |
| test/info_shaping_reward_mean  | -0.062     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.6491109 |
| test/Q_plus_P                  | -1.6491109 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 107200     |
| train/episodes                 | 10720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00405   |
| train/info_shaping_reward_mean | -0.0796    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 428800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 268        |
| stats_o/mean                   | 0.20198765 |
| stats_o/std                    | 0.09312835 |
| test/episodes                  | 2690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00156   |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0315262 |
| test/Q_plus_P                  | -1.0315262 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 107600     |
| train/episodes                 | 10760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00337   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 430400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 269         |
| stats_o/mean                   | 0.20198229  |
| stats_o/std                    | 0.093047924 |
| test/episodes                  | 2700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00281    |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.283347   |
| test/Q_plus_P                  | -1.283347   |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 108000      |
| train/episodes                 | 10800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.069      |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 432000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 270        |
| stats_o/mean                   | 0.20198378 |
| stats_o/std                    | 0.0929933  |
| test/episodes                  | 2710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00307   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.4183679 |
| test/Q_plus_P                  | -1.4183679 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 108400     |
| train/episodes                 | 10840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 433600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 271         |
| stats_o/mean                   | 0.2019928   |
| stats_o/std                    | 0.092905685 |
| test/episodes                  | 2720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00901    |
| test/info_shaping_reward_mean  | -0.0617     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.5707353  |
| test/Q_plus_P                  | -1.5707353  |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 108800      |
| train/episodes                 | 10880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.0701     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 435200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.202       |
| stats_o/std                    | 0.092843026 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00542    |
| test/info_shaping_reward_mean  | -0.0574     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.3508312  |
| test/Q_plus_P                  | -1.3508312  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00419    |
| train/info_shaping_reward_mean | -0.0646     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 273        |
| stats_o/mean                   | 0.20198931 |
| stats_o/std                    | 0.09276598 |
| test/episodes                  | 2740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00496   |
| test/info_shaping_reward_mean  | -0.0642    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.5751892 |
| test/Q_plus_P                  | -1.5751892 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 109600     |
| train/episodes                 | 10960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00384   |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 438400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 274        |
| stats_o/mean                   | 0.20197397 |
| stats_o/std                    | 0.09273856 |
| test/episodes                  | 2750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00738   |
| test/info_shaping_reward_mean  | -0.0606    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3750353 |
| test/Q_plus_P                  | -1.3750353 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 110000     |
| train/episodes                 | 11000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.528      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 440000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 275         |
| stats_o/mean                   | 0.20198323  |
| stats_o/std                    | 0.092656344 |
| test/episodes                  | 2760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00372    |
| test/info_shaping_reward_mean  | -0.0646     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.5899453  |
| test/Q_plus_P                  | -1.5899453  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 110400      |
| train/episodes                 | 11040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0034     |
| train/info_shaping_reward_mean | -0.067      |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 441600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 276         |
| stats_o/mean                   | 0.20199165  |
| stats_o/std                    | 0.092639774 |
| test/episodes                  | 2770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00456    |
| test/info_shaping_reward_mean  | -0.0527     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1339926  |
| test/Q_plus_P                  | -1.1339926  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 110800      |
| train/episodes                 | 11080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.576       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00539    |
| train/info_shaping_reward_mean | -0.078      |
| train/info_shaping_reward_min  | -0.225      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 443200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 277        |
| stats_o/mean                   | 0.20200752 |
| stats_o/std                    | 0.09256556 |
| test/episodes                  | 2780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00333   |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.4081035 |
| test/Q_plus_P                  | -1.4081035 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 111200     |
| train/episodes                 | 11120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00434   |
| train/info_shaping_reward_mean | -0.0757    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 444800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 278        |
| stats_o/mean                   | 0.20203234 |
| stats_o/std                    | 0.09252369 |
| test/episodes                  | 2790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000878  |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4135733 |
| test/Q_plus_P                  | -1.4135733 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 111600     |
| train/episodes                 | 11160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00484   |
| train/info_shaping_reward_mean | -0.0715    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 446400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 279        |
| stats_o/mean                   | 0.20203392 |
| stats_o/std                    | 0.0925518  |
| test/episodes                  | 2800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00542   |
| test/info_shaping_reward_mean  | -0.0596    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.474036  |
| test/Q_plus_P                  | -1.474036  |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 112000     |
| train/episodes                 | 11200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.0795    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 448000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 280        |
| stats_o/mean                   | 0.20204726 |
| stats_o/std                    | 0.09246473 |
| test/episodes                  | 2810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00138   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.418619  |
| test/Q_plus_P                  | -1.418619  |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 112400     |
| train/episodes                 | 11240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00443   |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 449600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.20204769 |
| stats_o/std                    | 0.09237235 |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00102   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.2998396 |
| test/Q_plus_P                  | -1.2998396 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00362   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.20202962 |
| stats_o/std                    | 0.09231216 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00277   |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.4389137 |
| test/Q_plus_P                  | -1.4389137 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00423   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.20203894 |
| stats_o/std                    | 0.09225839 |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00182   |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.3306907 |
| test/Q_plus_P                  | -1.3306907 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00339   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 284         |
| stats_o/mean                   | 0.20202918  |
| stats_o/std                    | 0.092233896 |
| test/episodes                  | 2850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00437    |
| test/info_shaping_reward_mean  | -0.0556     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.3902371  |
| test/Q_plus_P                  | -1.3902371  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 114000      |
| train/episodes                 | 11400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.542       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00372    |
| train/info_shaping_reward_mean | -0.0748     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.3       |
| train/steps                    | 456000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 285        |
| stats_o/mean                   | 0.20204434 |
| stats_o/std                    | 0.09220209 |
| test/episodes                  | 2860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00766   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1633075 |
| test/Q_plus_P                  | -1.1633075 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 114400     |
| train/episodes                 | 11440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00483   |
| train/info_shaping_reward_mean | -0.0934    |
| train/info_shaping_reward_min  | -0.317     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 457600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 286         |
| stats_o/mean                   | 0.20205496  |
| stats_o/std                    | 0.092150554 |
| test/episodes                  | 2870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00173    |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2163953  |
| test/Q_plus_P                  | -1.2163953  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 114800      |
| train/episodes                 | 11480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00427    |
| train/info_shaping_reward_mean | -0.0713     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 459200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.20205094 |
| stats_o/std                    | 0.0920888  |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00397   |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2139597 |
| test/Q_plus_P                  | -1.2139597 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00481   |
| train/info_shaping_reward_mean | -0.0686    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 288        |
| stats_o/mean                   | 0.20205292 |
| stats_o/std                    | 0.09204987 |
| test/episodes                  | 2890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00217   |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0698032 |
| test/Q_plus_P                  | -1.0698032 |
| test/reward_per_eps            | -9         |
| test/steps                     | 115600     |
| train/episodes                 | 11560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00342   |
| train/info_shaping_reward_mean | -0.0739    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 462400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.2020585   |
| stats_o/std                    | 0.092012316 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0031     |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2905934  |
| test/Q_plus_P                  | -1.2905934  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0696     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 290        |
| stats_o/mean                   | 0.20205007 |
| stats_o/std                    | 0.09197616 |
| test/episodes                  | 2910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0138    |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3008041 |
| test/Q_plus_P                  | -1.3008041 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 116400     |
| train/episodes                 | 11640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00506   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 465600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 291        |
| stats_o/mean                   | 0.20205104 |
| stats_o/std                    | 0.09189392 |
| test/episodes                  | 2920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00562   |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2428334 |
| test/Q_plus_P                  | -1.2428334 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 116800     |
| train/episodes                 | 11680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0722    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 467200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 292        |
| stats_o/mean                   | 0.20206165 |
| stats_o/std                    | 0.09184423 |
| test/episodes                  | 2930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00347   |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.2228051 |
| test/Q_plus_P                  | -1.2228051 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 117200     |
| train/episodes                 | 11720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.572      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0037    |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 468800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 293        |
| stats_o/mean                   | 0.20207043 |
| stats_o/std                    | 0.09180613 |
| test/episodes                  | 2940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00315   |
| test/info_shaping_reward_mean  | -0.0586    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3893303 |
| test/Q_plus_P                  | -1.3893303 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 117600     |
| train/episodes                 | 11760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.538      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.081     |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 470400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 294        |
| stats_o/mean                   | 0.2020783  |
| stats_o/std                    | 0.09174789 |
| test/episodes                  | 2950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00481   |
| test/info_shaping_reward_mean  | -0.0635    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.5241828 |
| test/Q_plus_P                  | -1.5241828 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 118000     |
| train/episodes                 | 11800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.0739    |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 472000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 295        |
| stats_o/mean                   | 0.20210041 |
| stats_o/std                    | 0.09170668 |
| test/episodes                  | 2960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00228   |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2463549 |
| test/Q_plus_P                  | -1.2463549 |
| test/reward_per_eps            | -10        |
| test/steps                     | 118400     |
| train/episodes                 | 11840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.0764    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 473600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 296        |
| stats_o/mean                   | 0.2021052  |
| stats_o/std                    | 0.09169712 |
| test/episodes                  | 2970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00357   |
| test/info_shaping_reward_mean  | -0.0666    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5885147 |
| test/Q_plus_P                  | -1.5885147 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 118800     |
| train/episodes                 | 11880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.582      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00544   |
| train/info_shaping_reward_mean | -0.0779    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 475200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 297        |
| stats_o/mean                   | 0.20208998 |
| stats_o/std                    | 0.09162979 |
| test/episodes                  | 2980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00319   |
| test/info_shaping_reward_mean  | -0.0569    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.4097713 |
| test/Q_plus_P                  | -1.4097713 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 119200     |
| train/episodes                 | 11920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00471   |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 476800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 298        |
| stats_o/mean                   | 0.20209828 |
| stats_o/std                    | 0.09157155 |
| test/episodes                  | 2990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00112   |
| test/info_shaping_reward_mean  | -0.0613    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4020844 |
| test/Q_plus_P                  | -1.4020844 |
| test/reward_per_eps            | -12        |
| test/steps                     | 119600     |
| train/episodes                 | 11960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00385   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 478400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 299         |
| stats_o/mean                   | 0.20209722  |
| stats_o/std                    | 0.091479495 |
| test/episodes                  | 3000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.0546     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.3426018  |
| test/Q_plus_P                  | -1.3426018  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 120000      |
| train/episodes                 | 12000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00408    |
| train/info_shaping_reward_mean | -0.066      |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 480000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.20209269  |
| stats_o/std                    | 0.091405764 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00201    |
| test/info_shaping_reward_mean  | -0.0585     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.2115873  |
| test/Q_plus_P                  | -1.2115873  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.602       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00488    |
| train/info_shaping_reward_mean | -0.0692     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.20212035 |
| stats_o/std                    | 0.09135993 |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00756   |
| test/info_shaping_reward_mean  | -0.0586    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.2745739 |
| test/Q_plus_P                  | -1.2745739 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00482   |
| train/info_shaping_reward_mean | -0.0679    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 302         |
| stats_o/mean                   | 0.20211865  |
| stats_o/std                    | 0.091298655 |
| test/episodes                  | 3030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.647       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00365    |
| test/info_shaping_reward_mean  | -0.0699     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.8162155  |
| test/Q_plus_P                  | -1.8162155  |
| test/reward_per_eps            | -14.1       |
| test/steps                     | 121200      |
| train/episodes                 | 12120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00392    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 484800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 303        |
| stats_o/mean                   | 0.20212506 |
| stats_o/std                    | 0.09124231 |
| test/episodes                  | 3040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00384   |
| test/info_shaping_reward_mean  | -0.0586    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4700551 |
| test/Q_plus_P                  | -1.4700551 |
| test/reward_per_eps            | -11        |
| test/steps                     | 121600     |
| train/episodes                 | 12160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00454   |
| train/info_shaping_reward_mean | -0.0761    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 486400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.20211096  |
| stats_o/std                    | 0.091180526 |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00179    |
| test/info_shaping_reward_mean  | -0.0598     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.4782234  |
| test/Q_plus_P                  | -1.4782234  |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.202096   |
| stats_o/std                    | 0.09115472 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00416   |
| test/info_shaping_reward_mean  | -0.0623    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6274796 |
| test/Q_plus_P                  | -1.6274796 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.555      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00512   |
| train/info_shaping_reward_mean | -0.0776    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 306        |
| stats_o/mean                   | 0.20210026 |
| stats_o/std                    | 0.09110707 |
| test/episodes                  | 3070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00229   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3577385 |
| test/Q_plus_P                  | -1.3577385 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 122800     |
| train/episodes                 | 12280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00366   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 491200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.20210701 |
| stats_o/std                    | 0.09100942 |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00423   |
| test/info_shaping_reward_mean  | -0.0604    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.4254678 |
| test/Q_plus_P                  | -1.4254678 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00353   |
| train/info_shaping_reward_mean | -0.0665    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 308         |
| stats_o/mean                   | 0.20211458  |
| stats_o/std                    | 0.090966456 |
| test/episodes                  | 3090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00245    |
| test/info_shaping_reward_mean  | -0.0567     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.3637626  |
| test/Q_plus_P                  | -1.3637626  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 123600      |
| train/episodes                 | 12360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.578       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00379    |
| train/info_shaping_reward_mean | -0.0751     |
| train/info_shaping_reward_min  | -0.219      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 494400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 309        |
| stats_o/mean                   | 0.20213282 |
| stats_o/std                    | 0.09091071 |
| test/episodes                  | 3100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00233   |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.0936221 |
| test/Q_plus_P                  | -1.0936221 |
| test/reward_per_eps            | -10        |
| test/steps                     | 124000     |
| train/episodes                 | 12400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00372   |
| train/info_shaping_reward_mean | -0.0757    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 496000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 310        |
| stats_o/mean                   | 0.20215319 |
| stats_o/std                    | 0.09086929 |
| test/episodes                  | 3110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000793  |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3449179 |
| test/Q_plus_P                  | -1.3449179 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 124400     |
| train/episodes                 | 12440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00421   |
| train/info_shaping_reward_mean | -0.074     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 497600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 311         |
| stats_o/mean                   | 0.2021491   |
| stats_o/std                    | 0.090797566 |
| test/episodes                  | 3120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00264    |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.286825   |
| test/Q_plus_P                  | -1.286825   |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 124800      |
| train/episodes                 | 12480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00437    |
| train/info_shaping_reward_mean | -0.0711     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 499200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 312        |
| stats_o/mean                   | 0.20213127 |
| stats_o/std                    | 0.09074037 |
| test/episodes                  | 3130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00304   |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -1.1679022 |
| test/Q_plus_P                  | -1.1679022 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 125200     |
| train/episodes                 | 12520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 500800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.20214222  |
| stats_o/std                    | 0.090660706 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00301    |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.2918395  |
| test/Q_plus_P                  | -1.2918395  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0046     |
| train/info_shaping_reward_mean | -0.0676     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 314        |
| stats_o/mean                   | 0.20212229 |
| stats_o/std                    | 0.09060219 |
| test/episodes                  | 3150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00746   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.2088012 |
| test/Q_plus_P                  | -1.2088012 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 126000     |
| train/episodes                 | 12600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.64       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00491   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 504000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 315         |
| stats_o/mean                   | 0.20211545  |
| stats_o/std                    | 0.09057541  |
| test/episodes                  | 3160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0095     |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -0.95748764 |
| test/Q_plus_P                  | -0.95748764 |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 126400      |
| train/episodes                 | 12640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.573       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00335    |
| train/info_shaping_reward_mean | -0.0785     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 505600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 316        |
| stats_o/mean                   | 0.20211282 |
| stats_o/std                    | 0.09049692 |
| test/episodes                  | 3170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00875   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2152694 |
| test/Q_plus_P                  | -1.2152694 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 126800     |
| train/episodes                 | 12680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0035    |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 507200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 317        |
| stats_o/mean                   | 0.20212501 |
| stats_o/std                    | 0.09047107 |
| test/episodes                  | 3180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00383   |
| test/info_shaping_reward_mean  | -0.0621    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5535331 |
| test/Q_plus_P                  | -1.5535331 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 127200     |
| train/episodes                 | 12720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00499   |
| train/info_shaping_reward_mean | -0.0775    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 508800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 318        |
| stats_o/mean                   | 0.20212755 |
| stats_o/std                    | 0.09042881 |
| test/episodes                  | 3190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00859   |
| test/info_shaping_reward_mean  | -0.0569    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0984929 |
| test/Q_plus_P                  | -1.0984929 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 127600     |
| train/episodes                 | 12760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00343   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 510400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 319        |
| stats_o/mean                   | 0.20212568 |
| stats_o/std                    | 0.09037611 |
| test/episodes                  | 3200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00356   |
| test/info_shaping_reward_mean  | -0.0568    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.3030211 |
| test/Q_plus_P                  | -1.3030211 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 128000     |
| train/episodes                 | 12800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00486   |
| train/info_shaping_reward_mean | -0.0693    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 512000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 320        |
| stats_o/mean                   | 0.20212843 |
| stats_o/std                    | 0.09031748 |
| test/episodes                  | 3210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00417   |
| test/info_shaping_reward_mean  | -0.0557    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.209275  |
| test/Q_plus_P                  | -1.209275  |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 128400     |
| train/episodes                 | 12840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00303   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 513600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.20213087  |
| stats_o/std                    | 0.090264775 |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00202    |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2269706  |
| test/Q_plus_P                  | -1.2269706  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00366    |
| train/info_shaping_reward_mean | -0.0713     |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 322        |
| stats_o/mean                   | 0.20213985 |
| stats_o/std                    | 0.09021262 |
| test/episodes                  | 3230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00149   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1721272 |
| test/Q_plus_P                  | -1.1721272 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 129200     |
| train/episodes                 | 12920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00331   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 516800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 323        |
| stats_o/mean                   | 0.20214863 |
| stats_o/std                    | 0.09016962 |
| test/episodes                  | 3240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00301   |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.3694184 |
| test/Q_plus_P                  | -1.3694184 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 129600     |
| train/episodes                 | 12960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00458   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 518400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 324        |
| stats_o/mean                   | 0.20213772 |
| stats_o/std                    | 0.09013107 |
| test/episodes                  | 3250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00313   |
| test/info_shaping_reward_mean  | -0.0674    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.7648921 |
| test/Q_plus_P                  | -1.7648921 |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 130000     |
| train/episodes                 | 13000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00523   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 520000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 325         |
| stats_o/mean                   | 0.20215566  |
| stats_o/std                    | 0.090071775 |
| test/episodes                  | 3260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00161    |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -1.457584   |
| test/Q_plus_P                  | -1.457584   |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 130400      |
| train/episodes                 | 13040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00371    |
| train/info_shaping_reward_mean | -0.0701     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 521600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.20215963  |
| stats_o/std                    | 0.090096496 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -1.0769094  |
| test/Q_plus_P                  | -1.0769094  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.575       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.0949     |
| train/info_shaping_reward_min  | -0.304      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 327        |
| stats_o/mean                   | 0.2021578  |
| stats_o/std                    | 0.09002891 |
| test/episodes                  | 3280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00203   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.0510793 |
| test/Q_plus_P                  | -1.0510793 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 131200     |
| train/episodes                 | 13120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00312   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 524800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 328         |
| stats_o/mean                   | 0.20216368  |
| stats_o/std                    | 0.089999095 |
| test/episodes                  | 3290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00218    |
| test/info_shaping_reward_mean  | -0.0608     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.5153618  |
| test/Q_plus_P                  | -1.5153618  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 131600      |
| train/episodes                 | 13160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.582       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00536    |
| train/info_shaping_reward_mean | -0.0719     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 526400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 329        |
| stats_o/mean                   | 0.20216402 |
| stats_o/std                    | 0.08992954 |
| test/episodes                  | 3300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00431   |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2863818 |
| test/Q_plus_P                  | -1.2863818 |
| test/reward_per_eps            | -10        |
| test/steps                     | 132000     |
| train/episodes                 | 13200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00288   |
| train/info_shaping_reward_mean | -0.07      |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 528000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.20217335  |
| stats_o/std                    | 0.089850605 |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00585    |
| test/info_shaping_reward_mean  | -0.06       |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.220561   |
| test/Q_plus_P                  | -1.220561   |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00393    |
| train/info_shaping_reward_mean | -0.0706     |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 331        |
| stats_o/mean                   | 0.20218153 |
| stats_o/std                    | 0.08981681 |
| test/episodes                  | 3320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00394   |
| test/info_shaping_reward_mean  | -0.0655    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4486684 |
| test/Q_plus_P                  | -1.4486684 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 132800     |
| train/episodes                 | 13280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00375   |
| train/info_shaping_reward_mean | -0.0741    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 531200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 332        |
| stats_o/mean                   | 0.20217255 |
| stats_o/std                    | 0.08975222 |
| test/episodes                  | 3330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0078    |
| test/info_shaping_reward_mean  | -0.0663    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.6261048 |
| test/Q_plus_P                  | -1.6261048 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 133200     |
| train/episodes                 | 13320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00347   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 532800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 333        |
| stats_o/mean                   | 0.20219532 |
| stats_o/std                    | 0.08970216 |
| test/episodes                  | 3340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0018    |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2069067 |
| test/Q_plus_P                  | -1.2069067 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 133600     |
| train/episodes                 | 13360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00428   |
| train/info_shaping_reward_mean | -0.0768    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 534400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 334        |
| stats_o/mean                   | 0.20220357 |
| stats_o/std                    | 0.08966619 |
| test/episodes                  | 3350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.013     |
| test/info_shaping_reward_mean  | -0.0616    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3668311 |
| test/Q_plus_P                  | -1.3668311 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 134000     |
| train/episodes                 | 13400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00336   |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 536000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 335         |
| stats_o/mean                   | 0.20219845  |
| stats_o/std                    | 0.089628905 |
| test/episodes                  | 3360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00421    |
| test/info_shaping_reward_mean  | -0.0635     |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -1.5779982  |
| test/Q_plus_P                  | -1.5779982  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 134400      |
| train/episodes                 | 13440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00489    |
| train/info_shaping_reward_mean | -0.074      |
| train/info_shaping_reward_min  | -0.197      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 537600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 336        |
| stats_o/mean                   | 0.20219855 |
| stats_o/std                    | 0.08958895 |
| test/episodes                  | 3370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4416581 |
| test/Q_plus_P                  | -1.4416581 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 134800     |
| train/episodes                 | 13480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00304   |
| train/info_shaping_reward_mean | -0.0743    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 539200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.20219433  |
| stats_o/std                    | 0.089596815 |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00486    |
| test/info_shaping_reward_mean  | -0.059      |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.409966   |
| test/Q_plus_P                  | -1.409966   |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.59        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00385    |
| train/info_shaping_reward_mean | -0.0774     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.20220251 |
| stats_o/std                    | 0.08956742 |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0025    |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.2989976 |
| test/Q_plus_P                  | -1.2989976 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00373   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.20222737  |
| stats_o/std                    | 0.089622825 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00312    |
| test/info_shaping_reward_mean  | -0.0571     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.4693302  |
| test/Q_plus_P                  | -1.4693302  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.505       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00496    |
| train/info_shaping_reward_mean | -0.0865     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.8       |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 340        |
| stats_o/mean                   | 0.20222455 |
| stats_o/std                    | 0.08958244 |
| test/episodes                  | 3410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000274  |
| test/info_shaping_reward_mean  | -0.0467    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0103137 |
| test/Q_plus_P                  | -1.0103137 |
| test/reward_per_eps            | -9         |
| test/steps                     | 136400     |
| train/episodes                 | 13640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00502   |
| train/info_shaping_reward_mean | -0.07      |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 545600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 341        |
| stats_o/mean                   | 0.20222989 |
| stats_o/std                    | 0.08950871 |
| test/episodes                  | 3420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0574    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3860254 |
| test/Q_plus_P                  | -1.3860254 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 136800     |
| train/episodes                 | 13680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.635      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00426   |
| train/info_shaping_reward_mean | -0.0647    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 547200     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 342       |
| stats_o/mean                   | 0.2022496 |
| stats_o/std                    | 0.0894514 |
| test/episodes                  | 3430      |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.645     |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.00235  |
| test/info_shaping_reward_mean  | -0.0654   |
| test/info_shaping_reward_min   | -0.178    |
| test/Q                         | -1.858901 |
| test/Q_plus_P                  | -1.858901 |
| test/reward_per_eps            | -14.2     |
| test/steps                     | 137200    |
| train/episodes                 | 13720     |
| train/info_is_success_max      | 1         |
| train/info_is_success_mean     | 0.581     |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.00415  |
| train/info_shaping_reward_mean | -0.0702   |
| train/info_shaping_reward_min  | -0.179    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -16.8     |
| train/steps                    | 548800    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 343        |
| stats_o/mean                   | 0.2022483  |
| stats_o/std                    | 0.08941232 |
| test/episodes                  | 3440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00315   |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3970381 |
| test/Q_plus_P                  | -1.3970381 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 137600     |
| train/episodes                 | 13760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00386   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 550400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 344        |
| stats_o/mean                   | 0.20226038 |
| stats_o/std                    | 0.08935508 |
| test/episodes                  | 3450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00349   |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3888099 |
| test/Q_plus_P                  | -1.3888099 |
| test/reward_per_eps            | -11        |
| test/steps                     | 138000     |
| train/episodes                 | 13800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00449   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 552000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.20225136  |
| stats_o/std                    | 0.089300014 |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000854   |
| test/info_shaping_reward_mean  | -0.0553     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.414226   |
| test/Q_plus_P                  | -1.414226   |
| test/reward_per_eps            | -11         |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00374    |
| train/info_shaping_reward_mean | -0.0664     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 346        |
| stats_o/mean                   | 0.20225568 |
| stats_o/std                    | 0.08926075 |
| test/episodes                  | 3470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0023    |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4880769 |
| test/Q_plus_P                  | -1.4880769 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 138800     |
| train/episodes                 | 13880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0715    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 555200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 347         |
| stats_o/mean                   | 0.20225735  |
| stats_o/std                    | 0.089219876 |
| test/episodes                  | 3480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00459    |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.1031275  |
| test/Q_plus_P                  | -1.1031275  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 139200      |
| train/episodes                 | 13920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00395    |
| train/info_shaping_reward_mean | -0.0656     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 556800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 348        |
| stats_o/mean                   | 0.20224857 |
| stats_o/std                    | 0.08916249 |
| test/episodes                  | 3490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000653  |
| test/info_shaping_reward_mean  | -0.0595    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.6269263 |
| test/Q_plus_P                  | -1.6269263 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 139600     |
| train/episodes                 | 13960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00446   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 558400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 349         |
| stats_o/mean                   | 0.20224604  |
| stats_o/std                    | 0.089110024 |
| test/episodes                  | 3500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00368    |
| test/info_shaping_reward_mean  | -0.0592     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.4695872  |
| test/Q_plus_P                  | -1.4695872  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 140000      |
| train/episodes                 | 14000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00472    |
| train/info_shaping_reward_mean | -0.0699     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 560000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 350         |
| stats_o/mean                   | 0.20224705  |
| stats_o/std                    | 0.089092284 |
| test/episodes                  | 3510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3544575  |
| test/Q_plus_P                  | -1.3544575  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 140400      |
| train/episodes                 | 14040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00337    |
| train/info_shaping_reward_mean | -0.077      |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 561600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 351         |
| stats_o/mean                   | 0.2022594   |
| stats_o/std                    | 0.089057624 |
| test/episodes                  | 3520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00279    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.3169861  |
| test/Q_plus_P                  | -1.3169861  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 140800      |
| train/episodes                 | 14080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00414    |
| train/info_shaping_reward_mean | -0.077      |
| train/info_shaping_reward_min  | -0.219      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 563200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 352         |
| stats_o/mean                   | 0.20229025  |
| stats_o/std                    | 0.089051716 |
| test/episodes                  | 3530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0599     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.344851   |
| test/Q_plus_P                  | -1.344851   |
| test/reward_per_eps            | -11         |
| test/steps                     | 141200      |
| train/episodes                 | 14120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.605       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00481    |
| train/info_shaping_reward_mean | -0.0728     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 564800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 353        |
| stats_o/mean                   | 0.20229709 |
| stats_o/std                    | 0.08902826 |
| test/episodes                  | 3540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00128   |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2844354 |
| test/Q_plus_P                  | -1.2844354 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 141600     |
| train/episodes                 | 14160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00413   |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 566400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 354        |
| stats_o/mean                   | 0.2022923  |
| stats_o/std                    | 0.08898156 |
| test/episodes                  | 3550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00236   |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3201959 |
| test/Q_plus_P                  | -1.3201959 |
| test/reward_per_eps            | -11        |
| test/steps                     | 142000     |
| train/episodes                 | 14200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.0719    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 568000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.20230241  |
| stats_o/std                    | 0.088946626 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00223    |
| test/info_shaping_reward_mean  | -0.0547     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.4862053  |
| test/Q_plus_P                  | -1.4862053  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00362    |
| train/info_shaping_reward_mean | -0.0682     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 356         |
| stats_o/mean                   | 0.2022998   |
| stats_o/std                    | 0.088907965 |
| test/episodes                  | 3570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00182    |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.194      |
| test/Q                         | -1.2854854  |
| test/Q_plus_P                  | -1.2854854  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 142800      |
| train/episodes                 | 14280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00361    |
| train/info_shaping_reward_mean | -0.0751     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 571200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 357        |
| stats_o/mean                   | 0.20231394 |
| stats_o/std                    | 0.08883487 |
| test/episodes                  | 3580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00188   |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2723601 |
| test/Q_plus_P                  | -1.2723601 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 143200     |
| train/episodes                 | 14320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00406   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 572800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 358        |
| stats_o/mean                   | 0.20231399 |
| stats_o/std                    | 0.08877013 |
| test/episodes                  | 3590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00265   |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5631677 |
| test/Q_plus_P                  | -1.5631677 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 143600     |
| train/episodes                 | 14360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00391   |
| train/info_shaping_reward_mean | -0.068     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 574400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 359        |
| stats_o/mean                   | 0.20231614 |
| stats_o/std                    | 0.08869488 |
| test/episodes                  | 3600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00564   |
| test/info_shaping_reward_mean  | -0.0604    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5284196 |
| test/Q_plus_P                  | -1.5284196 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 144000     |
| train/episodes                 | 14400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00375   |
| train/info_shaping_reward_mean | -0.0665    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 576000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 360         |
| stats_o/mean                   | 0.20230763  |
| stats_o/std                    | 0.088665426 |
| test/episodes                  | 3610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.0288414  |
| test/Q_plus_P                  | -1.0288414  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 144400      |
| train/episodes                 | 14440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00454    |
| train/info_shaping_reward_mean | -0.0687     |
| train/info_shaping_reward_min  | -0.199      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 577600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 361         |
| stats_o/mean                   | 0.20229182  |
| stats_o/std                    | 0.088608526 |
| test/episodes                  | 3620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00098    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.1777279  |
| test/Q_plus_P                  | -1.1777279  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 144800      |
| train/episodes                 | 14480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00373    |
| train/info_shaping_reward_mean | -0.0664     |
| train/info_shaping_reward_min  | -0.203      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 579200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 362        |
| stats_o/mean                   | 0.20228462 |
| stats_o/std                    | 0.0885978  |
| test/episodes                  | 3630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00542   |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.2610278 |
| test/Q_plus_P                  | -1.2610278 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 145200     |
| train/episodes                 | 14520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00412   |
| train/info_shaping_reward_mean | -0.0786    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 580800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.20229313  |
| stats_o/std                    | 0.088564634 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.677       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00277    |
| test/info_shaping_reward_mean  | -0.0639     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.7870215  |
| test/Q_plus_P                  | -1.7870215  |
| test/reward_per_eps            | -12.9       |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0691     |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 364         |
| stats_o/mean                   | 0.20229046  |
| stats_o/std                    | 0.088547505 |
| test/episodes                  | 3650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00484    |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.3518074  |
| test/Q_plus_P                  | -1.3518074  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 146000      |
| train/episodes                 | 14600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00398    |
| train/info_shaping_reward_mean | -0.0706     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 584000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 365        |
| stats_o/mean                   | 0.2023055  |
| stats_o/std                    | 0.08848852 |
| test/episodes                  | 3660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00215   |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3724906 |
| test/Q_plus_P                  | -1.3724906 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 146400     |
| train/episodes                 | 14640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 585600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 366        |
| stats_o/mean                   | 0.20230316 |
| stats_o/std                    | 0.08846532 |
| test/episodes                  | 3670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000543  |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3487247 |
| test/Q_plus_P                  | -1.3487247 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 146800     |
| train/episodes                 | 14680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00498   |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 587200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.20230755  |
| stats_o/std                    | 0.088386066 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0022     |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1719279  |
| test/Q_plus_P                  | -1.1719279  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00511    |
| train/info_shaping_reward_mean | -0.07       |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 368        |
| stats_o/mean                   | 0.20231572 |
| stats_o/std                    | 0.08833354 |
| test/episodes                  | 3690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00237   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4814439 |
| test/Q_plus_P                  | -1.4814439 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 147600     |
| train/episodes                 | 14760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0044    |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 590400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 369        |
| stats_o/mean                   | 0.20231545 |
| stats_o/std                    | 0.08829641 |
| test/episodes                  | 3700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0027    |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.0482695 |
| test/Q_plus_P                  | -1.0482695 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 148000     |
| train/episodes                 | 14800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00386   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 592000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 370        |
| stats_o/mean                   | 0.20232594 |
| stats_o/std                    | 0.08826381 |
| test/episodes                  | 3710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00369   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.203     |
| test/Q                         | -1.3592572 |
| test/Q_plus_P                  | -1.3592572 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 148400     |
| train/episodes                 | 14840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00467   |
| train/info_shaping_reward_mean | -0.0658    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 593600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 371        |
| stats_o/mean                   | 0.20231308 |
| stats_o/std                    | 0.0882463  |
| test/episodes                  | 3720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00455   |
| test/info_shaping_reward_mean  | -0.0564    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4445097 |
| test/Q_plus_P                  | -1.4445097 |
| test/reward_per_eps            | -11        |
| test/steps                     | 148800     |
| train/episodes                 | 14880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00317   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 595200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 372        |
| stats_o/mean                   | 0.20231971 |
| stats_o/std                    | 0.08817558 |
| test/episodes                  | 3730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00108   |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1371099 |
| test/Q_plus_P                  | -1.1371099 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 149200     |
| train/episodes                 | 14920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.635      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00286   |
| train/info_shaping_reward_mean | -0.0623    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 596800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 373        |
| stats_o/mean                   | 0.20231853 |
| stats_o/std                    | 0.08811764 |
| test/episodes                  | 3740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00317   |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3964729 |
| test/Q_plus_P                  | -1.3964729 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 149600     |
| train/episodes                 | 14960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00284   |
| train/info_shaping_reward_mean | -0.0639    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 598400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 374         |
| stats_o/mean                   | 0.20233473  |
| stats_o/std                    | 0.088075206 |
| test/episodes                  | 3750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00245    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1810746  |
| test/Q_plus_P                  | -1.1810746  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 150000      |
| train/episodes                 | 15000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.0695     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 600000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 375        |
| stats_o/mean                   | 0.20234355 |
| stats_o/std                    | 0.08804033 |
| test/episodes                  | 3760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000992  |
| test/info_shaping_reward_mean  | -0.0577    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -1.4488015 |
| test/Q_plus_P                  | -1.4488015 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 150400     |
| train/episodes                 | 15040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00421   |
| train/info_shaping_reward_mean | -0.0764    |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 601600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.20235196 |
| stats_o/std                    | 0.08799913 |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000658  |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.3358952 |
| test/Q_plus_P                  | -1.3358952 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0034    |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 377        |
| stats_o/mean                   | 0.20235187 |
| stats_o/std                    | 0.08798864 |
| test/episodes                  | 3780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00128   |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1016599 |
| test/Q_plus_P                  | -1.1016599 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 151200     |
| train/episodes                 | 15120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00318   |
| train/info_shaping_reward_mean | -0.0733    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 604800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 378         |
| stats_o/mean                   | 0.20236753  |
| stats_o/std                    | 0.088024944 |
| test/episodes                  | 3790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00226    |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.289637   |
| test/Q_plus_P                  | -1.289637   |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 151600      |
| train/episodes                 | 15160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00386    |
| train/info_shaping_reward_mean | -0.0813     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 606400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 379        |
| stats_o/mean                   | 0.20234184 |
| stats_o/std                    | 0.08802078 |
| test/episodes                  | 3800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00374   |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2928058 |
| test/Q_plus_P                  | -1.2928058 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 152000     |
| train/episodes                 | 15200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00387   |
| train/info_shaping_reward_mean | -0.0748    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 608000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 380        |
| stats_o/mean                   | 0.20234951 |
| stats_o/std                    | 0.08794846 |
| test/episodes                  | 3810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00196   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3435894 |
| test/Q_plus_P                  | -1.3435894 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 152400     |
| train/episodes                 | 15240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00326   |
| train/info_shaping_reward_mean | -0.0662    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 609600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 381        |
| stats_o/mean                   | 0.2023635  |
| stats_o/std                    | 0.08794722 |
| test/episodes                  | 3820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000341  |
| test/info_shaping_reward_mean  | -0.0571    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4127617 |
| test/Q_plus_P                  | -1.4127617 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 152800     |
| train/episodes                 | 15280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00398   |
| train/info_shaping_reward_mean | -0.082     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 611200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.20235641 |
| stats_o/std                    | 0.08788252 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00362   |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5021694 |
| test/Q_plus_P                  | -1.5021694 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00386   |
| train/info_shaping_reward_mean | -0.0674    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.20234756  |
| stats_o/std                    | 0.087874934 |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00364    |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1520393  |
| test/Q_plus_P                  | -1.1520393  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00477    |
| train/info_shaping_reward_mean | -0.0761     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 384        |
| stats_o/mean                   | 0.20234346 |
| stats_o/std                    | 0.0878085  |
| test/episodes                  | 3850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0044    |
| test/info_shaping_reward_mean  | -0.047     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.0376879 |
| test/Q_plus_P                  | -1.0376879 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 154000     |
| train/episodes                 | 15400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.63       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00371   |
| train/info_shaping_reward_mean | -0.0647    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 616000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 385        |
| stats_o/mean                   | 0.20234326 |
| stats_o/std                    | 0.08776566 |
| test/episodes                  | 3860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000691  |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2108356 |
| test/Q_plus_P                  | -1.2108356 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 154400     |
| train/episodes                 | 15440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00313   |
| train/info_shaping_reward_mean | -0.066     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 617600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 386        |
| stats_o/mean                   | 0.20234819 |
| stats_o/std                    | 0.0877062  |
| test/episodes                  | 3870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.675      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00355   |
| test/info_shaping_reward_mean  | -0.0662    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.6474322 |
| test/Q_plus_P                  | -1.6474322 |
| test/reward_per_eps            | -13        |
| test/steps                     | 154800     |
| train/episodes                 | 15480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.678      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.0591    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.9      |
| train/steps                    | 619200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 387        |
| stats_o/mean                   | 0.20235482 |
| stats_o/std                    | 0.0877081  |
| test/episodes                  | 3880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.675      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00278   |
| test/info_shaping_reward_mean  | -0.0624    |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -1.62674   |
| test/Q_plus_P                  | -1.62674   |
| test/reward_per_eps            | -13        |
| test/steps                     | 155200     |
| train/episodes                 | 15520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00421   |
| train/info_shaping_reward_mean | -0.0831    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 620800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 388        |
| stats_o/mean                   | 0.20235956 |
| stats_o/std                    | 0.0876457  |
| test/episodes                  | 3890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000329  |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.5204473 |
| test/Q_plus_P                  | -1.5204473 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 155600     |
| train/episodes                 | 15560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00312   |
| train/info_shaping_reward_mean | -0.066     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 622400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 389         |
| stats_o/mean                   | 0.20234749  |
| stats_o/std                    | 0.087659046 |
| test/episodes                  | 3900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00286    |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0932605  |
| test/Q_plus_P                  | -1.0932605  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 156000      |
| train/episodes                 | 15600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.573       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00442    |
| train/info_shaping_reward_mean | -0.0759     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 624000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 390        |
| stats_o/mean                   | 0.20234986 |
| stats_o/std                    | 0.08760953 |
| test/episodes                  | 3910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0026    |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -1.3519047 |
| test/Q_plus_P                  | -1.3519047 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 156400     |
| train/episodes                 | 15640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.59       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00477   |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 625600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 391        |
| stats_o/mean                   | 0.20234491 |
| stats_o/std                    | 0.08759328 |
| test/episodes                  | 3920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00257   |
| test/info_shaping_reward_mean  | -0.0439    |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -1.0177954 |
| test/Q_plus_P                  | -1.0177954 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 156800     |
| train/episodes                 | 15680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00367   |
| train/info_shaping_reward_mean | -0.0643    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 627200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 392        |
| stats_o/mean                   | 0.202354   |
| stats_o/std                    | 0.08758946 |
| test/episodes                  | 3930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00184   |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.3245584 |
| test/Q_plus_P                  | -1.3245584 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 157200     |
| train/episodes                 | 15720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.59       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00281   |
| train/info_shaping_reward_mean | -0.0777    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 628800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 393        |
| stats_o/mean                   | 0.20235777 |
| stats_o/std                    | 0.08756348 |
| test/episodes                  | 3940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00241   |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2712445 |
| test/Q_plus_P                  | -1.2712445 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 157600     |
| train/episodes                 | 15760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00404   |
| train/info_shaping_reward_mean | -0.0752    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 630400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 394        |
| stats_o/mean                   | 0.20235382 |
| stats_o/std                    | 0.08751833 |
| test/episodes                  | 3950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00123   |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.4054332 |
| test/Q_plus_P                  | -1.4054332 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 158000     |
| train/episodes                 | 15800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00375   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 632000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 395         |
| stats_o/mean                   | 0.2023583   |
| stats_o/std                    | 0.087506935 |
| test/episodes                  | 3960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.4603761  |
| test/Q_plus_P                  | -1.4603761  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 158400      |
| train/episodes                 | 15840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00455    |
| train/info_shaping_reward_mean | -0.0673     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 633600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 396        |
| stats_o/mean                   | 0.20236035 |
| stats_o/std                    | 0.0874554  |
| test/episodes                  | 3970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00782   |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4250232 |
| test/Q_plus_P                  | -1.4250232 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 158800     |
| train/episodes                 | 15880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0642    |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 635200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 397        |
| stats_o/mean                   | 0.2023538  |
| stats_o/std                    | 0.0874612  |
| test/episodes                  | 3980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00207   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.1564187 |
| test/Q_plus_P                  | -1.1564187 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 159200     |
| train/episodes                 | 15920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0036    |
| train/info_shaping_reward_mean | -0.0784    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 636800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 398         |
| stats_o/mean                   | 0.20235884  |
| stats_o/std                    | 0.087424584 |
| test/episodes                  | 3990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.0567     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.4562707  |
| test/Q_plus_P                  | -1.4562707  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 159600      |
| train/episodes                 | 15960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00336    |
| train/info_shaping_reward_mean | -0.0673     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 638400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 399        |
| stats_o/mean                   | 0.20234898 |
| stats_o/std                    | 0.08740739 |
| test/episodes                  | 4000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00252   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.1614727 |
| test/Q_plus_P                  | -1.1614727 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 160000     |
| train/episodes                 | 16000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00423   |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 640000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 400        |
| stats_o/mean                   | 0.202351   |
| stats_o/std                    | 0.08738859 |
| test/episodes                  | 4010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00198   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0576824 |
| test/Q_plus_P                  | -1.0576824 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 160400     |
| train/episodes                 | 16040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00431   |
| train/info_shaping_reward_mean | -0.0719    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 641600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 401        |
| stats_o/mean                   | 0.20235462 |
| stats_o/std                    | 0.08738924 |
| test/episodes                  | 4020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000526  |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0999706 |
| test/Q_plus_P                  | -1.0999706 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 160800     |
| train/episodes                 | 16080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00579   |
| train/info_shaping_reward_mean | -0.075     |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 643200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.20236233 |
| stats_o/std                    | 0.08734447 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00174   |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2454536 |
| test/Q_plus_P                  | -1.2454536 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00423   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 403        |
| stats_o/mean                   | 0.20234567 |
| stats_o/std                    | 0.08732022 |
| test/episodes                  | 4040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00133   |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1833973 |
| test/Q_plus_P                  | -1.1833973 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 161600     |
| train/episodes                 | 16160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00426   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 646400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 404        |
| stats_o/mean                   | 0.20233111 |
| stats_o/std                    | 0.08733173 |
| test/episodes                  | 4050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00209   |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2966309 |
| test/Q_plus_P                  | -1.2966309 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 162000     |
| train/episodes                 | 16200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0814    |
| train/info_shaping_reward_min  | -0.282     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 648000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.20232615  |
| stats_o/std                    | 0.087315604 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00184    |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.3247125  |
| test/Q_plus_P                  | -1.3247125  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00382    |
| train/info_shaping_reward_mean | -0.068      |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 406        |
| stats_o/mean                   | 0.20233142 |
| stats_o/std                    | 0.08724383 |
| test/episodes                  | 4070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00366   |
| test/info_shaping_reward_mean  | -0.0581    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.390884  |
| test/Q_plus_P                  | -1.390884  |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 162800     |
| train/episodes                 | 16280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00435   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 651200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 407        |
| stats_o/mean                   | 0.20233484 |
| stats_o/std                    | 0.08719791 |
| test/episodes                  | 4080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00125   |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2122136 |
| test/Q_plus_P                  | -1.2122136 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 163200     |
| train/episodes                 | 16320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00314   |
| train/info_shaping_reward_mean | -0.0637    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 652800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 408        |
| stats_o/mean                   | 0.20233269 |
| stats_o/std                    | 0.08723484 |
| test/episodes                  | 4090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000477  |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.3151741 |
| test/Q_plus_P                  | -1.3151741 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 163600     |
| train/episodes                 | 16360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00435   |
| train/info_shaping_reward_mean | -0.0964    |
| train/info_shaping_reward_min  | -0.274     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 654400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 409        |
| stats_o/mean                   | 0.20234226 |
| stats_o/std                    | 0.08721121 |
| test/episodes                  | 4100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00217   |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.6158273 |
| test/Q_plus_P                  | -1.6158273 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 164000     |
| train/episodes                 | 16400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00298   |
| train/info_shaping_reward_mean | -0.0655    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 656000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 410        |
| stats_o/mean                   | 0.20234229 |
| stats_o/std                    | 0.08715984 |
| test/episodes                  | 4110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00426   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.2992634 |
| test/Q_plus_P                  | -1.2992634 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 164400     |
| train/episodes                 | 16440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00275   |
| train/info_shaping_reward_mean | -0.0622    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 657600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 411        |
| stats_o/mean                   | 0.20234196 |
| stats_o/std                    | 0.08712149 |
| test/episodes                  | 4120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00334   |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.2185152 |
| test/Q_plus_P                  | -1.2185152 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 164800     |
| train/episodes                 | 16480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00357   |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 659200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 412        |
| stats_o/mean                   | 0.20234975 |
| stats_o/std                    | 0.08706393 |
| test/episodes                  | 4130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00211   |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3174248 |
| test/Q_plus_P                  | -1.3174248 |
| test/reward_per_eps            | -11        |
| test/steps                     | 165200     |
| train/episodes                 | 16520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.0637    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 660800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 413        |
| stats_o/mean                   | 0.20236021 |
| stats_o/std                    | 0.08701298 |
| test/episodes                  | 4140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0014    |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.3289043 |
| test/Q_plus_P                  | -1.3289043 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 165600     |
| train/episodes                 | 16560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00347   |
| train/info_shaping_reward_mean | -0.0658    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 662400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 414        |
| stats_o/mean                   | 0.20236805 |
| stats_o/std                    | 0.08700321 |
| test/episodes                  | 4150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00387   |
| test/info_shaping_reward_mean  | -0.061     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.7062362 |
| test/Q_plus_P                  | -1.7062362 |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 166000     |
| train/episodes                 | 16600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00431   |
| train/info_shaping_reward_mean | -0.0935    |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 664000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 415        |
| stats_o/mean                   | 0.20237982 |
| stats_o/std                    | 0.08694767 |
| test/episodes                  | 4160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5491904 |
| test/Q_plus_P                  | -1.5491904 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 166400     |
| train/episodes                 | 16640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00549   |
| train/info_shaping_reward_mean | -0.0741    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 665600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 416        |
| stats_o/mean                   | 0.20237961 |
| stats_o/std                    | 0.08692824 |
| test/episodes                  | 4170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000498  |
| test/info_shaping_reward_mean  | -0.06      |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5890088 |
| test/Q_plus_P                  | -1.5890088 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 166800     |
| train/episodes                 | 16680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00532   |
| train/info_shaping_reward_mean | -0.0715    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 667200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 417        |
| stats_o/mean                   | 0.20240171 |
| stats_o/std                    | 0.08693101 |
| test/episodes                  | 4180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00218   |
| test/info_shaping_reward_mean  | -0.0569    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5061803 |
| test/Q_plus_P                  | -1.5061803 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 167200     |
| train/episodes                 | 16720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00267   |
| train/info_shaping_reward_mean | -0.07      |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 668800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 418        |
| stats_o/mean                   | 0.20239305 |
| stats_o/std                    | 0.08692386 |
| test/episodes                  | 4190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00112   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3541418 |
| test/Q_plus_P                  | -1.3541418 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 167600     |
| train/episodes                 | 16760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00484   |
| train/info_shaping_reward_mean | -0.0752    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 670400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 419        |
| stats_o/mean                   | 0.20239654 |
| stats_o/std                    | 0.08686943 |
| test/episodes                  | 4200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00272   |
| test/info_shaping_reward_mean  | -0.0599    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.6305687 |
| test/Q_plus_P                  | -1.6305687 |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 168000     |
| train/episodes                 | 16800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00325   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 672000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 420        |
| stats_o/mean                   | 0.20238878 |
| stats_o/std                    | 0.0868267  |
| test/episodes                  | 4210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0022    |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4710684 |
| test/Q_plus_P                  | -1.4710684 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 168400     |
| train/episodes                 | 16840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.0658    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 673600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 421        |
| stats_o/mean                   | 0.20237145 |
| stats_o/std                    | 0.08678389 |
| test/episodes                  | 4220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00319   |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.5486015 |
| test/Q_plus_P                  | -1.5486015 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 168800     |
| train/episodes                 | 16880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.655      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00425   |
| train/info_shaping_reward_mean | -0.0635    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 675200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.20236887  |
| stats_o/std                    | 0.086723834 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.003      |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2777778  |
| test/Q_plus_P                  | -1.2777778  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00409    |
| train/info_shaping_reward_mean | -0.0663     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 423        |
| stats_o/mean                   | 0.202371   |
| stats_o/std                    | 0.08669002 |
| test/episodes                  | 4240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00163   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2576407 |
| test/Q_plus_P                  | -1.2576407 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 169600     |
| train/episodes                 | 16960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.585      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 678400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 424        |
| stats_o/mean                   | 0.20238449 |
| stats_o/std                    | 0.08668646 |
| test/episodes                  | 4250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00127   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2777169 |
| test/Q_plus_P                  | -1.2777169 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 170000     |
| train/episodes                 | 17000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.57       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00348   |
| train/info_shaping_reward_mean | -0.0781    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 680000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 425        |
| stats_o/mean                   | 0.20237003 |
| stats_o/std                    | 0.08667611 |
| test/episodes                  | 4260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00102   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.4631256 |
| test/Q_plus_P                  | -1.4631256 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 170400     |
| train/episodes                 | 17040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00411   |
| train/info_shaping_reward_mean | -0.0831    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 681600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 426        |
| stats_o/mean                   | 0.20238404 |
| stats_o/std                    | 0.08664466 |
| test/episodes                  | 4270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00247   |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3977904 |
| test/Q_plus_P                  | -1.3977904 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 170800     |
| train/episodes                 | 17080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00292   |
| train/info_shaping_reward_mean | -0.0719    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 683200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 427        |
| stats_o/mean                   | 0.20237604 |
| stats_o/std                    | 0.08660067 |
| test/episodes                  | 4280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00256   |
| test/info_shaping_reward_mean  | -0.0447    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.131803  |
| test/Q_plus_P                  | -1.131803  |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 171200     |
| train/episodes                 | 17120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.662      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00285   |
| train/info_shaping_reward_mean | -0.0606    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 684800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 428        |
| stats_o/mean                   | 0.20237331 |
| stats_o/std                    | 0.0865475  |
| test/episodes                  | 4290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00207   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.3803093 |
| test/Q_plus_P                  | -1.3803093 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 171600     |
| train/episodes                 | 17160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00406   |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 686400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 429        |
| stats_o/mean                   | 0.20237203 |
| stats_o/std                    | 0.08650936 |
| test/episodes                  | 4300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00028   |
| test/info_shaping_reward_mean  | -0.0515    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4609991 |
| test/Q_plus_P                  | -1.4609991 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 172000     |
| train/episodes                 | 17200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00424   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 688000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 430        |
| stats_o/mean                   | 0.20238242 |
| stats_o/std                    | 0.08644514 |
| test/episodes                  | 4310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00299   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3534341 |
| test/Q_plus_P                  | -1.3534341 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 172400     |
| train/episodes                 | 17240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.647      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0045    |
| train/info_shaping_reward_mean | -0.0629    |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 689600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 431        |
| stats_o/mean                   | 0.20238958 |
| stats_o/std                    | 0.0864219  |
| test/episodes                  | 4320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00118   |
| test/info_shaping_reward_mean  | -0.0417    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.070897  |
| test/Q_plus_P                  | -1.070897  |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 172800     |
| train/episodes                 | 17280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.59       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00407   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 691200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 432        |
| stats_o/mean                   | 0.20240022 |
| stats_o/std                    | 0.08639118 |
| test/episodes                  | 4330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000599  |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4733267 |
| test/Q_plus_P                  | -1.4733267 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 173200     |
| train/episodes                 | 17320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0042    |
| train/info_shaping_reward_mean | -0.0792    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 692800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 433        |
| stats_o/mean                   | 0.20239101 |
| stats_o/std                    | 0.08634046 |
| test/episodes                  | 4340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00154   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1263598 |
| test/Q_plus_P                  | -1.1263598 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 173600     |
| train/episodes                 | 17360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00363   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 694400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 434        |
| stats_o/mean                   | 0.20239826 |
| stats_o/std                    | 0.08633406 |
| test/episodes                  | 4350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0021    |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2550651 |
| test/Q_plus_P                  | -1.2550651 |
| test/reward_per_eps            | -10        |
| test/steps                     | 174000     |
| train/episodes                 | 17400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00358   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 696000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 435         |
| stats_o/mean                   | 0.20239283  |
| stats_o/std                    | 0.086369164 |
| test/episodes                  | 4360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00259    |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.326662   |
| test/Q_plus_P                  | -1.326662   |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 174400      |
| train/episodes                 | 17440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00397    |
| train/info_shaping_reward_mean | -0.0797     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 697600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 436        |
| stats_o/mean                   | 0.20240189 |
| stats_o/std                    | 0.08631542 |
| test/episodes                  | 4370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00341   |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1520743 |
| test/Q_plus_P                  | -1.1520743 |
| test/reward_per_eps            | -10        |
| test/steps                     | 174800     |
| train/episodes                 | 17480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0034    |
| train/info_shaping_reward_mean | -0.0617    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 699200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 437         |
| stats_o/mean                   | 0.20241252  |
| stats_o/std                    | 0.086296104 |
| test/episodes                  | 4380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000836   |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.3382175  |
| test/Q_plus_P                  | -1.3382175  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 175200      |
| train/episodes                 | 17520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0026     |
| train/info_shaping_reward_mean | -0.0732     |
| train/info_shaping_reward_min  | -0.225      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 700800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 438        |
| stats_o/mean                   | 0.20242546 |
| stats_o/std                    | 0.08627064 |
| test/episodes                  | 4390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00107   |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.4337693 |
| test/Q_plus_P                  | -1.4337693 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 175600     |
| train/episodes                 | 17560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00318   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 702400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 439        |
| stats_o/mean                   | 0.20241982 |
| stats_o/std                    | 0.0862143  |
| test/episodes                  | 4400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000649  |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.4093968 |
| test/Q_plus_P                  | -1.4093968 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 176000     |
| train/episodes                 | 17600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00372   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 704000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.20243141  |
| stats_o/std                    | 0.086178154 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00372    |
| test/info_shaping_reward_mean  | -0.0565     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5646963  |
| test/Q_plus_P                  | -1.5646963  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00343    |
| train/info_shaping_reward_mean | -0.0745     |
| train/info_shaping_reward_min  | -0.202      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.20243983  |
| stats_o/std                    | 0.086147346 |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00152    |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.2624011  |
| test/Q_plus_P                  | -1.2624011  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0047     |
| train/info_shaping_reward_mean | -0.0689     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 442         |
| stats_o/mean                   | 0.20244052  |
| stats_o/std                    | 0.086125374 |
| test/episodes                  | 4430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00305    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -1.2046115  |
| test/Q_plus_P                  | -1.2046115  |
| test/reward_per_eps            | -10         |
| test/steps                     | 177200      |
| train/episodes                 | 17720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00345    |
| train/info_shaping_reward_mean | -0.0689     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 708800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.20242295  |
| stats_o/std                    | 0.086118944 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00143    |
| test/info_shaping_reward_mean  | -0.0552     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -1.4041203  |
| test/Q_plus_P                  | -1.4041203  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00407    |
| train/info_shaping_reward_mean | -0.0716     |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 444        |
| stats_o/mean                   | 0.20241676 |
| stats_o/std                    | 0.08608112 |
| test/episodes                  | 4450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00066   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.232224  |
| test/Q_plus_P                  | -1.232224  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 178000     |
| train/episodes                 | 17800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00404   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 712000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 445        |
| stats_o/mean                   | 0.20240362 |
| stats_o/std                    | 0.08608279 |
| test/episodes                  | 4460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1425192 |
| test/Q_plus_P                  | -1.1425192 |
| test/reward_per_eps            | -10        |
| test/steps                     | 178400     |
| train/episodes                 | 17840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00324   |
| train/info_shaping_reward_mean | -0.0673    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 713600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 446        |
| stats_o/mean                   | 0.20240374 |
| stats_o/std                    | 0.08604272 |
| test/episodes                  | 4470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00262   |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.283876  |
| test/Q_plus_P                  | -1.283876  |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 178800     |
| train/episodes                 | 17880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.63       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00388   |
| train/info_shaping_reward_mean | -0.0636    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 715200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 447        |
| stats_o/mean                   | 0.2024047  |
| stats_o/std                    | 0.08605816 |
| test/episodes                  | 4480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00176   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2044392 |
| test/Q_plus_P                  | -1.2044392 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 179200     |
| train/episodes                 | 17920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.0754    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 716800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 448        |
| stats_o/mean                   | 0.20240168 |
| stats_o/std                    | 0.0860408  |
| test/episodes                  | 4490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -1.1698794 |
| test/Q_plus_P                  | -1.1698794 |
| test/reward_per_eps            | -10        |
| test/steps                     | 179600     |
| train/episodes                 | 17960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00256   |
| train/info_shaping_reward_mean | -0.0667    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 718400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 449        |
| stats_o/mean                   | 0.20241207 |
| stats_o/std                    | 0.08604234 |
| test/episodes                  | 4500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000341  |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1702847 |
| test/Q_plus_P                  | -1.1702847 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 180000     |
| train/episodes                 | 18000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.59       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00347   |
| train/info_shaping_reward_mean | -0.0812    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 720000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.20242622 |
| stats_o/std                    | 0.08603655 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.655      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00226   |
| test/info_shaping_reward_mean  | -0.0636    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.7435586 |
| test/Q_plus_P                  | -1.7435586 |
| test/reward_per_eps            | -13.8      |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00365   |
| train/info_shaping_reward_mean | -0.0771    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 451        |
| stats_o/mean                   | 0.20242248 |
| stats_o/std                    | 0.08604533 |
| test/episodes                  | 4520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00176   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2089881 |
| test/Q_plus_P                  | -1.2089881 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 180800     |
| train/episodes                 | 18080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.59       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00354   |
| train/info_shaping_reward_mean | -0.0757    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 723200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 452        |
| stats_o/mean                   | 0.20243034 |
| stats_o/std                    | 0.08602378 |
| test/episodes                  | 4530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00131   |
| test/info_shaping_reward_mean  | -0.0578    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.6059237 |
| test/Q_plus_P                  | -1.6059237 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 181200     |
| train/episodes                 | 18120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00341   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 724800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 453        |
| stats_o/mean                   | 0.20242637 |
| stats_o/std                    | 0.08598964 |
| test/episodes                  | 4540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3320285 |
| test/Q_plus_P                  | -1.3320285 |
| test/reward_per_eps            | -10        |
| test/steps                     | 181600     |
| train/episodes                 | 18160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00324   |
| train/info_shaping_reward_mean | -0.0645    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 726400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 454        |
| stats_o/mean                   | 0.2024185  |
| stats_o/std                    | 0.08597186 |
| test/episodes                  | 4550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00218   |
| test/info_shaping_reward_mean  | -0.047     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2033995 |
| test/Q_plus_P                  | -1.2033995 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 182000     |
| train/episodes                 | 18200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0034    |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 728000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 455        |
| stats_o/mean                   | 0.2024305  |
| stats_o/std                    | 0.08603717 |
| test/episodes                  | 4560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00108   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0841978 |
| test/Q_plus_P                  | -1.0841978 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 182400     |
| train/episodes                 | 18240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.565      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.0869    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 729600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 456         |
| stats_o/mean                   | 0.20243172  |
| stats_o/std                    | 0.086052544 |
| test/episodes                  | 4570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000483   |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.2663935  |
| test/Q_plus_P                  | -1.2663935  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 182800      |
| train/episodes                 | 18280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.541       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00495    |
| train/info_shaping_reward_mean | -0.0773     |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 731200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.20243189  |
| stats_o/std                    | 0.085999995 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00211    |
| test/info_shaping_reward_mean  | -0.0582     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.7123559  |
| test/Q_plus_P                  | -1.7123559  |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00456    |
| train/info_shaping_reward_mean | -0.066      |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 458        |
| stats_o/mean                   | 0.20243622 |
| stats_o/std                    | 0.08598849 |
| test/episodes                  | 4590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00109   |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3818665 |
| test/Q_plus_P                  | -1.3818665 |
| test/reward_per_eps            | -11        |
| test/steps                     | 183600     |
| train/episodes                 | 18360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00427   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 734400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 459         |
| stats_o/mean                   | 0.20244612  |
| stats_o/std                    | 0.085955545 |
| test/episodes                  | 4600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00231    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.0214114  |
| test/Q_plus_P                  | -1.0214114  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 184000      |
| train/episodes                 | 18400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0658     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 736000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.20245053  |
| stats_o/std                    | 0.085949935 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00406    |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.3102716  |
| test/Q_plus_P                  | -1.3102716  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.602       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00377    |
| train/info_shaping_reward_mean | -0.0676     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 461        |
| stats_o/mean                   | 0.20245676 |
| stats_o/std                    | 0.08590849 |
| test/episodes                  | 4620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00198   |
| test/info_shaping_reward_mean  | -0.0454    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1134658 |
| test/Q_plus_P                  | -1.1134658 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 184800     |
| train/episodes                 | 18480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.67       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00398   |
| train/info_shaping_reward_mean | -0.0612    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 739200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 462        |
| stats_o/mean                   | 0.20247151 |
| stats_o/std                    | 0.08591182 |
| test/episodes                  | 4630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00198   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.1264857 |
| test/Q_plus_P                  | -1.1264857 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 185200     |
| train/episodes                 | 18520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00323   |
| train/info_shaping_reward_mean | -0.0778    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 740800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 463        |
| stats_o/mean                   | 0.20247148 |
| stats_o/std                    | 0.08585432 |
| test/episodes                  | 4640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0013    |
| test/info_shaping_reward_mean  | -0.058     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.8636862 |
| test/Q_plus_P                  | -1.8636862 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 185600     |
| train/episodes                 | 18560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00382   |
| train/info_shaping_reward_mean | -0.0626    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 742400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 464        |
| stats_o/mean                   | 0.20248142 |
| stats_o/std                    | 0.08582852 |
| test/episodes                  | 4650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00247   |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2820463 |
| test/Q_plus_P                  | -1.2820463 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 186000     |
| train/episodes                 | 18600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.663      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00362   |
| train/info_shaping_reward_mean | -0.0619    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 744000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 465        |
| stats_o/mean                   | 0.20247437 |
| stats_o/std                    | 0.08582476 |
| test/episodes                  | 4660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0018    |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.0969867 |
| test/Q_plus_P                  | -1.0969867 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 186400     |
| train/episodes                 | 18640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00474   |
| train/info_shaping_reward_mean | -0.0681    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 745600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 466        |
| stats_o/mean                   | 0.20247492 |
| stats_o/std                    | 0.08582824 |
| test/episodes                  | 4670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00463   |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3388202 |
| test/Q_plus_P                  | -1.3388202 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 186800     |
| train/episodes                 | 18680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00317   |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 747200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 467        |
| stats_o/mean                   | 0.20248917 |
| stats_o/std                    | 0.08584575 |
| test/episodes                  | 4680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00321   |
| test/info_shaping_reward_mean  | -0.0601    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5544977 |
| test/Q_plus_P                  | -1.5544977 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 187200     |
| train/episodes                 | 18720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.547      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00262   |
| train/info_shaping_reward_mean | -0.0866    |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 748800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 468        |
| stats_o/mean                   | 0.2024957  |
| stats_o/std                    | 0.08582911 |
| test/episodes                  | 4690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00267   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2179703 |
| test/Q_plus_P                  | -1.2179703 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 187600     |
| train/episodes                 | 18760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00536   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 750400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 469         |
| stats_o/mean                   | 0.20248774  |
| stats_o/std                    | 0.085797966 |
| test/episodes                  | 4700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0019     |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2480745  |
| test/Q_plus_P                  | -1.2480745  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 188000      |
| train/episodes                 | 18800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.556       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00406    |
| train/info_shaping_reward_mean | -0.0733     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 752000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.2024869  |
| stats_o/std                    | 0.08580338 |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00297   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.174542  |
| test/Q_plus_P                  | -1.174542  |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00271   |
| train/info_shaping_reward_mean | -0.0744    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.20248957 |
| stats_o/std                    | 0.0857541  |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00273   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2293844 |
| test/Q_plus_P                  | -1.2293844 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00222   |
| train/info_shaping_reward_mean | -0.0637    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.20248924 |
| stats_o/std                    | 0.08571731 |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00212   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2328237 |
| test/Q_plus_P                  | -1.2328237 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00359   |
| train/info_shaping_reward_mean | -0.0625    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 473         |
| stats_o/mean                   | 0.20249029  |
| stats_o/std                    | 0.085687876 |
| test/episodes                  | 4740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00057    |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.3159431  |
| test/Q_plus_P                  | -1.3159431  |
| test/reward_per_eps            | -11         |
| test/steps                     | 189600      |
| train/episodes                 | 18960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00412    |
| train/info_shaping_reward_mean | -0.0636     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 758400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 474        |
| stats_o/mean                   | 0.20249414 |
| stats_o/std                    | 0.08572366 |
| test/episodes                  | 4750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00242   |
| test/info_shaping_reward_mean  | -0.0549    |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -1.6842704 |
| test/Q_plus_P                  | -1.6842704 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 190000     |
| train/episodes                 | 19000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.096     |
| train/info_shaping_reward_min  | -0.291     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 760000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 475        |
| stats_o/mean                   | 0.20250021 |
| stats_o/std                    | 0.08571954 |
| test/episodes                  | 4760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00198   |
| test/info_shaping_reward_mean  | -0.0434    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0353912 |
| test/Q_plus_P                  | -1.0353912 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 190400     |
| train/episodes                 | 19040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.54       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00358   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 761600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.20250778  |
| stats_o/std                    | 0.085725315 |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.667       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00564    |
| test/info_shaping_reward_mean  | -0.0629     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.6697507  |
| test/Q_plus_P                  | -1.6697507  |
| test/reward_per_eps            | -13.3       |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.552       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.0805     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.20250456  |
| stats_o/std                    | 0.085738786 |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.662       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0018     |
| test/info_shaping_reward_mean  | -0.064      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.74035    |
| test/Q_plus_P                  | -1.74035    |
| test/reward_per_eps            | -13.5       |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.549       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00403    |
| train/info_shaping_reward_mean | -0.0796     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18         |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 478        |
| stats_o/mean                   | 0.20252314 |
| stats_o/std                    | 0.08570569 |
| test/episodes                  | 4790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00371   |
| test/info_shaping_reward_mean  | -0.0454    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.140463  |
| test/Q_plus_P                  | -1.140463  |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 191600     |
| train/episodes                 | 19160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0715    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 766400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 479        |
| stats_o/mean                   | 0.20253853 |
| stats_o/std                    | 0.08567091 |
| test/episodes                  | 4800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0035    |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.958125  |
| test/Q_plus_P                  | -0.958125  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 192000     |
| train/episodes                 | 19200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.657      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00382   |
| train/info_shaping_reward_mean | -0.0639    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 768000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 480        |
| stats_o/mean                   | 0.20255093 |
| stats_o/std                    | 0.08565621 |
| test/episodes                  | 4810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000509  |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -1.210674  |
| test/Q_plus_P                  | -1.210674  |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 192400     |
| train/episodes                 | 19240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00431   |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 769600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 481        |
| stats_o/mean                   | 0.20255144 |
| stats_o/std                    | 0.08568445 |
| test/episodes                  | 4820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000515  |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2244294 |
| test/Q_plus_P                  | -1.2244294 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 192800     |
| train/episodes                 | 19280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.572      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00525   |
| train/info_shaping_reward_mean | -0.0804    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 771200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.20254512  |
| stats_o/std                    | 0.085671775 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00252    |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.4014894  |
| test/Q_plus_P                  | -1.4014894  |
| test/reward_per_eps            | -11         |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.576       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00316    |
| train/info_shaping_reward_mean | -0.0765     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 483         |
| stats_o/mean                   | 0.20255572  |
| stats_o/std                    | 0.085653156 |
| test/episodes                  | 4840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.2543415  |
| test/Q_plus_P                  | -1.2543415  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 193600      |
| train/episodes                 | 19360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0038     |
| train/info_shaping_reward_mean | -0.0703     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 774400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 484        |
| stats_o/mean                   | 0.20254703 |
| stats_o/std                    | 0.08562422 |
| test/episodes                  | 4850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1358099 |
| test/Q_plus_P                  | -1.1358099 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 194000     |
| train/episodes                 | 19400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00396   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 776000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 485        |
| stats_o/mean                   | 0.20254835 |
| stats_o/std                    | 0.08557974 |
| test/episodes                  | 4860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00141   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2115889 |
| test/Q_plus_P                  | -1.2115889 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 194400     |
| train/episodes                 | 19440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00299   |
| train/info_shaping_reward_mean | -0.0635    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 777600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 486        |
| stats_o/mean                   | 0.2025423  |
| stats_o/std                    | 0.08554112 |
| test/episodes                  | 4870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00134   |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0872805 |
| test/Q_plus_P                  | -1.0872805 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 194800     |
| train/episodes                 | 19480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00323   |
| train/info_shaping_reward_mean | -0.0656    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 779200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 487        |
| stats_o/mean                   | 0.20255071 |
| stats_o/std                    | 0.08550712 |
| test/episodes                  | 4880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00102   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.1419317 |
| test/Q_plus_P                  | -1.1419317 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 195200     |
| train/episodes                 | 19520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00411   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 780800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 488        |
| stats_o/mean                   | 0.20254406 |
| stats_o/std                    | 0.08549694 |
| test/episodes                  | 4890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00199   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1573477 |
| test/Q_plus_P                  | -1.1573477 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 195600     |
| train/episodes                 | 19560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00245   |
| train/info_shaping_reward_mean | -0.0647    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 782400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 489        |
| stats_o/mean                   | 0.20252717 |
| stats_o/std                    | 0.08546687 |
| test/episodes                  | 4900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00196   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -1.3329283 |
| test/Q_plus_P                  | -1.3329283 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 196000     |
| train/episodes                 | 19600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00265   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 784000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 490        |
| stats_o/mean                   | 0.2025272  |
| stats_o/std                    | 0.08545184 |
| test/episodes                  | 4910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000433  |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3952394 |
| test/Q_plus_P                  | -1.3952394 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 196400     |
| train/episodes                 | 19640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.073     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 785600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.20252909  |
| stats_o/std                    | 0.085418746 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00272    |
| test/info_shaping_reward_mean  | -0.0558     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.3185067  |
| test/Q_plus_P                  | -1.3185067  |
| test/reward_per_eps            | -11         |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00463    |
| train/info_shaping_reward_mean | -0.0631     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 492        |
| stats_o/mean                   | 0.20252639 |
| stats_o/std                    | 0.08538554 |
| test/episodes                  | 4930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00165   |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.3284163 |
| test/Q_plus_P                  | -1.3284163 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 197200     |
| train/episodes                 | 19720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.595      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00504   |
| train/info_shaping_reward_mean | -0.0686    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 788800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.20252605 |
| stats_o/std                    | 0.08541561 |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000354  |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.1718209 |
| test/Q_plus_P                  | -1.1718209 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.0853    |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 494        |
| stats_o/mean                   | 0.20252685 |
| stats_o/std                    | 0.08537264 |
| test/episodes                  | 4950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00165   |
| test/info_shaping_reward_mean  | -0.0515    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2773261 |
| test/Q_plus_P                  | -1.2773261 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 198000     |
| train/episodes                 | 19800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00423   |
| train/info_shaping_reward_mean | -0.0638    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 792000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 495        |
| stats_o/mean                   | 0.20252691 |
| stats_o/std                    | 0.08535358 |
| test/episodes                  | 4960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0026    |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.339396  |
| test/Q_plus_P                  | -1.339396  |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 198400     |
| train/episodes                 | 19840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00375   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 793600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 496        |
| stats_o/mean                   | 0.20252553 |
| stats_o/std                    | 0.08534278 |
| test/episodes                  | 4970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00161   |
| test/info_shaping_reward_mean  | -0.0472    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1640774 |
| test/Q_plus_P                  | -1.1640774 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 198800     |
| train/episodes                 | 19880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00412   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 795200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 497        |
| stats_o/mean                   | 0.20253149 |
| stats_o/std                    | 0.0853375  |
| test/episodes                  | 4980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000141  |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3771565 |
| test/Q_plus_P                  | -1.3771565 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 199200     |
| train/episodes                 | 19920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.587      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00373   |
| train/info_shaping_reward_mean | -0.0823    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 796800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 498        |
| stats_o/mean                   | 0.20253098 |
| stats_o/std                    | 0.0853483  |
| test/episodes                  | 4990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000891  |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.4407927 |
| test/Q_plus_P                  | -1.4407927 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 199600     |
| train/episodes                 | 19960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00336   |
| train/info_shaping_reward_mean | -0.074     |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 798400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 499        |
| stats_o/mean                   | 0.20251726 |
| stats_o/std                    | 0.08532251 |
| test/episodes                  | 5000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00257   |
| test/info_shaping_reward_mean  | -0.0599    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.4977884 |
| test/Q_plus_P                  | -1.4977884 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 200000     |
| train/episodes                 | 20000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.0639    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 800000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 500        |
| stats_o/mean                   | 0.2025194  |
| stats_o/std                    | 0.08529911 |
| test/episodes                  | 5010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00285   |
| test/info_shaping_reward_mean  | -0.0606    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.5456097 |
| test/Q_plus_P                  | -1.5456097 |
| test/reward_per_eps            | -12        |
| test/steps                     | 200400     |
| train/episodes                 | 20040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00397   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 801600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 501        |
| stats_o/mean                   | 0.20252745 |
| stats_o/std                    | 0.08526772 |
| test/episodes                  | 5020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000268  |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.3227849 |
| test/Q_plus_P                  | -1.3227849 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 200800     |
| train/episodes                 | 20080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 803200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 502        |
| stats_o/mean                   | 0.20252523 |
| stats_o/std                    | 0.08526789 |
| test/episodes                  | 5030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000825  |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2060673 |
| test/Q_plus_P                  | -1.2060673 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 201200     |
| train/episodes                 | 20120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00391   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 804800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 503        |
| stats_o/mean                   | 0.20253025 |
| stats_o/std                    | 0.08522678 |
| test/episodes                  | 5040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000463  |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2131371 |
| test/Q_plus_P                  | -1.2131371 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 201600     |
| train/episodes                 | 20160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00325   |
| train/info_shaping_reward_mean | -0.0674    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 806400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 504         |
| stats_o/mean                   | 0.20252924  |
| stats_o/std                    | 0.085193805 |
| test/episodes                  | 5050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000871   |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0422981  |
| test/Q_plus_P                  | -1.0422981  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 202000      |
| train/episodes                 | 20200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00314    |
| train/info_shaping_reward_mean | -0.0652     |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 808000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 505         |
| stats_o/mean                   | 0.20252843  |
| stats_o/std                    | 0.085180715 |
| test/episodes                  | 5060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00357    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -1.1182805  |
| test/Q_plus_P                  | -1.1182805  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 202400      |
| train/episodes                 | 20240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00465    |
| train/info_shaping_reward_mean | -0.069      |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 809600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 506        |
| stats_o/mean                   | 0.20253536 |
| stats_o/std                    | 0.08514503 |
| test/episodes                  | 5070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00379   |
| test/info_shaping_reward_mean  | -0.0531    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2330636 |
| test/Q_plus_P                  | -1.2330636 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 202800     |
| train/episodes                 | 20280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00374   |
| train/info_shaping_reward_mean | -0.0651    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 811200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 507        |
| stats_o/mean                   | 0.20253193 |
| stats_o/std                    | 0.08515235 |
| test/episodes                  | 5080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00128   |
| test/info_shaping_reward_mean  | -0.0575    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.6345956 |
| test/Q_plus_P                  | -1.6345956 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 203200     |
| train/episodes                 | 20320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00515   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 812800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 508        |
| stats_o/mean                   | 0.20253025 |
| stats_o/std                    | 0.08513097 |
| test/episodes                  | 5090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0012    |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2305603 |
| test/Q_plus_P                  | -1.2305603 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 203600     |
| train/episodes                 | 20360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.587      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00384   |
| train/info_shaping_reward_mean | -0.0734    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 814400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 509        |
| stats_o/mean                   | 0.20253892 |
| stats_o/std                    | 0.08512164 |
| test/episodes                  | 5100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0013    |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.2781062 |
| test/Q_plus_P                  | -1.2781062 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 204000     |
| train/episodes                 | 20400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00502   |
| train/info_shaping_reward_mean | -0.0733    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 816000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 510        |
| stats_o/mean                   | 0.20254059 |
| stats_o/std                    | 0.08510149 |
| test/episodes                  | 5110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.2660321 |
| test/Q_plus_P                  | -1.2660321 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 204400     |
| train/episodes                 | 20440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00438   |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 817600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 511        |
| stats_o/mean                   | 0.20254524 |
| stats_o/std                    | 0.08509442 |
| test/episodes                  | 5120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000894  |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3837461 |
| test/Q_plus_P                  | -1.3837461 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 204800     |
| train/episodes                 | 20480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00502   |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 819200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 512         |
| stats_o/mean                   | 0.20254463  |
| stats_o/std                    | 0.085054986 |
| test/episodes                  | 5130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.3178052  |
| test/Q_plus_P                  | -1.3178052  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 205200      |
| train/episodes                 | 20520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00318    |
| train/info_shaping_reward_mean | -0.0684     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 820800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 513         |
| stats_o/mean                   | 0.20254794  |
| stats_o/std                    | 0.085009806 |
| test/episodes                  | 5140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00197    |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.223354   |
| test/Q_plus_P                  | -1.223354   |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 205600      |
| train/episodes                 | 20560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00407    |
| train/info_shaping_reward_mean | -0.0641     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 822400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 514        |
| stats_o/mean                   | 0.20254673 |
| stats_o/std                    | 0.08502452 |
| test/episodes                  | 5150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00468   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2209482 |
| test/Q_plus_P                  | -1.2209482 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 206000     |
| train/episodes                 | 20600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00453   |
| train/info_shaping_reward_mean | -0.0771    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 824000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 515        |
| stats_o/mean                   | 0.20254086 |
| stats_o/std                    | 0.08500419 |
| test/episodes                  | 5160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.67       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00397   |
| test/info_shaping_reward_mean  | -0.0608    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.6506768 |
| test/Q_plus_P                  | -1.6506768 |
| test/reward_per_eps            | -13.2      |
| test/steps                     | 206400     |
| train/episodes                 | 20640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00398   |
| train/info_shaping_reward_mean | -0.0788    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 825600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 516        |
| stats_o/mean                   | 0.20253888 |
| stats_o/std                    | 0.08497579 |
| test/episodes                  | 5170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00434   |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4672719 |
| test/Q_plus_P                  | -1.4672719 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 206800     |
| train/episodes                 | 20680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00421   |
| train/info_shaping_reward_mean | -0.0662    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 827200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 517        |
| stats_o/mean                   | 0.20254694 |
| stats_o/std                    | 0.08497563 |
| test/episodes                  | 5180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00405   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2297359 |
| test/Q_plus_P                  | -1.2297359 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 207200     |
| train/episodes                 | 20720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00271   |
| train/info_shaping_reward_mean | -0.0815    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 828800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 518        |
| stats_o/mean                   | 0.20254932 |
| stats_o/std                    | 0.08497821 |
| test/episodes                  | 5190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3445233 |
| test/Q_plus_P                  | -1.3445233 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 207600     |
| train/episodes                 | 20760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00416   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 830400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 519         |
| stats_o/mean                   | 0.20254932  |
| stats_o/std                    | 0.084962346 |
| test/episodes                  | 5200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00219    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1366872  |
| test/Q_plus_P                  | -1.1366872  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 208000      |
| train/episodes                 | 20800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00411    |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 832000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 520        |
| stats_o/mean                   | 0.20255695 |
| stats_o/std                    | 0.08492977 |
| test/episodes                  | 5210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00149   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2787728 |
| test/Q_plus_P                  | -1.2787728 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 208400     |
| train/episodes                 | 20840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.682      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00343   |
| train/info_shaping_reward_mean | -0.0602    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 833600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 521        |
| stats_o/mean                   | 0.20255981 |
| stats_o/std                    | 0.08492789 |
| test/episodes                  | 5220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000533  |
| test/info_shaping_reward_mean  | -0.0588    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -1.710017  |
| test/Q_plus_P                  | -1.710017  |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 208800     |
| train/episodes                 | 20880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0043    |
| train/info_shaping_reward_mean | -0.0745    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 835200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 522        |
| stats_o/mean                   | 0.20256415 |
| stats_o/std                    | 0.08493053 |
| test/episodes                  | 5230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00132   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2017715 |
| test/Q_plus_P                  | -1.2017715 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 209200     |
| train/episodes                 | 20920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0036    |
| train/info_shaping_reward_mean | -0.0755    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 836800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 523        |
| stats_o/mean                   | 0.20258047 |
| stats_o/std                    | 0.08491962 |
| test/episodes                  | 5240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00174   |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.2874752 |
| test/Q_plus_P                  | -1.2874752 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 209600     |
| train/episodes                 | 20960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00421   |
| train/info_shaping_reward_mean | -0.0752    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 838400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 524        |
| stats_o/mean                   | 0.2025818  |
| stats_o/std                    | 0.08488066 |
| test/episodes                  | 5250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.531268  |
| test/Q_plus_P                  | -1.531268  |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 210000     |
| train/episodes                 | 21000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00473   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 840000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 525        |
| stats_o/mean                   | 0.20258208 |
| stats_o/std                    | 0.08488345 |
| test/episodes                  | 5260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00267   |
| test/info_shaping_reward_mean  | -0.0458    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.0774609 |
| test/Q_plus_P                  | -1.0774609 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 210400     |
| train/episodes                 | 21040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00314   |
| train/info_shaping_reward_mean | -0.0724    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 841600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 526        |
| stats_o/mean                   | 0.2025774  |
| stats_o/std                    | 0.0848598  |
| test/episodes                  | 5270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00252   |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5461931 |
| test/Q_plus_P                  | -1.5461931 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 210800     |
| train/episodes                 | 21080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0656    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 843200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 527        |
| stats_o/mean                   | 0.20256998 |
| stats_o/std                    | 0.08481994 |
| test/episodes                  | 5280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000922  |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4543235 |
| test/Q_plus_P                  | -1.4543235 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 211200     |
| train/episodes                 | 21120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.647      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00351   |
| train/info_shaping_reward_mean | -0.0628    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 844800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 528        |
| stats_o/mean                   | 0.20257448 |
| stats_o/std                    | 0.08478915 |
| test/episodes                  | 5290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00279   |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3395114 |
| test/Q_plus_P                  | -1.3395114 |
| test/reward_per_eps            | -10        |
| test/steps                     | 211600     |
| train/episodes                 | 21160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00442   |
| train/info_shaping_reward_mean | -0.0623    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 846400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.20258364  |
| stats_o/std                    | 0.084830254 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.0788715  |
| test/Q_plus_P                  | -1.0788715  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.526       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0037     |
| train/info_shaping_reward_mean | -0.0846     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 530        |
| stats_o/mean                   | 0.20258102 |
| stats_o/std                    | 0.0848314  |
| test/episodes                  | 5310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3795526 |
| test/Q_plus_P                  | -1.3795526 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 212400     |
| train/episodes                 | 21240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00569   |
| train/info_shaping_reward_mean | -0.0766    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 849600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 531        |
| stats_o/mean                   | 0.20258345 |
| stats_o/std                    | 0.08483359 |
| test/episodes                  | 5320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00149   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.4719199 |
| test/Q_plus_P                  | -1.4719199 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 212800     |
| train/episodes                 | 21280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00395   |
| train/info_shaping_reward_mean | -0.0713    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 851200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.20259681  |
| stats_o/std                    | 0.084815696 |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00281    |
| test/info_shaping_reward_mean  | -0.0497     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.3612442  |
| test/Q_plus_P                  | -1.3612442  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00408    |
| train/info_shaping_reward_mean | -0.0722     |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.20259245  |
| stats_o/std                    | 0.084776536 |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0052     |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.3943546  |
| test/Q_plus_P                  | -1.3943546  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00435    |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 534        |
| stats_o/mean                   | 0.20259589 |
| stats_o/std                    | 0.08474833 |
| test/episodes                  | 5350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00164   |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1288638 |
| test/Q_plus_P                  | -1.1288638 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 214000     |
| train/episodes                 | 21400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0048    |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 856000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 535         |
| stats_o/mean                   | 0.20260131  |
| stats_o/std                    | 0.084726155 |
| test/episodes                  | 5360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1638985  |
| test/Q_plus_P                  | -1.1638985  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 214400      |
| train/episodes                 | 21440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00465    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 857600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 536         |
| stats_o/mean                   | 0.20260394  |
| stats_o/std                    | 0.084725335 |
| test/episodes                  | 5370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00302    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.9783032  |
| test/Q_plus_P                  | -0.9783032  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 214800      |
| train/episodes                 | 21480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00339    |
| train/info_shaping_reward_mean | -0.0748     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 859200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 537         |
| stats_o/mean                   | 0.2026052   |
| stats_o/std                    | 0.084687606 |
| test/episodes                  | 5380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00193    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.2952346  |
| test/Q_plus_P                  | -1.2952346  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 215200      |
| train/episodes                 | 21520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00423    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 860800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 538        |
| stats_o/mean                   | 0.20261326 |
| stats_o/std                    | 0.08469255 |
| test/episodes                  | 5390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00359   |
| test/info_shaping_reward_mean  | -0.0569    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.5476216 |
| test/Q_plus_P                  | -1.5476216 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 215600     |
| train/episodes                 | 21560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00367   |
| train/info_shaping_reward_mean | -0.0759    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 862400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 539        |
| stats_o/mean                   | 0.20261613 |
| stats_o/std                    | 0.08471296 |
| test/episodes                  | 5400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00162   |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2594354 |
| test/Q_plus_P                  | -1.2594354 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 216000     |
| train/episodes                 | 21600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.57       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00361   |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 864000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 540        |
| stats_o/mean                   | 0.2026177  |
| stats_o/std                    | 0.08468058 |
| test/episodes                  | 5410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00223   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -1.2614791 |
| test/Q_plus_P                  | -1.2614791 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 216400     |
| train/episodes                 | 21640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00398   |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 865600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 541        |
| stats_o/mean                   | 0.20261927 |
| stats_o/std                    | 0.08465042 |
| test/episodes                  | 5420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00181   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.2950711 |
| test/Q_plus_P                  | -1.2950711 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 216800     |
| train/episodes                 | 21680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00337   |
| train/info_shaping_reward_mean | -0.066     |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 867200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 542        |
| stats_o/mean                   | 0.202633   |
| stats_o/std                    | 0.08462215 |
| test/episodes                  | 5430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00201   |
| test/info_shaping_reward_mean  | -0.057     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.4922202 |
| test/Q_plus_P                  | -1.4922202 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 217200     |
| train/episodes                 | 21720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.658      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00378   |
| train/info_shaping_reward_mean | -0.0631    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 868800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 543         |
| stats_o/mean                   | 0.20261797  |
| stats_o/std                    | 0.084617384 |
| test/episodes                  | 5440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00214    |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.3240209  |
| test/Q_plus_P                  | -1.3240209  |
| test/reward_per_eps            | -11         |
| test/steps                     | 217600      |
| train/episodes                 | 21760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0655     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 870400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 544        |
| stats_o/mean                   | 0.20262164 |
| stats_o/std                    | 0.08459367 |
| test/episodes                  | 5450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00205   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2007687 |
| test/Q_plus_P                  | -1.2007687 |
| test/reward_per_eps            | -10        |
| test/steps                     | 218000     |
| train/episodes                 | 21800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00275   |
| train/info_shaping_reward_mean | -0.0645    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 872000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 545        |
| stats_o/mean                   | 0.20261165 |
| stats_o/std                    | 0.08459615 |
| test/episodes                  | 5460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0029    |
| test/info_shaping_reward_mean  | -0.06      |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -1.612494  |
| test/Q_plus_P                  | -1.612494  |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 218400     |
| train/episodes                 | 21840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00408   |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 873600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 546        |
| stats_o/mean                   | 0.20262419 |
| stats_o/std                    | 0.08459725 |
| test/episodes                  | 5470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00365   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1016028 |
| test/Q_plus_P                  | -1.1016028 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 218800     |
| train/episodes                 | 21880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.565      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00581   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 875200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 547        |
| stats_o/mean                   | 0.2026252  |
| stats_o/std                    | 0.08459651 |
| test/episodes                  | 5480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00327   |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5294343 |
| test/Q_plus_P                  | -1.5294343 |
| test/reward_per_eps            | -12        |
| test/steps                     | 219200     |
| train/episodes                 | 21920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00393   |
| train/info_shaping_reward_mean | -0.0741    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 876800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 548        |
| stats_o/mean                   | 0.20263702 |
| stats_o/std                    | 0.08458845 |
| test/episodes                  | 5490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000604  |
| test/info_shaping_reward_mean  | -0.0515    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3858758 |
| test/Q_plus_P                  | -1.3858758 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 219600     |
| train/episodes                 | 21960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0033    |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 878400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.20263888  |
| stats_o/std                    | 0.084596485 |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000772   |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.3228282  |
| test/Q_plus_P                  | -1.3228282  |
| test/reward_per_eps            | -10         |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00464    |
| train/info_shaping_reward_mean | -0.078      |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 550        |
| stats_o/mean                   | 0.20263146 |
| stats_o/std                    | 0.08460456 |
| test/episodes                  | 5510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3371174 |
| test/Q_plus_P                  | -1.3371174 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 220400     |
| train/episodes                 | 22040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.595      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00408   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 881600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 551         |
| stats_o/mean                   | 0.20263441  |
| stats_o/std                    | 0.084587164 |
| test/episodes                  | 5520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0024     |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.0970119  |
| test/Q_plus_P                  | -1.0970119  |
| test/reward_per_eps            | -9          |
| test/steps                     | 220800      |
| train/episodes                 | 22080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.574       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00455    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 883200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 552        |
| stats_o/mean                   | 0.20262882 |
| stats_o/std                    | 0.08458248 |
| test/episodes                  | 5530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00405   |
| test/info_shaping_reward_mean  | -0.0446    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0497606 |
| test/Q_plus_P                  | -1.0497606 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 221200     |
| train/episodes                 | 22120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0034    |
| train/info_shaping_reward_mean | -0.0676    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 884800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 553        |
| stats_o/mean                   | 0.20261484 |
| stats_o/std                    | 0.08456404 |
| test/episodes                  | 5540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00225   |
| test/info_shaping_reward_mean  | -0.0458    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0430989 |
| test/Q_plus_P                  | -1.0430989 |
| test/reward_per_eps            | -9         |
| test/steps                     | 221600     |
| train/episodes                 | 22160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00357   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 886400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 554        |
| stats_o/mean                   | 0.20261852 |
| stats_o/std                    | 0.08452706 |
| test/episodes                  | 5550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00291   |
| test/info_shaping_reward_mean  | -0.0515    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3222388 |
| test/Q_plus_P                  | -1.3222388 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 222000     |
| train/episodes                 | 22200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00341   |
| train/info_shaping_reward_mean | -0.0635    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 888000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 555        |
| stats_o/mean                   | 0.20261627 |
| stats_o/std                    | 0.08451761 |
| test/episodes                  | 5560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00189   |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.204802  |
| test/Q_plus_P                  | -1.204802  |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 222400     |
| train/episodes                 | 22240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00417   |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 889600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 556        |
| stats_o/mean                   | 0.20260674 |
| stats_o/std                    | 0.08449717 |
| test/episodes                  | 5570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00216   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1698163 |
| test/Q_plus_P                  | -1.1698163 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 222800     |
| train/episodes                 | 22280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00372   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 891200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 557        |
| stats_o/mean                   | 0.2026148  |
| stats_o/std                    | 0.08449729 |
| test/episodes                  | 5580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00226   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.3505883 |
| test/Q_plus_P                  | -1.3505883 |
| test/reward_per_eps            | -10        |
| test/steps                     | 223200     |
| train/episodes                 | 22320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.582      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00376   |
| train/info_shaping_reward_mean | -0.075     |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 892800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 558        |
| stats_o/mean                   | 0.20260593 |
| stats_o/std                    | 0.08448239 |
| test/episodes                  | 5590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00223   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1693263 |
| test/Q_plus_P                  | -1.1693263 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 223600     |
| train/episodes                 | 22360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00316   |
| train/info_shaping_reward_mean | -0.0624    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 894400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 559         |
| stats_o/mean                   | 0.20260783  |
| stats_o/std                    | 0.084482476 |
| test/episodes                  | 5600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00295    |
| test/info_shaping_reward_mean  | -0.0434     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0697458  |
| test/Q_plus_P                  | -1.0697458  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 224000      |
| train/episodes                 | 22400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00486    |
| train/info_shaping_reward_mean | -0.076      |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 896000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 560        |
| stats_o/mean                   | 0.2026092  |
| stats_o/std                    | 0.08446829 |
| test/episodes                  | 5610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00226   |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1202884 |
| test/Q_plus_P                  | -1.1202884 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 224400     |
| train/episodes                 | 22440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00344   |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 897600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 561         |
| stats_o/mean                   | 0.20260958  |
| stats_o/std                    | 0.084441446 |
| test/episodes                  | 5620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00135    |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1823269  |
| test/Q_plus_P                  | -1.1823269  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 224800      |
| train/episodes                 | 22480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00351    |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 899200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.20260662  |
| stats_o/std                    | 0.084440775 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000746   |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -1.2923051  |
| test/Q_plus_P                  | -1.2923051  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.0714     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.20260495  |
| stats_o/std                    | 0.084449805 |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00165    |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.193      |
| test/Q                         | -0.9530032  |
| test/Q_plus_P                  | -0.9530032  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.605       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00469    |
| train/info_shaping_reward_mean | -0.0783     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 564        |
| stats_o/mean                   | 0.20260811 |
| stats_o/std                    | 0.08444741 |
| test/episodes                  | 5650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00337   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5008472 |
| test/Q_plus_P                  | -1.5008472 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 226000     |
| train/episodes                 | 22600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.0749    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 904000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 565        |
| stats_o/mean                   | 0.20261207 |
| stats_o/std                    | 0.0844492  |
| test/episodes                  | 5660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00237   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -1.2197437 |
| test/Q_plus_P                  | -1.2197437 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 226400     |
| train/episodes                 | 22640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00488   |
| train/info_shaping_reward_mean | -0.0782    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 905600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 566        |
| stats_o/mean                   | 0.20261608 |
| stats_o/std                    | 0.08444738 |
| test/episodes                  | 5670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00204   |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.5612134 |
| test/Q_plus_P                  | -1.5612134 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 226800     |
| train/episodes                 | 22680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00353   |
| train/info_shaping_reward_mean | -0.0769    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 907200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 567        |
| stats_o/mean                   | 0.20261484 |
| stats_o/std                    | 0.0844476  |
| test/episodes                  | 5680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00291   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2020363 |
| test/Q_plus_P                  | -1.2020363 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 227200     |
| train/episodes                 | 22720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.595      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00536   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 908800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 568        |
| stats_o/mean                   | 0.20261656 |
| stats_o/std                    | 0.08442181 |
| test/episodes                  | 5690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000792  |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0937358 |
| test/Q_plus_P                  | -1.0937358 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 227600     |
| train/episodes                 | 22760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00514   |
| train/info_shaping_reward_mean | -0.0646    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 910400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.20261526  |
| stats_o/std                    | 0.084395066 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.3921928  |
| test/Q_plus_P                  | -1.3921928  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0035     |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 570        |
| stats_o/mean                   | 0.2026115  |
| stats_o/std                    | 0.08439915 |
| test/episodes                  | 5710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00221   |
| test/info_shaping_reward_mean  | -0.0437    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0298216 |
| test/Q_plus_P                  | -1.0298216 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 228400     |
| train/episodes                 | 22840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00284   |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 913600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 571        |
| stats_o/mean                   | 0.20261724 |
| stats_o/std                    | 0.08442217 |
| test/episodes                  | 5720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00226   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2448876 |
| test/Q_plus_P                  | -1.2448876 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 228800     |
| train/episodes                 | 22880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00403   |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 915200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.20262247  |
| stats_o/std                    | 0.084393136 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00307    |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.4805284  |
| test/Q_plus_P                  | -1.4805284  |
| test/reward_per_eps            | -12         |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.0661     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 573        |
| stats_o/mean                   | 0.20262364 |
| stats_o/std                    | 0.08439824 |
| test/episodes                  | 5740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00196   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4310118 |
| test/Q_plus_P                  | -1.4310118 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 229600     |
| train/episodes                 | 22960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 918400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.20262767  |
| stats_o/std                    | 0.084387034 |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00178    |
| test/info_shaping_reward_mean  | -0.058      |
| test/info_shaping_reward_min   | -0.19       |
| test/Q                         | -1.4726297  |
| test/Q_plus_P                  | -1.4726297  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00414    |
| train/info_shaping_reward_mean | -0.0738     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.202627    |
| stats_o/std                    | 0.084377214 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.3253349  |
| test/Q_plus_P                  | -1.3253349  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00417    |
| train/info_shaping_reward_mean | -0.074      |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 576        |
| stats_o/mean                   | 0.20262024 |
| stats_o/std                    | 0.08434925 |
| test/episodes                  | 5770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00207   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1147937 |
| test/Q_plus_P                  | -1.1147937 |
| test/reward_per_eps            | -10        |
| test/steps                     | 230800     |
| train/episodes                 | 23080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00274   |
| train/info_shaping_reward_mean | -0.063     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 923200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 577         |
| stats_o/mean                   | 0.20261227  |
| stats_o/std                    | 0.084339626 |
| test/episodes                  | 5780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00237    |
| test/info_shaping_reward_mean  | -0.055      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.381772   |
| test/Q_plus_P                  | -1.381772   |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 231200      |
| train/episodes                 | 23120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.589       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00476    |
| train/info_shaping_reward_mean | -0.0727     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 924800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 578         |
| stats_o/mean                   | 0.20262107  |
| stats_o/std                    | 0.084346354 |
| test/episodes                  | 5790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00038    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2694968  |
| test/Q_plus_P                  | -1.2694968  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 231600      |
| train/episodes                 | 23160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.578       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00385    |
| train/info_shaping_reward_mean | -0.0803     |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 926400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 579        |
| stats_o/mean                   | 0.20263357 |
| stats_o/std                    | 0.08432824 |
| test/episodes                  | 5800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2340053 |
| test/Q_plus_P                  | -1.2340053 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 232000     |
| train/episodes                 | 23200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00369   |
| train/info_shaping_reward_mean | -0.0631    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 928000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 580        |
| stats_o/mean                   | 0.2026381  |
| stats_o/std                    | 0.08432713 |
| test/episodes                  | 5810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000763  |
| test/info_shaping_reward_mean  | -0.0468    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1491948 |
| test/Q_plus_P                  | -1.1491948 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 232400     |
| train/episodes                 | 23240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00433   |
| train/info_shaping_reward_mean | -0.0663    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 929600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 581         |
| stats_o/mean                   | 0.20263115  |
| stats_o/std                    | 0.084322765 |
| test/episodes                  | 5820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00388    |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.2664096  |
| test/Q_plus_P                  | -1.2664096  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 232800      |
| train/episodes                 | 23280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00478    |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 931200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 582        |
| stats_o/mean                   | 0.20263666 |
| stats_o/std                    | 0.08433346 |
| test/episodes                  | 5830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00172   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.1567378 |
| test/Q_plus_P                  | -1.1567378 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 233200     |
| train/episodes                 | 23320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00436   |
| train/info_shaping_reward_mean | -0.0856    |
| train/info_shaping_reward_min  | -0.265     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 932800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 583        |
| stats_o/mean                   | 0.20263316 |
| stats_o/std                    | 0.08430136 |
| test/episodes                  | 5840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00141   |
| test/info_shaping_reward_mean  | -0.0463    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1384517 |
| test/Q_plus_P                  | -1.1384517 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 233600     |
| train/episodes                 | 23360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.657      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.0621    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 934400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 584        |
| stats_o/mean                   | 0.20263372 |
| stats_o/std                    | 0.08430305 |
| test/episodes                  | 5850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00215   |
| test/info_shaping_reward_mean  | -0.0613    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.6503323 |
| test/Q_plus_P                  | -1.6503323 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 234000     |
| train/episodes                 | 23400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00396   |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 936000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 585        |
| stats_o/mean                   | 0.20264335 |
| stats_o/std                    | 0.08429341 |
| test/episodes                  | 5860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00306   |
| test/info_shaping_reward_mean  | -0.0658    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.8849548 |
| test/Q_plus_P                  | -1.8849548 |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 234400     |
| train/episodes                 | 23440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00553   |
| train/info_shaping_reward_mean | -0.0664    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 937600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 586        |
| stats_o/mean                   | 0.20264988 |
| stats_o/std                    | 0.08425869 |
| test/episodes                  | 5870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00226   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2644747 |
| test/Q_plus_P                  | -1.2644747 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 234800     |
| train/episodes                 | 23480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00534   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 939200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 587        |
| stats_o/mean                   | 0.20264189 |
| stats_o/std                    | 0.0842375  |
| test/episodes                  | 5880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00269   |
| test/info_shaping_reward_mean  | -0.0565    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.349629  |
| test/Q_plus_P                  | -1.349629  |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 235200     |
| train/episodes                 | 23520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00449   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 940800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 588        |
| stats_o/mean                   | 0.20263284 |
| stats_o/std                    | 0.08423079 |
| test/episodes                  | 5890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00585   |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0716372 |
| test/Q_plus_P                  | -1.0716372 |
| test/reward_per_eps            | -9         |
| test/steps                     | 235600     |
| train/episodes                 | 23560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00532   |
| train/info_shaping_reward_mean | -0.0833    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 942400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 589        |
| stats_o/mean                   | 0.20264542 |
| stats_o/std                    | 0.08425574 |
| test/episodes                  | 5900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000704  |
| test/info_shaping_reward_mean  | -0.0599    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6225247 |
| test/Q_plus_P                  | -1.6225247 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 236000     |
| train/episodes                 | 23600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.078     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 944000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 590        |
| stats_o/mean                   | 0.20264484 |
| stats_o/std                    | 0.08424374 |
| test/episodes                  | 5910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00114   |
| test/info_shaping_reward_mean  | -0.0594    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.6317259 |
| test/Q_plus_P                  | -1.6317259 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 236400     |
| train/episodes                 | 23640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00395   |
| train/info_shaping_reward_mean | -0.0691    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 945600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 591        |
| stats_o/mean                   | 0.20265587 |
| stats_o/std                    | 0.08423033 |
| test/episodes                  | 5920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00328   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2518975 |
| test/Q_plus_P                  | -1.2518975 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 236800     |
| train/episodes                 | 23680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00308   |
| train/info_shaping_reward_mean | -0.0732    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 947200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 592        |
| stats_o/mean                   | 0.20264804 |
| stats_o/std                    | 0.08421144 |
| test/episodes                  | 5930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00211   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3053002 |
| test/Q_plus_P                  | -1.3053002 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 237200     |
| train/episodes                 | 23720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00461   |
| train/info_shaping_reward_mean | -0.0674    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 948800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.20265396  |
| stats_o/std                    | 0.084184445 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00271    |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.313924   |
| test/Q_plus_P                  | -1.313924   |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.0655     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 594         |
| stats_o/mean                   | 0.20265938  |
| stats_o/std                    | 0.084163636 |
| test/episodes                  | 5950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.4460796  |
| test/Q_plus_P                  | -1.4460796  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 238000      |
| train/episodes                 | 23800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00518    |
| train/info_shaping_reward_mean | -0.0771     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 952000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 595        |
| stats_o/mean                   | 0.20264569 |
| stats_o/std                    | 0.08415881 |
| test/episodes                  | 5960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0409    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9801631 |
| test/Q_plus_P                  | -0.9801631 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 238400     |
| train/episodes                 | 23840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00381   |
| train/info_shaping_reward_mean | -0.065     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 953600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 596        |
| stats_o/mean                   | 0.20264906 |
| stats_o/std                    | 0.08412343 |
| test/episodes                  | 5970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00276   |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2832953 |
| test/Q_plus_P                  | -1.2832953 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 238800     |
| train/episodes                 | 23880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00319   |
| train/info_shaping_reward_mean | -0.0635    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 955200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 597         |
| stats_o/mean                   | 0.20265925  |
| stats_o/std                    | 0.084117696 |
| test/episodes                  | 5980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0029     |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1616235  |
| test/Q_plus_P                  | -1.1616235  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 239200      |
| train/episodes                 | 23920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.0652     |
| train/info_shaping_reward_min  | -0.203      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 956800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 598        |
| stats_o/mean                   | 0.20265254 |
| stats_o/std                    | 0.08412331 |
| test/episodes                  | 5990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00381   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1313344 |
| test/Q_plus_P                  | -1.1313344 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 239600     |
| train/episodes                 | 23960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00446   |
| train/info_shaping_reward_mean | -0.0646    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 958400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.20266034 |
| stats_o/std                    | 0.0841321  |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000438  |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2790632 |
| test/Q_plus_P                  | -1.2790632 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00403   |
| train/info_shaping_reward_mean | -0.0753    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
