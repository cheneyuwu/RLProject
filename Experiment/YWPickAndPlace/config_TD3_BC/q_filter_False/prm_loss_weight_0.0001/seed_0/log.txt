Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPickAndPlace/config_TD3_BC/q_filter_False/prm_loss_weight_0.0001/seed_0
-----------------------------------------------
| epoch                          | 0          |
| stats_o/mean                   | 0.20176055 |
| stats_o/std                    | 0.09542311 |
| test/episodes                  | 10         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.422      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00817   |
| test/info_shaping_reward_mean  | -0.0974    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -1.1414255 |
| test/Q_plus_P                  | -1.1414255 |
| test/reward_per_eps            | -23.1      |
| test/steps                     | 400        |
| train/episodes                 | 40         |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0156     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.106     |
| train/info_shaping_reward_mean | -0.216     |
| train/info_shaping_reward_min  | -0.467     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 1600       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 1          |
| stats_o/mean                   | 0.19863808 |
| stats_o/std                    | 0.1059269  |
| test/episodes                  | 20         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.253      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00486   |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.535     |
| test/Q                         | -1.620224  |
| test/Q_plus_P                  | -1.620224  |
| test/reward_per_eps            | -29.9      |
| test/steps                     | 800        |
| train/episodes                 | 80         |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.01       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.117     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.537     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 3200       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 2          |
| stats_o/mean                   | 0.19976431 |
| stats_o/std                    | 0.11590109 |
| test/episodes                  | 30         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.28       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00224   |
| test/info_shaping_reward_mean  | -0.149     |
| test/info_shaping_reward_min   | -0.503     |
| test/Q                         | -1.9050424 |
| test/Q_plus_P                  | -1.9050424 |
| test/reward_per_eps            | -28.8      |
| test/steps                     | 1200       |
| train/episodes                 | 120        |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0904    |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.579     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 4800       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.20034853 |
| stats_o/std                    | 0.1248578  |
| test/episodes                  | 40         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.375      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00859   |
| test/info_shaping_reward_mean  | -0.109     |
| test/info_shaping_reward_min   | -0.223     |
| test/Q                         | -1.9210457 |
| test/Q_plus_P                  | -1.9210457 |
| test/reward_per_eps            | -25        |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0381     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0914    |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.51      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 4          |
| stats_o/mean                   | 0.20009504 |
| stats_o/std                    | 0.12706159 |
| test/episodes                  | 50         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.146     |
| test/info_shaping_reward_min   | -0.508     |
| test/Q                         | -2.7114346 |
| test/Q_plus_P                  | -2.7114346 |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 2000       |
| train/episodes                 | 200        |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0294     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.101     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.622     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 8000       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 5          |
| stats_o/mean                   | 0.20066725 |
| stats_o/std                    | 0.12980415 |
| test/episodes                  | 60         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.165      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00558   |
| test/info_shaping_reward_mean  | -0.149     |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -3.0965588 |
| test/Q_plus_P                  | -3.0965588 |
| test/reward_per_eps            | -33.4      |
| test/steps                     | 2400       |
| train/episodes                 | 240        |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0381     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.101     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.666     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 9600       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 6          |
| stats_o/mean                   | 0.20055062 |
| stats_o/std                    | 0.13041858 |
| test/episodes                  | 70         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.278      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00438   |
| test/info_shaping_reward_mean  | -0.123     |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -3.0613697 |
| test/Q_plus_P                  | -3.0613697 |
| test/reward_per_eps            | -28.9      |
| test/steps                     | 2800       |
| train/episodes                 | 280        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.01       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.129     |
| train/info_shaping_reward_mean | -0.213     |
| train/info_shaping_reward_min  | -0.434     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 11200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 7          |
| stats_o/mean                   | 0.20069307 |
| stats_o/std                    | 0.12940574 |
| test/episodes                  | 80         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.405      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00337   |
| test/info_shaping_reward_mean  | -0.0954    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -3.0140674 |
| test/Q_plus_P                  | -3.0140674 |
| test/reward_per_eps            | -23.8      |
| test/steps                     | 3200       |
| train/episodes                 | 320        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.515     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 12800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 8          |
| stats_o/mean                   | 0.2008288  |
| stats_o/std                    | 0.13070993 |
| test/episodes                  | 90         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.443      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00568   |
| test/info_shaping_reward_mean  | -0.097     |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -3.1003938 |
| test/Q_plus_P                  | -3.1003938 |
| test/reward_per_eps            | -22.3      |
| test/steps                     | 3600       |
| train/episodes                 | 360        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0163     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.107     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.533     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 14400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 9          |
| stats_o/mean                   | 0.20048553 |
| stats_o/std                    | 0.1302124  |
| test/episodes                  | 100        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.448      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00207   |
| test/info_shaping_reward_mean  | -0.09      |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -3.1129484 |
| test/Q_plus_P                  | -3.1129484 |
| test/reward_per_eps            | -22.1      |
| test/steps                     | 4000       |
| train/episodes                 | 400        |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0206     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0945    |
| train/info_shaping_reward_mean | -0.2       |
| train/info_shaping_reward_min  | -0.392     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 16000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 10         |
| stats_o/mean                   | 0.20118134 |
| stats_o/std                    | 0.12833053 |
| test/episodes                  | 110        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.215      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00925   |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -4.430768  |
| test/Q_plus_P                  | -4.430768  |
| test/reward_per_eps            | -31.4      |
| test/steps                     | 4400       |
| train/episodes                 | 440        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0294     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.121     |
| train/info_shaping_reward_mean | -0.211     |
| train/info_shaping_reward_min  | -0.376     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 17600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 11         |
| stats_o/mean                   | 0.20071198 |
| stats_o/std                    | 0.12753642 |
| test/episodes                  | 120        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.343      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00211   |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.59      |
| test/Q                         | -4.2355213 |
| test/Q_plus_P                  | -4.2355213 |
| test/reward_per_eps            | -26.3      |
| test/steps                     | 4800       |
| train/episodes                 | 480        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0075     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.415     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 19200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 12         |
| stats_o/mean                   | 0.20154633 |
| stats_o/std                    | 0.12812364 |
| test/episodes                  | 130        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.427      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00283   |
| test/info_shaping_reward_mean  | -0.0974    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -3.9702945 |
| test/Q_plus_P                  | -3.9702945 |
| test/reward_per_eps            | -22.9      |
| test/steps                     | 5200       |
| train/episodes                 | 520        |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0388     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.076     |
| train/info_shaping_reward_mean | -0.217     |
| train/info_shaping_reward_min  | -0.431     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 20800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 13         |
| stats_o/mean                   | 0.2016646  |
| stats_o/std                    | 0.1278324  |
| test/episodes                  | 140        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.36       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.104     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -4.5879064 |
| test/Q_plus_P                  | -4.5879064 |
| test/reward_per_eps            | -25.6      |
| test/steps                     | 5600       |
| train/episodes                 | 560        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0238     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.106     |
| train/info_shaping_reward_mean | -0.206     |
| train/info_shaping_reward_min  | -0.413     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 22400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 14         |
| stats_o/mean                   | 0.20131046 |
| stats_o/std                    | 0.12653713 |
| test/episodes                  | 150        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.253      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00245   |
| test/info_shaping_reward_mean  | -0.125     |
| test/info_shaping_reward_min   | -0.223     |
| test/Q                         | -5.356295  |
| test/Q_plus_P                  | -5.356295  |
| test/reward_per_eps            | -29.9      |
| test/steps                     | 6000       |
| train/episodes                 | 600        |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0375     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0801    |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.354     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 24000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 15         |
| stats_o/mean                   | 0.20148247 |
| stats_o/std                    | 0.12788819 |
| test/episodes                  | 160        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.41       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000994  |
| test/info_shaping_reward_mean  | -0.1       |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -4.318871  |
| test/Q_plus_P                  | -4.318871  |
| test/reward_per_eps            | -23.6      |
| test/steps                     | 6400       |
| train/episodes                 | 640        |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0288     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.101     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.6       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 25600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 16         |
| stats_o/mean                   | 0.20159586 |
| stats_o/std                    | 0.12839998 |
| test/episodes                  | 170        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.365      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.114     |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -4.9635005 |
| test/Q_plus_P                  | -4.9635005 |
| test/reward_per_eps            | -25.4      |
| test/steps                     | 6800       |
| train/episodes                 | 680        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.12      |
| train/info_shaping_reward_mean | -0.205     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 27200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 17         |
| stats_o/mean                   | 0.2016688  |
| stats_o/std                    | 0.1276837  |
| test/episodes                  | 180        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.328      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00147   |
| test/info_shaping_reward_mean  | -0.116     |
| test/info_shaping_reward_min   | -0.203     |
| test/Q                         | -5.6469746 |
| test/Q_plus_P                  | -5.6469746 |
| test/reward_per_eps            | -26.9      |
| test/steps                     | 7200       |
| train/episodes                 | 720        |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.025      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0814    |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 28800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 18         |
| stats_o/mean                   | 0.20174553 |
| stats_o/std                    | 0.1275371  |
| test/episodes                  | 190        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.34       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00564   |
| test/info_shaping_reward_mean  | -0.116     |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -6.0388837 |
| test/Q_plus_P                  | -6.0388837 |
| test/reward_per_eps            | -26.4      |
| test/steps                     | 7600       |
| train/episodes                 | 760        |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.05       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0591    |
| train/info_shaping_reward_mean | -0.206     |
| train/info_shaping_reward_min  | -0.474     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38        |
| train/steps                    | 30400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.2017641  |
| stats_o/std                    | 0.12792622 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.445      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00506   |
| test/info_shaping_reward_mean  | -0.0916    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -5.1113515 |
| test/Q_plus_P                  | -5.1113515 |
| test/reward_per_eps            | -22.2      |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0131     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.108     |
| train/info_shaping_reward_mean | -0.205     |
| train/info_shaping_reward_min  | -0.434     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 20         |
| stats_o/mean                   | 0.20192842 |
| stats_o/std                    | 0.12769598 |
| test/episodes                  | 210        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.4        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00358   |
| test/info_shaping_reward_mean  | -0.111     |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -5.4212008 |
| test/Q_plus_P                  | -5.4212008 |
| test/reward_per_eps            | -24        |
| test/steps                     | 8400       |
| train/episodes                 | 840        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.473     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 33600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 21         |
| stats_o/mean                   | 0.20149614 |
| stats_o/std                    | 0.12857057 |
| test/episodes                  | 220        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.325      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00771   |
| test/info_shaping_reward_mean  | -0.12      |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -6.3452845 |
| test/Q_plus_P                  | -6.3452845 |
| test/reward_per_eps            | -27        |
| test/steps                     | 8800       |
| train/episodes                 | 880        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00437    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.118     |
| train/info_shaping_reward_mean | -0.205     |
| train/info_shaping_reward_min  | -0.391     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 35200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 22         |
| stats_o/mean                   | 0.20137553 |
| stats_o/std                    | 0.12817411 |
| test/episodes                  | 230        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.37       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00332   |
| test/info_shaping_reward_mean  | -0.106     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -6.31767   |
| test/Q_plus_P                  | -6.31767   |
| test/reward_per_eps            | -25.2      |
| test/steps                     | 9200       |
| train/episodes                 | 920        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.005      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.121     |
| train/info_shaping_reward_mean | -0.216     |
| train/info_shaping_reward_min  | -0.4       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 36800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 23         |
| stats_o/mean                   | 0.20151073 |
| stats_o/std                    | 0.12853493 |
| test/episodes                  | 240        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.297      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00147   |
| test/info_shaping_reward_mean  | -0.113     |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -6.498438  |
| test/Q_plus_P                  | -6.498438  |
| test/reward_per_eps            | -28.1      |
| test/steps                     | 9600       |
| train/episodes                 | 960        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0194     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.12      |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.467     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 38400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 24         |
| stats_o/mean                   | 0.2016204  |
| stats_o/std                    | 0.12928542 |
| test/episodes                  | 250        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.278      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00951   |
| test/info_shaping_reward_mean  | -0.118     |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -7.738273  |
| test/Q_plus_P                  | -7.738273  |
| test/reward_per_eps            | -28.9      |
| test/steps                     | 10000      |
| train/episodes                 | 1000       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0187     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0914    |
| train/info_shaping_reward_mean | -0.216     |
| train/info_shaping_reward_min  | -0.437     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 40000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 25         |
| stats_o/mean                   | 0.20156921 |
| stats_o/std                    | 0.12844926 |
| test/episodes                  | 260        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.388      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00512   |
| test/info_shaping_reward_mean  | -0.109     |
| test/info_shaping_reward_min   | -0.213     |
| test/Q                         | -6.3160396 |
| test/Q_plus_P                  | -6.3160396 |
| test/reward_per_eps            | -24.5      |
| test/steps                     | 10400      |
| train/episodes                 | 1040       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0394     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.081     |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.433     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 41600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 26         |
| stats_o/mean                   | 0.20150705 |
| stats_o/std                    | 0.12906961 |
| test/episodes                  | 270        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.39       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0029    |
| test/info_shaping_reward_mean  | -0.0986    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -6.134053  |
| test/Q_plus_P                  | -6.134053  |
| test/reward_per_eps            | -24.4      |
| test/steps                     | 10800      |
| train/episodes                 | 1080       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.03       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0968    |
| train/info_shaping_reward_mean | -0.211     |
| train/info_shaping_reward_min  | -0.483     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 43200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 27         |
| stats_o/mean                   | 0.20155084 |
| stats_o/std                    | 0.12877284 |
| test/episodes                  | 280        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.422      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00137   |
| test/info_shaping_reward_mean  | -0.1       |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -6.141997  |
| test/Q_plus_P                  | -6.141997  |
| test/reward_per_eps            | -23.1      |
| test/steps                     | 11200      |
| train/episodes                 | 1120       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0256     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.114     |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 44800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.20149347 |
| stats_o/std                    | 0.12850417 |
| test/episodes                  | 290        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.505      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00141   |
| test/info_shaping_reward_mean  | -0.0838    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -5.0766973 |
| test/Q_plus_P                  | -5.0766973 |
| test/reward_per_eps            | -19.8      |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0144     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.214     |
| train/info_shaping_reward_min  | -0.404     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.20134573 |
| stats_o/std                    | 0.1286736  |
| test/episodes                  | 300        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.258      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00456   |
| test/info_shaping_reward_mean  | -0.119     |
| test/info_shaping_reward_min   | -0.213     |
| test/Q                         | -8.289544  |
| test/Q_plus_P                  | -8.289544  |
| test/reward_per_eps            | -29.7      |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0262     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.101     |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.519     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 30         |
| stats_o/mean                   | 0.20122156 |
| stats_o/std                    | 0.1291477  |
| test/episodes                  | 310        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.31       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00482   |
| test/info_shaping_reward_mean  | -0.117     |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -7.7136497 |
| test/Q_plus_P                  | -7.7136497 |
| test/reward_per_eps            | -27.6      |
| test/steps                     | 12400      |
| train/episodes                 | 1240       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0369     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0811    |
| train/info_shaping_reward_mean | -0.203     |
| train/info_shaping_reward_min  | -0.375     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 49600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 31         |
| stats_o/mean                   | 0.2012246  |
| stats_o/std                    | 0.12924609 |
| test/episodes                  | 320        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.43       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00247   |
| test/info_shaping_reward_mean  | -0.0965    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -6.324882  |
| test/Q_plus_P                  | -6.324882  |
| test/reward_per_eps            | -22.8      |
| test/steps                     | 12800      |
| train/episodes                 | 1280       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.01       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.102     |
| train/info_shaping_reward_mean | -0.223     |
| train/info_shaping_reward_min  | -0.492     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 51200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 32         |
| stats_o/mean                   | 0.20133497 |
| stats_o/std                    | 0.1299712  |
| test/episodes                  | 330        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.23       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00613   |
| test/info_shaping_reward_mean  | -0.123     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -9.058761  |
| test/Q_plus_P                  | -9.058761  |
| test/reward_per_eps            | -30.8      |
| test/steps                     | 13200      |
| train/episodes                 | 1320       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0231     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.105     |
| train/info_shaping_reward_mean | -0.21      |
| train/info_shaping_reward_min  | -0.453     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 52800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 33         |
| stats_o/mean                   | 0.20133646 |
| stats_o/std                    | 0.12899369 |
| test/episodes                  | 340        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.263      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00499   |
| test/info_shaping_reward_mean  | -0.114     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -8.539096  |
| test/Q_plus_P                  | -8.539096  |
| test/reward_per_eps            | -29.5      |
| test/steps                     | 13600      |
| train/episodes                 | 1360       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0163     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.109     |
| train/info_shaping_reward_mean | -0.205     |
| train/info_shaping_reward_min  | -0.367     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 54400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 34         |
| stats_o/mean                   | 0.20119993 |
| stats_o/std                    | 0.12877277 |
| test/episodes                  | 350        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.39       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00188   |
| test/info_shaping_reward_mean  | -0.101     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -7.113724  |
| test/Q_plus_P                  | -7.113724  |
| test/reward_per_eps            | -24.4      |
| test/steps                     | 14000      |
| train/episodes                 | 1400       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0362     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0797    |
| train/info_shaping_reward_mean | -0.203     |
| train/info_shaping_reward_min  | -0.384     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 56000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.2010771  |
| stats_o/std                    | 0.12837978 |
| test/episodes                  | 360        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.307      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00369   |
| test/info_shaping_reward_mean  | -0.111     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -8.923392  |
| test/Q_plus_P                  | -8.923392  |
| test/reward_per_eps            | -27.7      |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0275     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.301     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 36         |
| stats_o/mean                   | 0.20091154 |
| stats_o/std                    | 0.12863187 |
| test/episodes                  | 370        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.398      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00149   |
| test/info_shaping_reward_mean  | -0.0982    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -6.9305115 |
| test/Q_plus_P                  | -6.9305115 |
| test/reward_per_eps            | -24.1      |
| test/steps                     | 14800      |
| train/episodes                 | 1480       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0275     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.102     |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 59200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 37         |
| stats_o/mean                   | 0.20085607 |
| stats_o/std                    | 0.12855583 |
| test/episodes                  | 380        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.355      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00121   |
| test/info_shaping_reward_mean  | -0.115     |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -8.343884  |
| test/Q_plus_P                  | -8.343884  |
| test/reward_per_eps            | -25.8      |
| test/steps                     | 15200      |
| train/episodes                 | 1520       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0263     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0973    |
| train/info_shaping_reward_mean | -0.203     |
| train/info_shaping_reward_min  | -0.428     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 60800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 38         |
| stats_o/mean                   | 0.20073847 |
| stats_o/std                    | 0.12807272 |
| test/episodes                  | 390        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.215      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.012     |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -11.104802 |
| test/Q_plus_P                  | -11.104802 |
| test/reward_per_eps            | -31.4      |
| test/steps                     | 15600      |
| train/episodes                 | 1560       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0125     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.104     |
| train/info_shaping_reward_mean | -0.205     |
| train/info_shaping_reward_min  | -0.391     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 62400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 39         |
| stats_o/mean                   | 0.20062576 |
| stats_o/std                    | 0.128337   |
| test/episodes                  | 400        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.403      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0026    |
| test/info_shaping_reward_mean  | -0.104     |
| test/info_shaping_reward_min   | -0.202     |
| test/Q                         | -7.694535  |
| test/Q_plus_P                  | -7.694535  |
| test/reward_per_eps            | -23.9      |
| test/steps                     | 16000      |
| train/episodes                 | 1600       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0225     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.118     |
| train/info_shaping_reward_mean | -0.208     |
| train/info_shaping_reward_min  | -0.431     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 64000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 40         |
| stats_o/mean                   | 0.20063566 |
| stats_o/std                    | 0.12807877 |
| test/episodes                  | 410        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.335      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0017    |
| test/info_shaping_reward_mean  | -0.114     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -8.994357  |
| test/Q_plus_P                  | -8.994357  |
| test/reward_per_eps            | -26.6      |
| test/steps                     | 16400      |
| train/episodes                 | 1640       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0412     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0827    |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 65600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 41         |
| stats_o/mean                   | 0.20072643 |
| stats_o/std                    | 0.12767221 |
| test/episodes                  | 420        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.11       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.136     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -13.295201 |
| test/Q_plus_P                  | -13.295201 |
| test/reward_per_eps            | -35.6      |
| test/steps                     | 16800      |
| train/episodes                 | 1680       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0406     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0781    |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.317     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 67200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 42         |
| stats_o/mean                   | 0.20073839 |
| stats_o/std                    | 0.12742661 |
| test/episodes                  | 430        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.328      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00485   |
| test/info_shaping_reward_mean  | -0.112     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -9.307396  |
| test/Q_plus_P                  | -9.307396  |
| test/reward_per_eps            | -26.9      |
| test/steps                     | 17200      |
| train/episodes                 | 1720       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0425     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0961    |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.3      |
| train/steps                    | 68800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 43         |
| stats_o/mean                   | 0.20080103 |
| stats_o/std                    | 0.12743379 |
| test/episodes                  | 440        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.487      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.091     |
| test/info_shaping_reward_min   | -0.215     |
| test/Q                         | -6.42126   |
| test/Q_plus_P                  | -6.42126   |
| test/reward_per_eps            | -20.5      |
| test/steps                     | 17600      |
| train/episodes                 | 1760       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0456     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0634    |
| train/info_shaping_reward_mean | -0.209     |
| train/info_shaping_reward_min  | -0.428     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.2      |
| train/steps                    | 70400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 44         |
| stats_o/mean                   | 0.20070194 |
| stats_o/std                    | 0.1280523  |
| test/episodes                  | 450        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.37       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00705   |
| test/info_shaping_reward_mean  | -0.11      |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -8.722393  |
| test/Q_plus_P                  | -8.722393  |
| test/reward_per_eps            | -25.2      |
| test/steps                     | 18000      |
| train/episodes                 | 1800       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.04       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0988    |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.507     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 72000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 45         |
| stats_o/mean                   | 0.20102176 |
| stats_o/std                    | 0.1282548  |
| test/episodes                  | 460        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.403      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00494   |
| test/info_shaping_reward_mean  | -0.0955    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -7.5295277 |
| test/Q_plus_P                  | -7.5295277 |
| test/reward_per_eps            | -23.9      |
| test/steps                     | 18400      |
| train/episodes                 | 1840       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0156     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0908    |
| train/info_shaping_reward_mean | -0.208     |
| train/info_shaping_reward_min  | -0.532     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 73600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 46         |
| stats_o/mean                   | 0.20099731 |
| stats_o/std                    | 0.12854843 |
| test/episodes                  | 470        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.152      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00716   |
| test/info_shaping_reward_mean  | -0.19      |
| test/info_shaping_reward_min   | -0.545     |
| test/Q                         | -13.871967 |
| test/Q_plus_P                  | -13.871967 |
| test/reward_per_eps            | -33.9      |
| test/steps                     | 18800      |
| train/episodes                 | 1880       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0212     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.091     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.523     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 75200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 47         |
| stats_o/mean                   | 0.20098026 |
| stats_o/std                    | 0.1284734  |
| test/episodes                  | 480        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.273      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0015    |
| test/info_shaping_reward_mean  | -0.125     |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -10.738861 |
| test/Q_plus_P                  | -10.738861 |
| test/reward_per_eps            | -29.1      |
| test/steps                     | 19200      |
| train/episodes                 | 1920       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.03       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0811    |
| train/info_shaping_reward_mean | -0.201     |
| train/info_shaping_reward_min  | -0.418     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 76800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 48         |
| stats_o/mean                   | 0.2010153  |
| stats_o/std                    | 0.12879032 |
| test/episodes                  | 490        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.427      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00329   |
| test/info_shaping_reward_mean  | -0.12      |
| test/info_shaping_reward_min   | -0.503     |
| test/Q                         | -8.230232  |
| test/Q_plus_P                  | -8.230232  |
| test/reward_per_eps            | -22.9      |
| test/steps                     | 19600      |
| train/episodes                 | 1960       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0512     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0772    |
| train/info_shaping_reward_mean | -0.212     |
| train/info_shaping_reward_min  | -0.496     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38        |
| train/steps                    | 78400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 49         |
| stats_o/mean                   | 0.20103323 |
| stats_o/std                    | 0.12917574 |
| test/episodes                  | 500        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.375      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00371   |
| test/info_shaping_reward_mean  | -0.109     |
| test/info_shaping_reward_min   | -0.212     |
| test/Q                         | -8.999142  |
| test/Q_plus_P                  | -8.999142  |
| test/reward_per_eps            | -25        |
| test/steps                     | 20000      |
| train/episodes                 | 2000       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0725     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0391    |
| train/info_shaping_reward_mean | -0.203     |
| train/info_shaping_reward_min  | -0.517     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.1      |
| train/steps                    | 80000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.20121323 |
| stats_o/std                    | 0.12895723 |
| test/episodes                  | 510        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.287      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00936   |
| test/info_shaping_reward_mean  | -0.116     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -10.239221 |
| test/Q_plus_P                  | -10.239221 |
| test/reward_per_eps            | -28.5      |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0619     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0431    |
| train/info_shaping_reward_mean | -0.2       |
| train/info_shaping_reward_min  | -0.527     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.20126678 |
| stats_o/std                    | 0.1291699  |
| test/episodes                  | 520        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.438      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.123     |
| test/info_shaping_reward_min   | -0.664     |
| test/Q                         | -8.403646  |
| test/Q_plus_P                  | -8.403646  |
| test/reward_per_eps            | -22.5      |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0781     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0472    |
| train/info_shaping_reward_mean | -0.2       |
| train/info_shaping_reward_min  | -0.547     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.9      |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.20146036 |
| stats_o/std                    | 0.12937342 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.507      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00497   |
| test/info_shaping_reward_mean  | -0.086     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -5.373838  |
| test/Q_plus_P                  | -5.373838  |
| test/reward_per_eps            | -19.7      |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.0819     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0296    |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.584     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.7      |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 53         |
| stats_o/mean                   | 0.20152855 |
| stats_o/std                    | 0.1292835  |
| test/episodes                  | 540        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.555      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00423   |
| test/info_shaping_reward_mean  | -0.0788    |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -5.392212  |
| test/Q_plus_P                  | -5.392212  |
| test/reward_per_eps            | -17.8      |
| test/steps                     | 21600      |
| train/episodes                 | 2160       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.07       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0481    |
| train/info_shaping_reward_mean | -0.162     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.2      |
| train/steps                    | 86400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 54         |
| stats_o/mean                   | 0.20142815 |
| stats_o/std                    | 0.1291565  |
| test/episodes                  | 550        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.287      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00382   |
| test/info_shaping_reward_mean  | -0.116     |
| test/info_shaping_reward_min   | -0.324     |
| test/Q                         | -8.533436  |
| test/Q_plus_P                  | -8.533436  |
| test/reward_per_eps            | -28.5      |
| test/steps                     | 22000      |
| train/episodes                 | 2200       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0531     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.034     |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.41      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.9      |
| train/steps                    | 88000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 55         |
| stats_o/mean                   | 0.2017207  |
| stats_o/std                    | 0.12941873 |
| test/episodes                  | 560        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.48       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00284   |
| test/info_shaping_reward_mean  | -0.0935    |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -6.7843847 |
| test/Q_plus_P                  | -6.7843847 |
| test/reward_per_eps            | -20.8      |
| test/steps                     | 22400      |
| train/episodes                 | 2240       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0275     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.046     |
| train/info_shaping_reward_mean | -0.192     |
| train/info_shaping_reward_min  | -0.45      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 89600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 56         |
| stats_o/mean                   | 0.20173387 |
| stats_o/std                    | 0.12942858 |
| test/episodes                  | 570        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.44       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00141   |
| test/info_shaping_reward_mean  | -0.0891    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -6.391431  |
| test/Q_plus_P                  | -6.391431  |
| test/reward_per_eps            | -22.4      |
| test/steps                     | 22800      |
| train/episodes                 | 2280       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.0519     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0394    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.9      |
| train/steps                    | 91200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 57         |
| stats_o/mean                   | 0.20184155 |
| stats_o/std                    | 0.12958989 |
| test/episodes                  | 580        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.41       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00305   |
| test/info_shaping_reward_mean  | -0.108     |
| test/info_shaping_reward_min   | -0.399     |
| test/Q                         | -6.905515  |
| test/Q_plus_P                  | -6.905515  |
| test/reward_per_eps            | -23.6      |
| test/steps                     | 23200      |
| train/episodes                 | 2320       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0625     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0359    |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.411     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 92800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 58         |
| stats_o/mean                   | 0.2019648  |
| stats_o/std                    | 0.12966263 |
| test/episodes                  | 590        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.435      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00614   |
| test/info_shaping_reward_mean  | -0.0974    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -6.4017463 |
| test/Q_plus_P                  | -6.4017463 |
| test/reward_per_eps            | -22.6      |
| test/steps                     | 23600      |
| train/episodes                 | 2360       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.0731     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0293    |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.388     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.1      |
| train/steps                    | 94400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 59         |
| stats_o/mean                   | 0.201889   |
| stats_o/std                    | 0.12949564 |
| test/episodes                  | 600        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.39       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00473   |
| test/info_shaping_reward_mean  | -0.0977    |
| test/info_shaping_reward_min   | -0.215     |
| test/Q                         | -6.6540065 |
| test/Q_plus_P                  | -6.6540065 |
| test/reward_per_eps            | -24.4      |
| test/steps                     | 24000      |
| train/episodes                 | 2400       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.0744     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.031     |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.376     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37        |
| train/steps                    | 96000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 60         |
| stats_o/mean                   | 0.20186923 |
| stats_o/std                    | 0.1293034  |
| test/episodes                  | 610        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.263      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0069    |
| test/info_shaping_reward_mean  | -0.123     |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -9.299573  |
| test/Q_plus_P                  | -9.299573  |
| test/reward_per_eps            | -29.5      |
| test/steps                     | 24400      |
| train/episodes                 | 2440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.0988     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0223    |
| train/info_shaping_reward_mean | -0.16      |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36        |
| train/steps                    | 97600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 61         |
| stats_o/mean                   | 0.20194884 |
| stats_o/std                    | 0.12920327 |
| test/episodes                  | 620        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.398      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00263   |
| test/info_shaping_reward_mean  | -0.0976    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -6.043771  |
| test/Q_plus_P                  | -6.043771  |
| test/reward_per_eps            | -24.1      |
| test/steps                     | 24800      |
| train/episodes                 | 2480       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0869     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0365    |
| train/info_shaping_reward_mean | -0.157     |
| train/info_shaping_reward_min  | -0.289     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.5      |
| train/steps                    | 99200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 62         |
| stats_o/mean                   | 0.20211476 |
| stats_o/std                    | 0.12909234 |
| test/episodes                  | 630        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.365      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.102     |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -7.4396152 |
| test/Q_plus_P                  | -7.4396152 |
| test/reward_per_eps            | -25.4      |
| test/steps                     | 25200      |
| train/episodes                 | 2520       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.07       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0445    |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.317     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.2      |
| train/steps                    | 100800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 63         |
| stats_o/mean                   | 0.20218448 |
| stats_o/std                    | 0.12875016 |
| test/episodes                  | 640        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.432      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0021    |
| test/info_shaping_reward_mean  | -0.0907    |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -5.7964344 |
| test/Q_plus_P                  | -5.7964344 |
| test/reward_per_eps            | -22.7      |
| test/steps                     | 25600      |
| train/episodes                 | 2560       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0825     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0453    |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.378     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.7      |
| train/steps                    | 102400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.20226437 |
| stats_o/std                    | 0.128328   |
| test/episodes                  | 650        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.247      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0047    |
| test/info_shaping_reward_mean  | -0.119     |
| test/info_shaping_reward_min   | -0.202     |
| test/Q                         | -8.1389475 |
| test/Q_plus_P                  | -8.1389475 |
| test/reward_per_eps            | -30.1      |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0956     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0292    |
| train/info_shaping_reward_mean | -0.144     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.2      |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 65         |
| stats_o/mean                   | 0.20222786 |
| stats_o/std                    | 0.12779813 |
| test/episodes                  | 660        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.42       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00308   |
| test/info_shaping_reward_mean  | -0.0951    |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -6.2941937 |
| test/Q_plus_P                  | -6.2941937 |
| test/reward_per_eps            | -23.2      |
| test/steps                     | 26400      |
| train/episodes                 | 2640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.181      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0182    |
| train/info_shaping_reward_mean | -0.124     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.8      |
| train/steps                    | 105600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 66         |
| stats_o/mean                   | 0.20225444 |
| stats_o/std                    | 0.12747861 |
| test/episodes                  | 670        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.492      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00452   |
| test/info_shaping_reward_mean  | -0.09      |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -5.602399  |
| test/Q_plus_P                  | -5.602399  |
| test/reward_per_eps            | -20.3      |
| test/steps                     | 26800      |
| train/episodes                 | 2680       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.116      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0247    |
| train/info_shaping_reward_mean | -0.148     |
| train/info_shaping_reward_min  | -0.31      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.4      |
| train/steps                    | 107200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 67         |
| stats_o/mean                   | 0.202306   |
| stats_o/std                    | 0.12706032 |
| test/episodes                  | 680        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.535      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00417   |
| test/info_shaping_reward_mean  | -0.0818    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -5.320955  |
| test/Q_plus_P                  | -5.320955  |
| test/reward_per_eps            | -18.6      |
| test/steps                     | 27200      |
| train/episodes                 | 2720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.181      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0182    |
| train/info_shaping_reward_mean | -0.138     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.8      |
| train/steps                    | 108800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 68         |
| stats_o/mean                   | 0.20246145 |
| stats_o/std                    | 0.12706488 |
| test/episodes                  | 690        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.49       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00487   |
| test/info_shaping_reward_mean  | -0.087     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -5.273584  |
| test/Q_plus_P                  | -5.273584  |
| test/reward_per_eps            | -20.4      |
| test/steps                     | 27600      |
| train/episodes                 | 2760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.123      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0255    |
| train/info_shaping_reward_mean | -0.144     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.1      |
| train/steps                    | 110400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 69         |
| stats_o/mean                   | 0.20255212 |
| stats_o/std                    | 0.1271223  |
| test/episodes                  | 700        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.497      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00365   |
| test/info_shaping_reward_mean  | -0.0898    |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -5.214685  |
| test/Q_plus_P                  | -5.214685  |
| test/reward_per_eps            | -20.1      |
| test/steps                     | 28000      |
| train/episodes                 | 2800       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.114      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0254    |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.404     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.4      |
| train/steps                    | 112000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.20263746 |
| stats_o/std                    | 0.12695049 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.522      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00207   |
| test/info_shaping_reward_mean  | -0.0827    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -5.3053246 |
| test/Q_plus_P                  | -5.3053246 |
| test/reward_per_eps            | -19.1      |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.224      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0137    |
| train/info_shaping_reward_mean | -0.146     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31        |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 71         |
| stats_o/mean                   | 0.20278572 |
| stats_o/std                    | 0.12682265 |
| test/episodes                  | 720        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.52       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00265   |
| test/info_shaping_reward_mean  | -0.0851    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -4.8268695 |
| test/Q_plus_P                  | -4.8268695 |
| test/reward_per_eps            | -19.2      |
| test/steps                     | 28800      |
| train/episodes                 | 2880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.181      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0146    |
| train/info_shaping_reward_mean | -0.154     |
| train/info_shaping_reward_min  | -0.396     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.8      |
| train/steps                    | 115200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 72         |
| stats_o/mean                   | 0.20293397 |
| stats_o/std                    | 0.12653536 |
| test/episodes                  | 730        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.598      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00212   |
| test/info_shaping_reward_mean  | -0.0715    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -3.9742901 |
| test/Q_plus_P                  | -3.9742901 |
| test/reward_per_eps            | -16.1      |
| test/steps                     | 29200      |
| train/episodes                 | 2920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.269      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0133    |
| train/info_shaping_reward_mean | -0.122     |
| train/info_shaping_reward_min  | -0.31      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.2      |
| train/steps                    | 116800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.20292583 |
| stats_o/std                    | 0.12641238 |
| test/episodes                  | 740        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.603      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00177   |
| test/info_shaping_reward_mean  | -0.071     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -4.1861157 |
| test/Q_plus_P                  | -4.1861157 |
| test/reward_per_eps            | -15.9      |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.268      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0128    |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.308     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.3      |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 74         |
| stats_o/mean                   | 0.2029934  |
| stats_o/std                    | 0.12603937 |
| test/episodes                  | 750        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.593      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00229   |
| test/info_shaping_reward_mean  | -0.0731    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -3.9507358 |
| test/Q_plus_P                  | -3.9507358 |
| test/reward_per_eps            | -16.3      |
| test/steps                     | 30000      |
| train/episodes                 | 3000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.236      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0103    |
| train/info_shaping_reward_mean | -0.131     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.6      |
| train/steps                    | 120000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 75         |
| stats_o/mean                   | 0.20304191 |
| stats_o/std                    | 0.1256534  |
| test/episodes                  | 760        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.066     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -3.639934  |
| test/Q_plus_P                  | -3.639934  |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 30400      |
| train/episodes                 | 3040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.289      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.011     |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.4      |
| train/steps                    | 121600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 76         |
| stats_o/mean                   | 0.20305136 |
| stats_o/std                    | 0.12520465 |
| test/episodes                  | 770        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.637      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000487  |
| test/info_shaping_reward_mean  | -0.0669    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -3.379029  |
| test/Q_plus_P                  | -3.379029  |
| test/reward_per_eps            | -14.5      |
| test/steps                     | 30800      |
| train/episodes                 | 3080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.289      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0101    |
| train/info_shaping_reward_mean | -0.13      |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.4      |
| train/steps                    | 123200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 77         |
| stats_o/mean                   | 0.2030492  |
| stats_o/std                    | 0.12486363 |
| test/episodes                  | 780        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00214   |
| test/info_shaping_reward_mean  | -0.0638    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -3.1158984 |
| test/Q_plus_P                  | -3.1158984 |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 31200      |
| train/episodes                 | 3120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.269      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0117    |
| train/info_shaping_reward_mean | -0.14      |
| train/info_shaping_reward_min  | -0.384     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.2      |
| train/steps                    | 124800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 78         |
| stats_o/mean                   | 0.20310171 |
| stats_o/std                    | 0.12449415 |
| test/episodes                  | 790        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.603      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00309   |
| test/info_shaping_reward_mean  | -0.0732    |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -3.74347   |
| test/Q_plus_P                  | -3.74347   |
| test/reward_per_eps            | -15.9      |
| test/steps                     | 31600      |
| train/episodes                 | 3160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.271      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0105    |
| train/info_shaping_reward_mean | -0.121     |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.2      |
| train/steps                    | 126400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 79          |
| stats_o/mean                   | 0.20317212  |
| stats_o/std                    | 0.123982765 |
| test/episodes                  | 800         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.605       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.071      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -3.4296386  |
| test/Q_plus_P                  | -3.4296386  |
| test/reward_per_eps            | -15.8       |
| test/steps                     | 32000       |
| train/episodes                 | 3200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.292       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00681    |
| train/info_shaping_reward_mean | -0.107      |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.3       |
| train/steps                    | 128000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 80         |
| stats_o/mean                   | 0.20326336 |
| stats_o/std                    | 0.1234727  |
| test/episodes                  | 810        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00181   |
| test/info_shaping_reward_mean  | -0.0691    |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -3.2633154 |
| test/Q_plus_P                  | -3.2633154 |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 32400      |
| train/episodes                 | 3240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.361      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00781   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.6      |
| train/steps                    | 129600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 81         |
| stats_o/mean                   | 0.20326626 |
| stats_o/std                    | 0.12311303 |
| test/episodes                  | 820        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.55       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00276   |
| test/info_shaping_reward_mean  | -0.0788    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -3.6077259 |
| test/Q_plus_P                  | -3.6077259 |
| test/reward_per_eps            | -18        |
| test/steps                     | 32800      |
| train/episodes                 | 3280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.211      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.016     |
| train/info_shaping_reward_mean | -0.12      |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.6      |
| train/steps                    | 131200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.20329371  |
| stats_o/std                    | 0.122913025 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.562       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000527   |
| test/info_shaping_reward_mean  | -0.0787     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -3.7765396  |
| test/Q_plus_P                  | -3.7765396  |
| test/reward_per_eps            | -17.5       |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.188       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0114     |
| train/info_shaping_reward_mean | -0.141      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.5       |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 83         |
| stats_o/mean                   | 0.20331074 |
| stats_o/std                    | 0.12268226 |
| test/episodes                  | 840        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.575      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00209   |
| test/info_shaping_reward_mean  | -0.0737    |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -3.757986  |
| test/Q_plus_P                  | -3.757986  |
| test/reward_per_eps            | -17        |
| test/steps                     | 33600      |
| train/episodes                 | 3360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.286      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0124    |
| train/info_shaping_reward_mean | -0.124     |
| train/info_shaping_reward_min  | -0.294     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.6      |
| train/steps                    | 134400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 84         |
| stats_o/mean                   | 0.20337886 |
| stats_o/std                    | 0.12216869 |
| test/episodes                  | 850        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.65       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00362   |
| test/info_shaping_reward_mean  | -0.0634    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.6357608 |
| test/Q_plus_P                  | -2.6357608 |
| test/reward_per_eps            | -14        |
| test/steps                     | 34000      |
| train/episodes                 | 3400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.341      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0108    |
| train/info_shaping_reward_mean | -0.1       |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.4      |
| train/steps                    | 136000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 85         |
| stats_o/mean                   | 0.20347494 |
| stats_o/std                    | 0.12182238 |
| test/episodes                  | 860        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.613      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00112   |
| test/info_shaping_reward_mean  | -0.0648    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -3.131052  |
| test/Q_plus_P                  | -3.131052  |
| test/reward_per_eps            | -15.5      |
| test/steps                     | 34400      |
| train/episodes                 | 3440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.269      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00948   |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.2      |
| train/steps                    | 137600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 86         |
| stats_o/mean                   | 0.20354865 |
| stats_o/std                    | 0.12141421 |
| test/episodes                  | 870        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.637      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00153   |
| test/info_shaping_reward_mean  | -0.0678    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.020368  |
| test/Q_plus_P                  | -3.020368  |
| test/reward_per_eps            | -14.5      |
| test/steps                     | 34800      |
| train/episodes                 | 3480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.32       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00787   |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.2      |
| train/steps                    | 139200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 87         |
| stats_o/mean                   | 0.20356612 |
| stats_o/std                    | 0.1209917  |
| test/episodes                  | 880        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.64       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00169   |
| test/info_shaping_reward_mean  | -0.0652    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -2.9563758 |
| test/Q_plus_P                  | -2.9563758 |
| test/reward_per_eps            | -14.4      |
| test/steps                     | 35200      |
| train/episodes                 | 3520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.32       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0054    |
| train/info_shaping_reward_mean | -0.106     |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.2      |
| train/steps                    | 140800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 88         |
| stats_o/mean                   | 0.20356053 |
| stats_o/std                    | 0.12054815 |
| test/episodes                  | 890        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.657      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00114   |
| test/info_shaping_reward_mean  | -0.0611    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.748038  |
| test/Q_plus_P                  | -2.748038  |
| test/reward_per_eps            | -13.7      |
| test/steps                     | 35600      |
| train/episodes                 | 3560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.399      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00662   |
| train/info_shaping_reward_mean | -0.0914    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.1      |
| train/steps                    | 142400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 89         |
| stats_o/mean                   | 0.20358402 |
| stats_o/std                    | 0.12009062 |
| test/episodes                  | 900        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.58       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00292   |
| test/info_shaping_reward_mean  | -0.0804    |
| test/info_shaping_reward_min   | -0.419     |
| test/Q                         | -4.2869024 |
| test/Q_plus_P                  | -4.2869024 |
| test/reward_per_eps            | -16.8      |
| test/steps                     | 36000      |
| train/episodes                 | 3600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.363      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00446   |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.5      |
| train/steps                    | 144000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 90          |
| stats_o/mean                   | 0.2035709   |
| stats_o/std                    | 0.119602226 |
| test/episodes                  | 910         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.635       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00325    |
| test/info_shaping_reward_mean  | -0.0662     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -2.7944117  |
| test/Q_plus_P                  | -2.7944117  |
| test/reward_per_eps            | -14.6       |
| test/steps                     | 36400       |
| train/episodes                 | 3640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.431       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00518    |
| train/info_shaping_reward_mean | -0.0871     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.8       |
| train/steps                    | 145600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 91         |
| stats_o/mean                   | 0.20355052 |
| stats_o/std                    | 0.11918949 |
| test/episodes                  | 920        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.615      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00264   |
| test/info_shaping_reward_mean  | -0.0684    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -3.0548916 |
| test/Q_plus_P                  | -3.0548916 |
| test/reward_per_eps            | -15.4      |
| test/steps                     | 36800      |
| train/episodes                 | 3680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.384      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00801   |
| train/info_shaping_reward_mean | -0.0962    |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.6      |
| train/steps                    | 147200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 92         |
| stats_o/mean                   | 0.20360082 |
| stats_o/std                    | 0.11872336 |
| test/episodes                  | 930        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.623      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00309   |
| test/info_shaping_reward_mean  | -0.0686    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.9778678 |
| test/Q_plus_P                  | -2.9778678 |
| test/reward_per_eps            | -15.1      |
| test/steps                     | 37200      |
| train/episodes                 | 3720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.42       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00431   |
| train/info_shaping_reward_mean | -0.0895    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.2      |
| train/steps                    | 148800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.20361446  |
| stats_o/std                    | 0.118357256 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.65        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00193    |
| test/info_shaping_reward_mean  | -0.063      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -2.729745   |
| test/Q_plus_P                  | -2.729745   |
| test/reward_per_eps            | -14         |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.401       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00729    |
| train/info_shaping_reward_mean | -0.0934     |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24         |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 94         |
| stats_o/mean                   | 0.203638   |
| stats_o/std                    | 0.11797571 |
| test/episodes                  | 950        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.642      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.0647    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.7078047 |
| test/Q_plus_P                  | -2.7078047 |
| test/reward_per_eps            | -14.3      |
| test/steps                     | 38000      |
| train/episodes                 | 3800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.419      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00657   |
| train/info_shaping_reward_mean | -0.0959    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.2      |
| train/steps                    | 152000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 95         |
| stats_o/mean                   | 0.20363195 |
| stats_o/std                    | 0.11757081 |
| test/episodes                  | 960        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.652      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00127   |
| test/info_shaping_reward_mean  | -0.0631    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.637025  |
| test/Q_plus_P                  | -2.637025  |
| test/reward_per_eps            | -13.9      |
| test/steps                     | 38400      |
| train/episodes                 | 3840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.414      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00475   |
| train/info_shaping_reward_mean | -0.0942    |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.4      |
| train/steps                    | 153600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 96          |
| stats_o/mean                   | 0.20365348  |
| stats_o/std                    | 0.117092274 |
| test/episodes                  | 970         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.667       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00191    |
| test/info_shaping_reward_mean  | -0.0618     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -2.558568   |
| test/Q_plus_P                  | -2.558568   |
| test/reward_per_eps            | -13.3       |
| test/steps                     | 38800       |
| train/episodes                 | 3880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.461       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00472    |
| train/info_shaping_reward_mean | -0.0889     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.6       |
| train/steps                    | 155200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 97         |
| stats_o/mean                   | 0.20361538 |
| stats_o/std                    | 0.11664854 |
| test/episodes                  | 980        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00213   |
| test/info_shaping_reward_mean  | -0.0595    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -2.5436792 |
| test/Q_plus_P                  | -2.5436792 |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 39200      |
| train/episodes                 | 3920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.373      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00534   |
| train/info_shaping_reward_mean | -0.0904    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.1      |
| train/steps                    | 156800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 98         |
| stats_o/mean                   | 0.2036319  |
| stats_o/std                    | 0.1162856  |
| test/episodes                  | 990        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.623      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00087   |
| test/info_shaping_reward_mean  | -0.0669    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.7736752 |
| test/Q_plus_P                  | -2.7736752 |
| test/reward_per_eps            | -15.1      |
| test/steps                     | 39600      |
| train/episodes                 | 3960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.428      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00511   |
| train/info_shaping_reward_mean | -0.0916    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.9      |
| train/steps                    | 158400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.20364457 |
| stats_o/std                    | 0.11589932 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.0616    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -2.6753428 |
| test/Q_plus_P                  | -2.6753428 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.403      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00596   |
| train/info_shaping_reward_mean | -0.0973    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.9      |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 100        |
| stats_o/mean                   | 0.20367132 |
| stats_o/std                    | 0.11549525 |
| test/episodes                  | 1010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.67       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.062     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.2888093 |
| test/Q_plus_P                  | -2.2888093 |
| test/reward_per_eps            | -13.2      |
| test/steps                     | 40400      |
| train/episodes                 | 4040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.472      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00542   |
| train/info_shaping_reward_mean | -0.0882    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 161600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.20367268  |
| stats_o/std                    | 0.115062386 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.623       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000885   |
| test/info_shaping_reward_mean  | -0.0693     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -2.8141222  |
| test/Q_plus_P                  | -2.8141222  |
| test/reward_per_eps            | -15.1       |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.411       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00377    |
| train/info_shaping_reward_mean | -0.0913     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.6       |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 102        |
| stats_o/mean                   | 0.20364869 |
| stats_o/std                    | 0.11472684 |
| test/episodes                  | 1030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0641    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -2.6464324 |
| test/Q_plus_P                  | -2.6464324 |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 41200      |
| train/episodes                 | 4120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.415      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00508   |
| train/info_shaping_reward_mean | -0.0957    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.4      |
| train/steps                    | 164800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 103        |
| stats_o/mean                   | 0.2036002  |
| stats_o/std                    | 0.11436343 |
| test/episodes                  | 1040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00142   |
| test/info_shaping_reward_mean  | -0.0634    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.462169  |
| test/Q_plus_P                  | -2.462169  |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 41600      |
| train/episodes                 | 4160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.468      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00381   |
| train/info_shaping_reward_mean | -0.0845    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.3      |
| train/steps                    | 166400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 104        |
| stats_o/mean                   | 0.20363319 |
| stats_o/std                    | 0.11403658 |
| test/episodes                  | 1050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000866  |
| test/info_shaping_reward_mean  | -0.0612    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.3501616 |
| test/Q_plus_P                  | -2.3501616 |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 42000      |
| train/episodes                 | 4200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.43       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00576   |
| train/info_shaping_reward_mean | -0.0911    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.8      |
| train/steps                    | 168000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 105        |
| stats_o/mean                   | 0.20362854 |
| stats_o/std                    | 0.11375067 |
| test/episodes                  | 1060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000901  |
| test/info_shaping_reward_mean  | -0.0595    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.246505  |
| test/Q_plus_P                  | -2.246505  |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 42400      |
| train/episodes                 | 4240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.371      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00465   |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.1      |
| train/steps                    | 169600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 106         |
| stats_o/mean                   | 0.20361593  |
| stats_o/std                    | 0.113372765 |
| test/episodes                  | 1070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00218    |
| test/info_shaping_reward_mean  | -0.058      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -2.1933396  |
| test/Q_plus_P                  | -2.1933396  |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 42800       |
| train/episodes                 | 4280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.476       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00608    |
| train/info_shaping_reward_mean | -0.0816     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21         |
| train/steps                    | 171200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 107        |
| stats_o/mean                   | 0.20360914 |
| stats_o/std                    | 0.11296562 |
| test/episodes                  | 1080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.64       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.0673    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -2.5324895 |
| test/Q_plus_P                  | -2.5324895 |
| test/reward_per_eps            | -14.4      |
| test/steps                     | 43200      |
| train/episodes                 | 4320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.469      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00738   |
| train/info_shaping_reward_mean | -0.0867    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 172800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 108        |
| stats_o/mean                   | 0.20362961 |
| stats_o/std                    | 0.11268407 |
| test/episodes                  | 1090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.627      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00112   |
| test/info_shaping_reward_mean  | -0.0658    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.5544925 |
| test/Q_plus_P                  | -2.5544925 |
| test/reward_per_eps            | -14.9      |
| test/steps                     | 43600      |
| train/episodes                 | 4360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.428      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00471   |
| train/info_shaping_reward_mean | -0.0876    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.9      |
| train/steps                    | 174400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 109        |
| stats_o/mean                   | 0.20363912 |
| stats_o/std                    | 0.11232706 |
| test/episodes                  | 1100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00113   |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.154882  |
| test/Q_plus_P                  | -2.154882  |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 44000      |
| train/episodes                 | 4400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.388      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00897   |
| train/info_shaping_reward_mean | -0.0904    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.5      |
| train/steps                    | 176000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 110         |
| stats_o/mean                   | 0.20360826  |
| stats_o/std                    | 0.111963406 |
| test/episodes                  | 1110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.662       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00135    |
| test/info_shaping_reward_mean  | -0.0613     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -2.3497214  |
| test/Q_plus_P                  | -2.3497214  |
| test/reward_per_eps            | -13.5       |
| test/steps                     | 44400       |
| train/episodes                 | 4440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.435       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00581    |
| train/info_shaping_reward_mean | -0.0868     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.6       |
| train/steps                    | 177600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 111        |
| stats_o/mean                   | 0.2036073  |
| stats_o/std                    | 0.11163801 |
| test/episodes                  | 1120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00172   |
| test/info_shaping_reward_mean  | -0.0577    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.166672  |
| test/Q_plus_P                  | -2.166672  |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 44800      |
| train/episodes                 | 4480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.486      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.0897    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 179200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 112        |
| stats_o/mean                   | 0.20360254 |
| stats_o/std                    | 0.11127173 |
| test/episodes                  | 1130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00199   |
| test/info_shaping_reward_mean  | -0.0571    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.125979  |
| test/Q_plus_P                  | -2.125979  |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 45200      |
| train/episodes                 | 4520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.449      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00462   |
| train/info_shaping_reward_mean | -0.0839    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22        |
| train/steps                    | 180800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 113        |
| stats_o/mean                   | 0.20360647 |
| stats_o/std                    | 0.11094341 |
| test/episodes                  | 1140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00067   |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.0917144 |
| test/Q_plus_P                  | -2.0917144 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 45600      |
| train/episodes                 | 4560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.476      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0053    |
| train/info_shaping_reward_mean | -0.0862    |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21        |
| train/steps                    | 182400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 114        |
| stats_o/mean                   | 0.20360163 |
| stats_o/std                    | 0.11067035 |
| test/episodes                  | 1150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.603      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00215   |
| test/info_shaping_reward_mean  | -0.0675    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.9431875 |
| test/Q_plus_P                  | -2.9431875 |
| test/reward_per_eps            | -15.9      |
| test/steps                     | 46000      |
| train/episodes                 | 4600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.476      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00496   |
| train/info_shaping_reward_mean | -0.0829    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 184000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 115        |
| stats_o/mean                   | 0.20357493 |
| stats_o/std                    | 0.11041396 |
| test/episodes                  | 1160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.645      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00107   |
| test/info_shaping_reward_mean  | -0.063     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.468543  |
| test/Q_plus_P                  | -2.468543  |
| test/reward_per_eps            | -14.2      |
| test/steps                     | 46400      |
| train/episodes                 | 4640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.448      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0042    |
| train/info_shaping_reward_mean | -0.0966    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.1      |
| train/steps                    | 185600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 116         |
| stats_o/mean                   | 0.20356934  |
| stats_o/std                    | 0.110082656 |
| test/episodes                  | 1170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.65        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00218    |
| test/info_shaping_reward_mean  | -0.0653     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.5303805  |
| test/Q_plus_P                  | -2.5303805  |
| test/reward_per_eps            | -14         |
| test/steps                     | 46800       |
| train/episodes                 | 4680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.524       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00506    |
| train/info_shaping_reward_mean | -0.0792     |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 187200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 117        |
| stats_o/mean                   | 0.20358515 |
| stats_o/std                    | 0.10988096 |
| test/episodes                  | 1180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.605      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00442   |
| test/info_shaping_reward_mean  | -0.0754    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -2.654818  |
| test/Q_plus_P                  | -2.654818  |
| test/reward_per_eps            | -15.8      |
| test/steps                     | 47200      |
| train/episodes                 | 4720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.409      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00764   |
| train/info_shaping_reward_mean | -0.0994    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.6      |
| train/steps                    | 188800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 118        |
| stats_o/mean                   | 0.20363922 |
| stats_o/std                    | 0.10957625 |
| test/episodes                  | 1190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.0694    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.7686443 |
| test/Q_plus_P                  | -2.7686443 |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 47600      |
| train/episodes                 | 4760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.411      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00671   |
| train/info_shaping_reward_mean | -0.0893    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.6      |
| train/steps                    | 190400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 119        |
| stats_o/mean                   | 0.2036212  |
| stats_o/std                    | 0.10931094 |
| test/episodes                  | 1200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00217   |
| test/info_shaping_reward_mean  | -0.0647    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.513001  |
| test/Q_plus_P                  | -2.513001  |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 48000      |
| train/episodes                 | 4800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.436      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00569   |
| train/info_shaping_reward_mean | -0.0923    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.6      |
| train/steps                    | 192000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 120        |
| stats_o/mean                   | 0.2035889  |
| stats_o/std                    | 0.10900107 |
| test/episodes                  | 1210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.652      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.003     |
| test/info_shaping_reward_mean  | -0.0641    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.4317646 |
| test/Q_plus_P                  | -2.4317646 |
| test/reward_per_eps            | -13.9      |
| test/steps                     | 48400      |
| train/episodes                 | 4840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.505      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00534   |
| train/info_shaping_reward_mean | -0.0792    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 193600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.20361276 |
| stats_o/std                    | 0.10867856 |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00303   |
| test/info_shaping_reward_mean  | -0.0638    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.608285  |
| test/Q_plus_P                  | -2.608285  |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.514      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00397   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 122         |
| stats_o/mean                   | 0.20361145  |
| stats_o/std                    | 0.108361386 |
| test/episodes                  | 1230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.652       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00174    |
| test/info_shaping_reward_mean  | -0.0629     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.4450257  |
| test/Q_plus_P                  | -2.4450257  |
| test/reward_per_eps            | -13.9       |
| test/steps                     | 49200       |
| train/episodes                 | 4920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.5         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00467    |
| train/info_shaping_reward_mean | -0.0849     |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20         |
| train/steps                    | 196800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 123        |
| stats_o/mean                   | 0.20359313 |
| stats_o/std                    | 0.10802842 |
| test/episodes                  | 1240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00203   |
| test/info_shaping_reward_mean  | -0.0587    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.1582918 |
| test/Q_plus_P                  | -2.1582918 |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 49600      |
| train/episodes                 | 4960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.461      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00412   |
| train/info_shaping_reward_mean | -0.0845    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 198400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 124        |
| stats_o/mean                   | 0.20357913 |
| stats_o/std                    | 0.10769517 |
| test/episodes                  | 1250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00241   |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.8788959 |
| test/Q_plus_P                  | -1.8788959 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 50000      |
| train/episodes                 | 5000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.543      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00337   |
| train/info_shaping_reward_mean | -0.0782    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 200000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 125        |
| stats_o/mean                   | 0.20360574 |
| stats_o/std                    | 0.10742658 |
| test/episodes                  | 1260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0012    |
| test/info_shaping_reward_mean  | -0.0569    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.8311592 |
| test/Q_plus_P                  | -1.8311592 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 50400      |
| train/episodes                 | 5040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.48       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00541   |
| train/info_shaping_reward_mean | -0.0908    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 201600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 126         |
| stats_o/mean                   | 0.2036212   |
| stats_o/std                    | 0.107184276 |
| test/episodes                  | 1270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.693       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00195    |
| test/info_shaping_reward_mean  | -0.0567     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.911989   |
| test/Q_plus_P                  | -1.911989   |
| test/reward_per_eps            | -12.3       |
| test/steps                     | 50800       |
| train/episodes                 | 5080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.488       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00507    |
| train/info_shaping_reward_mean | -0.0939     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.5       |
| train/steps                    | 203200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 127        |
| stats_o/mean                   | 0.20363674 |
| stats_o/std                    | 0.10687521 |
| test/episodes                  | 1280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00116   |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.915343  |
| test/Q_plus_P                  | -1.915343  |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 51200      |
| train/episodes                 | 5120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.544      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00569   |
| train/info_shaping_reward_mean | -0.0812    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 204800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.20366126 |
| stats_o/std                    | 0.10664725 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.652      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00153   |
| test/info_shaping_reward_mean  | -0.0617    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.203747  |
| test/Q_plus_P                  | -2.203747  |
| test/reward_per_eps            | -13.9      |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.493      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00388   |
| train/info_shaping_reward_mean | -0.0896    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.3      |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 129        |
| stats_o/mean                   | 0.2036621  |
| stats_o/std                    | 0.10645071 |
| test/episodes                  | 1300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.65       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0017    |
| test/info_shaping_reward_mean  | -0.0627    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -2.2431543 |
| test/Q_plus_P                  | -2.2431543 |
| test/reward_per_eps            | -14        |
| test/steps                     | 52000      |
| train/episodes                 | 5200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.477      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0054    |
| train/info_shaping_reward_mean | -0.0866    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 208000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 130        |
| stats_o/mean                   | 0.20368908 |
| stats_o/std                    | 0.10618118 |
| test/episodes                  | 1310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.65       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00239   |
| test/info_shaping_reward_mean  | -0.0634    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.3238585 |
| test/Q_plus_P                  | -2.3238585 |
| test/reward_per_eps            | -14        |
| test/steps                     | 52400      |
| train/episodes                 | 5240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00346   |
| train/info_shaping_reward_mean | -0.0836    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 209600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 131         |
| stats_o/mean                   | 0.20370162  |
| stats_o/std                    | 0.105901584 |
| test/episodes                  | 1320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.625       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00177    |
| test/info_shaping_reward_mean  | -0.0676     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -2.5586574  |
| test/Q_plus_P                  | -2.5586574  |
| test/reward_per_eps            | -15         |
| test/steps                     | 52800       |
| train/episodes                 | 5280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.497       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0047     |
| train/info_shaping_reward_mean | -0.0831     |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 211200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 132        |
| stats_o/mean                   | 0.20366159 |
| stats_o/std                    | 0.10570795 |
| test/episodes                  | 1330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00093   |
| test/info_shaping_reward_mean  | -0.0624    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.1775336 |
| test/Q_plus_P                  | -2.1775336 |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 53200      |
| train/episodes                 | 5320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.475      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0057    |
| train/info_shaping_reward_mean | -0.0929    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21        |
| train/steps                    | 212800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 133         |
| stats_o/mean                   | 0.20368962  |
| stats_o/std                    | 0.105464354 |
| test/episodes                  | 1340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000862   |
| test/info_shaping_reward_mean  | -0.0573     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -2.131512   |
| test/Q_plus_P                  | -2.131512   |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 53600       |
| train/episodes                 | 5360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.514       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00419    |
| train/info_shaping_reward_mean | -0.0807     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.4       |
| train/steps                    | 214400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 134         |
| stats_o/mean                   | 0.20371996  |
| stats_o/std                    | 0.105271466 |
| test/episodes                  | 1350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00174    |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -2.0362766  |
| test/Q_plus_P                  | -2.0362766  |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 54000       |
| train/episodes                 | 5400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.478       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00501    |
| train/info_shaping_reward_mean | -0.0996     |
| train/info_shaping_reward_min  | -0.285      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.9       |
| train/steps                    | 216000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 135        |
| stats_o/mean                   | 0.20373507 |
| stats_o/std                    | 0.10502254 |
| test/episodes                  | 1360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.0582    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.9541736 |
| test/Q_plus_P                  | -1.9541736 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 54400      |
| train/episodes                 | 5440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.454      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0057    |
| train/info_shaping_reward_mean | -0.0908    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 217600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 136        |
| stats_o/mean                   | 0.20376384 |
| stats_o/std                    | 0.10474157 |
| test/episodes                  | 1370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00241   |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.625544  |
| test/Q_plus_P                  | -1.625544  |
| test/reward_per_eps            | -11        |
| test/steps                     | 54800      |
| train/episodes                 | 5480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.502      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00535   |
| train/info_shaping_reward_mean | -0.0809    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 219200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 137        |
| stats_o/mean                   | 0.20378582 |
| stats_o/std                    | 0.10452368 |
| test/episodes                  | 1380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.625      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00236   |
| test/info_shaping_reward_mean  | -0.0687    |
| test/info_shaping_reward_min   | -0.203     |
| test/Q                         | -2.4374938 |
| test/Q_plus_P                  | -2.4374938 |
| test/reward_per_eps            | -15        |
| test/steps                     | 55200      |
| train/episodes                 | 5520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.507      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00461   |
| train/info_shaping_reward_mean | -0.0804    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.7      |
| train/steps                    | 220800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.20381247 |
| stats_o/std                    | 0.10425702 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.657      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00155   |
| test/info_shaping_reward_mean  | -0.0617    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.1854806 |
| test/Q_plus_P                  | -2.1854806 |
| test/reward_per_eps            | -13.7      |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.498      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00395   |
| train/info_shaping_reward_mean | -0.0818    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.20381862 |
| stats_o/std                    | 0.1039741  |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00138   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.7078645 |
| test/Q_plus_P                  | -1.7078645 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.0762    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 140         |
| stats_o/mean                   | 0.20381707  |
| stats_o/std                    | 0.103686005 |
| test/episodes                  | 1410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.637       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00182    |
| test/info_shaping_reward_mean  | -0.0654     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -2.2499955  |
| test/Q_plus_P                  | -2.2499955  |
| test/reward_per_eps            | -14.5       |
| test/steps                     | 56400       |
| train/episodes                 | 5640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.561       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00463    |
| train/info_shaping_reward_mean | -0.0733     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 225600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.20384714 |
| stats_o/std                    | 0.10352963 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00224   |
| test/info_shaping_reward_mean  | -0.0578    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.021034  |
| test/Q_plus_P                  | -2.021034  |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0898    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 142        |
| stats_o/mean                   | 0.20384517 |
| stats_o/std                    | 0.10335742 |
| test/episodes                  | 1430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.637      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00252   |
| test/info_shaping_reward_mean  | -0.0655    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.513374  |
| test/Q_plus_P                  | -2.513374  |
| test/reward_per_eps            | -14.5      |
| test/steps                     | 57200      |
| train/episodes                 | 5720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.504      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00446   |
| train/info_shaping_reward_mean | -0.0872    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 228800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 143        |
| stats_o/mean                   | 0.20383522 |
| stats_o/std                    | 0.10314547 |
| test/episodes                  | 1440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -1.550067  |
| test/Q_plus_P                  | -1.550067  |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 57600      |
| train/episodes                 | 5760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.538      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00482   |
| train/info_shaping_reward_mean | -0.0837    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 230400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.2038814   |
| stats_o/std                    | 0.102925666 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00156    |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.8573152  |
| test/Q_plus_P                  | -1.8573152  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.507       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00531    |
| train/info_shaping_reward_mean | -0.0804     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.7       |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 145         |
| stats_o/mean                   | 0.20387915  |
| stats_o/std                    | 0.102779604 |
| test/episodes                  | 1460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00156    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.5124917  |
| test/Q_plus_P                  | -1.5124917  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 58400       |
| train/episodes                 | 5840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.494       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00497    |
| train/info_shaping_reward_mean | -0.085      |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.2       |
| train/steps                    | 233600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 146         |
| stats_o/mean                   | 0.20387056  |
| stats_o/std                    | 0.102546446 |
| test/episodes                  | 1470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00173    |
| test/info_shaping_reward_mean  | -0.0573     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.8342979  |
| test/Q_plus_P                  | -1.8342979  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 58800       |
| train/episodes                 | 5880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.544       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00509    |
| train/info_shaping_reward_mean | -0.0785     |
| train/info_shaping_reward_min  | -0.202      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 235200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 147        |
| stats_o/mean                   | 0.20387632 |
| stats_o/std                    | 0.10240056 |
| test/episodes                  | 1480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.67       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000892  |
| test/info_shaping_reward_mean  | -0.06      |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -2.1209993 |
| test/Q_plus_P                  | -2.1209993 |
| test/reward_per_eps            | -13.2      |
| test/steps                     | 59200      |
| train/episodes                 | 5920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0879    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 236800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 148        |
| stats_o/mean                   | 0.20388119 |
| stats_o/std                    | 0.10218132 |
| test/episodes                  | 1490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00403   |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.725037  |
| test/Q_plus_P                  | -1.725037  |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 59600      |
| train/episodes                 | 5960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.543      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00553   |
| train/info_shaping_reward_mean | -0.0747    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 238400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 149        |
| stats_o/mean                   | 0.20391256 |
| stats_o/std                    | 0.10197339 |
| test/episodes                  | 1500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00209   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5698974 |
| test/Q_plus_P                  | -1.5698974 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 60000      |
| train/episodes                 | 6000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.527      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00427   |
| train/info_shaping_reward_mean | -0.0835    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 240000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 150        |
| stats_o/mean                   | 0.20390993 |
| stats_o/std                    | 0.10177409 |
| test/episodes                  | 1510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00178   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.7606723 |
| test/Q_plus_P                  | -1.7606723 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 60400      |
| train/episodes                 | 6040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.488      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0864    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.5      |
| train/steps                    | 241600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 151         |
| stats_o/mean                   | 0.20392227  |
| stats_o/std                    | 0.101553656 |
| test/episodes                  | 1520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00145    |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.4662536  |
| test/Q_plus_P                  | -1.4662536  |
| test/reward_per_eps            | -11         |
| test/steps                     | 60800       |
| train/episodes                 | 6080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.551       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.0789     |
| train/info_shaping_reward_min  | -0.194      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18         |
| train/steps                    | 243200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 152         |
| stats_o/mean                   | 0.20394997  |
| stats_o/std                    | 0.101434745 |
| test/episodes                  | 1530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00052    |
| test/info_shaping_reward_mean  | -0.0572     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.8322197  |
| test/Q_plus_P                  | -1.8322197  |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 61200       |
| train/episodes                 | 6120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.489       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00433    |
| train/info_shaping_reward_mean | -0.0914     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.4       |
| train/steps                    | 244800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 153        |
| stats_o/mean                   | 0.20393887 |
| stats_o/std                    | 0.10126675 |
| test/episodes                  | 1540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00235   |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.6330085 |
| test/Q_plus_P                  | -1.6330085 |
| test/reward_per_eps            | -12        |
| test/steps                     | 61600      |
| train/episodes                 | 6160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.518      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00556   |
| train/info_shaping_reward_mean | -0.0853    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 246400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.2039378  |
| stats_o/std                    | 0.10109987 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00135   |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.7514427 |
| test/Q_plus_P                  | -1.7514427 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.538      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00448   |
| train/info_shaping_reward_mean | -0.0895    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.20392638 |
| stats_o/std                    | 0.10087947 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00043   |
| test/info_shaping_reward_mean  | -0.0578    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.7452263 |
| test/Q_plus_P                  | -1.7452263 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00493   |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 156         |
| stats_o/mean                   | 0.20394373  |
| stats_o/std                    | 0.100654565 |
| test/episodes                  | 1570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0015     |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.7382716  |
| test/Q_plus_P                  | -1.7382716  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 62800       |
| train/episodes                 | 6280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.571       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00423    |
| train/info_shaping_reward_mean | -0.0753     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 251200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 157         |
| stats_o/mean                   | 0.20392679  |
| stats_o/std                    | 0.100466006 |
| test/episodes                  | 1580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.632       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00232    |
| test/info_shaping_reward_mean  | -0.0638     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -3.0999842  |
| test/Q_plus_P                  | -3.0999842  |
| test/reward_per_eps            | -14.7       |
| test/steps                     | 63200       |
| train/episodes                 | 6320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.535       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00529    |
| train/info_shaping_reward_mean | -0.0745     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 252800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.20393261  |
| stats_o/std                    | 0.100263625 |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.667       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.0612     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.9340243  |
| test/Q_plus_P                  | -1.9340243  |
| test/reward_per_eps            | -13.3       |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.566       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0038     |
| train/info_shaping_reward_mean | -0.0706     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.20393644  |
| stats_o/std                    | 0.100018464 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00169    |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.596727   |
| test/Q_plus_P                  | -1.596727   |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.563       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00401    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 160        |
| stats_o/mean                   | 0.20395061 |
| stats_o/std                    | 0.09983938 |
| test/episodes                  | 1610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00196   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.7042031 |
| test/Q_plus_P                  | -1.7042031 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 64400      |
| train/episodes                 | 6440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.544      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0041    |
| train/info_shaping_reward_mean | -0.0738    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 257600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.20394224  |
| stats_o/std                    | 0.099652395 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.0579     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.7129469  |
| test/Q_plus_P                  | -1.7129469  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.498       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00506    |
| train/info_shaping_reward_mean | -0.081      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 162        |
| stats_o/mean                   | 0.20399398 |
| stats_o/std                    | 0.09951158 |
| test/episodes                  | 1630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.67       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0012    |
| test/info_shaping_reward_mean  | -0.0596    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.9663011 |
| test/Q_plus_P                  | -1.9663011 |
| test/reward_per_eps            | -13.2      |
| test/steps                     | 65200      |
| train/episodes                 | 6520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.473      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00492   |
| train/info_shaping_reward_mean | -0.0917    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 260800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 163        |
| stats_o/mean                   | 0.20397991 |
| stats_o/std                    | 0.09936025 |
| test/episodes                  | 1640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.0594    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.9087052 |
| test/Q_plus_P                  | -1.9087052 |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 65600      |
| train/episodes                 | 6560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.5        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00448   |
| train/info_shaping_reward_mean | -0.087     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20        |
| train/steps                    | 262400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 164        |
| stats_o/mean                   | 0.20397379 |
| stats_o/std                    | 0.09912657 |
| test/episodes                  | 1650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00186   |
| test/info_shaping_reward_mean  | -0.0611    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.9184079 |
| test/Q_plus_P                  | -1.9184079 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 66000      |
| train/episodes                 | 6600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00405   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 264000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 165        |
| stats_o/mean                   | 0.20398107 |
| stats_o/std                    | 0.09896187 |
| test/episodes                  | 1660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00194   |
| test/info_shaping_reward_mean  | -0.0622    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.9952773 |
| test/Q_plus_P                  | -1.9952773 |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 66400      |
| train/episodes                 | 6640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0041    |
| train/info_shaping_reward_mean | -0.0842    |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 265600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 166        |
| stats_o/mean                   | 0.20396473 |
| stats_o/std                    | 0.09889738 |
| test/episodes                  | 1670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00186   |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.77996   |
| test/Q_plus_P                  | -1.77996   |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 66800      |
| train/episodes                 | 6680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.484      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00522   |
| train/info_shaping_reward_mean | -0.0938    |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 267200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.20395839 |
| stats_o/std                    | 0.09867386 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00229   |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6937336 |
| test/Q_plus_P                  | -1.6937336 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00477   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 168         |
| stats_o/mean                   | 0.20395319  |
| stats_o/std                    | 0.098510884 |
| test/episodes                  | 1690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00236    |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.6973011  |
| test/Q_plus_P                  | -1.6973011  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 67600       |
| train/episodes                 | 6760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.557       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00488    |
| train/info_shaping_reward_mean | -0.081      |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 270400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.20396891 |
| stats_o/std                    | 0.09837869 |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00146   |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.6416377 |
| test/Q_plus_P                  | -1.6416377 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.534      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00573   |
| train/info_shaping_reward_mean | -0.0789    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 170        |
| stats_o/mean                   | 0.20395023 |
| stats_o/std                    | 0.09822509 |
| test/episodes                  | 1710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.819395  |
| test/Q_plus_P                  | -1.819395  |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 68400      |
| train/episodes                 | 6840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00465   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 273600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 171        |
| stats_o/mean                   | 0.20394592 |
| stats_o/std                    | 0.09803963 |
| test/episodes                  | 1720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00267   |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6660943 |
| test/Q_plus_P                  | -1.6660943 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 68800      |
| train/episodes                 | 6880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00348   |
| train/info_shaping_reward_mean | -0.0854    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 275200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 172        |
| stats_o/mean                   | 0.20393199 |
| stats_o/std                    | 0.09787661 |
| test/episodes                  | 1730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00106   |
| test/info_shaping_reward_mean  | -0.0609    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.0063183 |
| test/Q_plus_P                  | -2.0063183 |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 69200      |
| train/episodes                 | 6920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.547      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00436   |
| train/info_shaping_reward_mean | -0.0776    |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 276800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 173         |
| stats_o/mean                   | 0.20393836  |
| stats_o/std                    | 0.097656205 |
| test/episodes                  | 1740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0585     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.7370006  |
| test/Q_plus_P                  | -1.7370006  |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 69600       |
| train/episodes                 | 6960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.575       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0721     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 278400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 174        |
| stats_o/mean                   | 0.20395413 |
| stats_o/std                    | 0.09748608 |
| test/episodes                  | 1750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000436  |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.7432644 |
| test/Q_plus_P                  | -1.7432644 |
| test/reward_per_eps            | -12        |
| test/steps                     | 70000      |
| train/episodes                 | 7000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00453   |
| train/info_shaping_reward_mean | -0.0774    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 280000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 175        |
| stats_o/mean                   | 0.20396362 |
| stats_o/std                    | 0.09731866 |
| test/episodes                  | 1760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00132   |
| test/info_shaping_reward_mean  | -0.0565    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6936274 |
| test/Q_plus_P                  | -1.6936274 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 70400      |
| train/episodes                 | 7040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.548      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 281600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 176        |
| stats_o/mean                   | 0.2039835  |
| stats_o/std                    | 0.09717349 |
| test/episodes                  | 1770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000961  |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.5421408 |
| test/Q_plus_P                  | -1.5421408 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 70800      |
| train/episodes                 | 7080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.543      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00523   |
| train/info_shaping_reward_mean | -0.0744    |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 283200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 177        |
| stats_o/mean                   | 0.20400152 |
| stats_o/std                    | 0.09709814 |
| test/episodes                  | 1780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00216   |
| test/info_shaping_reward_mean  | -0.0531    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5152508 |
| test/Q_plus_P                  | -1.5152508 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 71200      |
| train/episodes                 | 7120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.51       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00488   |
| train/info_shaping_reward_mean | -0.0909    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 284800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.2040152  |
| stats_o/std                    | 0.09693642 |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00234   |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5523522 |
| test/Q_plus_P                  | -1.5523522 |
| test/reward_per_eps            | -11        |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00428   |
| train/info_shaping_reward_mean | -0.073     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 179         |
| stats_o/mean                   | 0.20403601  |
| stats_o/std                    | 0.096792676 |
| test/episodes                  | 1800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.0579     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.7585143  |
| test/Q_plus_P                  | -1.7585143  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 72000       |
| train/episodes                 | 7200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.536       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00458    |
| train/info_shaping_reward_mean | -0.0753     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 288000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 180        |
| stats_o/mean                   | 0.20402735 |
| stats_o/std                    | 0.09661307 |
| test/episodes                  | 1810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00125   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.7283124 |
| test/Q_plus_P                  | -1.7283124 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 72400      |
| train/episodes                 | 7240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.513      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0057    |
| train/info_shaping_reward_mean | -0.0795    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.5      |
| train/steps                    | 289600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 181        |
| stats_o/mean                   | 0.20405969 |
| stats_o/std                    | 0.0965168  |
| test/episodes                  | 1820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00138   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4771686 |
| test/Q_plus_P                  | -1.4771686 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 72800      |
| train/episodes                 | 7280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.527      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00571   |
| train/info_shaping_reward_mean | -0.0829    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 291200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 182        |
| stats_o/mean                   | 0.20407723 |
| stats_o/std                    | 0.09634564 |
| test/episodes                  | 1830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00247   |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5291325 |
| test/Q_plus_P                  | -1.5291325 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 73200      |
| train/episodes                 | 7320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00549   |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 292800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 183         |
| stats_o/mean                   | 0.20405024  |
| stats_o/std                    | 0.096228816 |
| test/episodes                  | 1840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0017     |
| test/info_shaping_reward_mean  | -0.055      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.58475    |
| test/Q_plus_P                  | -1.58475    |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 73600       |
| train/episodes                 | 7360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.549       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00481    |
| train/info_shaping_reward_mean | -0.0738     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 294400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 184        |
| stats_o/mean                   | 0.20406435 |
| stats_o/std                    | 0.0961068  |
| test/episodes                  | 1850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00157   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5250188 |
| test/Q_plus_P                  | -1.5250188 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 74000      |
| train/episodes                 | 7400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.535      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00492   |
| train/info_shaping_reward_mean | -0.0855    |
| train/info_shaping_reward_min  | -0.307     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 296000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 185         |
| stats_o/mean                   | 0.20407991  |
| stats_o/std                    | 0.095936865 |
| test/episodes                  | 1860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00216    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.2911782  |
| test/Q_plus_P                  | -1.2911782  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 74400       |
| train/episodes                 | 7440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00464    |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 297600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 186        |
| stats_o/mean                   | 0.2040931  |
| stats_o/std                    | 0.09586604 |
| test/episodes                  | 1870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0011    |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.8406938 |
| test/Q_plus_P                  | -1.8406938 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 74800      |
| train/episodes                 | 7480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.538      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00438   |
| train/info_shaping_reward_mean | -0.0839    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 299200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.20410308 |
| stats_o/std                    | 0.09567209 |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00181   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4585937 |
| test/Q_plus_P                  | -1.4585937 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00482   |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 188        |
| stats_o/mean                   | 0.2040991  |
| stats_o/std                    | 0.09551089 |
| test/episodes                  | 1890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0022    |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3356717 |
| test/Q_plus_P                  | -1.3356717 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 75600      |
| train/episodes                 | 7560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.55       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00401   |
| train/info_shaping_reward_mean | -0.0812    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 302400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 189        |
| stats_o/mean                   | 0.20408235 |
| stats_o/std                    | 0.09536042 |
| test/episodes                  | 1900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5081041 |
| test/Q_plus_P                  | -1.5081041 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 76000      |
| train/episodes                 | 7600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00424   |
| train/info_shaping_reward_mean | -0.0739    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 304000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 190        |
| stats_o/mean                   | 0.2041057  |
| stats_o/std                    | 0.09524302 |
| test/episodes                  | 1910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00345   |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.4895707 |
| test/Q_plus_P                  | -1.4895707 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 76400      |
| train/episodes                 | 7640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.565      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00428   |
| train/info_shaping_reward_mean | -0.0816    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 305600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 191         |
| stats_o/mean                   | 0.20408821  |
| stats_o/std                    | 0.095121786 |
| test/episodes                  | 1920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00223    |
| test/info_shaping_reward_mean  | -0.0595     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.6687719  |
| test/Q_plus_P                  | -1.6687719  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 76800       |
| train/episodes                 | 7680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.534       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00473    |
| train/info_shaping_reward_mean | -0.0847     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 307200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.20408331  |
| stats_o/std                    | 0.094948895 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00523    |
| test/info_shaping_reward_mean  | -0.0591     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.6605101  |
| test/Q_plus_P                  | -1.6605101  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00399    |
| train/info_shaping_reward_mean | -0.0708     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 193         |
| stats_o/mean                   | 0.20408931  |
| stats_o/std                    | 0.094946645 |
| test/episodes                  | 1940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5546308  |
| test/Q_plus_P                  | -1.5546308  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 77600       |
| train/episodes                 | 7760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.484       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00437    |
| train/info_shaping_reward_mean | -0.0996     |
| train/info_shaping_reward_min  | -0.296      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.6       |
| train/steps                    | 310400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 194        |
| stats_o/mean                   | 0.2040908  |
| stats_o/std                    | 0.09479598 |
| test/episodes                  | 1950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00436   |
| test/info_shaping_reward_mean  | -0.0622    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.6235528 |
| test/Q_plus_P                  | -1.6235528 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 78000      |
| train/episodes                 | 7800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.0733    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 312000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 195        |
| stats_o/mean                   | 0.20410267 |
| stats_o/std                    | 0.09467451 |
| test/episodes                  | 1960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00369   |
| test/info_shaping_reward_mean  | -0.0632    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.7886691 |
| test/Q_plus_P                  | -1.7886691 |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 78400      |
| train/episodes                 | 7840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00391   |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 313600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 196        |
| stats_o/mean                   | 0.20412281 |
| stats_o/std                    | 0.0945137  |
| test/episodes                  | 1970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.675      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00127   |
| test/info_shaping_reward_mean  | -0.0619    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.8716936 |
| test/Q_plus_P                  | -1.8716936 |
| test/reward_per_eps            | -13        |
| test/steps                     | 78800      |
| train/episodes                 | 7880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00376   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 315200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 197         |
| stats_o/mean                   | 0.20414118  |
| stats_o/std                    | 0.094392926 |
| test/episodes                  | 1980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0039     |
| test/info_shaping_reward_mean  | -0.0602     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.7254906  |
| test/Q_plus_P                  | -1.7254906  |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 79200       |
| train/episodes                 | 7920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.557       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00385    |
| train/info_shaping_reward_mean | -0.0727     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 316800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 198        |
| stats_o/mean                   | 0.2041477  |
| stats_o/std                    | 0.09435275 |
| test/episodes                  | 1990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00124   |
| test/info_shaping_reward_mean  | -0.0605    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.637     |
| test/Q_plus_P                  | -1.637     |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 79600      |
| train/episodes                 | 7960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00387   |
| train/info_shaping_reward_mean | -0.0882    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 318400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.20414476 |
| stats_o/std                    | 0.09420186 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00639   |
| test/info_shaping_reward_mean  | -0.0612    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6381104 |
| test/Q_plus_P                  | -1.6381104 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00426   |
| train/info_shaping_reward_mean | -0.0808    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 200        |
| stats_o/mean                   | 0.20417266 |
| stats_o/std                    | 0.09408467 |
| test/episodes                  | 2010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00865   |
| test/info_shaping_reward_mean  | -0.0621    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.7241755 |
| test/Q_plus_P                  | -1.7241755 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 80400      |
| train/episodes                 | 8040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00546   |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 321600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 201         |
| stats_o/mean                   | 0.20417385  |
| stats_o/std                    | 0.093916185 |
| test/episodes                  | 2020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00554    |
| test/info_shaping_reward_mean  | -0.0554     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3732767  |
| test/Q_plus_P                  | -1.3732767  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 80800       |
| train/episodes                 | 8080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00349    |
| train/info_shaping_reward_mean | -0.0686     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 323200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 202        |
| stats_o/mean                   | 0.20416541 |
| stats_o/std                    | 0.09383119 |
| test/episodes                  | 2030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00138   |
| test/info_shaping_reward_mean  | -0.0584    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.5952809 |
| test/Q_plus_P                  | -1.5952809 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 81200      |
| train/episodes                 | 8120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.55       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00584   |
| train/info_shaping_reward_mean | -0.0832    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 324800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 203        |
| stats_o/mean                   | 0.20418686 |
| stats_o/std                    | 0.09371342 |
| test/episodes                  | 2040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000826  |
| test/info_shaping_reward_mean  | -0.0607    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.6369691 |
| test/Q_plus_P                  | -1.6369691 |
| test/reward_per_eps            | -12        |
| test/steps                     | 81600      |
| train/episodes                 | 8160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00331   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 326400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 204        |
| stats_o/mean                   | 0.20419815 |
| stats_o/std                    | 0.09354964 |
| test/episodes                  | 2050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00114   |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5812314 |
| test/Q_plus_P                  | -1.5812314 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 82000      |
| train/episodes                 | 8200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00488   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 328000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 205         |
| stats_o/mean                   | 0.20421149  |
| stats_o/std                    | 0.093431205 |
| test/episodes                  | 2060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00255    |
| test/info_shaping_reward_mean  | -0.0621     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5619993  |
| test/Q_plus_P                  | -1.5619993  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 82400       |
| train/episodes                 | 8240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.571       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00528    |
| train/info_shaping_reward_mean | -0.0738     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 329600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 206        |
| stats_o/mean                   | 0.20420356 |
| stats_o/std                    | 0.09328043 |
| test/episodes                  | 2070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.665      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00637   |
| test/info_shaping_reward_mean  | -0.0664    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.9291246 |
| test/Q_plus_P                  | -1.9291246 |
| test/reward_per_eps            | -13.4      |
| test/steps                     | 82800      |
| train/episodes                 | 8280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00364   |
| train/info_shaping_reward_mean | -0.0681    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 331200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.20421615 |
| stats_o/std                    | 0.09314269 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.625      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00218   |
| test/info_shaping_reward_mean  | -0.0704    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -2.0364523 |
| test/Q_plus_P                  | -2.0364523 |
| test/reward_per_eps            | -15        |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00488   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 208         |
| stats_o/mean                   | 0.204223    |
| stats_o/std                    | 0.093052566 |
| test/episodes                  | 2090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.613       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00241    |
| test/info_shaping_reward_mean  | -0.0724     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -2.179696   |
| test/Q_plus_P                  | -2.179696   |
| test/reward_per_eps            | -15.5       |
| test/steps                     | 83600       |
| train/episodes                 | 8360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.545       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0049     |
| train/info_shaping_reward_mean | -0.0792     |
| train/info_shaping_reward_min  | -0.199      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 334400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 209        |
| stats_o/mean                   | 0.20424543 |
| stats_o/std                    | 0.0929543  |
| test/episodes                  | 2100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00521   |
| test/info_shaping_reward_mean  | -0.0641    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.721756  |
| test/Q_plus_P                  | -1.721756  |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 84000      |
| train/episodes                 | 8400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.512      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00492   |
| train/info_shaping_reward_mean | -0.08      |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.5      |
| train/steps                    | 336000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 210         |
| stats_o/mean                   | 0.20427936  |
| stats_o/std                    | 0.092854954 |
| test/episodes                  | 2110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000563   |
| test/info_shaping_reward_mean  | -0.0561     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.3762424  |
| test/Q_plus_P                  | -1.3762424  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 84400       |
| train/episodes                 | 8440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.558       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0057     |
| train/info_shaping_reward_mean | -0.0728     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 337600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 211        |
| stats_o/mean                   | 0.20428972 |
| stats_o/std                    | 0.09275618 |
| test/episodes                  | 2120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00603   |
| test/info_shaping_reward_mean  | -0.0657    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.7569827 |
| test/Q_plus_P                  | -1.7569827 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 84800      |
| train/episodes                 | 8480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.528      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00677   |
| train/info_shaping_reward_mean | -0.0793    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 339200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 212         |
| stats_o/mean                   | 0.2042955   |
| stats_o/std                    | 0.092585154 |
| test/episodes                  | 2130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00297    |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.409164   |
| test/Q_plus_P                  | -1.409164   |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 85200       |
| train/episodes                 | 8520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00444    |
| train/info_shaping_reward_mean | -0.073      |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 340800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 213        |
| stats_o/mean                   | 0.20430757 |
| stats_o/std                    | 0.09246924 |
| test/episodes                  | 2140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00394   |
| test/info_shaping_reward_mean  | -0.0711    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.2138977 |
| test/Q_plus_P                  | -2.2138977 |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 85600      |
| train/episodes                 | 8560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00388   |
| train/info_shaping_reward_mean | -0.0776    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 342400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 214        |
| stats_o/mean                   | 0.20431925 |
| stats_o/std                    | 0.09242783 |
| test/episodes                  | 2150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00465   |
| test/info_shaping_reward_mean  | -0.0603    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.6843255 |
| test/Q_plus_P                  | -1.6843255 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 86000      |
| train/episodes                 | 8600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.455      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00534   |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 344000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 215        |
| stats_o/mean                   | 0.20433405 |
| stats_o/std                    | 0.09231275 |
| test/episodes                  | 2160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00832   |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5567659 |
| test/Q_plus_P                  | -1.5567659 |
| test/reward_per_eps            | -11        |
| test/steps                     | 86400      |
| train/episodes                 | 8640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.535      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00525   |
| train/info_shaping_reward_mean | -0.0824    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 345600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 216         |
| stats_o/mean                   | 0.20435537  |
| stats_o/std                    | 0.092297494 |
| test/episodes                  | 2170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.575       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00313    |
| test/info_shaping_reward_mean  | -0.0807     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -2.666283   |
| test/Q_plus_P                  | -2.666283   |
| test/reward_per_eps            | -17         |
| test/steps                     | 86800       |
| train/episodes                 | 8680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.497       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00668    |
| train/info_shaping_reward_mean | -0.0987     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 347200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 217        |
| stats_o/mean                   | 0.20437922 |
| stats_o/std                    | 0.09230199 |
| test/episodes                  | 2180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.492      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0054    |
| test/info_shaping_reward_mean  | -0.0965    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -4.377677  |
| test/Q_plus_P                  | -4.377677  |
| test/reward_per_eps            | -20.3      |
| test/steps                     | 87200      |
| train/episodes                 | 8720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.442      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00518   |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.3      |
| train/steps                    | 348800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 218        |
| stats_o/mean                   | 0.20441143 |
| stats_o/std                    | 0.0923909  |
| test/episodes                  | 2190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.562      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00183   |
| test/info_shaping_reward_mean  | -0.0823    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.4044418 |
| test/Q_plus_P                  | -3.4044418 |
| test/reward_per_eps            | -17.5      |
| test/steps                     | 87600      |
| train/episodes                 | 8760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.323      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00748   |
| train/info_shaping_reward_mean | -0.115     |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.1      |
| train/steps                    | 350400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 219        |
| stats_o/mean                   | 0.20441306 |
| stats_o/std                    | 0.0923437  |
| test/episodes                  | 2200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00479   |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3101513 |
| test/Q_plus_P                  | -1.3101513 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 88000      |
| train/episodes                 | 8800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00534   |
| train/info_shaping_reward_mean | -0.0834    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 352000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 220        |
| stats_o/mean                   | 0.20444007 |
| stats_o/std                    | 0.09240821 |
| test/episodes                  | 2210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.578      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00741   |
| test/info_shaping_reward_mean  | -0.0799    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -3.3499374 |
| test/Q_plus_P                  | -3.3499374 |
| test/reward_per_eps            | -16.9      |
| test/steps                     | 88400      |
| train/episodes                 | 8840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.42       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00495   |
| train/info_shaping_reward_mean | -0.1       |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.2      |
| train/steps                    | 353600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 221         |
| stats_o/mean                   | 0.20446095  |
| stats_o/std                    | 0.092528924 |
| test/episodes                  | 2220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.672       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00668    |
| test/info_shaping_reward_mean  | -0.0638     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.7463604  |
| test/Q_plus_P                  | -1.7463604  |
| test/reward_per_eps            | -13.1       |
| test/steps                     | 88800       |
| train/episodes                 | 8880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.393       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00478    |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.3       |
| train/steps                    | 355200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.20447499 |
| stats_o/std                    | 0.09257528 |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0035    |
| test/info_shaping_reward_mean  | -0.0563    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5003345 |
| test/Q_plus_P                  | -1.5003345 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.424      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0039    |
| train/info_shaping_reward_mean | -0.0977    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.1      |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 223        |
| stats_o/mean                   | 0.20449394 |
| stats_o/std                    | 0.09273258 |
| test/episodes                  | 2240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.637      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00289   |
| test/info_shaping_reward_mean  | -0.0675    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.9849201 |
| test/Q_plus_P                  | -1.9849201 |
| test/reward_per_eps            | -14.5      |
| test/steps                     | 89600      |
| train/episodes                 | 8960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.422      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.099     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.1      |
| train/steps                    | 358400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.20448944 |
| stats_o/std                    | 0.09293166 |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.425      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00647   |
| test/info_shaping_reward_mean  | -0.104     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -6.097641  |
| test/Q_plus_P                  | -6.097641  |
| test/reward_per_eps            | -23        |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.394      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0056    |
| train/info_shaping_reward_mean | -0.113     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.2      |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 225        |
| stats_o/mean                   | 0.20450598 |
| stats_o/std                    | 0.09299883 |
| test/episodes                  | 2260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.453      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00109   |
| test/info_shaping_reward_mean  | -0.101     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -6.560022  |
| test/Q_plus_P                  | -6.560022  |
| test/reward_per_eps            | -21.9      |
| test/steps                     | 90400      |
| train/episodes                 | 9040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.44       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00297   |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.4      |
| train/steps                    | 361600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 226         |
| stats_o/mean                   | 0.20452398  |
| stats_o/std                    | 0.093010336 |
| test/episodes                  | 2270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.598       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0122     |
| test/info_shaping_reward_mean  | -0.0768     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -3.8626416  |
| test/Q_plus_P                  | -3.8626416  |
| test/reward_per_eps            | -16.1       |
| test/steps                     | 90800       |
| train/episodes                 | 9080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.484       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00452    |
| train/info_shaping_reward_mean | -0.0939     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.6       |
| train/steps                    | 363200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 227        |
| stats_o/mean                   | 0.20452927 |
| stats_o/std                    | 0.0930009  |
| test/episodes                  | 2280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00262   |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.355513  |
| test/Q_plus_P                  | -1.355513  |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 91200      |
| train/episodes                 | 9120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.507      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00432   |
| train/info_shaping_reward_mean | -0.0823    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.7      |
| train/steps                    | 364800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 228        |
| stats_o/mean                   | 0.20453335 |
| stats_o/std                    | 0.09300454 |
| test/episodes                  | 2290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00254   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2906324 |
| test/Q_plus_P                  | -1.2906324 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 91600      |
| train/episodes                 | 9160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.51       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00586   |
| train/info_shaping_reward_mean | -0.093     |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 366400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 229        |
| stats_o/mean                   | 0.20456164 |
| stats_o/std                    | 0.09290804 |
| test/episodes                  | 2300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00333   |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.399305  |
| test/Q_plus_P                  | -1.399305  |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 92000      |
| train/episodes                 | 9200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.587      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00316   |
| train/info_shaping_reward_mean | -0.0679    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 368000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 230        |
| stats_o/mean                   | 0.20457028 |
| stats_o/std                    | 0.09279284 |
| test/episodes                  | 2310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00751   |
| test/info_shaping_reward_mean  | -0.0585    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4716713 |
| test/Q_plus_P                  | -1.4716713 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 92400      |
| train/episodes                 | 9240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0676    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 369600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 231         |
| stats_o/mean                   | 0.2045504   |
| stats_o/std                    | 0.092703946 |
| test/episodes                  | 2320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00306    |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.589397   |
| test/Q_plus_P                  | -1.589397   |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 92800       |
| train/episodes                 | 9280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.559       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0773     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 371200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 232        |
| stats_o/mean                   | 0.20454882 |
| stats_o/std                    | 0.09260075 |
| test/episodes                  | 2330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00358   |
| test/info_shaping_reward_mean  | -0.0589    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5591531 |
| test/Q_plus_P                  | -1.5591531 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 93200      |
| train/episodes                 | 9320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00386   |
| train/info_shaping_reward_mean | -0.074     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 372800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 233        |
| stats_o/mean                   | 0.20455402 |
| stats_o/std                    | 0.09246653 |
| test/episodes                  | 2340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00356   |
| test/info_shaping_reward_mean  | -0.0635    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.6671292 |
| test/Q_plus_P                  | -1.6671292 |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 93600      |
| train/episodes                 | 9360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00546   |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 374400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.20455712 |
| stats_o/std                    | 0.09237976 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00176   |
| test/info_shaping_reward_mean  | -0.0601    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.5766209 |
| test/Q_plus_P                  | -1.5766209 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.515      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00575   |
| train/info_shaping_reward_mean | -0.0793    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 235        |
| stats_o/mean                   | 0.20457229 |
| stats_o/std                    | 0.09226977 |
| test/episodes                  | 2360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000997  |
| test/info_shaping_reward_mean  | -0.0598    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.760304  |
| test/Q_plus_P                  | -1.760304  |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 94400      |
| train/episodes                 | 9440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.0782    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 377600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 236         |
| stats_o/mean                   | 0.20458561  |
| stats_o/std                    | 0.092139654 |
| test/episodes                  | 2370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00194    |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.4223077  |
| test/Q_plus_P                  | -1.4223077  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 94800       |
| train/episodes                 | 9480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.0686     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 379200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 237        |
| stats_o/mean                   | 0.2045794  |
| stats_o/std                    | 0.09203284 |
| test/episodes                  | 2380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.494132  |
| test/Q_plus_P                  | -1.494132  |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 95200      |
| train/episodes                 | 9520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.003     |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 380800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 238        |
| stats_o/mean                   | 0.20458621 |
| stats_o/std                    | 0.0919273  |
| test/episodes                  | 2390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00209   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3551433 |
| test/Q_plus_P                  | -1.3551433 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 95600      |
| train/episodes                 | 9560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00338   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 382400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 239         |
| stats_o/mean                   | 0.20458788  |
| stats_o/std                    | 0.091812484 |
| test/episodes                  | 2400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00223    |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.5204636  |
| test/Q_plus_P                  | -1.5204636  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 96000       |
| train/episodes                 | 9600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.602       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00438    |
| train/info_shaping_reward_mean | -0.0699     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 384000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 240        |
| stats_o/mean                   | 0.20459552 |
| stats_o/std                    | 0.09168209 |
| test/episodes                  | 2410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00972   |
| test/info_shaping_reward_mean  | -0.0625    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.6287069 |
| test/Q_plus_P                  | -1.6287069 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 96400      |
| train/episodes                 | 9640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00396   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 385600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 241         |
| stats_o/mean                   | 0.20459384  |
| stats_o/std                    | 0.091556095 |
| test/episodes                  | 2420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00425    |
| test/info_shaping_reward_mean  | -0.0597     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.5905957  |
| test/Q_plus_P                  | -1.5905957  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 96800       |
| train/episodes                 | 9680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.57        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00519    |
| train/info_shaping_reward_mean | -0.0744     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 387200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 242        |
| stats_o/mean                   | 0.20459647 |
| stats_o/std                    | 0.09144913 |
| test/episodes                  | 2430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0102    |
| test/info_shaping_reward_mean  | -0.067     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.9184228 |
| test/Q_plus_P                  | -1.9184228 |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 97200      |
| train/episodes                 | 9720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00446   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 388800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 243        |
| stats_o/mean                   | 0.20461744 |
| stats_o/std                    | 0.09135999 |
| test/episodes                  | 2440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0588    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.6604482 |
| test/Q_plus_P                  | -1.6604482 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 97600      |
| train/episodes                 | 9760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00426   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 390400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.20462078 |
| stats_o/std                    | 0.09128035 |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00743   |
| test/info_shaping_reward_mean  | -0.0628    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.7064494 |
| test/Q_plus_P                  | -1.7064494 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00526   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 245        |
| stats_o/mean                   | 0.20461564 |
| stats_o/std                    | 0.09121851 |
| test/episodes                  | 2460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.627      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00374   |
| test/info_shaping_reward_mean  | -0.0892    |
| test/info_shaping_reward_min   | -0.535     |
| test/Q                         | -4.0915117 |
| test/Q_plus_P                  | -4.0915117 |
| test/reward_per_eps            | -14.9      |
| test/steps                     | 98400      |
| train/episodes                 | 9840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.544      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00385   |
| train/info_shaping_reward_mean | -0.0839    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 393600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 246        |
| stats_o/mean                   | 0.20462914 |
| stats_o/std                    | 0.09111634 |
| test/episodes                  | 2470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00805   |
| test/info_shaping_reward_mean  | -0.0645    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.721107  |
| test/Q_plus_P                  | -1.721107  |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 98800      |
| train/episodes                 | 9880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00454   |
| train/info_shaping_reward_mean | -0.072     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 395200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 247        |
| stats_o/mean                   | 0.20463066 |
| stats_o/std                    | 0.09105325 |
| test/episodes                  | 2480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.665      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00338   |
| test/info_shaping_reward_mean  | -0.0665    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.8326877 |
| test/Q_plus_P                  | -1.8326877 |
| test/reward_per_eps            | -13.4      |
| test/steps                     | 99200      |
| train/episodes                 | 9920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.0754    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 396800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.2046564  |
| stats_o/std                    | 0.09103247 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00486   |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3753635 |
| test/Q_plus_P                  | -1.3753635 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00348   |
| train/info_shaping_reward_mean | -0.0817    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 249         |
| stats_o/mean                   | 0.20466714  |
| stats_o/std                    | 0.090951554 |
| test/episodes                  | 2500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00415    |
| test/info_shaping_reward_mean  | -0.0643     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.6169466  |
| test/Q_plus_P                  | -1.6169466  |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 100000      |
| train/episodes                 | 10000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.556       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00451    |
| train/info_shaping_reward_mean | -0.0734     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 400000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 250        |
| stats_o/mean                   | 0.20468774 |
| stats_o/std                    | 0.09093598 |
| test/episodes                  | 2510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0085    |
| test/info_shaping_reward_mean  | -0.0604    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.7078869 |
| test/Q_plus_P                  | -1.7078869 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 100400     |
| train/episodes                 | 10040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00393   |
| train/info_shaping_reward_mean | -0.0819    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 401600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 251         |
| stats_o/mean                   | 0.2047078   |
| stats_o/std                    | 0.090866506 |
| test/episodes                  | 2520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00845    |
| test/info_shaping_reward_mean  | -0.0667     |
| test/info_shaping_reward_min   | -0.2        |
| test/Q                         | -1.737659   |
| test/Q_plus_P                  | -1.737659   |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 100800      |
| train/episodes                 | 10080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.57        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00402    |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 403200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 252        |
| stats_o/mean                   | 0.20470576 |
| stats_o/std                    | 0.09085689 |
| test/episodes                  | 2530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00418   |
| test/info_shaping_reward_mean  | -0.0584    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.4134022 |
| test/Q_plus_P                  | -1.4134022 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 101200     |
| train/episodes                 | 10120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.0817    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 404800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 253        |
| stats_o/mean                   | 0.20471169 |
| stats_o/std                    | 0.09076213 |
| test/episodes                  | 2540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0119    |
| test/info_shaping_reward_mean  | -0.0598    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.5140395 |
| test/Q_plus_P                  | -1.5140395 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 101600     |
| train/episodes                 | 10160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00512   |
| train/info_shaping_reward_mean | -0.066     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 406400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 254        |
| stats_o/mean                   | 0.20468135 |
| stats_o/std                    | 0.09075445 |
| test/episodes                  | 2550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00784   |
| test/info_shaping_reward_mean  | -0.0581    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5566632 |
| test/Q_plus_P                  | -1.5566632 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 102000     |
| train/episodes                 | 10200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.483      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00547   |
| train/info_shaping_reward_mean | -0.0862    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.7      |
| train/steps                    | 408000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 255        |
| stats_o/mean                   | 0.20469338 |
| stats_o/std                    | 0.09072378 |
| test/episodes                  | 2560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00445   |
| test/info_shaping_reward_mean  | -0.0646    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.7073388 |
| test/Q_plus_P                  | -1.7073388 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 102400     |
| train/episodes                 | 10240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00481   |
| train/info_shaping_reward_mean | -0.0946    |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 409600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 256        |
| stats_o/mean                   | 0.20471151 |
| stats_o/std                    | 0.09061385 |
| test/episodes                  | 2570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0099    |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.3452944 |
| test/Q_plus_P                  | -1.3452944 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 102800     |
| train/episodes                 | 10280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00307   |
| train/info_shaping_reward_mean | -0.0674    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 411200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 257        |
| stats_o/mean                   | 0.20471396 |
| stats_o/std                    | 0.09052358 |
| test/episodes                  | 2580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.65       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00145   |
| test/info_shaping_reward_mean  | -0.0684    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.8732158 |
| test/Q_plus_P                  | -1.8732158 |
| test/reward_per_eps            | -14        |
| test/steps                     | 103200     |
| train/episodes                 | 10320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 412800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 258         |
| stats_o/mean                   | 0.20471765  |
| stats_o/std                    | 0.090432785 |
| test/episodes                  | 2590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00547    |
| test/info_shaping_reward_mean  | -0.0609     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.5401125  |
| test/Q_plus_P                  | -1.5401125  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 103600      |
| train/episodes                 | 10360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.574       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00552    |
| train/info_shaping_reward_mean | -0.0732     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 414400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 259        |
| stats_o/mean                   | 0.20472008 |
| stats_o/std                    | 0.09035659 |
| test/episodes                  | 2600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00397   |
| test/info_shaping_reward_mean  | -0.0594    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5690265 |
| test/Q_plus_P                  | -1.5690265 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 104000     |
| train/episodes                 | 10400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00406   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 416000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 260         |
| stats_o/mean                   | 0.20470046  |
| stats_o/std                    | 0.090300724 |
| test/episodes                  | 2610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00713    |
| test/info_shaping_reward_mean  | -0.0585     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5095116  |
| test/Q_plus_P                  | -1.5095116  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 104400      |
| train/episodes                 | 10440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.526       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00389    |
| train/info_shaping_reward_mean | -0.0783     |
| train/info_shaping_reward_min  | -0.201      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19         |
| train/steps                    | 417600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 261        |
| stats_o/mean                   | 0.20469931 |
| stats_o/std                    | 0.09021    |
| test/episodes                  | 2620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00857   |
| test/info_shaping_reward_mean  | -0.0589    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5091369 |
| test/Q_plus_P                  | -1.5091369 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 104800     |
| train/episodes                 | 10480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00403   |
| train/info_shaping_reward_mean | -0.0722    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 419200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.20470504  |
| stats_o/std                    | 0.090109386 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0123     |
| test/info_shaping_reward_mean  | -0.0567     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.4043224  |
| test/Q_plus_P                  | -1.4043224  |
| test/reward_per_eps            | -10         |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00398    |
| train/info_shaping_reward_mean | -0.0697     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.20471212  |
| stats_o/std                    | 0.090077884 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00244    |
| test/info_shaping_reward_mean  | -0.0635     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.8269789  |
| test/Q_plus_P                  | -1.8269789  |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.561       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00386    |
| train/info_shaping_reward_mean | -0.0796     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 264        |
| stats_o/mean                   | 0.2047043  |
| stats_o/std                    | 0.0900404  |
| test/episodes                  | 2650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00393   |
| test/info_shaping_reward_mean  | -0.0595    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.6104033 |
| test/Q_plus_P                  | -1.6104033 |
| test/reward_per_eps            | -12        |
| test/steps                     | 106000     |
| train/episodes                 | 10600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.484      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00484   |
| train/info_shaping_reward_mean | -0.0849    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 424000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 265        |
| stats_o/mean                   | 0.20471069 |
| stats_o/std                    | 0.08993164 |
| test/episodes                  | 2660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00218   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3995594 |
| test/Q_plus_P                  | -1.3995594 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 106400     |
| train/episodes                 | 10640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00288   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 425600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 266        |
| stats_o/mean                   | 0.2046999  |
| stats_o/std                    | 0.08987073 |
| test/episodes                  | 2670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00441   |
| test/info_shaping_reward_mean  | -0.0578    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.5120506 |
| test/Q_plus_P                  | -1.5120506 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 106800     |
| train/episodes                 | 10680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00469   |
| train/info_shaping_reward_mean | -0.0738    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 427200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 267        |
| stats_o/mean                   | 0.20468421 |
| stats_o/std                    | 0.08984622 |
| test/episodes                  | 2680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.61       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00157   |
| test/info_shaping_reward_mean  | -0.0712    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.198098  |
| test/Q_plus_P                  | -2.198098  |
| test/reward_per_eps            | -15.6      |
| test/steps                     | 107200     |
| train/episodes                 | 10720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00468   |
| train/info_shaping_reward_mean | -0.0899    |
| train/info_shaping_reward_min  | -0.288     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 428800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 268        |
| stats_o/mean                   | 0.20468344 |
| stats_o/std                    | 0.08978824 |
| test/episodes                  | 2690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.65       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00358   |
| test/info_shaping_reward_mean  | -0.0674    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.8800802 |
| test/Q_plus_P                  | -1.8800802 |
| test/reward_per_eps            | -14        |
| test/steps                     | 107600     |
| train/episodes                 | 10760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00627   |
| train/info_shaping_reward_mean | -0.0843    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 430400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 269        |
| stats_o/mean                   | 0.20468144 |
| stats_o/std                    | 0.08967804 |
| test/episodes                  | 2700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.642      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00197   |
| test/info_shaping_reward_mean  | -0.0666    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.9094311 |
| test/Q_plus_P                  | -1.9094311 |
| test/reward_per_eps            | -14.3      |
| test/steps                     | 108000     |
| train/episodes                 | 10800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00362   |
| train/info_shaping_reward_mean | -0.0747    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 432000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 270        |
| stats_o/mean                   | 0.20466223 |
| stats_o/std                    | 0.08957797 |
| test/episodes                  | 2710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00504   |
| test/info_shaping_reward_mean  | -0.0645    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.6944788 |
| test/Q_plus_P                  | -1.6944788 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 108400     |
| train/episodes                 | 10840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00525   |
| train/info_shaping_reward_mean | -0.0734    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 433600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 271        |
| stats_o/mean                   | 0.20466104 |
| stats_o/std                    | 0.0894845  |
| test/episodes                  | 2720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00402   |
| test/info_shaping_reward_mean  | -0.0582    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.5437729 |
| test/Q_plus_P                  | -1.5437729 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 108800     |
| train/episodes                 | 10880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00558   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 435200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 272        |
| stats_o/mean                   | 0.20466055 |
| stats_o/std                    | 0.08942614 |
| test/episodes                  | 2730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00206   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5339906 |
| test/Q_plus_P                  | -1.5339906 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 109200     |
| train/episodes                 | 10920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00486   |
| train/info_shaping_reward_mean | -0.0786    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 436800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 273        |
| stats_o/mean                   | 0.20467295 |
| stats_o/std                    | 0.08937099 |
| test/episodes                  | 2740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00808   |
| test/info_shaping_reward_mean  | -0.0541    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4027982 |
| test/Q_plus_P                  | -1.4027982 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 109600     |
| train/episodes                 | 10960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0679    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 438400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 274        |
| stats_o/mean                   | 0.20466714 |
| stats_o/std                    | 0.0892834  |
| test/episodes                  | 2750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0038    |
| test/info_shaping_reward_mean  | -0.0617    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.5330125 |
| test/Q_plus_P                  | -1.5330125 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 110000     |
| train/episodes                 | 11000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.567      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00416   |
| train/info_shaping_reward_mean | -0.0723    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 440000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 275        |
| stats_o/mean                   | 0.20467827 |
| stats_o/std                    | 0.08920991 |
| test/episodes                  | 2760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00144   |
| test/info_shaping_reward_mean  | -0.0608    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6894232 |
| test/Q_plus_P                  | -1.6894232 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 110400     |
| train/episodes                 | 11040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00369   |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 441600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 276         |
| stats_o/mean                   | 0.20469072  |
| stats_o/std                    | 0.089130275 |
| test/episodes                  | 2770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00318    |
| test/info_shaping_reward_mean  | -0.059      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.5021397  |
| test/Q_plus_P                  | -1.5021397  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 110800      |
| train/episodes                 | 11080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.556       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00407    |
| train/info_shaping_reward_mean | -0.0755     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 443200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 277        |
| stats_o/mean                   | 0.2046858  |
| stats_o/std                    | 0.08907535 |
| test/episodes                  | 2780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.331409  |
| test/Q_plus_P                  | -1.331409  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 111200     |
| train/episodes                 | 11120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 444800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 278         |
| stats_o/mean                   | 0.20466274  |
| stats_o/std                    | 0.089010864 |
| test/episodes                  | 2790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00213    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2219576  |
| test/Q_plus_P                  | -1.2219576  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 111600      |
| train/episodes                 | 11160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0674     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 446400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 279         |
| stats_o/mean                   | 0.20466928  |
| stats_o/std                    | 0.088936366 |
| test/episodes                  | 2800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00176    |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.3179765  |
| test/Q_plus_P                  | -1.3179765  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 112000      |
| train/episodes                 | 11200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.585       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0714     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 448000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 280        |
| stats_o/mean                   | 0.20465231 |
| stats_o/std                    | 0.08889862 |
| test/episodes                  | 2810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00257   |
| test/info_shaping_reward_mean  | -0.0609    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.7353485 |
| test/Q_plus_P                  | -1.7353485 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 112400     |
| train/episodes                 | 11240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00331   |
| train/info_shaping_reward_mean | -0.0774    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 449600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.20466644 |
| stats_o/std                    | 0.08884836 |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00601   |
| test/info_shaping_reward_mean  | -0.0642    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.2967799 |
| test/Q_plus_P                  | -2.2967799 |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00427   |
| train/info_shaping_reward_mean | -0.0746    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.20465487 |
| stats_o/std                    | 0.08875295 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00656   |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4389268 |
| test/Q_plus_P                  | -1.4389268 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00372   |
| train/info_shaping_reward_mean | -0.0664    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.20465614 |
| stats_o/std                    | 0.08874525 |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00266   |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4447851 |
| test/Q_plus_P                  | -1.4447851 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.544      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00359   |
| train/info_shaping_reward_mean | -0.078     |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 284        |
| stats_o/mean                   | 0.20467617 |
| stats_o/std                    | 0.08867675 |
| test/episodes                  | 2850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00305   |
| test/info_shaping_reward_mean  | -0.0592    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5016088 |
| test/Q_plus_P                  | -1.5016088 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 114000     |
| train/episodes                 | 11400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00372   |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 456000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 285        |
| stats_o/mean                   | 0.20466974 |
| stats_o/std                    | 0.08864353 |
| test/episodes                  | 2860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00439   |
| test/info_shaping_reward_mean  | -0.0577    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3945975 |
| test/Q_plus_P                  | -1.3945975 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 114400     |
| train/episodes                 | 11440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.555      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00472   |
| train/info_shaping_reward_mean | -0.0803    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 457600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 286        |
| stats_o/mean                   | 0.20466323 |
| stats_o/std                    | 0.08856594 |
| test/episodes                  | 2870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0155    |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2371604 |
| test/Q_plus_P                  | -1.2371604 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 114800     |
| train/episodes                 | 11480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00499   |
| train/info_shaping_reward_mean | -0.0673    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 459200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.20468274 |
| stats_o/std                    | 0.08849816 |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.665      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00212   |
| test/info_shaping_reward_mean  | -0.0643    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.7096353 |
| test/Q_plus_P                  | -1.7096353 |
| test/reward_per_eps            | -13.4      |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00366   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 288         |
| stats_o/mean                   | 0.2046989   |
| stats_o/std                    | 0.088417575 |
| test/episodes                  | 2890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00679    |
| test/info_shaping_reward_mean  | -0.0624     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.6214175  |
| test/Q_plus_P                  | -1.6214175  |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 115600      |
| train/episodes                 | 11560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00502    |
| train/info_shaping_reward_mean | -0.0705     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 462400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 289        |
| stats_o/mean                   | 0.20470819 |
| stats_o/std                    | 0.08835671 |
| test/episodes                  | 2900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000283  |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3976407 |
| test/Q_plus_P                  | -1.3976407 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 116000     |
| train/episodes                 | 11600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0043    |
| train/info_shaping_reward_mean | -0.0786    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 464000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 290         |
| stats_o/mean                   | 0.20471428  |
| stats_o/std                    | 0.088283814 |
| test/episodes                  | 2910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00482    |
| test/info_shaping_reward_mean  | -0.0553     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.3942538  |
| test/Q_plus_P                  | -1.3942538  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 116400      |
| train/episodes                 | 11640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00421    |
| train/info_shaping_reward_mean | -0.0676     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 465600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 291        |
| stats_o/mean                   | 0.20471698 |
| stats_o/std                    | 0.08818572 |
| test/episodes                  | 2920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00477   |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2661681 |
| test/Q_plus_P                  | -1.2661681 |
| test/reward_per_eps            | -10        |
| test/steps                     | 116800     |
| train/episodes                 | 11680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.585      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.0683    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 467200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 292        |
| stats_o/mean                   | 0.20470537 |
| stats_o/std                    | 0.08814649 |
| test/episodes                  | 2930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.014     |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -1.3429887 |
| test/Q_plus_P                  | -1.3429887 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 117200     |
| train/episodes                 | 11720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.523      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00427   |
| train/info_shaping_reward_mean | -0.0818    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 468800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.2047297   |
| stats_o/std                    | 0.088132754 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000905   |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.414216   |
| test/Q_plus_P                  | -1.414216   |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.562       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00343    |
| train/info_shaping_reward_mean | -0.0838     |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 294        |
| stats_o/mean                   | 0.20472214 |
| stats_o/std                    | 0.08806647 |
| test/episodes                  | 2950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00174   |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.491708  |
| test/Q_plus_P                  | -1.491708  |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 118000     |
| train/episodes                 | 11800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00434   |
| train/info_shaping_reward_mean | -0.0683    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 472000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 295        |
| stats_o/mean                   | 0.20472908 |
| stats_o/std                    | 0.08798154 |
| test/episodes                  | 2960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00343   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.3659391 |
| test/Q_plus_P                  | -1.3659391 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 118400     |
| train/episodes                 | 11840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 473600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 296        |
| stats_o/mean                   | 0.20474213 |
| stats_o/std                    | 0.08789714 |
| test/episodes                  | 2970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00828   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3285928 |
| test/Q_plus_P                  | -1.3285928 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 118800     |
| train/episodes                 | 11880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00314   |
| train/info_shaping_reward_mean | -0.0667    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 475200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 297         |
| stats_o/mean                   | 0.20473514  |
| stats_o/std                    | 0.087861024 |
| test/episodes                  | 2980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00747    |
| test/info_shaping_reward_mean  | -0.0589     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.5493447  |
| test/Q_plus_P                  | -1.5493447  |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 119200      |
| train/episodes                 | 11920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00353    |
| train/info_shaping_reward_mean | -0.0771     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 476800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 298        |
| stats_o/mean                   | 0.20473541 |
| stats_o/std                    | 0.08778847 |
| test/episodes                  | 2990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00271   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2521    |
| test/Q_plus_P                  | -1.2521    |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 119600     |
| train/episodes                 | 11960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00387   |
| train/info_shaping_reward_mean | -0.0673    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 478400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 299        |
| stats_o/mean                   | 0.20473841 |
| stats_o/std                    | 0.08770429 |
| test/episodes                  | 3000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.0563    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4933951 |
| test/Q_plus_P                  | -1.4933951 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 120000     |
| train/episodes                 | 12000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.0646    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 480000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.20474187  |
| stats_o/std                    | 0.087674595 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00254    |
| test/info_shaping_reward_mean  | -0.0613     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.7122291  |
| test/Q_plus_P                  | -1.7122291  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00391    |
| train/info_shaping_reward_mean | -0.0802     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.20475002 |
| stats_o/std                    | 0.08762022 |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00114   |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5828011 |
| test/Q_plus_P                  | -1.5828011 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00359   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 302        |
| stats_o/mean                   | 0.2047589  |
| stats_o/std                    | 0.0875849  |
| test/episodes                  | 3030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00403   |
| test/info_shaping_reward_mean  | -0.0608    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5654062 |
| test/Q_plus_P                  | -1.5654062 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 121200     |
| train/episodes                 | 12120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00372   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 484800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 303         |
| stats_o/mean                   | 0.20477451  |
| stats_o/std                    | 0.087527506 |
| test/episodes                  | 3040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00598    |
| test/info_shaping_reward_mean  | -0.0595     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.5703092  |
| test/Q_plus_P                  | -1.5703092  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 121600      |
| train/episodes                 | 12160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.533       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00477    |
| train/info_shaping_reward_mean | -0.0853     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 486400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 304        |
| stats_o/mean                   | 0.20478769 |
| stats_o/std                    | 0.08746655 |
| test/episodes                  | 3050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00122   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3411305 |
| test/Q_plus_P                  | -1.3411305 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 122000     |
| train/episodes                 | 12200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00424   |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 488000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.20479302 |
| stats_o/std                    | 0.08738524 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00676   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2290858 |
| test/Q_plus_P                  | -1.2290858 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00347   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 306        |
| stats_o/mean                   | 0.2048012  |
| stats_o/std                    | 0.08732122 |
| test/episodes                  | 3070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00616   |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3287245 |
| test/Q_plus_P                  | -1.3287245 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 122800     |
| train/episodes                 | 12280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.64       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00359   |
| train/info_shaping_reward_mean | -0.0696    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 491200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.20480318 |
| stats_o/std                    | 0.0872641  |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0025    |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3894447 |
| test/Q_plus_P                  | -1.3894447 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00377   |
| train/info_shaping_reward_mean | -0.0738    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 308         |
| stats_o/mean                   | 0.20479393  |
| stats_o/std                    | 0.087179616 |
| test/episodes                  | 3090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00267    |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.3973025  |
| test/Q_plus_P                  | -1.3973025  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 123600      |
| train/episodes                 | 12360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00368    |
| train/info_shaping_reward_mean | -0.0677     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 494400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 309        |
| stats_o/mean                   | 0.20478314 |
| stats_o/std                    | 0.08710608 |
| test/episodes                  | 3100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0012    |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.289298  |
| test/Q_plus_P                  | -1.289298  |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 124000     |
| train/episodes                 | 12400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00344   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 496000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 310        |
| stats_o/mean                   | 0.20478581 |
| stats_o/std                    | 0.08702012 |
| test/episodes                  | 3110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00227   |
| test/info_shaping_reward_mean  | -0.0598    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.6166894 |
| test/Q_plus_P                  | -1.6166894 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 124400     |
| train/episodes                 | 12440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00425   |
| train/info_shaping_reward_mean | -0.066     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 497600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.2047838  |
| stats_o/std                    | 0.08695794 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00274   |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.5708274 |
| test/Q_plus_P                  | -1.5708274 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0045    |
| train/info_shaping_reward_mean | -0.074     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 312        |
| stats_o/mean                   | 0.20478652 |
| stats_o/std                    | 0.08688478 |
| test/episodes                  | 3130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00269   |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4666442 |
| test/Q_plus_P                  | -1.4666442 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 125200     |
| train/episodes                 | 12520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00415   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 500800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.20479918  |
| stats_o/std                    | 0.086829394 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0024     |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.0997671  |
| test/Q_plus_P                  | -1.0997671  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00313    |
| train/info_shaping_reward_mean | -0.0715     |
| train/info_shaping_reward_min  | -0.208      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 314         |
| stats_o/mean                   | 0.20480505  |
| stats_o/std                    | 0.086805455 |
| test/episodes                  | 3150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00353    |
| test/info_shaping_reward_mean  | -0.0559     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.4767258  |
| test/Q_plus_P                  | -1.4767258  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 126000      |
| train/episodes                 | 12600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.544       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0037     |
| train/info_shaping_reward_mean | -0.0794     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 504000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 315        |
| stats_o/mean                   | 0.20481877 |
| stats_o/std                    | 0.08675186 |
| test/episodes                  | 3160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00294   |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4630891 |
| test/Q_plus_P                  | -1.4630891 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 126400     |
| train/episodes                 | 12640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 505600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 316        |
| stats_o/mean                   | 0.20481132 |
| stats_o/std                    | 0.08667053 |
| test/episodes                  | 3170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000772  |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2331275 |
| test/Q_plus_P                  | -1.2331275 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 126800     |
| train/episodes                 | 12680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.0623    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 507200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 317         |
| stats_o/mean                   | 0.20481244  |
| stats_o/std                    | 0.086608194 |
| test/episodes                  | 3180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00181    |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.4472017  |
| test/Q_plus_P                  | -1.4472017  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 127200      |
| train/episodes                 | 12720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.585       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.004      |
| train/info_shaping_reward_mean | -0.0684     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 508800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 318         |
| stats_o/mean                   | 0.20482454  |
| stats_o/std                    | 0.086537905 |
| test/episodes                  | 3190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00277    |
| test/info_shaping_reward_mean  | -0.0527     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.2841684  |
| test/Q_plus_P                  | -1.2841684  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 127600      |
| train/episodes                 | 12760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.605       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00441    |
| train/info_shaping_reward_mean | -0.0679     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 510400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 319        |
| stats_o/mean                   | 0.2048324  |
| stats_o/std                    | 0.08644967 |
| test/episodes                  | 3200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00277   |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4099087 |
| test/Q_plus_P                  | -1.4099087 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 128000     |
| train/episodes                 | 12800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0745    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 512000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 320        |
| stats_o/mean                   | 0.20482506 |
| stats_o/std                    | 0.08643744 |
| test/episodes                  | 3210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00244   |
| test/info_shaping_reward_mean  | -0.0571    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4910531 |
| test/Q_plus_P                  | -1.4910531 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 128400     |
| train/episodes                 | 12840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 513600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.20483255  |
| stats_o/std                    | 0.086393125 |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00465    |
| test/info_shaping_reward_mean  | -0.0551     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.3664566  |
| test/Q_plus_P                  | -1.3664566  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.57        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0037     |
| train/info_shaping_reward_mean | -0.0704     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 322        |
| stats_o/mean                   | 0.2048216  |
| stats_o/std                    | 0.08639103 |
| test/episodes                  | 3230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00188   |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.6297426 |
| test/Q_plus_P                  | -1.6297426 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 129200     |
| train/episodes                 | 12920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00367   |
| train/info_shaping_reward_mean | -0.0828    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 516800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 323        |
| stats_o/mean                   | 0.20481697 |
| stats_o/std                    | 0.08630579 |
| test/episodes                  | 3240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00182   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5117625 |
| test/Q_plus_P                  | -1.5117625 |
| test/reward_per_eps            | -11        |
| test/steps                     | 129600     |
| train/episodes                 | 12960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00415   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 518400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.20482016  |
| stats_o/std                    | 0.086242475 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00558    |
| test/info_shaping_reward_mean  | -0.0555     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.3560498  |
| test/Q_plus_P                  | -1.3560498  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.575       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00335    |
| train/info_shaping_reward_mean | -0.0706     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 325        |
| stats_o/mean                   | 0.20482752 |
| stats_o/std                    | 0.08617663 |
| test/episodes                  | 3260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00241   |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3029337 |
| test/Q_plus_P                  | -1.3029337 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 130400     |
| train/episodes                 | 13040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00393   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 521600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.20481776  |
| stats_o/std                    | 0.086131915 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00298    |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.469025   |
| test/Q_plus_P                  | -1.469025   |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0699     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 327        |
| stats_o/mean                   | 0.2048263  |
| stats_o/std                    | 0.08609465 |
| test/episodes                  | 3280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0007    |
| test/info_shaping_reward_mean  | -0.0564    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.384544  |
| test/Q_plus_P                  | -1.384544  |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 131200     |
| train/episodes                 | 13120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00312   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 524800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 328        |
| stats_o/mean                   | 0.20485243 |
| stats_o/std                    | 0.08611325 |
| test/episodes                  | 3290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00208   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3682235 |
| test/Q_plus_P                  | -1.3682235 |
| test/reward_per_eps            | -10        |
| test/steps                     | 131600     |
| train/episodes                 | 13160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.582      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00408   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 526400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 329        |
| stats_o/mean                   | 0.20485345 |
| stats_o/std                    | 0.08605395 |
| test/episodes                  | 3300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0021    |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3729353 |
| test/Q_plus_P                  | -1.3729353 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 132000     |
| train/episodes                 | 13200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0038    |
| train/info_shaping_reward_mean | -0.0647    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 528000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 330        |
| stats_o/mean                   | 0.20485145 |
| stats_o/std                    | 0.08597568 |
| test/episodes                  | 3310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00203   |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4617157 |
| test/Q_plus_P                  | -1.4617157 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 132400     |
| train/episodes                 | 13240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00433   |
| train/info_shaping_reward_mean | -0.0664    |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 529600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 331        |
| stats_o/mean                   | 0.20485887 |
| stats_o/std                    | 0.08590654 |
| test/episodes                  | 3320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.652      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00325   |
| test/info_shaping_reward_mean  | -0.0655    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.8353566 |
| test/Q_plus_P                  | -1.8353566 |
| test/reward_per_eps            | -13.9      |
| test/steps                     | 132800     |
| train/episodes                 | 13280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00341   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 531200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 332         |
| stats_o/mean                   | 0.20485449  |
| stats_o/std                    | 0.085853994 |
| test/episodes                  | 3330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00254    |
| test/info_shaping_reward_mean  | -0.056      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.5541217  |
| test/Q_plus_P                  | -1.5541217  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 133200      |
| train/episodes                 | 13320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00377    |
| train/info_shaping_reward_mean | -0.0695     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 532800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 333        |
| stats_o/mean                   | 0.20485137 |
| stats_o/std                    | 0.08580962 |
| test/episodes                  | 3340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00272   |
| test/info_shaping_reward_mean  | -0.0592    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.6023985 |
| test/Q_plus_P                  | -1.6023985 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 133600     |
| train/episodes                 | 13360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.567      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0041    |
| train/info_shaping_reward_mean | -0.0753    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 534400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 334        |
| stats_o/mean                   | 0.20485388 |
| stats_o/std                    | 0.08575708 |
| test/episodes                  | 3350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00174   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1850172 |
| test/Q_plus_P                  | -1.1850172 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 134000     |
| train/episodes                 | 13400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0059    |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 536000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.20486355 |
| stats_o/std                    | 0.08567541 |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00318   |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5648696 |
| test/Q_plus_P                  | -1.5648696 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00378   |
| train/info_shaping_reward_mean | -0.0664    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 336        |
| stats_o/mean                   | 0.20485474 |
| stats_o/std                    | 0.0856043  |
| test/episodes                  | 3370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00336   |
| test/info_shaping_reward_mean  | -0.0595    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.541845  |
| test/Q_plus_P                  | -1.541845  |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 134800     |
| train/episodes                 | 13480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00448   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 539200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.20485117  |
| stats_o/std                    | 0.085552014 |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00647    |
| test/info_shaping_reward_mean  | -0.0582     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.420573   |
| test/Q_plus_P                  | -1.420573   |
| test/reward_per_eps            | -11         |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00335    |
| train/info_shaping_reward_mean | -0.0727     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.20483594 |
| stats_o/std                    | 0.08553882 |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00178   |
| test/info_shaping_reward_mean  | -0.0596    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6968122 |
| test/Q_plus_P                  | -1.6968122 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.524      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0043    |
| train/info_shaping_reward_mean | -0.0817    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 339        |
| stats_o/mean                   | 0.20483978 |
| stats_o/std                    | 0.08548036 |
| test/episodes                  | 3400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00108   |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4832028 |
| test/Q_plus_P                  | -1.4832028 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 136000     |
| train/episodes                 | 13600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0036    |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 544000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 340         |
| stats_o/mean                   | 0.20483404  |
| stats_o/std                    | 0.085458845 |
| test/episodes                  | 3410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00846    |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2044563  |
| test/Q_plus_P                  | -1.2044563  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 136400      |
| train/episodes                 | 13640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0669     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 545600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 341        |
| stats_o/mean                   | 0.20482889 |
| stats_o/std                    | 0.08537921 |
| test/episodes                  | 3420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00417   |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.4378024 |
| test/Q_plus_P                  | -1.4378024 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 136800     |
| train/episodes                 | 13680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.003     |
| train/info_shaping_reward_mean | -0.065     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 547200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 342        |
| stats_o/mean                   | 0.2048371  |
| stats_o/std                    | 0.08540086 |
| test/episodes                  | 3430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00276   |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4429367 |
| test/Q_plus_P                  | -1.4429367 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 137200     |
| train/episodes                 | 13720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00364   |
| train/info_shaping_reward_mean | -0.0889    |
| train/info_shaping_reward_min  | -0.276     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 548800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 343        |
| stats_o/mean                   | 0.20482595 |
| stats_o/std                    | 0.08536039 |
| test/episodes                  | 3440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3236911 |
| test/Q_plus_P                  | -1.3236911 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 137600     |
| train/episodes                 | 13760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.0674    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 550400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 344        |
| stats_o/mean                   | 0.20481724 |
| stats_o/std                    | 0.08531876 |
| test/episodes                  | 3450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00244   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3773586 |
| test/Q_plus_P                  | -1.3773586 |
| test/reward_per_eps            | -10        |
| test/steps                     | 138000     |
| train/episodes                 | 13800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00388   |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 552000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 345        |
| stats_o/mean                   | 0.20482327 |
| stats_o/std                    | 0.08528014 |
| test/episodes                  | 3460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00243   |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3196906 |
| test/Q_plus_P                  | -1.3196906 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 138400     |
| train/episodes                 | 13840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.59       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0036    |
| train/info_shaping_reward_mean | -0.0756    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 553600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 346         |
| stats_o/mean                   | 0.20481479  |
| stats_o/std                    | 0.085228965 |
| test/episodes                  | 3470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00254    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.3413658  |
| test/Q_plus_P                  | -1.3413658  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 138800      |
| train/episodes                 | 13880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 555200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 347        |
| stats_o/mean                   | 0.2048358  |
| stats_o/std                    | 0.08520283 |
| test/episodes                  | 3480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00406   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.5421606 |
| test/Q_plus_P                  | -1.5421606 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 139200     |
| train/episodes                 | 13920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.548      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00412   |
| train/info_shaping_reward_mean | -0.08      |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 556800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 348        |
| stats_o/mean                   | 0.204851   |
| stats_o/std                    | 0.08517058 |
| test/episodes                  | 3490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00102   |
| test/info_shaping_reward_mean  | -0.0592    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.7665497 |
| test/Q_plus_P                  | -1.7665497 |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 139600     |
| train/episodes                 | 13960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00427   |
| train/info_shaping_reward_mean | -0.0792    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 558400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 349         |
| stats_o/mean                   | 0.20485549  |
| stats_o/std                    | 0.085151225 |
| test/episodes                  | 3500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000711   |
| test/info_shaping_reward_mean  | -0.0571     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.6017902  |
| test/Q_plus_P                  | -1.6017902  |
| test/reward_per_eps            | -12         |
| test/steps                     | 140000      |
| train/episodes                 | 14000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.566       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.0731     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 560000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 350        |
| stats_o/mean                   | 0.20485951 |
| stats_o/std                    | 0.08508716 |
| test/episodes                  | 3510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00266   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3388933 |
| test/Q_plus_P                  | -1.3388933 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 140400     |
| train/episodes                 | 14040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 561600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 351        |
| stats_o/mean                   | 0.20486456 |
| stats_o/std                    | 0.08503889 |
| test/episodes                  | 3520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0039    |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3650936 |
| test/Q_plus_P                  | -1.3650936 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 140800     |
| train/episodes                 | 14080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0037    |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 563200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 352        |
| stats_o/mean                   | 0.20486872 |
| stats_o/std                    | 0.08500825 |
| test/episodes                  | 3530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00133   |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4935293 |
| test/Q_plus_P                  | -1.4935293 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 141200     |
| train/episodes                 | 14120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00403   |
| train/info_shaping_reward_mean | -0.0767    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 564800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 353        |
| stats_o/mean                   | 0.20487392 |
| stats_o/std                    | 0.08494766 |
| test/episodes                  | 3540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00443   |
| test/info_shaping_reward_mean  | -0.0607    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.6970657 |
| test/Q_plus_P                  | -1.6970657 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 141600     |
| train/episodes                 | 14160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00398   |
| train/info_shaping_reward_mean | -0.0688    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 566400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 354        |
| stats_o/mean                   | 0.204871   |
| stats_o/std                    | 0.08488273 |
| test/episodes                  | 3550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00858   |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3304673 |
| test/Q_plus_P                  | -1.3304673 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 142000     |
| train/episodes                 | 14200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00353   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 568000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 355        |
| stats_o/mean                   | 0.20488903 |
| stats_o/std                    | 0.08489069 |
| test/episodes                  | 3560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00112   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.3592438 |
| test/Q_plus_P                  | -1.3592438 |
| test/reward_per_eps            | -10        |
| test/steps                     | 142400     |
| train/episodes                 | 14240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.0796    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 569600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 356        |
| stats_o/mean                   | 0.20487899 |
| stats_o/std                    | 0.08484562 |
| test/episodes                  | 3570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.5127792 |
| test/Q_plus_P                  | -1.5127792 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 142800     |
| train/episodes                 | 14280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 571200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 357        |
| stats_o/mean                   | 0.20489162 |
| stats_o/std                    | 0.08483326 |
| test/episodes                  | 3580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0044    |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.8720565 |
| test/Q_plus_P                  | -1.8720565 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 143200     |
| train/episodes                 | 14320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00443   |
| train/info_shaping_reward_mean | -0.0767    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 572800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 358         |
| stats_o/mean                   | 0.20489317  |
| stats_o/std                    | 0.084870346 |
| test/episodes                  | 3590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00162    |
| test/info_shaping_reward_mean  | -0.0585     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.6843976  |
| test/Q_plus_P                  | -1.6843976  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 143600      |
| train/episodes                 | 14360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.556       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0046     |
| train/info_shaping_reward_mean | -0.0881     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 574400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 359        |
| stats_o/mean                   | 0.20488921 |
| stats_o/std                    | 0.08485642 |
| test/episodes                  | 3600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.608      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0026    |
| test/info_shaping_reward_mean  | -0.0712    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -2.0768664 |
| test/Q_plus_P                  | -2.0768664 |
| test/reward_per_eps            | -15.7      |
| test/steps                     | 144000     |
| train/episodes                 | 14400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00441   |
| train/info_shaping_reward_mean | -0.0805    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 576000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 360        |
| stats_o/mean                   | 0.20489997 |
| stats_o/std                    | 0.0847992  |
| test/episodes                  | 3610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00201   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.3382226 |
| test/Q_plus_P                  | -1.3382226 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 144400     |
| train/episodes                 | 14440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0058    |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 577600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 361        |
| stats_o/mean                   | 0.20488833 |
| stats_o/std                    | 0.08475361 |
| test/episodes                  | 3620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00354   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3066448 |
| test/Q_plus_P                  | -1.3066448 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 144800     |
| train/episodes                 | 14480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00343   |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 579200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 362        |
| stats_o/mean                   | 0.20489275 |
| stats_o/std                    | 0.08468268 |
| test/episodes                  | 3630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00759   |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4362106 |
| test/Q_plus_P                  | -1.4362106 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 145200     |
| train/episodes                 | 14520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0033    |
| train/info_shaping_reward_mean | -0.0636    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 580800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 363        |
| stats_o/mean                   | 0.20490152 |
| stats_o/std                    | 0.08462951 |
| test/episodes                  | 3640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0031    |
| test/info_shaping_reward_mean  | -0.0591    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.5472898 |
| test/Q_plus_P                  | -1.5472898 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 145600     |
| train/episodes                 | 14560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00508   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 582400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 364        |
| stats_o/mean                   | 0.20489739 |
| stats_o/std                    | 0.08456688 |
| test/episodes                  | 3650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00106   |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2816129 |
| test/Q_plus_P                  | -1.2816129 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 146000     |
| train/episodes                 | 14600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00357   |
| train/info_shaping_reward_mean | -0.0683    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 584000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.20489593  |
| stats_o/std                    | 0.084508136 |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000836   |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.4097664  |
| test/Q_plus_P                  | -1.4097664  |
| test/reward_per_eps            | -10         |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.566       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00338    |
| train/info_shaping_reward_mean | -0.0771     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 366         |
| stats_o/mean                   | 0.20489986  |
| stats_o/std                    | 0.084511906 |
| test/episodes                  | 3670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2950438  |
| test/Q_plus_P                  | -1.2950438  |
| test/reward_per_eps            | -10         |
| test/steps                     | 146800      |
| train/episodes                 | 14680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.557       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00421    |
| train/info_shaping_reward_mean | -0.0933     |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 587200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.20489879  |
| stats_o/std                    | 0.084449865 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00321    |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2940652  |
| test/Q_plus_P                  | -1.2940652  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00441    |
| train/info_shaping_reward_mean | -0.0706     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 368        |
| stats_o/mean                   | 0.20490135 |
| stats_o/std                    | 0.08441744 |
| test/episodes                  | 3690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00484   |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1313751 |
| test/Q_plus_P                  | -1.1313751 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 147600     |
| train/episodes                 | 14760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00369   |
| train/info_shaping_reward_mean | -0.0686    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 590400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 369        |
| stats_o/mean                   | 0.2049127  |
| stats_o/std                    | 0.08435932 |
| test/episodes                  | 3700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00186   |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4500717 |
| test/Q_plus_P                  | -1.4500717 |
| test/reward_per_eps            | -11        |
| test/steps                     | 148000     |
| train/episodes                 | 14800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00568   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 592000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 370        |
| stats_o/mean                   | 0.20491348 |
| stats_o/std                    | 0.08429544 |
| test/episodes                  | 3710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00165   |
| test/info_shaping_reward_mean  | -0.05      |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.1745499 |
| test/Q_plus_P                  | -1.1745499 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 148400     |
| train/episodes                 | 14840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00338   |
| train/info_shaping_reward_mean | -0.0641    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 593600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 371        |
| stats_o/mean                   | 0.20491111 |
| stats_o/std                    | 0.08424651 |
| test/episodes                  | 3720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00214   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2594625 |
| test/Q_plus_P                  | -1.2594625 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 148800     |
| train/episodes                 | 14880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00373   |
| train/info_shaping_reward_mean | -0.0696    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 595200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 372        |
| stats_o/mean                   | 0.20490837 |
| stats_o/std                    | 0.08421874 |
| test/episodes                  | 3730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00152   |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6529738 |
| test/Q_plus_P                  | -1.6529738 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 149200     |
| train/episodes                 | 14920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 596800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 373         |
| stats_o/mean                   | 0.20490555  |
| stats_o/std                    | 0.084166594 |
| test/episodes                  | 3740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.3941101  |
| test/Q_plus_P                  | -1.3941101  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 149600      |
| train/episodes                 | 14960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00418    |
| train/info_shaping_reward_mean | -0.0691     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 598400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.20490654 |
| stats_o/std                    | 0.08413697 |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000984  |
| test/info_shaping_reward_mean  | -0.0417    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0613786 |
| test/Q_plus_P                  | -1.0613786 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00406   |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 375        |
| stats_o/mean                   | 0.20490569 |
| stats_o/std                    | 0.08408041 |
| test/episodes                  | 3760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000869  |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.3414363 |
| test/Q_plus_P                  | -1.3414363 |
| test/reward_per_eps            | -10        |
| test/steps                     | 150400     |
| train/episodes                 | 15040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00348   |
| train/info_shaping_reward_mean | -0.0634    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 601600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.20491028 |
| stats_o/std                    | 0.08403193 |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2796787 |
| test/Q_plus_P                  | -1.2796787 |
| test/reward_per_eps            | -10        |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00325   |
| train/info_shaping_reward_mean | -0.0665    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.20491014  |
| stats_o/std                    | 0.083992586 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00233    |
| test/info_shaping_reward_mean  | -0.0554     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5847725  |
| test/Q_plus_P                  | -1.5847725  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00349    |
| train/info_shaping_reward_mean | -0.0696     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 378        |
| stats_o/mean                   | 0.20491226 |
| stats_o/std                    | 0.08392832 |
| test/episodes                  | 3790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00125   |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4698539 |
| test/Q_plus_P                  | -1.4698539 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 151600     |
| train/episodes                 | 15160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00327   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 606400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 379        |
| stats_o/mean                   | 0.20491539 |
| stats_o/std                    | 0.08386909 |
| test/episodes                  | 3800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00096   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.4390141 |
| test/Q_plus_P                  | -1.4390141 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 152000     |
| train/episodes                 | 15200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00238   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 608000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 380        |
| stats_o/mean                   | 0.20491758 |
| stats_o/std                    | 0.08384044 |
| test/episodes                  | 3810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000866  |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4568807 |
| test/Q_plus_P                  | -1.4568807 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 152400     |
| train/episodes                 | 15240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00358   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 609600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 381         |
| stats_o/mean                   | 0.20491213  |
| stats_o/std                    | 0.083800256 |
| test/episodes                  | 3820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00167    |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.2863828  |
| test/Q_plus_P                  | -1.2863828  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 152800      |
| train/episodes                 | 15280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00401    |
| train/info_shaping_reward_mean | -0.0716     |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 611200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.20489554 |
| stats_o/std                    | 0.08377316 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000841  |
| test/info_shaping_reward_mean  | -0.0467    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2763327 |
| test/Q_plus_P                  | -1.2763327 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.555      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00329   |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 383        |
| stats_o/mean                   | 0.20489675 |
| stats_o/std                    | 0.08371577 |
| test/episodes                  | 3840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00122   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.6107416 |
| test/Q_plus_P                  | -1.6107416 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 153600     |
| train/episodes                 | 15360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0032    |
| train/info_shaping_reward_mean | -0.0636    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 614400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 384        |
| stats_o/mean                   | 0.20490564 |
| stats_o/std                    | 0.08366989 |
| test/episodes                  | 3850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0032    |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3639377 |
| test/Q_plus_P                  | -1.3639377 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 154000     |
| train/episodes                 | 15400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00349   |
| train/info_shaping_reward_mean | -0.0748    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 616000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 385        |
| stats_o/mean                   | 0.20490994 |
| stats_o/std                    | 0.08362163 |
| test/episodes                  | 3860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000714  |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3184366 |
| test/Q_plus_P                  | -1.3184366 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 154400     |
| train/episodes                 | 15440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00303   |
| train/info_shaping_reward_mean | -0.0673    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 617600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 386         |
| stats_o/mean                   | 0.20490378  |
| stats_o/std                    | 0.083556645 |
| test/episodes                  | 3870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0018     |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3438667  |
| test/Q_plus_P                  | -1.3438667  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 154800      |
| train/episodes                 | 15480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00427    |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 619200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.20489927  |
| stats_o/std                    | 0.083524436 |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3709282  |
| test/Q_plus_P                  | -1.3709282  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0722     |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 388        |
| stats_o/mean                   | 0.2049004  |
| stats_o/std                    | 0.08351404 |
| test/episodes                  | 3890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00178   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2658718 |
| test/Q_plus_P                  | -1.2658718 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 155600     |
| train/episodes                 | 15560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00323   |
| train/info_shaping_reward_mean | -0.085     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 622400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 389        |
| stats_o/mean                   | 0.20490858 |
| stats_o/std                    | 0.08351895 |
| test/episodes                  | 3900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000999  |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3142962 |
| test/Q_plus_P                  | -1.3142962 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 156000     |
| train/episodes                 | 15600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00321   |
| train/info_shaping_reward_mean | -0.078     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 624000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 390        |
| stats_o/mean                   | 0.20490217 |
| stats_o/std                    | 0.08346304 |
| test/episodes                  | 3910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00201   |
| test/info_shaping_reward_mean  | -0.0571    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6239922 |
| test/Q_plus_P                  | -1.6239922 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 156400     |
| train/episodes                 | 15640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00434   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 625600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.20489775  |
| stats_o/std                    | 0.083423555 |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.5586282  |
| test/Q_plus_P                  | -1.5586282  |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.575       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00425    |
| train/info_shaping_reward_mean | -0.0699     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 392        |
| stats_o/mean                   | 0.20489258 |
| stats_o/std                    | 0.08338305 |
| test/episodes                  | 3930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00086   |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4981732 |
| test/Q_plus_P                  | -1.4981732 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 157200     |
| train/episodes                 | 15720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00442   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 628800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 393         |
| stats_o/mean                   | 0.20489435  |
| stats_o/std                    | 0.083350636 |
| test/episodes                  | 3940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000478   |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.4450347  |
| test/Q_plus_P                  | -1.4450347  |
| test/reward_per_eps            | -11         |
| test/steps                     | 157600      |
| train/episodes                 | 15760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.553       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00485    |
| train/info_shaping_reward_mean | -0.0754     |
| train/info_shaping_reward_min  | -0.198      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 630400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 394        |
| stats_o/mean                   | 0.20489204 |
| stats_o/std                    | 0.08330305 |
| test/episodes                  | 3950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000416  |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3301867 |
| test/Q_plus_P                  | -1.3301867 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 158000     |
| train/episodes                 | 15800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00276   |
| train/info_shaping_reward_mean | -0.0732    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 632000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 395        |
| stats_o/mean                   | 0.2048998  |
| stats_o/std                    | 0.08331452 |
| test/episodes                  | 3960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000916  |
| test/info_shaping_reward_mean  | -0.0429    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1698582 |
| test/Q_plus_P                  | -1.1698582 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 158400     |
| train/episodes                 | 15840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.517      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00397   |
| train/info_shaping_reward_mean | -0.0846    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 633600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 396        |
| stats_o/mean                   | 0.20490026 |
| stats_o/std                    | 0.08327906 |
| test/episodes                  | 3970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0015    |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3556762 |
| test/Q_plus_P                  | -1.3556762 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 158800     |
| train/episodes                 | 15880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.59       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.0738    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 635200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.20490463  |
| stats_o/std                    | 0.083273746 |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00108    |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.5262375  |
| test/Q_plus_P                  | -1.5262375  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.602       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00369    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 398         |
| stats_o/mean                   | 0.20490259  |
| stats_o/std                    | 0.083237536 |
| test/episodes                  | 3990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00113    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.3215685  |
| test/Q_plus_P                  | -1.3215685  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 159600      |
| train/episodes                 | 15960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00362    |
| train/info_shaping_reward_mean | -0.0673     |
| train/info_shaping_reward_min  | -0.203      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 638400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 399        |
| stats_o/mean                   | 0.20489359 |
| stats_o/std                    | 0.08319886 |
| test/episodes                  | 4000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0013    |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4875027 |
| test/Q_plus_P                  | -1.4875027 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 160000     |
| train/episodes                 | 16000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.544      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0035    |
| train/info_shaping_reward_mean | -0.0733    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 640000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 400        |
| stats_o/mean                   | 0.204897   |
| stats_o/std                    | 0.08314388 |
| test/episodes                  | 4010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00164   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3425473 |
| test/Q_plus_P                  | -1.3425473 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 160400     |
| train/episodes                 | 16040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00315   |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 641600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 401        |
| stats_o/mean                   | 0.2048985  |
| stats_o/std                    | 0.08307641 |
| test/episodes                  | 4020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000805  |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1958327 |
| test/Q_plus_P                  | -1.1958327 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 160800     |
| train/episodes                 | 16080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.647      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 643200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.20489353 |
| stats_o/std                    | 0.08302052 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.4430838 |
| test/Q_plus_P                  | -1.4430838 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00331   |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 403        |
| stats_o/mean                   | 0.20489496 |
| stats_o/std                    | 0.08299994 |
| test/episodes                  | 4040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2691951 |
| test/Q_plus_P                  | -1.2691951 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 161600     |
| train/episodes                 | 16160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 646400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.20489801  |
| stats_o/std                    | 0.082937315 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000838   |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.3721476  |
| test/Q_plus_P                  | -1.3721476  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 405        |
| stats_o/mean                   | 0.20488322 |
| stats_o/std                    | 0.0829099  |
| test/episodes                  | 4060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00186   |
| test/info_shaping_reward_mean  | -0.0472    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2531933 |
| test/Q_plus_P                  | -1.2531933 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 162400     |
| train/episodes                 | 16240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.544      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 649600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 406        |
| stats_o/mean                   | 0.20488483 |
| stats_o/std                    | 0.08287148 |
| test/episodes                  | 4070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00114   |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1055284 |
| test/Q_plus_P                  | -1.1055284 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 162800     |
| train/episodes                 | 16280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 651200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.20488036  |
| stats_o/std                    | 0.082832344 |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00156    |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3086777  |
| test/Q_plus_P                  | -1.3086777  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.6         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00372    |
| train/info_shaping_reward_mean | -0.0687     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 408        |
| stats_o/mean                   | 0.2048826  |
| stats_o/std                    | 0.08283086 |
| test/episodes                  | 4090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00291   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3425041 |
| test/Q_plus_P                  | -1.3425041 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 163600     |
| train/episodes                 | 16360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.0846    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 654400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 409        |
| stats_o/mean                   | 0.20487872 |
| stats_o/std                    | 0.08278421 |
| test/episodes                  | 4100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00187   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.4239582 |
| test/Q_plus_P                  | -1.4239582 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 164000     |
| train/episodes                 | 16400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.653      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00401   |
| train/info_shaping_reward_mean | -0.0683    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 656000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 410         |
| stats_o/mean                   | 0.20486407  |
| stats_o/std                    | 0.082760096 |
| test/episodes                  | 4110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000642   |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1563098  |
| test/Q_plus_P                  | -1.1563098  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 164400      |
| train/episodes                 | 16440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.556       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.0721     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 657600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 411        |
| stats_o/mean                   | 0.20486234 |
| stats_o/std                    | 0.0827161  |
| test/episodes                  | 4120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00189   |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.3775694 |
| test/Q_plus_P                  | -1.3775694 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 164800     |
| train/episodes                 | 16480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00299   |
| train/info_shaping_reward_mean | -0.065     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 659200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 412        |
| stats_o/mean                   | 0.20486796 |
| stats_o/std                    | 0.08273561 |
| test/episodes                  | 4130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00145   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3485585 |
| test/Q_plus_P                  | -1.3485585 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 165200     |
| train/episodes                 | 16520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00395   |
| train/info_shaping_reward_mean | -0.076     |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 660800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 413        |
| stats_o/mean                   | 0.20486897 |
| stats_o/std                    | 0.08268316 |
| test/episodes                  | 4140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00133   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3496693 |
| test/Q_plus_P                  | -1.3496693 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 165600     |
| train/episodes                 | 16560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00373   |
| train/info_shaping_reward_mean | -0.0654    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 662400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 414        |
| stats_o/mean                   | 0.20485055 |
| stats_o/std                    | 0.08266187 |
| test/episodes                  | 4150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5855178 |
| test/Q_plus_P                  | -1.5855178 |
| test/reward_per_eps            | -12        |
| test/steps                     | 166000     |
| train/episodes                 | 16600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0042    |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 664000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 415        |
| stats_o/mean                   | 0.20485722 |
| stats_o/std                    | 0.08262989 |
| test/episodes                  | 4160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00304   |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4353489 |
| test/Q_plus_P                  | -1.4353489 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 166400     |
| train/episodes                 | 16640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0026    |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 665600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 416         |
| stats_o/mean                   | 0.20485209  |
| stats_o/std                    | 0.082622975 |
| test/episodes                  | 4170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0023     |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.4179661  |
| test/Q_plus_P                  | -1.4179661  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 166800      |
| train/episodes                 | 16680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.577       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00421    |
| train/info_shaping_reward_mean | -0.0714     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 667200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 417        |
| stats_o/mean                   | 0.20484242 |
| stats_o/std                    | 0.08259561 |
| test/episodes                  | 4180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.1018656 |
| test/Q_plus_P                  | -1.1018656 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 167200     |
| train/episodes                 | 16720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00447   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 668800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 418        |
| stats_o/mean                   | 0.20484571 |
| stats_o/std                    | 0.08257133 |
| test/episodes                  | 4190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00101   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3185387 |
| test/Q_plus_P                  | -1.3185387 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 167600     |
| train/episodes                 | 16760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0769    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 670400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 419        |
| stats_o/mean                   | 0.20486268 |
| stats_o/std                    | 0.08256787 |
| test/episodes                  | 4200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.003     |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3235981 |
| test/Q_plus_P                  | -1.3235981 |
| test/reward_per_eps            | -10        |
| test/steps                     | 168000     |
| train/episodes                 | 16800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00398   |
| train/info_shaping_reward_mean | -0.0777    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 672000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 420        |
| stats_o/mean                   | 0.20485535 |
| stats_o/std                    | 0.08251799 |
| test/episodes                  | 4210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000285  |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.256249  |
| test/Q_plus_P                  | -1.256249  |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 168400     |
| train/episodes                 | 16840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0034    |
| train/info_shaping_reward_mean | -0.0632    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 673600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 421        |
| stats_o/mean                   | 0.20484285 |
| stats_o/std                    | 0.08251526 |
| test/episodes                  | 4220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00113   |
| test/info_shaping_reward_mean  | -0.0568    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5518254 |
| test/Q_plus_P                  | -1.5518254 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 168800     |
| train/episodes                 | 16880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00271   |
| train/info_shaping_reward_mean | -0.0777    |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 675200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 422        |
| stats_o/mean                   | 0.20485212 |
| stats_o/std                    | 0.08249956 |
| test/episodes                  | 4230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00149   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1785691 |
| test/Q_plus_P                  | -1.1785691 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 169200     |
| train/episodes                 | 16920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.575      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00388   |
| train/info_shaping_reward_mean | -0.0768    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 676800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 423        |
| stats_o/mean                   | 0.20484306 |
| stats_o/std                    | 0.08250137 |
| test/episodes                  | 4240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00338   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1905932 |
| test/Q_plus_P                  | -1.1905932 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 169600     |
| train/episodes                 | 16960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 678400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 424        |
| stats_o/mean                   | 0.20484608 |
| stats_o/std                    | 0.08245544 |
| test/episodes                  | 4250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00486   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.428885  |
| test/Q_plus_P                  | -1.428885  |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 170000     |
| train/episodes                 | 17000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00233   |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 680000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 425        |
| stats_o/mean                   | 0.20484829 |
| stats_o/std                    | 0.08246257 |
| test/episodes                  | 4260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00089   |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3321266 |
| test/Q_plus_P                  | -1.3321266 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 170400     |
| train/episodes                 | 17040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00348   |
| train/info_shaping_reward_mean | -0.0769    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 681600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 426        |
| stats_o/mean                   | 0.20484346 |
| stats_o/std                    | 0.08244544 |
| test/episodes                  | 4270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00205   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.324862  |
| test/Q_plus_P                  | -1.324862  |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 170800     |
| train/episodes                 | 17080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0034    |
| train/info_shaping_reward_mean | -0.073     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 683200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 427        |
| stats_o/mean                   | 0.20483582 |
| stats_o/std                    | 0.08242866 |
| test/episodes                  | 4280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00302   |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2175758 |
| test/Q_plus_P                  | -1.2175758 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 171200     |
| train/episodes                 | 17120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00331   |
| train/info_shaping_reward_mean | -0.0639    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 684800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 428        |
| stats_o/mean                   | 0.20482963 |
| stats_o/std                    | 0.08238372 |
| test/episodes                  | 4290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00297   |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1395128 |
| test/Q_plus_P                  | -1.1395128 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 171600     |
| train/episodes                 | 17160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.0667    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 686400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 429        |
| stats_o/mean                   | 0.20483074 |
| stats_o/std                    | 0.08233218 |
| test/episodes                  | 4300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000829  |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1801641 |
| test/Q_plus_P                  | -1.1801641 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 172000     |
| train/episodes                 | 17200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 688000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 430        |
| stats_o/mean                   | 0.20482683 |
| stats_o/std                    | 0.08232892 |
| test/episodes                  | 4310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00164   |
| test/info_shaping_reward_mean  | -0.0463    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.013191  |
| test/Q_plus_P                  | -1.013191  |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 172400     |
| train/episodes                 | 17240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00385   |
| train/info_shaping_reward_mean | -0.0812    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 689600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 431        |
| stats_o/mean                   | 0.20483583 |
| stats_o/std                    | 0.08232561 |
| test/episodes                  | 4320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00187   |
| test/info_shaping_reward_mean  | -0.0598    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.6189008 |
| test/Q_plus_P                  | -1.6189008 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 172800     |
| train/episodes                 | 17280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0046    |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 691200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 432        |
| stats_o/mean                   | 0.20484175 |
| stats_o/std                    | 0.08229323 |
| test/episodes                  | 4330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000855  |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.4622188 |
| test/Q_plus_P                  | -1.4622188 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 173200     |
| train/episodes                 | 17320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00311   |
| train/info_shaping_reward_mean | -0.0739    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 692800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 433        |
| stats_o/mean                   | 0.20483723 |
| stats_o/std                    | 0.08226065 |
| test/episodes                  | 4340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00129   |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.508915  |
| test/Q_plus_P                  | -1.508915  |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 173600     |
| train/episodes                 | 17360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00378   |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 694400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 434        |
| stats_o/mean                   | 0.20483609 |
| stats_o/std                    | 0.08224971 |
| test/episodes                  | 4350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00108   |
| test/info_shaping_reward_mean  | -0.0601    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.7094232 |
| test/Q_plus_P                  | -1.7094232 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 174000     |
| train/episodes                 | 17400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.0776    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 696000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 435        |
| stats_o/mean                   | 0.20483536 |
| stats_o/std                    | 0.08220339 |
| test/episodes                  | 4360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00275   |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4157534 |
| test/Q_plus_P                  | -1.4157534 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 174400     |
| train/episodes                 | 17440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00349   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 697600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 436        |
| stats_o/mean                   | 0.20483427 |
| stats_o/std                    | 0.08218446 |
| test/episodes                  | 4370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00184   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.4259465 |
| test/Q_plus_P                  | -1.4259465 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 174800     |
| train/episodes                 | 17480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00321   |
| train/info_shaping_reward_mean | -0.0771    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 699200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 437         |
| stats_o/mean                   | 0.20484109  |
| stats_o/std                    | 0.082132556 |
| test/episodes                  | 4380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0518     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.3512144  |
| test/Q_plus_P                  | -1.3512144  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 175200      |
| train/episodes                 | 17520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00528    |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 700800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 438        |
| stats_o/mean                   | 0.20482917 |
| stats_o/std                    | 0.08208969 |
| test/episodes                  | 4390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00392   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0819137 |
| test/Q_plus_P                  | -1.0819137 |
| test/reward_per_eps            | -9         |
| test/steps                     | 175600     |
| train/episodes                 | 17560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00356   |
| train/info_shaping_reward_mean | -0.0645    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 702400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 439        |
| stats_o/mean                   | 0.20483516 |
| stats_o/std                    | 0.08207055 |
| test/episodes                  | 4400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.287276  |
| test/Q_plus_P                  | -1.287276  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 176000     |
| train/episodes                 | 17600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00458   |
| train/info_shaping_reward_mean | -0.0745    |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 704000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 440        |
| stats_o/mean                   | 0.20483747 |
| stats_o/std                    | 0.08201235 |
| test/episodes                  | 4410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00116   |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.4205719 |
| test/Q_plus_P                  | -1.4205719 |
| test/reward_per_eps            | -11        |
| test/steps                     | 176400     |
| train/episodes                 | 17640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00258   |
| train/info_shaping_reward_mean | -0.0624    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 705600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.2048328   |
| stats_o/std                    | 0.081967026 |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00364    |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.2286719  |
| test/Q_plus_P                  | -1.2286719  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00388    |
| train/info_shaping_reward_mean | -0.0634     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 442        |
| stats_o/mean                   | 0.20483513 |
| stats_o/std                    | 0.08190817 |
| test/episodes                  | 4430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00217   |
| test/info_shaping_reward_mean  | -0.0547    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.3582525 |
| test/Q_plus_P                  | -1.3582525 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 177200     |
| train/episodes                 | 17720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 708800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.20483267  |
| stats_o/std                    | 0.081889905 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00364    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.0929095  |
| test/Q_plus_P                  | -1.0929095  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0776     |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 444        |
| stats_o/mean                   | 0.2048372  |
| stats_o/std                    | 0.081897   |
| test/episodes                  | 4450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00295   |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2977362 |
| test/Q_plus_P                  | -1.2977362 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 178000     |
| train/episodes                 | 17800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0771    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 712000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.20482886  |
| stats_o/std                    | 0.081888445 |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00276    |
| test/info_shaping_reward_mean  | -0.055      |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.382275   |
| test/Q_plus_P                  | -1.382275   |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.534       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0758     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 446        |
| stats_o/mean                   | 0.2048228  |
| stats_o/std                    | 0.08189167 |
| test/episodes                  | 4470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00379   |
| test/info_shaping_reward_mean  | -0.0467    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1073014 |
| test/Q_plus_P                  | -1.1073014 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 178800     |
| train/episodes                 | 17880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00314   |
| train/info_shaping_reward_mean | -0.0771    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 715200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 447        |
| stats_o/mean                   | 0.20483758 |
| stats_o/std                    | 0.0819176  |
| test/episodes                  | 4480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0024    |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.17992   |
| test/Q_plus_P                  | -1.17992   |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 179200     |
| train/episodes                 | 17920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.567      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00307   |
| train/info_shaping_reward_mean | -0.0923    |
| train/info_shaping_reward_min  | -0.305     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 716800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 448        |
| stats_o/mean                   | 0.20484433 |
| stats_o/std                    | 0.08188019 |
| test/episodes                  | 4490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00223   |
| test/info_shaping_reward_mean  | -0.0436    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0956422 |
| test/Q_plus_P                  | -1.0956422 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 179600     |
| train/episodes                 | 17960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00454   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 718400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 449        |
| stats_o/mean                   | 0.20484953 |
| stats_o/std                    | 0.08186074 |
| test/episodes                  | 4500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00226   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0720396 |
| test/Q_plus_P                  | -1.0720396 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 180000     |
| train/episodes                 | 18000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00433   |
| train/info_shaping_reward_mean | -0.0722    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 720000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.20484954 |
| stats_o/std                    | 0.08180945 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00131   |
| test/info_shaping_reward_mean  | -0.0421    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0235566 |
| test/Q_plus_P                  | -1.0235566 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.0641    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 451        |
| stats_o/mean                   | 0.2048498  |
| stats_o/std                    | 0.08175649 |
| test/episodes                  | 4520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4242618 |
| test/Q_plus_P                  | -1.4242618 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 180800     |
| train/episodes                 | 18080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00371   |
| train/info_shaping_reward_mean | -0.0708    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 723200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 452         |
| stats_o/mean                   | 0.20485047  |
| stats_o/std                    | 0.081697084 |
| test/episodes                  | 4530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00334    |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5231361  |
| test/Q_plus_P                  | -1.5231361  |
| test/reward_per_eps            | -12         |
| test/steps                     | 181200      |
| train/episodes                 | 18120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00468    |
| train/info_shaping_reward_mean | -0.0699     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 724800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 453        |
| stats_o/mean                   | 0.20486012 |
| stats_o/std                    | 0.08165028 |
| test/episodes                  | 4540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00164   |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4994392 |
| test/Q_plus_P                  | -1.4994392 |
| test/reward_per_eps            | -12        |
| test/steps                     | 181600     |
| train/episodes                 | 18160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00347   |
| train/info_shaping_reward_mean | -0.0683    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 726400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.20485191  |
| stats_o/std                    | 0.081657544 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00183    |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.4778992  |
| test/Q_plus_P                  | -1.4778992  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00429    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.2048566   |
| stats_o/std                    | 0.081645906 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00191    |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.4334873  |
| test/Q_plus_P                  | -1.4334873  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.0803     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 456        |
| stats_o/mean                   | 0.2048526  |
| stats_o/std                    | 0.08158675 |
| test/episodes                  | 4570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00434   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.4209651 |
| test/Q_plus_P                  | -1.4209651 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 182800     |
| train/episodes                 | 18280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00306   |
| train/info_shaping_reward_mean | -0.0656    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 731200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.20485108  |
| stats_o/std                    | 0.081559226 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00317    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.2758021  |
| test/Q_plus_P                  | -1.2758021  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.573       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.0706     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 458        |
| stats_o/mean                   | 0.20484671 |
| stats_o/std                    | 0.08151362 |
| test/episodes                  | 4590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.0448    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1013416 |
| test/Q_plus_P                  | -1.1013416 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 183600     |
| train/episodes                 | 18360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 734400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 459        |
| stats_o/mean                   | 0.20485874 |
| stats_o/std                    | 0.08148738 |
| test/episodes                  | 4600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00139   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2318785 |
| test/Q_plus_P                  | -1.2318785 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 184000     |
| train/episodes                 | 18400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.59       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00413   |
| train/info_shaping_reward_mean | -0.0691    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 736000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 460        |
| stats_o/mean                   | 0.20485699 |
| stats_o/std                    | 0.08144011 |
| test/episodes                  | 4610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00287   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.3514272 |
| test/Q_plus_P                  | -1.3514272 |
| test/reward_per_eps            | -11        |
| test/steps                     | 184400     |
| train/episodes                 | 18440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00409   |
| train/info_shaping_reward_mean | -0.0679    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 737600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 461        |
| stats_o/mean                   | 0.20485474 |
| stats_o/std                    | 0.08138853 |
| test/episodes                  | 4620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00181   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.2346137 |
| test/Q_plus_P                  | -1.2346137 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 184800     |
| train/episodes                 | 18480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00325   |
| train/info_shaping_reward_mean | -0.0681    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 739200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 462        |
| stats_o/mean                   | 0.20485869 |
| stats_o/std                    | 0.08134451 |
| test/episodes                  | 4630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0014    |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3378147 |
| test/Q_plus_P                  | -1.3378147 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 185200     |
| train/episodes                 | 18520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00343   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 740800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 463        |
| stats_o/mean                   | 0.20485054 |
| stats_o/std                    | 0.08131299 |
| test/episodes                  | 4640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00116   |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3081577 |
| test/Q_plus_P                  | -1.3081577 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 185600     |
| train/episodes                 | 18560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00306   |
| train/info_shaping_reward_mean | -0.0632    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 742400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 464         |
| stats_o/mean                   | 0.2048544   |
| stats_o/std                    | 0.081326574 |
| test/episodes                  | 4650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00205    |
| test/info_shaping_reward_mean  | -0.0472     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.2869574  |
| test/Q_plus_P                  | -1.2869574  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 186000      |
| train/episodes                 | 18600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00378    |
| train/info_shaping_reward_mean | -0.0755     |
| train/info_shaping_reward_min  | -0.219      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 744000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 465        |
| stats_o/mean                   | 0.20485634 |
| stats_o/std                    | 0.08129882 |
| test/episodes                  | 4660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00226   |
| test/info_shaping_reward_mean  | -0.0602    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.6642957 |
| test/Q_plus_P                  | -1.6642957 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 186400     |
| train/episodes                 | 18640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.565      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00431   |
| train/info_shaping_reward_mean | -0.0713    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 745600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 466        |
| stats_o/mean                   | 0.20485666 |
| stats_o/std                    | 0.08127555 |
| test/episodes                  | 4670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000427  |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.529305  |
| test/Q_plus_P                  | -1.529305  |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 186800     |
| train/episodes                 | 18680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00433   |
| train/info_shaping_reward_mean | -0.0778    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 747200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 467        |
| stats_o/mean                   | 0.20486122 |
| stats_o/std                    | 0.08124119 |
| test/episodes                  | 4680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00241   |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4518427 |
| test/Q_plus_P                  | -1.4518427 |
| test/reward_per_eps            | -11        |
| test/steps                     | 187200     |
| train/episodes                 | 18720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00432   |
| train/info_shaping_reward_mean | -0.0717    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 748800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 468        |
| stats_o/mean                   | 0.20486332 |
| stats_o/std                    | 0.08120878 |
| test/episodes                  | 4690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00278   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4935178 |
| test/Q_plus_P                  | -1.4935178 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 187600     |
| train/episodes                 | 18760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00381   |
| train/info_shaping_reward_mean | -0.0778    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 750400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 469        |
| stats_o/mean                   | 0.20486668 |
| stats_o/std                    | 0.08117444 |
| test/episodes                  | 4700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00164   |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2513556 |
| test/Q_plus_P                  | -1.2513556 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 188000     |
| train/episodes                 | 18800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00377   |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 752000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.20487928 |
| stats_o/std                    | 0.08113773 |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00513   |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5326664 |
| test/Q_plus_P                  | -1.5326664 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0049    |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.20487556 |
| stats_o/std                    | 0.08110728 |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000652  |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2726346 |
| test/Q_plus_P                  | -1.2726346 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00472   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.20486952 |
| stats_o/std                    | 0.08108511 |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00116   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.3957676 |
| test/Q_plus_P                  | -1.3957676 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0031    |
| train/info_shaping_reward_mean | -0.0634    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 473        |
| stats_o/mean                   | 0.20486496 |
| stats_o/std                    | 0.08105129 |
| test/episodes                  | 4740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000698  |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5521556 |
| test/Q_plus_P                  | -1.5521556 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 189600     |
| train/episodes                 | 18960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00428   |
| train/info_shaping_reward_mean | -0.0652    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 758400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 474        |
| stats_o/mean                   | 0.204866   |
| stats_o/std                    | 0.08101797 |
| test/episodes                  | 4750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00249   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2141606 |
| test/Q_plus_P                  | -1.2141606 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 190000     |
| train/episodes                 | 19000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00233   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 760000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 475         |
| stats_o/mean                   | 0.20486817  |
| stats_o/std                    | 0.080992915 |
| test/episodes                  | 4760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000746   |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2504026  |
| test/Q_plus_P                  | -1.2504026  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 190400      |
| train/episodes                 | 19040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0652     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 761600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 476        |
| stats_o/mean                   | 0.20486838 |
| stats_o/std                    | 0.08095941 |
| test/episodes                  | 4770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00217   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4540335 |
| test/Q_plus_P                  | -1.4540335 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 190800     |
| train/episodes                 | 19080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.63       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00302   |
| train/info_shaping_reward_mean | -0.0644    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 763200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 477        |
| stats_o/mean                   | 0.2048705  |
| stats_o/std                    | 0.08091006 |
| test/episodes                  | 4780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00292   |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -1.3887112 |
| test/Q_plus_P                  | -1.3887112 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 191200     |
| train/episodes                 | 19120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00367   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 764800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 478         |
| stats_o/mean                   | 0.20487043  |
| stats_o/std                    | 0.080880076 |
| test/episodes                  | 4790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0019     |
| test/info_shaping_reward_mean  | -0.0527     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -1.3189023  |
| test/Q_plus_P                  | -1.3189023  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 191600      |
| train/episodes                 | 19160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00403    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 766400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 479        |
| stats_o/mean                   | 0.20486578 |
| stats_o/std                    | 0.08085462 |
| test/episodes                  | 4800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00174   |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4792682 |
| test/Q_plus_P                  | -1.4792682 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 192000     |
| train/episodes                 | 19200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00462   |
| train/info_shaping_reward_mean | -0.0733    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 768000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 480         |
| stats_o/mean                   | 0.20486623  |
| stats_o/std                    | 0.080825575 |
| test/episodes                  | 4810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00197    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2362342  |
| test/Q_plus_P                  | -1.2362342  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 192400      |
| train/episodes                 | 19240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0713     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 769600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 481        |
| stats_o/mean                   | 0.2048687  |
| stats_o/std                    | 0.08079353 |
| test/episodes                  | 4820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00169   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2910018 |
| test/Q_plus_P                  | -1.2910018 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 192800     |
| train/episodes                 | 19280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00467   |
| train/info_shaping_reward_mean | -0.0683    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 771200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.20486507  |
| stats_o/std                    | 0.080752224 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00274    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1192495  |
| test/Q_plus_P                  | -1.1192495  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 483        |
| stats_o/mean                   | 0.20487744 |
| stats_o/std                    | 0.08073142 |
| test/episodes                  | 4840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1569175 |
| test/Q_plus_P                  | -1.1569175 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 193600     |
| train/episodes                 | 19360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.072     |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 774400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 484        |
| stats_o/mean                   | 0.20487164 |
| stats_o/std                    | 0.0807088  |
| test/episodes                  | 4850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00159   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3665463 |
| test/Q_plus_P                  | -1.3665463 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 194000     |
| train/episodes                 | 19400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0043    |
| train/info_shaping_reward_mean | -0.0651    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 776000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 485        |
| stats_o/mean                   | 0.20488241 |
| stats_o/std                    | 0.08070667 |
| test/episodes                  | 4860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00332   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.217501  |
| test/Q_plus_P                  | -1.217501  |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 194400     |
| train/episodes                 | 19440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00449   |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 777600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 486         |
| stats_o/mean                   | 0.20487241  |
| stats_o/std                    | 0.080713674 |
| test/episodes                  | 4870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00251    |
| test/info_shaping_reward_mean  | -0.0568     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -1.519909   |
| test/Q_plus_P                  | -1.519909   |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 194800      |
| train/episodes                 | 19480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00377    |
| train/info_shaping_reward_mean | -0.0806     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 779200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 487        |
| stats_o/mean                   | 0.2048679  |
| stats_o/std                    | 0.08070531 |
| test/episodes                  | 4880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00271   |
| test/info_shaping_reward_mean  | -0.0591    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.6565129 |
| test/Q_plus_P                  | -1.6565129 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 195200     |
| train/episodes                 | 19520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.0717    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 780800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 488        |
| stats_o/mean                   | 0.2048704  |
| stats_o/std                    | 0.08066774 |
| test/episodes                  | 4890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00134   |
| test/info_shaping_reward_mean  | -0.0578    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.6250113 |
| test/Q_plus_P                  | -1.6250113 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 195600     |
| train/episodes                 | 19560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00338   |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 782400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 489         |
| stats_o/mean                   | 0.20487133  |
| stats_o/std                    | 0.080624126 |
| test/episodes                  | 4900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.3203279  |
| test/Q_plus_P                  | -1.3203279  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 196000      |
| train/episodes                 | 19600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00491    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 784000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 490        |
| stats_o/mean                   | 0.20487577 |
| stats_o/std                    | 0.08061327 |
| test/episodes                  | 4910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000475  |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5334616 |
| test/Q_plus_P                  | -1.5334616 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 196400     |
| train/episodes                 | 19640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00387   |
| train/info_shaping_reward_mean | -0.0831    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 785600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.2048809   |
| stats_o/std                    | 0.080588266 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.153164   |
| test/Q_plus_P                  | -1.153164   |
| test/reward_per_eps            | -9          |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00456    |
| train/info_shaping_reward_mean | -0.0731     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 492         |
| stats_o/mean                   | 0.20488717  |
| stats_o/std                    | 0.080572784 |
| test/episodes                  | 4930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00187    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1918398  |
| test/Q_plus_P                  | -1.1918398  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 197200      |
| train/episodes                 | 19720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00456    |
| train/info_shaping_reward_mean | -0.0678     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 788800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.20488054 |
| stats_o/std                    | 0.08053693 |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.0437    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1288316 |
| test/Q_plus_P                  | -1.1288316 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00384   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 494        |
| stats_o/mean                   | 0.20488036 |
| stats_o/std                    | 0.08051187 |
| test/episodes                  | 4950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00216   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.260869  |
| test/Q_plus_P                  | -1.260869  |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 198000     |
| train/episodes                 | 19800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00374   |
| train/info_shaping_reward_mean | -0.0715    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 792000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 495        |
| stats_o/mean                   | 0.20488028 |
| stats_o/std                    | 0.08051314 |
| test/episodes                  | 4960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00354   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.2048981 |
| test/Q_plus_P                  | -1.2048981 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 198400     |
| train/episodes                 | 19840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00472   |
| train/info_shaping_reward_mean | -0.0741    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 793600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 496        |
| stats_o/mean                   | 0.2048767  |
| stats_o/std                    | 0.08051713 |
| test/episodes                  | 4970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000904  |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2080914 |
| test/Q_plus_P                  | -1.2080914 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 198800     |
| train/episodes                 | 19880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.48       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00396   |
| train/info_shaping_reward_mean | -0.0804    |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 795200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 497        |
| stats_o/mean                   | 0.20486885 |
| stats_o/std                    | 0.0804817  |
| test/episodes                  | 4980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0448    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1734279 |
| test/Q_plus_P                  | -1.1734279 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 199200     |
| train/episodes                 | 19920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00296   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 796800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 498        |
| stats_o/mean                   | 0.20486833 |
| stats_o/std                    | 0.08044604 |
| test/episodes                  | 4990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00228   |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.0989386 |
| test/Q_plus_P                  | -1.0989386 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 199600     |
| train/episodes                 | 19960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.57       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00494   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 798400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 499        |
| stats_o/mean                   | 0.2048684  |
| stats_o/std                    | 0.08043896 |
| test/episodes                  | 5000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000895  |
| test/info_shaping_reward_mean  | -0.0396    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.9766249 |
| test/Q_plus_P                  | -0.9766249 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 200000     |
| train/episodes                 | 20000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00379   |
| train/info_shaping_reward_mean | -0.0723    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 800000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 500        |
| stats_o/mean                   | 0.20486823 |
| stats_o/std                    | 0.08042178 |
| test/episodes                  | 5010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00193   |
| test/info_shaping_reward_mean  | -0.0441    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1865562 |
| test/Q_plus_P                  | -1.1865562 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 200400     |
| train/episodes                 | 20040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00406   |
| train/info_shaping_reward_mean | -0.0726    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 801600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 501        |
| stats_o/mean                   | 0.20487982 |
| stats_o/std                    | 0.08040242 |
| test/episodes                  | 5020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00129   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.50099   |
| test/Q_plus_P                  | -1.50099   |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 200800     |
| train/episodes                 | 20080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.585      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00384   |
| train/info_shaping_reward_mean | -0.0774    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 803200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 502        |
| stats_o/mean                   | 0.20487227 |
| stats_o/std                    | 0.08041037 |
| test/episodes                  | 5030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00105   |
| test/info_shaping_reward_mean  | -0.0451    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2066256 |
| test/Q_plus_P                  | -1.2066256 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 201200     |
| train/episodes                 | 20120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00363   |
| train/info_shaping_reward_mean | -0.0772    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 804800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 503        |
| stats_o/mean                   | 0.20487641 |
| stats_o/std                    | 0.0803689  |
| test/episodes                  | 5040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00349   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3146263 |
| test/Q_plus_P                  | -1.3146263 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 201600     |
| train/episodes                 | 20160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.595      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00533   |
| train/info_shaping_reward_mean | -0.0684    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 806400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 504        |
| stats_o/mean                   | 0.20487024 |
| stats_o/std                    | 0.0803393  |
| test/episodes                  | 5050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3626071 |
| test/Q_plus_P                  | -1.3626071 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 202000     |
| train/episodes                 | 20200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00307   |
| train/info_shaping_reward_mean | -0.0641    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 808000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 505        |
| stats_o/mean                   | 0.20486796 |
| stats_o/std                    | 0.08031446 |
| test/episodes                  | 5060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00186   |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.3456564 |
| test/Q_plus_P                  | -1.3456564 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 202400     |
| train/episodes                 | 20240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00361   |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 809600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.2048685   |
| stats_o/std                    | 0.080296904 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00231    |
| test/info_shaping_reward_mean  | -0.0574     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.6109217  |
| test/Q_plus_P                  | -1.6109217  |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00406    |
| train/info_shaping_reward_mean | -0.0777     |
| train/info_shaping_reward_min  | -0.225      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 507        |
| stats_o/mean                   | 0.20486727 |
| stats_o/std                    | 0.08027847 |
| test/episodes                  | 5080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00118   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.3255188 |
| test/Q_plus_P                  | -1.3255188 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 203200     |
| train/episodes                 | 20320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00345   |
| train/info_shaping_reward_mean | -0.0734    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 812800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 508        |
| stats_o/mean                   | 0.20486915 |
| stats_o/std                    | 0.08023234 |
| test/episodes                  | 5090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4876355 |
| test/Q_plus_P                  | -1.4876355 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 203600     |
| train/episodes                 | 20360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00451   |
| train/info_shaping_reward_mean | -0.0641    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 814400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 509        |
| stats_o/mean                   | 0.20488039 |
| stats_o/std                    | 0.08024253 |
| test/episodes                  | 5100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0039    |
| test/info_shaping_reward_mean  | -0.0463    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2358012 |
| test/Q_plus_P                  | -1.2358012 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 204000     |
| train/episodes                 | 20400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.567      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.0801    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 816000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 510         |
| stats_o/mean                   | 0.20487691  |
| stats_o/std                    | 0.080268726 |
| test/episodes                  | 5110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00284    |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.2836926  |
| test/Q_plus_P                  | -1.2836926  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 204400      |
| train/episodes                 | 20440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.573       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00458    |
| train/info_shaping_reward_mean | -0.0713     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 817600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 511        |
| stats_o/mean                   | 0.20488276 |
| stats_o/std                    | 0.08024828 |
| test/episodes                  | 5120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00184   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3329971 |
| test/Q_plus_P                  | -1.3329971 |
| test/reward_per_eps            | -11        |
| test/steps                     | 204800     |
| train/episodes                 | 20480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00303   |
| train/info_shaping_reward_mean | -0.0644    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 819200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 512        |
| stats_o/mean                   | 0.20488268 |
| stats_o/std                    | 0.08023469 |
| test/episodes                  | 5130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000372  |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.1660942 |
| test/Q_plus_P                  | -1.1660942 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 205200     |
| train/episodes                 | 20520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00361   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 820800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 513        |
| stats_o/mean                   | 0.20488839 |
| stats_o/std                    | 0.08025394 |
| test/episodes                  | 5140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00225   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3862149 |
| test/Q_plus_P                  | -1.3862149 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 205600     |
| train/episodes                 | 20560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.541      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00388   |
| train/info_shaping_reward_mean | -0.0748    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 822400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 514        |
| stats_o/mean                   | 0.20487852 |
| stats_o/std                    | 0.08022697 |
| test/episodes                  | 5150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0014    |
| test/info_shaping_reward_mean  | -0.0429    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0898943 |
| test/Q_plus_P                  | -1.0898943 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 206000     |
| train/episodes                 | 20600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00509   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 824000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 515        |
| stats_o/mean                   | 0.20487861 |
| stats_o/std                    | 0.08019625 |
| test/episodes                  | 5160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00271   |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2664741 |
| test/Q_plus_P                  | -1.2664741 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 206400     |
| train/episodes                 | 20640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00379   |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 825600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 516         |
| stats_o/mean                   | 0.20487969  |
| stats_o/std                    | 0.080167234 |
| test/episodes                  | 5170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.98501974 |
| test/Q_plus_P                  | -0.98501974 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 206800      |
| train/episodes                 | 20680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.62        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0682     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 827200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 517        |
| stats_o/mean                   | 0.20486698 |
| stats_o/std                    | 0.0801557  |
| test/episodes                  | 5180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0014    |
| test/info_shaping_reward_mean  | -0.0457    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.1239816 |
| test/Q_plus_P                  | -1.1239816 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 207200     |
| train/episodes                 | 20720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00342   |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 828800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 518         |
| stats_o/mean                   | 0.20487307  |
| stats_o/std                    | 0.080143064 |
| test/episodes                  | 5190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000721   |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1450887  |
| test/Q_plus_P                  | -1.1450887  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 207600      |
| train/episodes                 | 20760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.605       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00411    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.197      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 830400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 519        |
| stats_o/mean                   | 0.20487641 |
| stats_o/std                    | 0.08011508 |
| test/episodes                  | 5200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000758  |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -1.3982533 |
| test/Q_plus_P                  | -1.3982533 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 208000     |
| train/episodes                 | 20800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.63       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 832000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 520        |
| stats_o/mean                   | 0.20486261 |
| stats_o/std                    | 0.08010817 |
| test/episodes                  | 5210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00115   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2288241 |
| test/Q_plus_P                  | -1.2288241 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 208400     |
| train/episodes                 | 20840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00483   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 833600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 521         |
| stats_o/mean                   | 0.20486574  |
| stats_o/std                    | 0.080093004 |
| test/episodes                  | 5220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0974919  |
| test/Q_plus_P                  | -1.0974919  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 208800      |
| train/episodes                 | 20880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.072      |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 835200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 522         |
| stats_o/mean                   | 0.20487933  |
| stats_o/std                    | 0.080074064 |
| test/episodes                  | 5230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00304    |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.1156533  |
| test/Q_plus_P                  | -1.1156533  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 209200      |
| train/episodes                 | 20920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00419    |
| train/info_shaping_reward_mean | -0.0693     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 836800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 523        |
| stats_o/mean                   | 0.20486984 |
| stats_o/std                    | 0.08005316 |
| test/episodes                  | 5240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3113594 |
| test/Q_plus_P                  | -1.3113594 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 209600     |
| train/episodes                 | 20960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00325   |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 838400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 524        |
| stats_o/mean                   | 0.20487042 |
| stats_o/std                    | 0.08004527 |
| test/episodes                  | 5250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00137   |
| test/info_shaping_reward_mean  | -0.0418    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1191958 |
| test/Q_plus_P                  | -1.1191958 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 210000     |
| train/episodes                 | 21000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00382   |
| train/info_shaping_reward_mean | -0.0673    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 840000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 525         |
| stats_o/mean                   | 0.20487465  |
| stats_o/std                    | 0.080010355 |
| test/episodes                  | 5260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00323    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1914237  |
| test/Q_plus_P                  | -1.1914237  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 210400      |
| train/episodes                 | 21040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00412    |
| train/info_shaping_reward_mean | -0.07       |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 841600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 526        |
| stats_o/mean                   | 0.20486772 |
| stats_o/std                    | 0.07998632 |
| test/episodes                  | 5270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00243   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3938419 |
| test/Q_plus_P                  | -1.3938419 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 210800     |
| train/episodes                 | 21080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0046    |
| train/info_shaping_reward_mean | -0.0663    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 843200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 527        |
| stats_o/mean                   | 0.2048651  |
| stats_o/std                    | 0.07995218 |
| test/episodes                  | 5280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00062   |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1178895 |
| test/Q_plus_P                  | -1.1178895 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 211200     |
| train/episodes                 | 21120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00357   |
| train/info_shaping_reward_mean | -0.0651    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 844800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 528        |
| stats_o/mean                   | 0.20486802 |
| stats_o/std                    | 0.07992017 |
| test/episodes                  | 5290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3570035 |
| test/Q_plus_P                  | -1.3570035 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 211600     |
| train/episodes                 | 21160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00471   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 846400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 529        |
| stats_o/mean                   | 0.20487873 |
| stats_o/std                    | 0.07994037 |
| test/episodes                  | 5300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00154   |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5881162 |
| test/Q_plus_P                  | -1.5881162 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 212000     |
| train/episodes                 | 21200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00352   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 848000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 530        |
| stats_o/mean                   | 0.20487614 |
| stats_o/std                    | 0.07990427 |
| test/episodes                  | 5310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00147   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5553277 |
| test/Q_plus_P                  | -1.5553277 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 212400     |
| train/episodes                 | 21240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.657      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00411   |
| train/info_shaping_reward_mean | -0.0616    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 849600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 531         |
| stats_o/mean                   | 0.20487629  |
| stats_o/std                    | 0.079878315 |
| test/episodes                  | 5320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00315    |
| test/info_shaping_reward_mean  | -0.0488     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.2038646  |
| test/Q_plus_P                  | -1.2038646  |
| test/reward_per_eps            | -10         |
| test/steps                     | 212800      |
| train/episodes                 | 21280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.0715     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 851200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.20487803  |
| stats_o/std                    | 0.079892516 |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00173    |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.4825879  |
| test/Q_plus_P                  | -1.4825879  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.566       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.0809     |
| train/info_shaping_reward_min  | -0.219      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 533        |
| stats_o/mean                   | 0.20487797 |
| stats_o/std                    | 0.0798775  |
| test/episodes                  | 5340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00115   |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3519642 |
| test/Q_plus_P                  | -1.3519642 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 213600     |
| train/episodes                 | 21360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00393   |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 854400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 534        |
| stats_o/mean                   | 0.20488277 |
| stats_o/std                    | 0.07985553 |
| test/episodes                  | 5350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.326151  |
| test/Q_plus_P                  | -1.326151  |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 214000     |
| train/episodes                 | 21400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00466   |
| train/info_shaping_reward_mean | -0.0727    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 856000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 535         |
| stats_o/mean                   | 0.20488785  |
| stats_o/std                    | 0.079827316 |
| test/episodes                  | 5360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00155    |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.363175   |
| test/Q_plus_P                  | -1.363175   |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 214400      |
| train/episodes                 | 21440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.574       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00386    |
| train/info_shaping_reward_mean | -0.0768     |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 857600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 536        |
| stats_o/mean                   | 0.20488594 |
| stats_o/std                    | 0.07980298 |
| test/episodes                  | 5370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00197   |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3010597 |
| test/Q_plus_P                  | -1.3010597 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 214800     |
| train/episodes                 | 21480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 859200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 537        |
| stats_o/mean                   | 0.20488375 |
| stats_o/std                    | 0.07979321 |
| test/episodes                  | 5380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000767  |
| test/info_shaping_reward_mean  | -0.0423    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1557586 |
| test/Q_plus_P                  | -1.1557586 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 215200     |
| train/episodes                 | 21520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00471   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 860800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 538        |
| stats_o/mean                   | 0.20488618 |
| stats_o/std                    | 0.07979044 |
| test/episodes                  | 5390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00229   |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4936883 |
| test/Q_plus_P                  | -1.4936883 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 215600     |
| train/episodes                 | 21560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00472   |
| train/info_shaping_reward_mean | -0.0722    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 862400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 539         |
| stats_o/mean                   | 0.20487615  |
| stats_o/std                    | 0.079782985 |
| test/episodes                  | 5400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.675       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00143    |
| test/info_shaping_reward_mean  | -0.0622     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.5096909  |
| test/Q_plus_P                  | -1.5096909  |
| test/reward_per_eps            | -13         |
| test/steps                     | 216000      |
| train/episodes                 | 21600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.577       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00436    |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 864000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 540        |
| stats_o/mean                   | 0.20487595 |
| stats_o/std                    | 0.07974227 |
| test/episodes                  | 5410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00172   |
| test/info_shaping_reward_mean  | -0.0582    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5375807 |
| test/Q_plus_P                  | -1.5375807 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 216400     |
| train/episodes                 | 21640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00539   |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 865600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.2048843   |
| stats_o/std                    | 0.079716094 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000745   |
| test/info_shaping_reward_mean  | -0.0588     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.6628081  |
| test/Q_plus_P                  | -1.6628081  |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00371    |
| train/info_shaping_reward_mean | -0.0747     |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 542        |
| stats_o/mean                   | 0.20488362 |
| stats_o/std                    | 0.079698   |
| test/episodes                  | 5430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00143   |
| test/info_shaping_reward_mean  | -0.0541    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2256534 |
| test/Q_plus_P                  | -1.2256534 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 217200     |
| train/episodes                 | 21720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0041    |
| train/info_shaping_reward_mean | -0.0655    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 868800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 543        |
| stats_o/mean                   | 0.20488323 |
| stats_o/std                    | 0.07968882 |
| test/episodes                  | 5440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5560906 |
| test/Q_plus_P                  | -1.5560906 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 217600     |
| train/episodes                 | 21760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00431   |
| train/info_shaping_reward_mean | -0.0662    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 870400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.20487662  |
| stats_o/std                    | 0.079673484 |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00171    |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.2200853  |
| test/Q_plus_P                  | -1.2200853  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00411    |
| train/info_shaping_reward_mean | -0.0723     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 545        |
| stats_o/mean                   | 0.20487139 |
| stats_o/std                    | 0.0796546  |
| test/episodes                  | 5460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00177   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0695982 |
| test/Q_plus_P                  | -1.0695982 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 218400     |
| train/episodes                 | 21840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00315   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 873600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 546        |
| stats_o/mean                   | 0.2048655  |
| stats_o/std                    | 0.07966767 |
| test/episodes                  | 5470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000942  |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5558583 |
| test/Q_plus_P                  | -1.5558583 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 218800     |
| train/episodes                 | 21880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.567      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00461   |
| train/info_shaping_reward_mean | -0.0823    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 875200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 547        |
| stats_o/mean                   | 0.20485763 |
| stats_o/std                    | 0.07964975 |
| test/episodes                  | 5480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00113   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.5673636 |
| test/Q_plus_P                  | -1.5673636 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 219200     |
| train/episodes                 | 21920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.673      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00236   |
| train/info_shaping_reward_mean | -0.0611    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 876800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 548        |
| stats_o/mean                   | 0.20485349 |
| stats_o/std                    | 0.07965027 |
| test/episodes                  | 5490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00079   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.2008891 |
| test/Q_plus_P                  | -1.2008891 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 219600     |
| train/episodes                 | 21960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 878400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.20484915  |
| stats_o/std                    | 0.079631574 |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00135    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0905558  |
| test/Q_plus_P                  | -1.0905558  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00374    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 550        |
| stats_o/mean                   | 0.20485777 |
| stats_o/std                    | 0.07963349 |
| test/episodes                  | 5510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000307  |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0801022 |
| test/Q_plus_P                  | -1.0801022 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 220400     |
| train/episodes                 | 22040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00564   |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 881600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 551         |
| stats_o/mean                   | 0.2048581   |
| stats_o/std                    | 0.079597905 |
| test/episodes                  | 5520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.002      |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.4637867  |
| test/Q_plus_P                  | -1.4637867  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 220800      |
| train/episodes                 | 22080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 883200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 552        |
| stats_o/mean                   | 0.20484848 |
| stats_o/std                    | 0.07960537 |
| test/episodes                  | 5530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0021    |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2313236 |
| test/Q_plus_P                  | -1.2313236 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 221200     |
| train/episodes                 | 22120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00279   |
| train/info_shaping_reward_mean | -0.0829    |
| train/info_shaping_reward_min  | -0.265     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 884800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 553        |
| stats_o/mean                   | 0.20484865 |
| stats_o/std                    | 0.07957332 |
| test/episodes                  | 5540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00158   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3169374 |
| test/Q_plus_P                  | -1.3169374 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 221600     |
| train/episodes                 | 22160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00408   |
| train/info_shaping_reward_mean | -0.0666    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 886400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 554        |
| stats_o/mean                   | 0.20485401 |
| stats_o/std                    | 0.07956732 |
| test/episodes                  | 5550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00364   |
| test/info_shaping_reward_mean  | -0.0609    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.6084714 |
| test/Q_plus_P                  | -1.6084714 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 222000     |
| train/episodes                 | 22200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00375   |
| train/info_shaping_reward_mean | -0.0755    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 888000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 555        |
| stats_o/mean                   | 0.20485644 |
| stats_o/std                    | 0.07955254 |
| test/episodes                  | 5560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.675      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00317   |
| test/info_shaping_reward_mean  | -0.057     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5634838 |
| test/Q_plus_P                  | -1.5634838 |
| test/reward_per_eps            | -13        |
| test/steps                     | 222400     |
| train/episodes                 | 22240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00332   |
| train/info_shaping_reward_mean | -0.0681    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 889600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 556         |
| stats_o/mean                   | 0.20485969  |
| stats_o/std                    | 0.079538986 |
| test/episodes                  | 5570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000862   |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.204117   |
| test/Q_plus_P                  | -1.204117   |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 222800      |
| train/episodes                 | 22280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.559       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.0848     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 891200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 557        |
| stats_o/mean                   | 0.20486155 |
| stats_o/std                    | 0.07956238 |
| test/episodes                  | 5580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00247   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4233508 |
| test/Q_plus_P                  | -1.4233508 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 223200     |
| train/episodes                 | 22320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00386   |
| train/info_shaping_reward_mean | -0.086     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 892800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 558         |
| stats_o/mean                   | 0.20485492  |
| stats_o/std                    | 0.079528354 |
| test/episodes                  | 5590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00356    |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.3955172  |
| test/Q_plus_P                  | -1.3955172  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 223600      |
| train/episodes                 | 22360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00291    |
| train/info_shaping_reward_mean | -0.0649     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 894400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 559        |
| stats_o/mean                   | 0.20485298 |
| stats_o/std                    | 0.07951203 |
| test/episodes                  | 5600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00172   |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.4184875 |
| test/Q_plus_P                  | -1.4184875 |
| test/reward_per_eps            | -11        |
| test/steps                     | 224000     |
| train/episodes                 | 22400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00293   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 896000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 560        |
| stats_o/mean                   | 0.20485114 |
| stats_o/std                    | 0.07950731 |
| test/episodes                  | 5610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00292   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3110434 |
| test/Q_plus_P                  | -1.3110434 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 224400     |
| train/episodes                 | 22440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00413   |
| train/info_shaping_reward_mean | -0.0656    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 897600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 561         |
| stats_o/mean                   | 0.20485668  |
| stats_o/std                    | 0.079484686 |
| test/episodes                  | 5620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000969   |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1194625  |
| test/Q_plus_P                  | -1.1194625  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 224800      |
| train/episodes                 | 22480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00403    |
| train/info_shaping_reward_mean | -0.0659     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 899200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.20484945  |
| stats_o/std                    | 0.079498164 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000892   |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.4162602  |
| test/Q_plus_P                  | -1.4162602  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00352    |
| train/info_shaping_reward_mean | -0.074      |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.20485485  |
| stats_o/std                    | 0.079466276 |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0434     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1649259  |
| test/Q_plus_P                  | -1.1649259  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00439    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 564        |
| stats_o/mean                   | 0.20485687 |
| stats_o/std                    | 0.079429   |
| test/episodes                  | 5650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00116   |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4595865 |
| test/Q_plus_P                  | -1.4595865 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 226000     |
| train/episodes                 | 22600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.658      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00339   |
| train/info_shaping_reward_mean | -0.0623    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 904000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 565        |
| stats_o/mean                   | 0.20485666 |
| stats_o/std                    | 0.07940098 |
| test/episodes                  | 5660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00133   |
| test/info_shaping_reward_mean  | -0.0429    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.112271  |
| test/Q_plus_P                  | -1.112271  |
| test/reward_per_eps            | -9         |
| test/steps                     | 226400     |
| train/episodes                 | 22640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0039    |
| train/info_shaping_reward_mean | -0.0658    |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 905600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.20486103  |
| stats_o/std                    | 0.079366975 |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00258    |
| test/info_shaping_reward_mean  | -0.0574     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.3865985  |
| test/Q_plus_P                  | -1.3865985  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 567        |
| stats_o/mean                   | 0.20485626 |
| stats_o/std                    | 0.07937851 |
| test/episodes                  | 5680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.655      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0027    |
| test/info_shaping_reward_mean  | -0.0676    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.7542622 |
| test/Q_plus_P                  | -1.7542622 |
| test/reward_per_eps            | -13.8      |
| test/steps                     | 227200     |
| train/episodes                 | 22720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00461   |
| train/info_shaping_reward_mean | -0.0726    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 908800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.20484942  |
| stats_o/std                    | 0.079343155 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.647       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00342    |
| test/info_shaping_reward_mean  | -0.0676     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.8607054  |
| test/Q_plus_P                  | -1.8607054  |
| test/reward_per_eps            | -14.1       |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0659     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.20484418  |
| stats_o/std                    | 0.079329126 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00116    |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.5251802  |
| test/Q_plus_P                  | -1.5251802  |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00399    |
| train/info_shaping_reward_mean | -0.0689     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.20484228  |
| stats_o/std                    | 0.079279274 |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000894   |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1498692  |
| test/Q_plus_P                  | -1.1498692  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00397    |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 571        |
| stats_o/mean                   | 0.20484671 |
| stats_o/std                    | 0.07927226 |
| test/episodes                  | 5720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00382   |
| test/info_shaping_reward_mean  | -0.0653    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.7505472 |
| test/Q_plus_P                  | -1.7505472 |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 228800     |
| train/episodes                 | 22880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00473   |
| train/info_shaping_reward_mean | -0.0757    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 915200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 572        |
| stats_o/mean                   | 0.20484613 |
| stats_o/std                    | 0.07929469 |
| test/episodes                  | 5730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00242   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3954321 |
| test/Q_plus_P                  | -1.3954321 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 229200     |
| train/episodes                 | 22920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0841    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 916800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 573         |
| stats_o/mean                   | 0.20484374  |
| stats_o/std                    | 0.079277605 |
| test/episodes                  | 5740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.688       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000582   |
| test/info_shaping_reward_mean  | -0.0568     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.6724632  |
| test/Q_plus_P                  | -1.6724632  |
| test/reward_per_eps            | -12.5       |
| test/steps                     | 229600      |
| train/episodes                 | 22960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.576       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0054     |
| train/info_shaping_reward_mean | -0.0702     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 918400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.20484161  |
| stats_o/std                    | 0.079271734 |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00385    |
| test/info_shaping_reward_mean  | -0.0593     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.6947322  |
| test/Q_plus_P                  | -1.6947322  |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0048     |
| train/info_shaping_reward_mean | -0.078      |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 575        |
| stats_o/mean                   | 0.20483387 |
| stats_o/std                    | 0.07928645 |
| test/episodes                  | 5760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000406  |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3326498 |
| test/Q_plus_P                  | -1.3326498 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 230400     |
| train/episodes                 | 23040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00366   |
| train/info_shaping_reward_mean | -0.0696    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 921600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 576        |
| stats_o/mean                   | 0.204831   |
| stats_o/std                    | 0.079271   |
| test/episodes                  | 5770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00104   |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.4862924 |
| test/Q_plus_P                  | -1.4862924 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 230800     |
| train/episodes                 | 23080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00306   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 923200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 577        |
| stats_o/mean                   | 0.2048235  |
| stats_o/std                    | 0.07924542 |
| test/episodes                  | 5780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.581469  |
| test/Q_plus_P                  | -1.581469  |
| test/reward_per_eps            | -12        |
| test/steps                     | 231200     |
| train/episodes                 | 23120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00417   |
| train/info_shaping_reward_mean | -0.0684    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 924800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.20482989 |
| stats_o/std                    | 0.07922729 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00208   |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5348397 |
| test/Q_plus_P                  | -1.5348397 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00427   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 579        |
| stats_o/mean                   | 0.2048274  |
| stats_o/std                    | 0.07919696 |
| test/episodes                  | 5800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000606  |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.348716  |
| test/Q_plus_P                  | -1.348716  |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 232000     |
| train/episodes                 | 23200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00408   |
| train/info_shaping_reward_mean | -0.0641    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 928000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 580        |
| stats_o/mean                   | 0.20483345 |
| stats_o/std                    | 0.07916867 |
| test/episodes                  | 5810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00301   |
| test/info_shaping_reward_mean  | -0.0585    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.6212162 |
| test/Q_plus_P                  | -1.6212162 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 232400     |
| train/episodes                 | 23240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00348   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 929600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 581         |
| stats_o/mean                   | 0.2048341   |
| stats_o/std                    | 0.079165354 |
| test/episodes                  | 5820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.218868   |
| test/Q_plus_P                  | -1.218868   |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 232800      |
| train/episodes                 | 23280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.556       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0717     |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 931200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 582        |
| stats_o/mean                   | 0.20482974 |
| stats_o/std                    | 0.0791623  |
| test/episodes                  | 5830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000985  |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5437524 |
| test/Q_plus_P                  | -1.5437524 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 233200     |
| train/episodes                 | 23320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00505   |
| train/info_shaping_reward_mean | -0.0758    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 932800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 583        |
| stats_o/mean                   | 0.20482454 |
| stats_o/std                    | 0.07917847 |
| test/episodes                  | 5840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00107   |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.4701948 |
| test/Q_plus_P                  | -1.4701948 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 233600     |
| train/episodes                 | 23360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00403   |
| train/info_shaping_reward_mean | -0.0849    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 934400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 584        |
| stats_o/mean                   | 0.20482962 |
| stats_o/std                    | 0.07916221 |
| test/episodes                  | 5850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00209   |
| test/info_shaping_reward_mean  | -0.0448    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2138414 |
| test/Q_plus_P                  | -1.2138414 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 234000     |
| train/episodes                 | 23400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00388   |
| train/info_shaping_reward_mean | -0.0787    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 936000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 585        |
| stats_o/mean                   | 0.20483008 |
| stats_o/std                    | 0.07916269 |
| test/episodes                  | 5860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00177   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4482815 |
| test/Q_plus_P                  | -1.4482815 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 234400     |
| train/episodes                 | 23440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00339   |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 937600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 586        |
| stats_o/mean                   | 0.20482668 |
| stats_o/std                    | 0.07914441 |
| test/episodes                  | 5870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00238   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3393856 |
| test/Q_plus_P                  | -1.3393856 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 234800     |
| train/episodes                 | 23480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00385   |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 939200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 587        |
| stats_o/mean                   | 0.20482935 |
| stats_o/std                    | 0.07910831 |
| test/episodes                  | 5880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00123   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.1995343 |
| test/Q_plus_P                  | -1.1995343 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 235200     |
| train/episodes                 | 23520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00549   |
| train/info_shaping_reward_mean | -0.0679    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 940800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 588        |
| stats_o/mean                   | 0.20482391 |
| stats_o/std                    | 0.07907143 |
| test/episodes                  | 5890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00333   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3179685 |
| test/Q_plus_P                  | -1.3179685 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 235600     |
| train/episodes                 | 23560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00432   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 942400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 589        |
| stats_o/mean                   | 0.20482023 |
| stats_o/std                    | 0.07907217 |
| test/episodes                  | 5900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000876  |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.4571719 |
| test/Q_plus_P                  | -1.4571719 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 236000     |
| train/episodes                 | 23600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.582      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00388   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 944000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.20482327  |
| stats_o/std                    | 0.079053424 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00156    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.179591   |
| test/Q_plus_P                  | -1.179591   |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00388    |
| train/info_shaping_reward_mean | -0.0668     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 591        |
| stats_o/mean                   | 0.20481677 |
| stats_o/std                    | 0.07905386 |
| test/episodes                  | 5920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0018    |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0190369 |
| test/Q_plus_P                  | -1.0190369 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 236800     |
| train/episodes                 | 23680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00412   |
| train/info_shaping_reward_mean | -0.0724    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 947200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 592        |
| stats_o/mean                   | 0.20481627 |
| stats_o/std                    | 0.07904289 |
| test/episodes                  | 5930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000419  |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.3313872 |
| test/Q_plus_P                  | -1.3313872 |
| test/reward_per_eps            | -10        |
| test/steps                     | 237200     |
| train/episodes                 | 23720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00533   |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 948800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 593        |
| stats_o/mean                   | 0.20480989 |
| stats_o/std                    | 0.07902496 |
| test/episodes                  | 5940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0011    |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2842457 |
| test/Q_plus_P                  | -1.2842457 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 237600     |
| train/episodes                 | 23760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00431   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 950400     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 594       |
| stats_o/mean                   | 0.2048091 |
| stats_o/std                    | 0.0790004 |
| test/episodes                  | 5950      |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.705     |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.00265  |
| test/info_shaping_reward_mean  | -0.0557   |
| test/info_shaping_reward_min   | -0.175    |
| test/Q                         | -1.4656   |
| test/Q_plus_P                  | -1.4656   |
| test/reward_per_eps            | -11.8     |
| test/steps                     | 238000    |
| train/episodes                 | 23800     |
| train/info_is_success_max      | 1         |
| train/info_is_success_mean     | 0.619     |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.00383  |
| train/info_shaping_reward_mean | -0.0653   |
| train/info_shaping_reward_min  | -0.185    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -15.2     |
| train/steps                    | 952000    |
----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 595         |
| stats_o/mean                   | 0.2048139   |
| stats_o/std                    | 0.079003625 |
| test/episodes                  | 5960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.055      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.540313   |
| test/Q_plus_P                  | -1.540313   |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 238400      |
| train/episodes                 | 23840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00454    |
| train/info_shaping_reward_mean | -0.0677     |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 953600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 596        |
| stats_o/mean                   | 0.20481895 |
| stats_o/std                    | 0.07900255 |
| test/episodes                  | 5970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00294   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.3761275 |
| test/Q_plus_P                  | -1.3761275 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 238800     |
| train/episodes                 | 23880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.548      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00481   |
| train/info_shaping_reward_mean | -0.0787    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 955200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 597        |
| stats_o/mean                   | 0.20482247 |
| stats_o/std                    | 0.07899339 |
| test/episodes                  | 5980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00193   |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5942045 |
| test/Q_plus_P                  | -1.5942045 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 239200     |
| train/episodes                 | 23920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0032    |
| train/info_shaping_reward_mean | -0.0688    |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 956800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 598        |
| stats_o/mean                   | 0.2048152  |
| stats_o/std                    | 0.07897938 |
| test/episodes                  | 5990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0012    |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3029099 |
| test/Q_plus_P                  | -1.3029099 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 239600     |
| train/episodes                 | 23960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00367   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 958400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.20481652 |
| stats_o/std                    | 0.0789566  |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000576  |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2688015 |
| test/Q_plus_P                  | -1.2688015 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00408   |
| train/info_shaping_reward_mean | -0.0645    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
