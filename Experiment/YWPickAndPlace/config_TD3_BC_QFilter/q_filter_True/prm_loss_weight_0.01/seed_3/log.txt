Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPickAndPlace/config_TD3_BC_QFilter/q_filter_True/prm_loss_weight_0.01/seed_3
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.20078842  |
| stats_o/std                    | 0.064812474 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.4842469  |
| test/Q_plus_P                  | -1.4842469  |
| test/reward_per_eps            | -40         |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.191      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 1          |
| stats_o/mean                   | 0.20117636 |
| stats_o/std                    | 0.06668118 |
| test/episodes                  | 20         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.144     |
| test/info_shaping_reward_mean  | -0.179     |
| test/info_shaping_reward_min   | -0.212     |
| test/Q                         | -2.0898445 |
| test/Q_plus_P                  | -2.0898445 |
| test/reward_per_eps            | -40        |
| test/steps                     | 800        |
| train/episodes                 | 80         |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 3200       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 2          |
| stats_o/mean                   | 0.20043385 |
| stats_o/std                    | 0.07197273 |
| test/episodes                  | 30         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.178     |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -2.1539576 |
| test/Q_plus_P                  | -2.1539576 |
| test/reward_per_eps            | -40        |
| test/steps                     | 1200       |
| train/episodes                 | 120        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.28      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 4800       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.20048538 |
| stats_o/std                    | 0.07906562 |
| test/episodes                  | 40         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.141     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.206     |
| test/Q                         | -2.5453837 |
| test/Q_plus_P                  | -2.5453837 |
| test/reward_per_eps            | -40        |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.209     |
| train/info_shaping_reward_min  | -0.446     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 4           |
| stats_o/mean                   | 0.2013806   |
| stats_o/std                    | 0.082837924 |
| test/episodes                  | 50          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.14       |
| test/info_shaping_reward_mean  | -0.172      |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -2.967658   |
| test/Q_plus_P                  | -2.967658   |
| test/reward_per_eps            | -40         |
| test/steps                     | 2000        |
| train/episodes                 | 200         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.194      |
| train/info_shaping_reward_min  | -0.39       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 8000        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 5           |
| stats_o/mean                   | 0.20183909  |
| stats_o/std                    | 0.082853585 |
| test/episodes                  | 60          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0225      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0284     |
| test/info_shaping_reward_mean  | -0.162      |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -3.3429687  |
| test/Q_plus_P                  | -3.3429687  |
| test/reward_per_eps            | -39.1       |
| test/steps                     | 2400        |
| train/episodes                 | 240         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.194      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 9600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 6           |
| stats_o/mean                   | 0.20207445  |
| stats_o/std                    | 0.079921596 |
| test/episodes                  | 70          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.202      |
| test/Q                         | -3.7439487  |
| test/Q_plus_P                  | -3.7439487  |
| test/reward_per_eps            | -40         |
| test/steps                     | 2800        |
| train/episodes                 | 280         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.18       |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 11200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 7          |
| stats_o/mean                   | 0.20247607 |
| stats_o/std                    | 0.0808238  |
| test/episodes                  | 80         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.178     |
| test/info_shaping_reward_min   | -0.213     |
| test/Q                         | -4.139615  |
| test/Q_plus_P                  | -4.139615  |
| test/reward_per_eps            | -40        |
| test/steps                     | 3200       |
| train/episodes                 | 320        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.299     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 12800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 8           |
| stats_o/mean                   | 0.203174    |
| stats_o/std                    | 0.081857815 |
| test/episodes                  | 90          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0525      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0217     |
| test/info_shaping_reward_mean  | -0.169      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -4.8689885  |
| test/Q_plus_P                  | -4.8689885  |
| test/reward_per_eps            | -37.9       |
| test/steps                     | 3600        |
| train/episodes                 | 360         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.184      |
| train/info_shaping_reward_min  | -0.29       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 14400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 9          |
| stats_o/mean                   | 0.20359722 |
| stats_o/std                    | 0.08479318 |
| test/episodes                  | 100        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.114     |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.728     |
| test/Q                         | -5.2321258 |
| test/Q_plus_P                  | -5.2321258 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4000       |
| train/episodes                 | 400        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 16000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 10          |
| stats_o/mean                   | 0.20386787  |
| stats_o/std                    | 0.089450076 |
| test/episodes                  | 110         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.015       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0234     |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.629      |
| test/Q                         | -5.6459565  |
| test/Q_plus_P                  | -5.6459565  |
| test/reward_per_eps            | -39.4       |
| test/steps                     | 4400        |
| train/episodes                 | 440         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00125     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.118      |
| train/info_shaping_reward_mean | -0.203      |
| train/info_shaping_reward_min  | -0.487      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 17600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 11         |
| stats_o/mean                   | 0.20394483 |
| stats_o/std                    | 0.09239879 |
| test/episodes                  | 120        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.186     |
| test/info_shaping_reward_min   | -0.345     |
| test/Q                         | -5.9436097 |
| test/Q_plus_P                  | -5.9436097 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4800       |
| train/episodes                 | 480        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0075     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.214     |
| train/info_shaping_reward_min  | -0.499     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 19200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 12         |
| stats_o/mean                   | 0.2041576  |
| stats_o/std                    | 0.09505634 |
| test/episodes                  | 130        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.105      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00818   |
| test/info_shaping_reward_mean  | -0.142     |
| test/info_shaping_reward_min   | -0.222     |
| test/Q                         | -6.136923  |
| test/Q_plus_P                  | -6.136923  |
| test/reward_per_eps            | -35.8      |
| test/steps                     | 5200       |
| train/episodes                 | 520        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.207     |
| train/info_shaping_reward_min  | -0.384     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 20800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 13         |
| stats_o/mean                   | 0.20438425 |
| stats_o/std                    | 0.09697809 |
| test/episodes                  | 140        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0384    |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -6.5777416 |
| test/Q_plus_P                  | -6.5777416 |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 5600       |
| train/episodes                 | 560        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.201     |
| train/info_shaping_reward_min  | -0.439     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 22400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 14          |
| stats_o/mean                   | 0.20437561  |
| stats_o/std                    | 0.097450405 |
| test/episodes                  | 150         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.065       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0295     |
| test/info_shaping_reward_mean  | -0.152      |
| test/info_shaping_reward_min   | -0.195      |
| test/Q                         | -6.882681   |
| test/Q_plus_P                  | -6.882681   |
| test/reward_per_eps            | -37.4       |
| test/steps                     | 6000        |
| train/episodes                 | 600         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.194      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 24000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 15          |
| stats_o/mean                   | 0.20430326  |
| stats_o/std                    | 0.096776634 |
| test/episodes                  | 160         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.05        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0142     |
| test/info_shaping_reward_mean  | -0.158      |
| test/info_shaping_reward_min   | -0.208      |
| test/Q                         | -7.1913013  |
| test/Q_plus_P                  | -7.1913013  |
| test/reward_per_eps            | -38         |
| test/steps                     | 6400        |
| train/episodes                 | 640         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0194      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.116      |
| train/info_shaping_reward_mean | -0.197      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 25600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 16          |
| stats_o/mean                   | 0.20439644  |
| stats_o/std                    | 0.096769094 |
| test/episodes                  | 170         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.113       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00308    |
| test/info_shaping_reward_mean  | -0.128      |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -7.405487   |
| test/Q_plus_P                  | -7.405487   |
| test/reward_per_eps            | -35.5       |
| test/steps                     | 6800        |
| train/episodes                 | 680         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.005       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.189      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.8       |
| train/steps                    | 27200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 17         |
| stats_o/mean                   | 0.2044354  |
| stats_o/std                    | 0.09701399 |
| test/episodes                  | 180        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0574    |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.224     |
| test/Q                         | -8.109994  |
| test/Q_plus_P                  | -8.109994  |
| test/reward_per_eps            | -40        |
| test/steps                     | 7200       |
| train/episodes                 | 720        |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0231     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.114     |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.413     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 28800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 18         |
| stats_o/mean                   | 0.20429516 |
| stats_o/std                    | 0.09600601 |
| test/episodes                  | 190        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.055      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0155    |
| test/info_shaping_reward_mean  | -0.19      |
| test/info_shaping_reward_min   | -0.532     |
| test/Q                         | -8.40971   |
| test/Q_plus_P                  | -8.40971   |
| test/reward_per_eps            | -37.8      |
| test/steps                     | 7600       |
| train/episodes                 | 760        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 30400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.20424509 |
| stats_o/std                    | 0.09581551 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.217      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00416   |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -8.394854  |
| test/Q_plus_P                  | -8.394854  |
| test/reward_per_eps            | -31.3      |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.29      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 20         |
| stats_o/mean                   | 0.20402683 |
| stats_o/std                    | 0.0963602  |
| test/episodes                  | 210        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.117      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.026     |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -9.043661  |
| test/Q_plus_P                  | -9.043661  |
| test/reward_per_eps            | -35.3      |
| test/steps                     | 8400       |
| train/episodes                 | 840        |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0181     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0925    |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 33600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 21         |
| stats_o/mean                   | 0.20408855 |
| stats_o/std                    | 0.09707921 |
| test/episodes                  | 220        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.155      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.01      |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -8.973128  |
| test/Q_plus_P                  | -8.973128  |
| test/reward_per_eps            | -33.8      |
| test/steps                     | 8800       |
| train/episodes                 | 880        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 35200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 22         |
| stats_o/mean                   | 0.20387805 |
| stats_o/std                    | 0.09758117 |
| test/episodes                  | 230        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.107      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0176    |
| test/info_shaping_reward_mean  | -0.133     |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -9.421447  |
| test/Q_plus_P                  | -9.421447  |
| test/reward_per_eps            | -35.7      |
| test/steps                     | 9200       |
| train/episodes                 | 920        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00313    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 36800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 23         |
| stats_o/mean                   | 0.20380737 |
| stats_o/std                    | 0.09705805 |
| test/episodes                  | 240        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.275      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00612   |
| test/info_shaping_reward_mean  | -0.108     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -9.123074  |
| test/Q_plus_P                  | -9.123074  |
| test/reward_per_eps            | -29        |
| test/steps                     | 9600       |
| train/episodes                 | 960        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00562    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.113     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 38400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.2036231   |
| stats_o/std                    | 0.096912995 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0775      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.023      |
| test/info_shaping_reward_mean  | -0.151      |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -10.439864  |
| test/Q_plus_P                  | -10.439864  |
| test/reward_per_eps            | -36.9       |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00875     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.189      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 25         |
| stats_o/mean                   | 0.20364068 |
| stats_o/std                    | 0.09637352 |
| test/episodes                  | 260        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.28       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.012     |
| test/info_shaping_reward_mean  | -0.126     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -10.080991 |
| test/Q_plus_P                  | -10.080991 |
| test/reward_per_eps            | -28.8      |
| test/steps                     | 10400      |
| train/episodes                 | 1040       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.035      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.101     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 41600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 26         |
| stats_o/mean                   | 0.20356283 |
| stats_o/std                    | 0.09600381 |
| test/episodes                  | 270        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.125      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00545   |
| test/info_shaping_reward_mean  | -0.135     |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -10.618098 |
| test/Q_plus_P                  | -10.618098 |
| test/reward_per_eps            | -35        |
| test/steps                     | 10800      |
| train/episodes                 | 1080       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.005      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 43200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 27          |
| stats_o/mean                   | 0.20387934  |
| stats_o/std                    | 0.096398786 |
| test/episodes                  | 280         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0625      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0297     |
| test/info_shaping_reward_mean  | -0.143      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -11.106052  |
| test/Q_plus_P                  | -11.106052  |
| test/reward_per_eps            | -37.5       |
| test/steps                     | 11200       |
| train/episodes                 | 1120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.192      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 44800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.20387346 |
| stats_o/std                    | 0.09598794 |
| test/episodes                  | 290        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.21       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0114    |
| test/info_shaping_reward_mean  | -0.117     |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -10.59532  |
| test/Q_plus_P                  | -10.59532  |
| test/reward_per_eps            | -31.6      |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.118     |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.20372951 |
| stats_o/std                    | 0.09587803 |
| test/episodes                  | 300        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.14       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00774   |
| test/info_shaping_reward_mean  | -0.13      |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -11.318369 |
| test/Q_plus_P                  | -11.318369 |
| test/reward_per_eps            | -34.4      |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0112     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0928    |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 30         |
| stats_o/mean                   | 0.20379224 |
| stats_o/std                    | 0.09563365 |
| test/episodes                  | 310        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0975     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00747   |
| test/info_shaping_reward_mean  | -0.14      |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -12.020226 |
| test/Q_plus_P                  | -12.020226 |
| test/reward_per_eps            | -36.1      |
| test/steps                     | 12400      |
| train/episodes                 | 1240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 49600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 31         |
| stats_o/mean                   | 0.20380822 |
| stats_o/std                    | 0.0958472  |
| test/episodes                  | 320        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.035      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0152    |
| test/info_shaping_reward_mean  | -0.123     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -11.838113 |
| test/Q_plus_P                  | -11.838113 |
| test/reward_per_eps            | -38.6      |
| test/steps                     | 12800      |
| train/episodes                 | 1280       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0131     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.112     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 51200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 32          |
| stats_o/mean                   | 0.20388006  |
| stats_o/std                    | 0.09596988  |
| test/episodes                  | 330         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.14        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0117     |
| test/info_shaping_reward_mean  | -0.135      |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -12.2324295 |
| test/Q_plus_P                  | -12.2324295 |
| test/reward_per_eps            | -34.4       |
| test/steps                     | 13200       |
| train/episodes                 | 1320        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0025      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.189      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 52800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 33         |
| stats_o/mean                   | 0.20384899 |
| stats_o/std                    | 0.09582829 |
| test/episodes                  | 340        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0725     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0183    |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -12.632922 |
| test/Q_plus_P                  | -12.632922 |
| test/reward_per_eps            | -37.1      |
| test/steps                     | 13600      |
| train/episodes                 | 1360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.308     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 54400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 34         |
| stats_o/mean                   | 0.20383129 |
| stats_o/std                    | 0.09598387 |
| test/episodes                  | 350        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.14       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0109    |
| test/info_shaping_reward_mean  | -0.133     |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -12.390799 |
| test/Q_plus_P                  | -12.390799 |
| test/reward_per_eps            | -34.4      |
| test/steps                     | 14000      |
| train/episodes                 | 1400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.42      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 56000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.2036973  |
| stats_o/std                    | 0.09625094 |
| test/episodes                  | 360        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.175      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0208    |
| test/info_shaping_reward_mean  | -0.124     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -12.53426  |
| test/Q_plus_P                  | -12.53426  |
| test/reward_per_eps            | -33        |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0138     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.106     |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.383     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 36          |
| stats_o/mean                   | 0.2037801   |
| stats_o/std                    | 0.096387535 |
| test/episodes                  | 370         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.175       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.013      |
| test/info_shaping_reward_mean  | -0.135      |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -12.932827  |
| test/Q_plus_P                  | -12.932827  |
| test/reward_per_eps            | -33         |
| test/steps                     | 14800       |
| train/episodes                 | 1480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.191      |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 59200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 37         |
| stats_o/mean                   | 0.20372085 |
| stats_o/std                    | 0.09627944 |
| test/episodes                  | 380        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.3        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0111    |
| test/info_shaping_reward_mean  | -0.116     |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -11.83388  |
| test/Q_plus_P                  | -11.83388  |
| test/reward_per_eps            | -28        |
| test/steps                     | 15200      |
| train/episodes                 | 1520       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.02       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 60800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 38          |
| stats_o/mean                   | 0.20362638  |
| stats_o/std                    | 0.096517466 |
| test/episodes                  | 390         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.228       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0168     |
| test/info_shaping_reward_mean  | -0.13       |
| test/info_shaping_reward_min   | -0.211      |
| test/Q                         | -13.125078  |
| test/Q_plus_P                  | -13.125078  |
| test/reward_per_eps            | -30.9       |
| test/steps                     | 15600       |
| train/episodes                 | 1560        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.000625    |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.206      |
| train/info_shaping_reward_min  | -0.401      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 62400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 39          |
| stats_o/mean                   | 0.20360455  |
| stats_o/std                    | 0.097051315 |
| test/episodes                  | 400         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0875      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.026      |
| test/info_shaping_reward_mean  | -0.163      |
| test/info_shaping_reward_min   | -0.324      |
| test/Q                         | -14.371509  |
| test/Q_plus_P                  | -14.371509  |
| test/reward_per_eps            | -36.5       |
| test/steps                     | 16000       |
| train/episodes                 | 1600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.193      |
| train/info_shaping_reward_min  | -0.376      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 64000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.2032673   |
| stats_o/std                    | 0.097847134 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.015       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0472     |
| test/info_shaping_reward_mean  | -0.146      |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -14.285308  |
| test/Q_plus_P                  | -14.285308  |
| test/reward_per_eps            | -39.4       |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.217      |
| train/info_shaping_reward_min  | -0.467      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.20324112  |
| stats_o/std                    | 0.098445505 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.253       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00957    |
| test/info_shaping_reward_mean  | -0.117      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -12.91353   |
| test/Q_plus_P                  | -12.91353   |
| test/reward_per_eps            | -29.9       |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00813     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.12       |
| train/info_shaping_reward_mean | -0.196      |
| train/info_shaping_reward_min  | -0.431      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 42          |
| stats_o/mean                   | 0.20309007  |
| stats_o/std                    | 0.0989556   |
| test/episodes                  | 430         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.203       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00581    |
| test/info_shaping_reward_mean  | -0.115      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -13.2632675 |
| test/Q_plus_P                  | -13.2632675 |
| test/reward_per_eps            | -31.9       |
| test/steps                     | 17200       |
| train/episodes                 | 1720        |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0194      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0948     |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.646      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 68800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 43         |
| stats_o/mean                   | 0.20308794 |
| stats_o/std                    | 0.09978158 |
| test/episodes                  | 440        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.365      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0108    |
| test/info_shaping_reward_mean  | -0.099     |
| test/info_shaping_reward_min   | -0.21      |
| test/Q                         | -12.434834 |
| test/Q_plus_P                  | -12.434834 |
| test/reward_per_eps            | -25.4      |
| test/steps                     | 17600      |
| train/episodes                 | 1760       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.12      |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.487     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 70400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 44          |
| stats_o/mean                   | 0.20292641  |
| stats_o/std                    | 0.100582965 |
| test/episodes                  | 450         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.163       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0161     |
| test/info_shaping_reward_mean  | -0.133      |
| test/info_shaping_reward_min   | -0.196      |
| test/Q                         | -14.543178  |
| test/Q_plus_P                  | -14.543178  |
| test/reward_per_eps            | -33.5       |
| test/steps                     | 18000       |
| train/episodes                 | 1800        |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0181      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.099      |
| train/info_shaping_reward_mean | -0.204      |
| train/info_shaping_reward_min  | -0.461      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 72000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 45         |
| stats_o/mean                   | 0.20290829 |
| stats_o/std                    | 0.10145667 |
| test/episodes                  | 460        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.41       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00503   |
| test/info_shaping_reward_mean  | -0.0989    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -11.660226 |
| test/Q_plus_P                  | -11.660226 |
| test/reward_per_eps            | -23.6      |
| test/steps                     | 18400      |
| train/episodes                 | 1840       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0075     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.09      |
| train/info_shaping_reward_mean | -0.209     |
| train/info_shaping_reward_min  | -0.394     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 73600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 46         |
| stats_o/mean                   | 0.20292921 |
| stats_o/std                    | 0.10177063 |
| test/episodes                  | 470        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.223      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00829   |
| test/info_shaping_reward_mean  | -0.12      |
| test/info_shaping_reward_min   | -0.368     |
| test/Q                         | -13.13644  |
| test/Q_plus_P                  | -13.13644  |
| test/reward_per_eps            | -31.1      |
| test/steps                     | 18800      |
| train/episodes                 | 1880       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00688    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0942    |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.523     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 75200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 47         |
| stats_o/mean                   | 0.20303786 |
| stats_o/std                    | 0.10351048 |
| test/episodes                  | 480        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.378      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00245   |
| test/info_shaping_reward_mean  | -0.121     |
| test/info_shaping_reward_min   | -0.517     |
| test/Q                         | -12.671693 |
| test/Q_plus_P                  | -12.671693 |
| test/reward_per_eps            | -24.9      |
| test/steps                     | 19200      |
| train/episodes                 | 1920       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0231     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0703    |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.552     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 76800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 48         |
| stats_o/mean                   | 0.2030888  |
| stats_o/std                    | 0.10436616 |
| test/episodes                  | 490        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.287      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00431   |
| test/info_shaping_reward_mean  | -0.114     |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -12.92876  |
| test/Q_plus_P                  | -12.92876  |
| test/reward_per_eps            | -28.5      |
| test/steps                     | 19600      |
| train/episodes                 | 1960       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0612     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.05      |
| train/info_shaping_reward_mean | -0.196     |
| train/info_shaping_reward_min  | -0.428     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 78400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 49          |
| stats_o/mean                   | 0.20316143  |
| stats_o/std                    | 0.105428785 |
| test/episodes                  | 500         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.268       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00749    |
| test/info_shaping_reward_mean  | -0.118      |
| test/info_shaping_reward_min   | -0.199      |
| test/Q                         | -12.683201  |
| test/Q_plus_P                  | -12.683201  |
| test/reward_per_eps            | -29.3       |
| test/steps                     | 20000       |
| train/episodes                 | 2000        |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.015       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0752     |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.536      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 80000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.20316526 |
| stats_o/std                    | 0.10661214 |
| test/episodes                  | 510        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.158      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00916   |
| test/info_shaping_reward_mean  | -0.145     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -13.406868 |
| test/Q_plus_P                  | -13.406868 |
| test/reward_per_eps            | -33.7      |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0306     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0685    |
| train/info_shaping_reward_mean | -0.207     |
| train/info_shaping_reward_min  | -0.479     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.20318417 |
| stats_o/std                    | 0.10674585 |
| test/episodes                  | 520        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.415      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00233   |
| test/info_shaping_reward_mean  | -0.102     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -10.800404 |
| test/Q_plus_P                  | -10.800404 |
| test/reward_per_eps            | -23.4      |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.03       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0565    |
| train/info_shaping_reward_mean | -0.205     |
| train/info_shaping_reward_min  | -0.453     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.20304537 |
| stats_o/std                    | 0.10684862 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.325      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00644   |
| test/info_shaping_reward_mean  | -0.117     |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -12.254748 |
| test/Q_plus_P                  | -12.254748 |
| test/reward_per_eps            | -27        |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0412     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0651    |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.311     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 53         |
| stats_o/mean                   | 0.20304957 |
| stats_o/std                    | 0.10663138 |
| test/episodes                  | 540        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.25       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00707   |
| test/info_shaping_reward_mean  | -0.111     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -11.003918 |
| test/Q_plus_P                  | -11.003918 |
| test/reward_per_eps            | -30        |
| test/steps                     | 21600      |
| train/episodes                 | 2160       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0594     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0589    |
| train/info_shaping_reward_mean | -0.16      |
| train/info_shaping_reward_min  | -0.302     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.6      |
| train/steps                    | 86400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 54         |
| stats_o/mean                   | 0.20313749 |
| stats_o/std                    | 0.10647032 |
| test/episodes                  | 550        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.45       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00351   |
| test/info_shaping_reward_mean  | -0.094     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -10.531132 |
| test/Q_plus_P                  | -10.531132 |
| test/reward_per_eps            | -22        |
| test/steps                     | 22000      |
| train/episodes                 | 2200       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0606     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0505    |
| train/info_shaping_reward_mean | -0.155     |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.6      |
| train/steps                    | 88000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 55         |
| stats_o/mean                   | 0.2032412  |
| stats_o/std                    | 0.10615452 |
| test/episodes                  | 560        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.335      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.0999    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -10.600192 |
| test/Q_plus_P                  | -10.600192 |
| test/reward_per_eps            | -26.6      |
| test/steps                     | 22400      |
| train/episodes                 | 2240       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0563     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0271    |
| train/info_shaping_reward_mean | -0.153     |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 89600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 56          |
| stats_o/mean                   | 0.20329355  |
| stats_o/std                    | 0.106034964 |
| test/episodes                  | 570         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.48        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00253    |
| test/info_shaping_reward_mean  | -0.0849     |
| test/info_shaping_reward_min   | -0.2        |
| test/Q                         | -9.798054   |
| test/Q_plus_P                  | -9.798054   |
| test/reward_per_eps            | -20.8       |
| test/steps                     | 22800       |
| train/episodes                 | 2280        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.0744      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0292     |
| train/info_shaping_reward_mean | -0.162      |
| train/info_shaping_reward_min  | -0.284      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37         |
| train/steps                    | 91200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 57          |
| stats_o/mean                   | 0.20340946  |
| stats_o/std                    | 0.10573246  |
| test/episodes                  | 580         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.427       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00477    |
| test/info_shaping_reward_mean  | -0.0913     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -10.0236435 |
| test/Q_plus_P                  | -10.0236435 |
| test/reward_per_eps            | -22.9       |
| test/steps                     | 23200       |
| train/episodes                 | 2320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.108       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0192     |
| train/info_shaping_reward_mean | -0.144      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.7       |
| train/steps                    | 92800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 58         |
| stats_o/mean                   | 0.20348047 |
| stats_o/std                    | 0.10560501 |
| test/episodes                  | 590        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.42       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00265   |
| test/info_shaping_reward_mean  | -0.0858    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -9.4950905 |
| test/Q_plus_P                  | -9.4950905 |
| test/reward_per_eps            | -23.2      |
| test/steps                     | 23600      |
| train/episodes                 | 2360       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.113      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0323    |
| train/info_shaping_reward_mean | -0.152     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.5      |
| train/steps                    | 94400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 59          |
| stats_o/mean                   | 0.20360628  |
| stats_o/std                    | 0.105492346 |
| test/episodes                  | 600         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.463       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0117     |
| test/info_shaping_reward_mean  | -0.0838     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -9.146135   |
| test/Q_plus_P                  | -9.146135   |
| test/reward_per_eps            | -21.5       |
| test/steps                     | 24000       |
| train/episodes                 | 2400        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.143       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0272     |
| train/info_shaping_reward_mean | -0.143      |
| train/info_shaping_reward_min  | -0.279      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.3       |
| train/steps                    | 96000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 60         |
| stats_o/mean                   | 0.20360163 |
| stats_o/std                    | 0.10523434 |
| test/episodes                  | 610        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.552      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00155   |
| test/info_shaping_reward_mean  | -0.083     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -9.684685  |
| test/Q_plus_P                  | -9.684685  |
| test/reward_per_eps            | -17.9      |
| test/steps                     | 24400      |
| train/episodes                 | 2440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.117      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0181    |
| train/info_shaping_reward_mean | -0.146     |
| train/info_shaping_reward_min  | -0.287     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.3      |
| train/steps                    | 97600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 61         |
| stats_o/mean                   | 0.20369093 |
| stats_o/std                    | 0.10490604 |
| test/episodes                  | 620        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.623      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00318   |
| test/info_shaping_reward_mean  | -0.0678    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -7.9187074 |
| test/Q_plus_P                  | -7.9187074 |
| test/reward_per_eps            | -15.1      |
| test/steps                     | 24800      |
| train/episodes                 | 2480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.221      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.013     |
| train/info_shaping_reward_mean | -0.127     |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.1      |
| train/steps                    | 99200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 62         |
| stats_o/mean                   | 0.20368129 |
| stats_o/std                    | 0.10452159 |
| test/episodes                  | 630        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.657      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00247   |
| test/info_shaping_reward_mean  | -0.0669    |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -7.51864   |
| test/Q_plus_P                  | -7.51864   |
| test/reward_per_eps            | -13.7      |
| test/steps                     | 25200      |
| train/episodes                 | 2520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.194      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.013     |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.2      |
| train/steps                    | 100800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 63         |
| stats_o/mean                   | 0.20373665 |
| stats_o/std                    | 0.10420455 |
| test/episodes                  | 640        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.59       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00518   |
| test/info_shaping_reward_mean  | -0.0706    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -7.892872  |
| test/Q_plus_P                  | -7.892872  |
| test/reward_per_eps            | -16.4      |
| test/steps                     | 25600      |
| train/episodes                 | 2560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.219      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0106    |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.2      |
| train/steps                    | 102400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.20368066 |
| stats_o/std                    | 0.10374095 |
| test/episodes                  | 650        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.57       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00261   |
| test/info_shaping_reward_mean  | -0.0743    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -7.9852138 |
| test/Q_plus_P                  | -7.9852138 |
| test/reward_per_eps            | -17.2      |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.249      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.011     |
| train/info_shaping_reward_mean | -0.112     |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30        |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 65         |
| stats_o/mean                   | 0.20367928 |
| stats_o/std                    | 0.10328367 |
| test/episodes                  | 660        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00103   |
| test/info_shaping_reward_mean  | -0.0699    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -6.8567176 |
| test/Q_plus_P                  | -6.8567176 |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 26400      |
| train/episodes                 | 2640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.22       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0106    |
| train/info_shaping_reward_mean | -0.123     |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.2      |
| train/steps                    | 105600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 66          |
| stats_o/mean                   | 0.2036623   |
| stats_o/std                    | 0.102980316 |
| test/episodes                  | 670         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.635       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00167    |
| test/info_shaping_reward_mean  | -0.0687     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -7.0387683  |
| test/Q_plus_P                  | -7.0387683  |
| test/reward_per_eps            | -14.6       |
| test/steps                     | 26800       |
| train/episodes                 | 2680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.294       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00875    |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.2       |
| train/steps                    | 107200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 67         |
| stats_o/mean                   | 0.20364845 |
| stats_o/std                    | 0.10255532 |
| test/episodes                  | 680        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00125   |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -6.496442  |
| test/Q_plus_P                  | -6.496442  |
| test/reward_per_eps            | -12        |
| test/steps                     | 27200      |
| train/episodes                 | 2720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.297      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.009     |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.1      |
| train/steps                    | 108800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 68          |
| stats_o/mean                   | 0.20376697  |
| stats_o/std                    | 0.102176294 |
| test/episodes                  | 690         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.393       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00235    |
| test/info_shaping_reward_mean  | -0.0946     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -10.464647  |
| test/Q_plus_P                  | -10.464647  |
| test/reward_per_eps            | -24.3       |
| test/steps                     | 27600       |
| train/episodes                 | 2760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.343       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00696    |
| train/info_shaping_reward_mean | -0.105      |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.3       |
| train/steps                    | 110400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 69         |
| stats_o/mean                   | 0.2037551  |
| stats_o/std                    | 0.10167014 |
| test/episodes                  | 700        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.64       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00233   |
| test/info_shaping_reward_mean  | -0.0647    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -6.7132764 |
| test/Q_plus_P                  | -6.7132764 |
| test/reward_per_eps            | -14.4      |
| test/steps                     | 28000      |
| train/episodes                 | 2800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.329      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00784   |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.8      |
| train/steps                    | 112000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.20370835 |
| stats_o/std                    | 0.10119747 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.637      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0029    |
| test/info_shaping_reward_mean  | -0.0641    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -6.499265  |
| test/Q_plus_P                  | -6.499265  |
| test/reward_per_eps            | -14.5      |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.307      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00774   |
| train/info_shaping_reward_mean | -0.0997    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.7      |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 71         |
| stats_o/mean                   | 0.20372196 |
| stats_o/std                    | 0.10077122 |
| test/episodes                  | 720        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00292   |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -5.772712  |
| test/Q_plus_P                  | -5.772712  |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 28800      |
| train/episodes                 | 2880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.402      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00766   |
| train/info_shaping_reward_mean | -0.0921    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.9      |
| train/steps                    | 115200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 72         |
| stats_o/mean                   | 0.20371914 |
| stats_o/std                    | 0.1002742  |
| test/episodes                  | 730        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00449   |
| test/info_shaping_reward_mean  | -0.0588    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -5.832219  |
| test/Q_plus_P                  | -5.832219  |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 29200      |
| train/episodes                 | 2920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.43       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00412   |
| train/info_shaping_reward_mean | -0.0878    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.8      |
| train/steps                    | 116800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.20369539 |
| stats_o/std                    | 0.09985547 |
| test/episodes                  | 740        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.657      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00474   |
| test/info_shaping_reward_mean  | -0.061     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -5.8478346 |
| test/Q_plus_P                  | -5.8478346 |
| test/reward_per_eps            | -13.7      |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.347      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00645   |
| train/info_shaping_reward_mean | -0.0996    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.1      |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 74          |
| stats_o/mean                   | 0.20370667  |
| stats_o/std                    | 0.099439934 |
| test/episodes                  | 750         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00395    |
| test/info_shaping_reward_mean  | -0.0588     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -5.6986127  |
| test/Q_plus_P                  | -5.6986127  |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 30000       |
| train/episodes                 | 3000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.456       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00475    |
| train/info_shaping_reward_mean | -0.0855     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.8       |
| train/steps                    | 120000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 75         |
| stats_o/mean                   | 0.20376207 |
| stats_o/std                    | 0.0990994  |
| test/episodes                  | 760        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0063    |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -4.7736754 |
| test/Q_plus_P                  | -4.7736754 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 30400      |
| train/episodes                 | 3040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.444      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0071    |
| train/info_shaping_reward_mean | -0.0854    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.2      |
| train/steps                    | 121600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 76         |
| stats_o/mean                   | 0.20377113 |
| stats_o/std                    | 0.0986604  |
| test/episodes                  | 770        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0015    |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -4.989342  |
| test/Q_plus_P                  | -4.989342  |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 30800      |
| train/episodes                 | 3080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.459      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00476   |
| train/info_shaping_reward_mean | -0.0834    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 123200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 77          |
| stats_o/mean                   | 0.20373802  |
| stats_o/std                    | 0.098359145 |
| test/episodes                  | 780         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00334    |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -4.771291   |
| test/Q_plus_P                  | -4.771291   |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 31200       |
| train/episodes                 | 3120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.458       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00545    |
| train/info_shaping_reward_mean | -0.0892     |
| train/info_shaping_reward_min  | -0.201      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.7       |
| train/steps                    | 124800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 78          |
| stats_o/mean                   | 0.20377988  |
| stats_o/std                    | 0.098137006 |
| test/episodes                  | 790         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00958    |
| test/info_shaping_reward_mean  | -0.0637     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -4.716959   |
| test/Q_plus_P                  | -4.716959   |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 31600       |
| train/episodes                 | 3160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.435       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00474    |
| train/info_shaping_reward_mean | -0.0945     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.6       |
| train/steps                    | 126400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 79         |
| stats_o/mean                   | 0.20379972 |
| stats_o/std                    | 0.09770258 |
| test/episodes                  | 800        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00757   |
| test/info_shaping_reward_mean  | -0.0594    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -4.682666  |
| test/Q_plus_P                  | -4.682666  |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 32000      |
| train/episodes                 | 3200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.47       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00488   |
| train/info_shaping_reward_mean | -0.0851    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 128000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.20379837  |
| stats_o/std                    | 0.097258955 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00108    |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -4.479887   |
| test/Q_plus_P                  | -4.479887   |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.523       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00544    |
| train/info_shaping_reward_mean | -0.0764     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 81         |
| stats_o/mean                   | 0.20381854 |
| stats_o/std                    | 0.09696477 |
| test/episodes                  | 820        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.06      |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -4.548966  |
| test/Q_plus_P                  | -4.548966  |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 32800      |
| train/episodes                 | 3280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.407      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00653   |
| train/info_shaping_reward_mean | -0.1       |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.7      |
| train/steps                    | 131200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 82         |
| stats_o/mean                   | 0.203829   |
| stats_o/std                    | 0.09665953 |
| test/episodes                  | 830        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00443   |
| test/info_shaping_reward_mean  | -0.0569    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -4.1648884 |
| test/Q_plus_P                  | -4.1648884 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 33200      |
| train/episodes                 | 3320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00538   |
| train/info_shaping_reward_mean | -0.0823    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 132800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 83         |
| stats_o/mean                   | 0.20383158 |
| stats_o/std                    | 0.09647162 |
| test/episodes                  | 840        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00492   |
| test/info_shaping_reward_mean  | -0.0608    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -3.9413605 |
| test/Q_plus_P                  | -3.9413605 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 33600      |
| train/episodes                 | 3360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.456      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00484   |
| train/info_shaping_reward_mean | -0.0937    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 134400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 84          |
| stats_o/mean                   | 0.20393297  |
| stats_o/std                    | 0.096296184 |
| test/episodes                  | 850         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00287    |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -3.657754   |
| test/Q_plus_P                  | -3.657754   |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 34000       |
| train/episodes                 | 3400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.424       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00705    |
| train/info_shaping_reward_mean | -0.0966     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23         |
| train/steps                    | 136000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.20391773  |
| stats_o/std                    | 0.095902614 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.688       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00266    |
| test/info_shaping_reward_mean  | -0.0581     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -4.0292177  |
| test/Q_plus_P                  | -4.0292177  |
| test/reward_per_eps            | -12.5       |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.496       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00621    |
| train/info_shaping_reward_mean | -0.0819     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.2       |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 86         |
| stats_o/mean                   | 0.20394829 |
| stats_o/std                    | 0.09569852 |
| test/episodes                  | 870        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00341   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -3.4849765 |
| test/Q_plus_P                  | -3.4849765 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 34800      |
| train/episodes                 | 3480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.478      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0935    |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 139200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 87          |
| stats_o/mean                   | 0.20394497  |
| stats_o/std                    | 0.095310666 |
| test/episodes                  | 880         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00223    |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -3.3983526  |
| test/Q_plus_P                  | -3.3983526  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 35200       |
| train/episodes                 | 3520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.524       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00534    |
| train/info_shaping_reward_mean | -0.0773     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 140800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 88         |
| stats_o/mean                   | 0.2039426  |
| stats_o/std                    | 0.09491692 |
| test/episodes                  | 890        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00321   |
| test/info_shaping_reward_mean  | -0.0629    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.8384235 |
| test/Q_plus_P                  | -3.8384235 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 35600      |
| train/episodes                 | 3560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00426   |
| train/info_shaping_reward_mean | -0.0779    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 142400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 89         |
| stats_o/mean                   | 0.20391689 |
| stats_o/std                    | 0.09473787 |
| test/episodes                  | 900        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00267   |
| test/info_shaping_reward_mean  | -0.057     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.4797895 |
| test/Q_plus_P                  | -3.4797895 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 36000      |
| train/episodes                 | 3600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.419      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00523   |
| train/info_shaping_reward_mean | -0.0981    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.2      |
| train/steps                    | 144000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 90          |
| stats_o/mean                   | 0.20394973  |
| stats_o/std                    | 0.094462566 |
| test/episodes                  | 910         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00195    |
| test/info_shaping_reward_mean  | -0.0594     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -3.580078   |
| test/Q_plus_P                  | -3.580078   |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 36400       |
| train/episodes                 | 3640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.449       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00552    |
| train/info_shaping_reward_mean | -0.0899     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22         |
| train/steps                    | 145600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 91          |
| stats_o/mean                   | 0.20398371  |
| stats_o/std                    | 0.094172515 |
| test/episodes                  | 920         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00442    |
| test/info_shaping_reward_mean  | -0.0639     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -3.3827202  |
| test/Q_plus_P                  | -3.3827202  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 36800       |
| train/episodes                 | 3680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.492       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00676    |
| train/info_shaping_reward_mean | -0.0819     |
| train/info_shaping_reward_min  | -0.194      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.3       |
| train/steps                    | 147200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 92         |
| stats_o/mean                   | 0.20403023 |
| stats_o/std                    | 0.09390318 |
| test/episodes                  | 930        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0035    |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -3.022984  |
| test/Q_plus_P                  | -3.022984  |
| test/reward_per_eps            | -10        |
| test/steps                     | 37200      |
| train/episodes                 | 3720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.446      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00554   |
| train/info_shaping_reward_mean | -0.0881    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.1      |
| train/steps                    | 148800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 93         |
| stats_o/mean                   | 0.20404598 |
| stats_o/std                    | 0.09353453 |
| test/episodes                  | 940        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00322   |
| test/info_shaping_reward_mean  | -0.0603    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -3.1141858 |
| test/Q_plus_P                  | -3.1141858 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 37600      |
| train/episodes                 | 3760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.468      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00618   |
| train/info_shaping_reward_mean | -0.0826    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.3      |
| train/steps                    | 150400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 94         |
| stats_o/mean                   | 0.20403112 |
| stats_o/std                    | 0.09327061 |
| test/episodes                  | 950        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0021    |
| test/info_shaping_reward_mean  | -0.06      |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -3.084691  |
| test/Q_plus_P                  | -3.084691  |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 38000      |
| train/episodes                 | 3800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.476      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00496   |
| train/info_shaping_reward_mean | -0.0936    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21        |
| train/steps                    | 152000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 95         |
| stats_o/mean                   | 0.20405447 |
| stats_o/std                    | 0.09303659 |
| test/episodes                  | 960        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00309   |
| test/info_shaping_reward_mean  | -0.0531    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.8955488 |
| test/Q_plus_P                  | -2.8955488 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 38400      |
| train/episodes                 | 3840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.48       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.0863    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 153600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 96         |
| stats_o/mean                   | 0.20406887 |
| stats_o/std                    | 0.0928032  |
| test/episodes                  | 970        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00491   |
| test/info_shaping_reward_mean  | -0.0624    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.9702668 |
| test/Q_plus_P                  | -2.9702668 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 38800      |
| train/episodes                 | 3880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.461      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00606   |
| train/info_shaping_reward_mean | -0.0884    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 155200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 97         |
| stats_o/mean                   | 0.20408703 |
| stats_o/std                    | 0.09254623 |
| test/episodes                  | 980        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00208   |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.5392535 |
| test/Q_plus_P                  | -2.5392535 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 39200      |
| train/episodes                 | 3920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.474      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0845    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21        |
| train/steps                    | 156800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 98         |
| stats_o/mean                   | 0.20415142 |
| stats_o/std                    | 0.09235165 |
| test/episodes                  | 990        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00181   |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.639179  |
| test/Q_plus_P                  | -2.639179  |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 39600      |
| train/episodes                 | 3960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0791    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 158400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 99          |
| stats_o/mean                   | 0.20417239  |
| stats_o/std                    | 0.092042655 |
| test/episodes                  | 1000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00584    |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.574747   |
| test/Q_plus_P                  | -2.574747   |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 40000       |
| train/episodes                 | 4000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.556       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00567    |
| train/info_shaping_reward_mean | -0.0768     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 160000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 100         |
| stats_o/mean                   | 0.2041792   |
| stats_o/std                    | 0.091725394 |
| test/episodes                  | 1010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00201    |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -2.424367   |
| test/Q_plus_P                  | -2.424367   |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 40400       |
| train/episodes                 | 4040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.558       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00367    |
| train/info_shaping_reward_mean | -0.0749     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 161600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 101        |
| stats_o/mean                   | 0.20416464 |
| stats_o/std                    | 0.09145331 |
| test/episodes                  | 1020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00373   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.1351042 |
| test/Q_plus_P                  | -2.1351042 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 40800      |
| train/episodes                 | 4080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00541   |
| train/info_shaping_reward_mean | -0.0741    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 163200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 102        |
| stats_o/mean                   | 0.20416832 |
| stats_o/std                    | 0.09115058 |
| test/episodes                  | 1030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00439   |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.4002945 |
| test/Q_plus_P                  | -2.4002945 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 41200      |
| train/episodes                 | 4120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00435   |
| train/info_shaping_reward_mean | -0.0775    |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 164800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 103        |
| stats_o/mean                   | 0.2041729  |
| stats_o/std                    | 0.09095434 |
| test/episodes                  | 1040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00349   |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.477984  |
| test/Q_plus_P                  | -2.477984  |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 41600      |
| train/episodes                 | 4160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.441      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.005     |
| train/info_shaping_reward_mean | -0.0875    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.4      |
| train/steps                    | 166400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 104        |
| stats_o/mean                   | 0.20416719 |
| stats_o/std                    | 0.09079582 |
| test/episodes                  | 1050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00215   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.3367715 |
| test/Q_plus_P                  | -2.3367715 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 42000      |
| train/episodes                 | 4200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.461      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00475   |
| train/info_shaping_reward_mean | -0.0904    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 168000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 105        |
| stats_o/mean                   | 0.20416743 |
| stats_o/std                    | 0.09052498 |
| test/episodes                  | 1060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00174   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.245802  |
| test/Q_plus_P                  | -2.245802  |
| test/reward_per_eps            | -10        |
| test/steps                     | 42400      |
| train/episodes                 | 4240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.436      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00548   |
| train/info_shaping_reward_mean | -0.0885    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.6      |
| train/steps                    | 169600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 106        |
| stats_o/mean                   | 0.20421234 |
| stats_o/std                    | 0.09053963 |
| test/episodes                  | 1070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00131   |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -2.5108144 |
| test/Q_plus_P                  | -2.5108144 |
| test/reward_per_eps            | -11        |
| test/steps                     | 42800      |
| train/episodes                 | 4280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.494      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0958    |
| train/info_shaping_reward_min  | -0.282     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 171200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 107        |
| stats_o/mean                   | 0.20421013 |
| stats_o/std                    | 0.09045944 |
| test/episodes                  | 1080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00328   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.1616654 |
| test/Q_plus_P                  | -2.1616654 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 43200      |
| train/episodes                 | 4320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.514      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.0919    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 172800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 108        |
| stats_o/mean                   | 0.20423767 |
| stats_o/std                    | 0.09045513 |
| test/episodes                  | 1090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00699   |
| test/info_shaping_reward_mean  | -0.0614    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.4327676 |
| test/Q_plus_P                  | -2.4327676 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 43600      |
| train/episodes                 | 4360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.426      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00542   |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23        |
| train/steps                    | 174400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 109         |
| stats_o/mean                   | 0.20423548  |
| stats_o/std                    | 0.090233974 |
| test/episodes                  | 1100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00343    |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -2.2504692  |
| test/Q_plus_P                  | -2.2504692  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 44000       |
| train/episodes                 | 4400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.448       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00545    |
| train/info_shaping_reward_mean | -0.0879     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.1       |
| train/steps                    | 176000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 110        |
| stats_o/mean                   | 0.2042446  |
| stats_o/std                    | 0.09007339 |
| test/episodes                  | 1110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.67       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00805   |
| test/info_shaping_reward_mean  | -0.0634    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.4909723 |
| test/Q_plus_P                  | -2.4909723 |
| test/reward_per_eps            | -13.2      |
| test/steps                     | 44400      |
| train/episodes                 | 4440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.494      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00551   |
| train/info_shaping_reward_mean | -0.0835    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 177600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 111         |
| stats_o/mean                   | 0.20426023  |
| stats_o/std                    | 0.089978196 |
| test/episodes                  | 1120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00134    |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.053183   |
| test/Q_plus_P                  | -2.053183   |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 44800       |
| train/episodes                 | 4480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.493       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00546    |
| train/info_shaping_reward_mean | -0.0902     |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.3       |
| train/steps                    | 179200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 112        |
| stats_o/mean                   | 0.20426907 |
| stats_o/std                    | 0.08971865 |
| test/episodes                  | 1130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.9051852 |
| test/Q_plus_P                  | -1.9051852 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 45200      |
| train/episodes                 | 4520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.506      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00644   |
| train/info_shaping_reward_mean | -0.0779    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 180800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 113        |
| stats_o/mean                   | 0.20431636 |
| stats_o/std                    | 0.08955652 |
| test/episodes                  | 1140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00253   |
| test/info_shaping_reward_mean  | -0.0515    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.8636371 |
| test/Q_plus_P                  | -1.8636371 |
| test/reward_per_eps            | -10        |
| test/steps                     | 45600      |
| train/episodes                 | 4560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00588   |
| train/info_shaping_reward_mean | -0.0785    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 182400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 114         |
| stats_o/mean                   | 0.20434862  |
| stats_o/std                    | 0.089336805 |
| test/episodes                  | 1150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00239    |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.9168537  |
| test/Q_plus_P                  | -1.9168537  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 46000       |
| train/episodes                 | 4600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.535       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00412    |
| train/info_shaping_reward_mean | -0.0773     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 184000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 115        |
| stats_o/mean                   | 0.20435381 |
| stats_o/std                    | 0.08915108 |
| test/episodes                  | 1160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.8057576 |
| test/Q_plus_P                  | -1.8057576 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 46400      |
| train/episodes                 | 4640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.555      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.0814    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 185600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 116        |
| stats_o/mean                   | 0.20434822 |
| stats_o/std                    | 0.08893102 |
| test/episodes                  | 1170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00272   |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.9339283 |
| test/Q_plus_P                  | -1.9339283 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 46800      |
| train/episodes                 | 4680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.513      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0039    |
| train/info_shaping_reward_mean | -0.0809    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.5      |
| train/steps                    | 187200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 117        |
| stats_o/mean                   | 0.20434007 |
| stats_o/std                    | 0.08871834 |
| test/episodes                  | 1180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00187   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.8337528 |
| test/Q_plus_P                  | -1.8337528 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 47200      |
| train/episodes                 | 4720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.0754    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 188800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.20436041  |
| stats_o/std                    | 0.088531725 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00233    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.733715   |
| test/Q_plus_P                  | -1.733715   |
| test/reward_per_eps            | -10         |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.521       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00436    |
| train/info_shaping_reward_mean | -0.0868     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 119         |
| stats_o/mean                   | 0.20436317  |
| stats_o/std                    | 0.088312365 |
| test/episodes                  | 1200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00428    |
| test/info_shaping_reward_mean  | -0.0559     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.890438   |
| test/Q_plus_P                  | -1.890438   |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 48000       |
| train/episodes                 | 4800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.556       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00447    |
| train/info_shaping_reward_mean | -0.0794     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 192000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 120         |
| stats_o/mean                   | 0.20438789  |
| stats_o/std                    | 0.088127814 |
| test/episodes                  | 1210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00454    |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.9604111  |
| test/Q_plus_P                  | -1.9604111  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 48400       |
| train/episodes                 | 4840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.484       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00452    |
| train/info_shaping_reward_mean | -0.0845     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.6       |
| train/steps                    | 193600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 121         |
| stats_o/mean                   | 0.20439225  |
| stats_o/std                    | 0.088089205 |
| test/episodes                  | 1220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00784    |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3796952  |
| test/Q_plus_P                  | -1.3796952  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 48800       |
| train/episodes                 | 4880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.436       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00581    |
| train/info_shaping_reward_mean | -0.0957     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.6       |
| train/steps                    | 195200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 122        |
| stats_o/mean                   | 0.20439439 |
| stats_o/std                    | 0.0878814  |
| test/episodes                  | 1230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00302   |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.0107474 |
| test/Q_plus_P                  | -2.0107474 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 49200      |
| train/episodes                 | 4920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.478      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00708   |
| train/info_shaping_reward_mean | -0.0869    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 196800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 123         |
| stats_o/mean                   | 0.2044177   |
| stats_o/std                    | 0.087744236 |
| test/episodes                  | 1240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00327    |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.6777469  |
| test/Q_plus_P                  | -1.6777469  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 49600       |
| train/episodes                 | 4960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.496       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00739    |
| train/info_shaping_reward_mean | -0.0883     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 198400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 124        |
| stats_o/mean                   | 0.20442112 |
| stats_o/std                    | 0.0875723  |
| test/episodes                  | 1250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6863931 |
| test/Q_plus_P                  | -1.6863931 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 50000      |
| train/episodes                 | 5000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.535      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00449   |
| train/info_shaping_reward_mean | -0.082     |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 200000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 125         |
| stats_o/mean                   | 0.20445365  |
| stats_o/std                    | 0.087441586 |
| test/episodes                  | 1260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00436    |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5907875  |
| test/Q_plus_P                  | -1.5907875  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 50400       |
| train/episodes                 | 5040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.501       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00701    |
| train/info_shaping_reward_mean | -0.0821     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20         |
| train/steps                    | 201600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 126        |
| stats_o/mean                   | 0.20444089 |
| stats_o/std                    | 0.08739515 |
| test/episodes                  | 1270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00145   |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6097928 |
| test/Q_plus_P                  | -1.6097928 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 50800      |
| train/episodes                 | 5080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.457      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00458   |
| train/info_shaping_reward_mean | -0.1       |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.7      |
| train/steps                    | 203200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 127        |
| stats_o/mean                   | 0.20446827 |
| stats_o/std                    | 0.08742056 |
| test/episodes                  | 1280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0025    |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3540738 |
| test/Q_plus_P                  | -1.3540738 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 51200      |
| train/episodes                 | 5120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.485      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0053    |
| train/info_shaping_reward_mean | -0.094     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 204800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.2044671  |
| stats_o/std                    | 0.08722476 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00817   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2130722 |
| test/Q_plus_P                  | -1.2130722 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.563      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00469   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 129        |
| stats_o/mean                   | 0.20443629 |
| stats_o/std                    | 0.08709266 |
| test/episodes                  | 1300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0043    |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6975425 |
| test/Q_plus_P                  | -1.6975425 |
| test/reward_per_eps            | -10        |
| test/steps                     | 52000      |
| train/episodes                 | 5200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.0835    |
| train/info_shaping_reward_min  | -0.277     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 208000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 130         |
| stats_o/mean                   | 0.20442334  |
| stats_o/std                    | 0.086932726 |
| test/episodes                  | 1310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00425    |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.4760777  |
| test/Q_plus_P                  | -1.4760777  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 52400       |
| train/episodes                 | 5240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.487       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00545    |
| train/info_shaping_reward_mean | -0.0805     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.5       |
| train/steps                    | 209600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 131        |
| stats_o/mean                   | 0.2044474  |
| stats_o/std                    | 0.08675552 |
| test/episodes                  | 1320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00467   |
| test/info_shaping_reward_mean  | -0.071     |
| test/info_shaping_reward_min   | -0.339     |
| test/Q                         | -3.0605292 |
| test/Q_plus_P                  | -3.0605292 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 52800      |
| train/episodes                 | 5280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00453   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 211200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 132         |
| stats_o/mean                   | 0.20444836  |
| stats_o/std                    | 0.086540766 |
| test/episodes                  | 1330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00637    |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.6097555  |
| test/Q_plus_P                  | -1.6097555  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 53200       |
| train/episodes                 | 5320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.537       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00469    |
| train/info_shaping_reward_mean | -0.0777     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.5       |
| train/steps                    | 212800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 133         |
| stats_o/mean                   | 0.20444523  |
| stats_o/std                    | 0.086318254 |
| test/episodes                  | 1340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00562    |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.2403862  |
| test/Q_plus_P                  | -1.2403862  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 53600       |
| train/episodes                 | 5360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.582       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00625    |
| train/info_shaping_reward_mean | -0.0698     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 214400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 134        |
| stats_o/mean                   | 0.20445627 |
| stats_o/std                    | 0.08608819 |
| test/episodes                  | 1350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00351   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.7147292 |
| test/Q_plus_P                  | -1.7147292 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 54000      |
| train/episodes                 | 5400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00446   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 216000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 135        |
| stats_o/mean                   | 0.2044512  |
| stats_o/std                    | 0.08586294 |
| test/episodes                  | 1360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0052    |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5484824 |
| test/Q_plus_P                  | -1.5484824 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 54400      |
| train/episodes                 | 5440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00466   |
| train/info_shaping_reward_mean | -0.0768    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 217600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 136         |
| stats_o/mean                   | 0.2044651   |
| stats_o/std                    | 0.085759915 |
| test/episodes                  | 1370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00291    |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.5380735  |
| test/Q_plus_P                  | -1.5380735  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 54800       |
| train/episodes                 | 5480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.523       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0046     |
| train/info_shaping_reward_mean | -0.0786     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 219200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 137        |
| stats_o/mean                   | 0.20447282 |
| stats_o/std                    | 0.08562274 |
| test/episodes                  | 1380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00529   |
| test/info_shaping_reward_mean  | -0.0574    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5485579 |
| test/Q_plus_P                  | -1.5485579 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 55200      |
| train/episodes                 | 5520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00585   |
| train/info_shaping_reward_mean | -0.0895    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 220800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.20448127 |
| stats_o/std                    | 0.08547758 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00127   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3983982 |
| test/Q_plus_P                  | -1.3983982 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.553      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0046    |
| train/info_shaping_reward_mean | -0.0792    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.2044953  |
| stats_o/std                    | 0.08530335 |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00511   |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3767445 |
| test/Q_plus_P                  | -1.3767445 |
| test/reward_per_eps            | -10        |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00405   |
| train/info_shaping_reward_mean | -0.0842    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 140         |
| stats_o/mean                   | 0.20451784  |
| stats_o/std                    | 0.085149124 |
| test/episodes                  | 1410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00339    |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.5553787  |
| test/Q_plus_P                  | -1.5553787  |
| test/reward_per_eps            | -11         |
| test/steps                     | 56400       |
| train/episodes                 | 5640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.551       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00484    |
| train/info_shaping_reward_mean | -0.0781     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 225600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.20452651 |
| stats_o/std                    | 0.08495509 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00368   |
| test/info_shaping_reward_mean  | -0.0571    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5906072 |
| test/Q_plus_P                  | -1.5906072 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00499   |
| train/info_shaping_reward_mean | -0.0754    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 142        |
| stats_o/mean                   | 0.20451957 |
| stats_o/std                    | 0.0848376  |
| test/episodes                  | 1430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0041    |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.5291052 |
| test/Q_plus_P                  | -1.5291052 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 57200      |
| train/episodes                 | 5720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.553      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00562   |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 228800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 143        |
| stats_o/mean                   | 0.20454012 |
| stats_o/std                    | 0.08471922 |
| test/episodes                  | 1440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00825   |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3863416 |
| test/Q_plus_P                  | -1.3863416 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 57600      |
| train/episodes                 | 5760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 230400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 144        |
| stats_o/mean                   | 0.20451742 |
| stats_o/std                    | 0.08460746 |
| test/episodes                  | 1450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00248   |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2054502 |
| test/Q_plus_P                  | -1.2054502 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 58000      |
| train/episodes                 | 5800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00408   |
| train/info_shaping_reward_mean | -0.0766    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 232000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 145         |
| stats_o/mean                   | 0.2045344   |
| stats_o/std                    | 0.084584214 |
| test/episodes                  | 1460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0041     |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2114486  |
| test/Q_plus_P                  | -1.2114486  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 58400       |
| train/episodes                 | 5840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.522       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00498    |
| train/info_shaping_reward_mean | -0.083      |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 233600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 146        |
| stats_o/mean                   | 0.20452736 |
| stats_o/std                    | 0.08442387 |
| test/episodes                  | 1470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000725  |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3531275 |
| test/Q_plus_P                  | -1.3531275 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 58800      |
| train/episodes                 | 5880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.497      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0052    |
| train/info_shaping_reward_mean | -0.0796    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 235200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 147        |
| stats_o/mean                   | 0.20452859 |
| stats_o/std                    | 0.0843467  |
| test/episodes                  | 1480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00345   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1239579 |
| test/Q_plus_P                  | -1.1239579 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 59200      |
| train/episodes                 | 5920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.451      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00726   |
| train/info_shaping_reward_mean | -0.0929    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.9      |
| train/steps                    | 236800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 148         |
| stats_o/mean                   | 0.2045262   |
| stats_o/std                    | 0.084156446 |
| test/episodes                  | 1490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0091     |
| test/info_shaping_reward_mean  | -0.0579     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.4792846  |
| test/Q_plus_P                  | -1.4792846  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 59600       |
| train/episodes                 | 5960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.513       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0057     |
| train/info_shaping_reward_mean | -0.0823     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.5       |
| train/steps                    | 238400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 149        |
| stats_o/mean                   | 0.20453744 |
| stats_o/std                    | 0.08402275 |
| test/episodes                  | 1500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0057    |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4354036 |
| test/Q_plus_P                  | -1.4354036 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 60000      |
| train/episodes                 | 6000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.538      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00432   |
| train/info_shaping_reward_mean | -0.0844    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 240000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 150        |
| stats_o/mean                   | 0.20455156 |
| stats_o/std                    | 0.08386919 |
| test/episodes                  | 1510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00382   |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2398413 |
| test/Q_plus_P                  | -1.2398413 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 60400      |
| train/episodes                 | 6040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00572   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 241600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 151        |
| stats_o/mean                   | 0.20456    |
| stats_o/std                    | 0.08372972 |
| test/episodes                  | 1520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.23632   |
| test/Q_plus_P                  | -1.23632   |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 60800      |
| train/episodes                 | 6080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.54       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00508   |
| train/info_shaping_reward_mean | -0.0744    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 243200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 152        |
| stats_o/mean                   | 0.20454824 |
| stats_o/std                    | 0.08355754 |
| test/episodes                  | 1530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00552   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0836058 |
| test/Q_plus_P                  | -1.0836058 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 61200      |
| train/episodes                 | 6120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00412   |
| train/info_shaping_reward_mean | -0.0676    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 244800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 153        |
| stats_o/mean                   | 0.20454599 |
| stats_o/std                    | 0.08340133 |
| test/episodes                  | 1540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00217   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.1358334 |
| test/Q_plus_P                  | -1.1358334 |
| test/reward_per_eps            | -9         |
| test/steps                     | 61600      |
| train/episodes                 | 6160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00376   |
| train/info_shaping_reward_mean | -0.0776    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 246400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.20455873 |
| stats_o/std                    | 0.08321169 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0025    |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2023573 |
| test/Q_plus_P                  | -1.2023573 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00463   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.20457146 |
| stats_o/std                    | 0.08319794 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00302   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2400494 |
| test/Q_plus_P                  | -1.2400494 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00345   |
| train/info_shaping_reward_mean | -0.0859    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 156        |
| stats_o/mean                   | 0.20457865 |
| stats_o/std                    | 0.08302601 |
| test/episodes                  | 1570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00715   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0998503 |
| test/Q_plus_P                  | -1.0998503 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 62800      |
| train/episodes                 | 6280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00563   |
| train/info_shaping_reward_mean | -0.0723    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 251200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 157        |
| stats_o/mean                   | 0.20458493 |
| stats_o/std                    | 0.08285091 |
| test/episodes                  | 1580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00584   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2878559 |
| test/Q_plus_P                  | -1.2878559 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 63200      |
| train/episodes                 | 6320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.587      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00462   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 252800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.20459698  |
| stats_o/std                    | 0.082742795 |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00693    |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2422507  |
| test/Q_plus_P                  | -1.2422507  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00447    |
| train/info_shaping_reward_mean | -0.0746     |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.20460734  |
| stats_o/std                    | 0.082564555 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00636    |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.1904705  |
| test/Q_plus_P                  | -1.1904705  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.559       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00476    |
| train/info_shaping_reward_mean | -0.073      |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 160         |
| stats_o/mean                   | 0.20461395  |
| stats_o/std                    | 0.082363434 |
| test/episodes                  | 1610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00705    |
| test/info_shaping_reward_mean  | -0.0533     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.301451   |
| test/Q_plus_P                  | -1.301451   |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 64400       |
| train/episodes                 | 6440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00472    |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 257600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 161        |
| stats_o/mean                   | 0.20462829 |
| stats_o/std                    | 0.08224872 |
| test/episodes                  | 1620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00619   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1708777 |
| test/Q_plus_P                  | -1.1708777 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 64800      |
| train/episodes                 | 6480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.0831    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 259200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 162        |
| stats_o/mean                   | 0.20464136 |
| stats_o/std                    | 0.08209262 |
| test/episodes                  | 1630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00398   |
| test/info_shaping_reward_mean  | -0.0541    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.326802  |
| test/Q_plus_P                  | -1.326802  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 65200      |
| train/episodes                 | 6520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00575   |
| train/info_shaping_reward_mean | -0.0764    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 260800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 163         |
| stats_o/mean                   | 0.20465094  |
| stats_o/std                    | 0.082001925 |
| test/episodes                  | 1640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00496    |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.50429    |
| test/Q_plus_P                  | -1.50429    |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 65600       |
| train/episodes                 | 6560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.548       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00472    |
| train/info_shaping_reward_mean | -0.082      |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 262400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 164        |
| stats_o/mean                   | 0.20466001 |
| stats_o/std                    | 0.08183773 |
| test/episodes                  | 1650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000854  |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.2376213 |
| test/Q_plus_P                  | -1.2376213 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 66000      |
| train/episodes                 | 6600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.0719    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 264000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 165        |
| stats_o/mean                   | 0.20465788 |
| stats_o/std                    | 0.08171171 |
| test/episodes                  | 1660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00258   |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4535758 |
| test/Q_plus_P                  | -1.4535758 |
| test/reward_per_eps            | -10        |
| test/steps                     | 66400      |
| train/episodes                 | 6640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.548      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00358   |
| train/info_shaping_reward_mean | -0.0767    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 265600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 166        |
| stats_o/mean                   | 0.20467262 |
| stats_o/std                    | 0.08157247 |
| test/episodes                  | 1670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000493  |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.5241547 |
| test/Q_plus_P                  | -1.5241547 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 66800      |
| train/episodes                 | 6680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00494   |
| train/info_shaping_reward_mean | -0.0798    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 267200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.20466466 |
| stats_o/std                    | 0.08145986 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0027    |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.177829  |
| test/Q_plus_P                  | -1.177829  |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00329   |
| train/info_shaping_reward_mean | -0.073     |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 168        |
| stats_o/mean                   | 0.20468555 |
| stats_o/std                    | 0.08140485 |
| test/episodes                  | 1690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00282   |
| test/info_shaping_reward_mean  | -0.0515    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3606076 |
| test/Q_plus_P                  | -1.3606076 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 67600      |
| train/episodes                 | 6760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.513      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0037    |
| train/info_shaping_reward_mean | -0.0875    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.5      |
| train/steps                    | 270400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.20468985 |
| stats_o/std                    | 0.08123453 |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00424   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2039436 |
| test/Q_plus_P                  | -1.2039436 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00448   |
| train/info_shaping_reward_mean | -0.0643    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 170         |
| stats_o/mean                   | 0.20470257  |
| stats_o/std                    | 0.081193775 |
| test/episodes                  | 1710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00711    |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.0954741  |
| test/Q_plus_P                  | -1.0954741  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 68400       |
| train/episodes                 | 6840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.555       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00349    |
| train/info_shaping_reward_mean | -0.0881     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 273600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 171         |
| stats_o/mean                   | 0.20470653  |
| stats_o/std                    | 0.08109165  |
| test/episodes                  | 1720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00353    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.99663424 |
| test/Q_plus_P                  | -0.99663424 |
| test/reward_per_eps            | -9          |
| test/steps                     | 68800       |
| train/episodes                 | 6880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0048     |
| train/info_shaping_reward_mean | -0.076      |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 275200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 172        |
| stats_o/mean                   | 0.20471779 |
| stats_o/std                    | 0.08096551 |
| test/episodes                  | 1730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00286   |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9767966 |
| test/Q_plus_P                  | -0.9767966 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 69200      |
| train/episodes                 | 6920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00345   |
| train/info_shaping_reward_mean | -0.074     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 276800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 173        |
| stats_o/mean                   | 0.20471688 |
| stats_o/std                    | 0.08081668 |
| test/episodes                  | 1740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00322   |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0785902 |
| test/Q_plus_P                  | -1.0785902 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 69600      |
| train/episodes                 | 6960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0049    |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 278400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 174         |
| stats_o/mean                   | 0.20472465  |
| stats_o/std                    | 0.080667704 |
| test/episodes                  | 1750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00381    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1859677  |
| test/Q_plus_P                  | -1.1859677  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 70000       |
| train/episodes                 | 7000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.566       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00469    |
| train/info_shaping_reward_mean | -0.0765     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 280000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 175        |
| stats_o/mean                   | 0.20473026 |
| stats_o/std                    | 0.08056034 |
| test/episodes                  | 1760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00342   |
| test/info_shaping_reward_mean  | -0.057     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4159057 |
| test/Q_plus_P                  | -1.4159057 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 70400      |
| train/episodes                 | 7040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0748    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 281600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 176        |
| stats_o/mean                   | 0.20472763 |
| stats_o/std                    | 0.08042905 |
| test/episodes                  | 1770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00198   |
| test/info_shaping_reward_mean  | -0.0458    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.056638  |
| test/Q_plus_P                  | -1.056638  |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 70800      |
| train/episodes                 | 7080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 283200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 177        |
| stats_o/mean                   | 0.2047276  |
| stats_o/std                    | 0.08033835 |
| test/episodes                  | 1780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00213   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2976679 |
| test/Q_plus_P                  | -1.2976679 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 71200      |
| train/episodes                 | 7120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00417   |
| train/info_shaping_reward_mean | -0.0829    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 284800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.20472427 |
| stats_o/std                    | 0.08025451 |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0032    |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.1230966 |
| test/Q_plus_P                  | -1.1230966 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00412   |
| train/info_shaping_reward_mean | -0.0801    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 179        |
| stats_o/mean                   | 0.20473419 |
| stats_o/std                    | 0.08024738 |
| test/episodes                  | 1800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00377   |
| test/info_shaping_reward_mean  | -0.0587    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.6821362 |
| test/Q_plus_P                  | -1.6821362 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 72000      |
| train/episodes                 | 7200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.499      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00503   |
| train/info_shaping_reward_mean | -0.0846    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20        |
| train/steps                    | 288000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 180        |
| stats_o/mean                   | 0.20473795 |
| stats_o/std                    | 0.08012882 |
| test/episodes                  | 1810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00113   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2389338 |
| test/Q_plus_P                  | -1.2389338 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 72400      |
| train/episodes                 | 7240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00322   |
| train/info_shaping_reward_mean | -0.0757    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 289600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 181        |
| stats_o/mean                   | 0.20474249 |
| stats_o/std                    | 0.08002377 |
| test/episodes                  | 1820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0412    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1886551 |
| test/Q_plus_P                  | -1.1886551 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 72800      |
| train/episodes                 | 7280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00339   |
| train/info_shaping_reward_mean | -0.0733    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 291200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 182        |
| stats_o/mean                   | 0.20474668 |
| stats_o/std                    | 0.0799875  |
| test/episodes                  | 1830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00275   |
| test/info_shaping_reward_mean  | -0.0626    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.7341777 |
| test/Q_plus_P                  | -1.7341777 |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 73200      |
| train/episodes                 | 7320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.507      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00436   |
| train/info_shaping_reward_mean | -0.0791    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.7      |
| train/steps                    | 292800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 183         |
| stats_o/mean                   | 0.20475633  |
| stats_o/std                    | 0.079854615 |
| test/episodes                  | 1840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00288    |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.3283266  |
| test/Q_plus_P                  | -1.3283266  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 73600       |
| train/episodes                 | 7360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.471       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00485    |
| train/info_shaping_reward_mean | -0.0862     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.2       |
| train/steps                    | 294400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 184         |
| stats_o/mean                   | 0.20473939  |
| stats_o/std                    | 0.079734586 |
| test/episodes                  | 1850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2245511  |
| test/Q_plus_P                  | -1.2245511  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 74000       |
| train/episodes                 | 7400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.542       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00482    |
| train/info_shaping_reward_mean | -0.0779     |
| train/info_shaping_reward_min  | -0.198      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.3       |
| train/steps                    | 296000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 185        |
| stats_o/mean                   | 0.2047413  |
| stats_o/std                    | 0.07967564 |
| test/episodes                  | 1860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00236   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0908949 |
| test/Q_plus_P                  | -1.0908949 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 74400      |
| train/episodes                 | 7440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00432   |
| train/info_shaping_reward_mean | -0.0772    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 297600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 186         |
| stats_o/mean                   | 0.20472574  |
| stats_o/std                    | 0.079598606 |
| test/episodes                  | 1870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00544    |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1211689  |
| test/Q_plus_P                  | -1.1211689  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 74800       |
| train/episodes                 | 7480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.531       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00406    |
| train/info_shaping_reward_mean | -0.0799     |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.8       |
| train/steps                    | 299200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.20472874 |
| stats_o/std                    | 0.07948086 |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00248   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1319299 |
| test/Q_plus_P                  | -1.1319299 |
| test/reward_per_eps            | -9         |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00436   |
| train/info_shaping_reward_mean | -0.0733    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 188         |
| stats_o/mean                   | 0.20472722  |
| stats_o/std                    | 0.07933248  |
| test/episodes                  | 1890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000806   |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.97255087 |
| test/Q_plus_P                  | -0.97255087 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 75600       |
| train/episodes                 | 7560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00346    |
| train/info_shaping_reward_mean | -0.0673     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 302400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 189        |
| stats_o/mean                   | 0.20472904 |
| stats_o/std                    | 0.07924041 |
| test/episodes                  | 1900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000395  |
| test/info_shaping_reward_mean  | -0.0451    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1732752 |
| test/Q_plus_P                  | -1.1732752 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 76000      |
| train/episodes                 | 7600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00417   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 304000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 190         |
| stats_o/mean                   | 0.20472826  |
| stats_o/std                    | 0.079154946 |
| test/episodes                  | 1910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00183    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1395059  |
| test/Q_plus_P                  | -1.1395059  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 76400       |
| train/episodes                 | 7640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.577       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00443    |
| train/info_shaping_reward_mean | -0.0744     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 305600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 191        |
| stats_o/mean                   | 0.2047299  |
| stats_o/std                    | 0.07901261 |
| test/episodes                  | 1920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00255   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.169994  |
| test/Q_plus_P                  | -1.169994  |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 76800      |
| train/episodes                 | 7680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.538      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00503   |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 307200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.20472565  |
| stats_o/std                    | 0.07886919  |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00283    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.99949455 |
| test/Q_plus_P                  | -0.99949455 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00486    |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 193        |
| stats_o/mean                   | 0.20471315 |
| stats_o/std                    | 0.07877819 |
| test/episodes                  | 1940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00471   |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1171733 |
| test/Q_plus_P                  | -1.1171733 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 77600      |
| train/episodes                 | 7760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00409   |
| train/info_shaping_reward_mean | -0.0744    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 310400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 194        |
| stats_o/mean                   | 0.20472497 |
| stats_o/std                    | 0.07874849 |
| test/episodes                  | 1950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00657   |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.053202  |
| test/Q_plus_P                  | -1.053202  |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 78000      |
| train/episodes                 | 7800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00489   |
| train/info_shaping_reward_mean | -0.0822    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 312000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 195         |
| stats_o/mean                   | 0.20473945  |
| stats_o/std                    | 0.078655966 |
| test/episodes                  | 1960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0045     |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.93579584 |
| test/Q_plus_P                  | -0.93579584 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 78400       |
| train/episodes                 | 7840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.563       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00474    |
| train/info_shaping_reward_mean | -0.074      |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 313600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 196         |
| stats_o/mean                   | 0.20472784  |
| stats_o/std                    | 0.078529075 |
| test/episodes                  | 1970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.667       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.065      |
| test/info_shaping_reward_min   | -0.195      |
| test/Q                         | -2.8816159  |
| test/Q_plus_P                  | -2.8816159  |
| test/reward_per_eps            | -13.3       |
| test/steps                     | 78800       |
| train/episodes                 | 7880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00383    |
| train/info_shaping_reward_mean | -0.0674     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 315200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 197         |
| stats_o/mean                   | 0.20472422  |
| stats_o/std                    | 0.078485884 |
| test/episodes                  | 1980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.675       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00857    |
| test/info_shaping_reward_mean  | -0.0659     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.747895   |
| test/Q_plus_P                  | -1.747895   |
| test/reward_per_eps            | -13         |
| test/steps                     | 79200       |
| train/episodes                 | 7920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.562       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00435    |
| train/info_shaping_reward_mean | -0.0799     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 316800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 198        |
| stats_o/mean                   | 0.2047166  |
| stats_o/std                    | 0.07843519 |
| test/episodes                  | 1990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00428   |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3184209 |
| test/Q_plus_P                  | -1.3184209 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 79600      |
| train/episodes                 | 7960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.513      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00592   |
| train/info_shaping_reward_mean | -0.0816    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.5      |
| train/steps                    | 318400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.20472084 |
| stats_o/std                    | 0.07830476 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000967  |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1334004 |
| test/Q_plus_P                  | -1.1334004 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0039    |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 200        |
| stats_o/mean                   | 0.20471813 |
| stats_o/std                    | 0.07821587 |
| test/episodes                  | 2010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000721  |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1527452 |
| test/Q_plus_P                  | -1.1527452 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 80400      |
| train/episodes                 | 8040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.558      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00518   |
| train/info_shaping_reward_mean | -0.0753    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 321600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 201        |
| stats_o/mean                   | 0.20471513 |
| stats_o/std                    | 0.07817389 |
| test/episodes                  | 2020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000578  |
| test/info_shaping_reward_mean  | -0.0443    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0049819 |
| test/Q_plus_P                  | -1.0049819 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 80800      |
| train/episodes                 | 8080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00348   |
| train/info_shaping_reward_mean | -0.0819    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 323200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 202         |
| stats_o/mean                   | 0.20473234  |
| stats_o/std                    | 0.078064226 |
| test/episodes                  | 2030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00239    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.9755848  |
| test/Q_plus_P                  | -0.9755848  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 81200       |
| train/episodes                 | 8120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00447    |
| train/info_shaping_reward_mean | -0.0704     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 324800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 203        |
| stats_o/mean                   | 0.20472692 |
| stats_o/std                    | 0.07796781 |
| test/episodes                  | 2040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00125   |
| test/info_shaping_reward_mean  | -0.0454    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -1.1624286 |
| test/Q_plus_P                  | -1.1624286 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 81600      |
| train/episodes                 | 8160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0043    |
| train/info_shaping_reward_mean | -0.0686    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 326400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 204        |
| stats_o/mean                   | 0.20473345 |
| stats_o/std                    | 0.07787507 |
| test/episodes                  | 2050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00138   |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2474154 |
| test/Q_plus_P                  | -1.2474154 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 82000      |
| train/episodes                 | 8200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00457   |
| train/info_shaping_reward_mean | -0.0804    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 328000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 205        |
| stats_o/mean                   | 0.20473523 |
| stats_o/std                    | 0.07779253 |
| test/episodes                  | 2060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00145   |
| test/info_shaping_reward_mean  | -0.0437    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0474242 |
| test/Q_plus_P                  | -1.0474242 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 82400      |
| train/episodes                 | 8240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.56       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0045    |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 329600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 206         |
| stats_o/mean                   | 0.20473182  |
| stats_o/std                    | 0.077699035 |
| test/episodes                  | 2070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00241    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.0323049  |
| test/Q_plus_P                  | -1.0323049  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 82800       |
| train/episodes                 | 8280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00496    |
| train/info_shaping_reward_mean | -0.0738     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 331200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.20473193 |
| stats_o/std                    | 0.07771631 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00152   |
| test/info_shaping_reward_mean  | -0.0434    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1067717 |
| test/Q_plus_P                  | -1.1067717 |
| test/reward_per_eps            | -9         |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00451   |
| train/info_shaping_reward_mean | -0.0849    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 208        |
| stats_o/mean                   | 0.20472729 |
| stats_o/std                    | 0.07763751 |
| test/episodes                  | 2090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00199   |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1521343 |
| test/Q_plus_P                  | -1.1521343 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 83600      |
| train/episodes                 | 8360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 334400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 209        |
| stats_o/mean                   | 0.2047508  |
| stats_o/std                    | 0.07764042 |
| test/episodes                  | 2100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000554  |
| test/info_shaping_reward_mean  | -0.0399    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.092136  |
| test/Q_plus_P                  | -1.092136  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 84000      |
| train/episodes                 | 8400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0046    |
| train/info_shaping_reward_mean | -0.0867    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 336000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 210        |
| stats_o/mean                   | 0.20475465 |
| stats_o/std                    | 0.07752368 |
| test/episodes                  | 2110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00214   |
| test/info_shaping_reward_mean  | -0.0451    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1288754 |
| test/Q_plus_P                  | -1.1288754 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 84400      |
| train/episodes                 | 8440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00375   |
| train/info_shaping_reward_mean | -0.066     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 337600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 211         |
| stats_o/mean                   | 0.20473793  |
| stats_o/std                    | 0.077501476 |
| test/episodes                  | 2120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00203    |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.92160416 |
| test/Q_plus_P                  | -0.92160416 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 84800       |
| train/episodes                 | 8480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.548       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00421    |
| train/info_shaping_reward_mean | -0.0783     |
| train/info_shaping_reward_min  | -0.194      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 339200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 212        |
| stats_o/mean                   | 0.20473877 |
| stats_o/std                    | 0.07740583 |
| test/episodes                  | 2130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0013    |
| test/info_shaping_reward_mean  | -0.0454    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1272086 |
| test/Q_plus_P                  | -1.1272086 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 85200      |
| train/episodes                 | 8520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00361   |
| train/info_shaping_reward_mean | -0.0715    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 340800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 213        |
| stats_o/mean                   | 0.20473433 |
| stats_o/std                    | 0.07732339 |
| test/episodes                  | 2140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000745  |
| test/info_shaping_reward_mean  | -0.0402    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0792782 |
| test/Q_plus_P                  | -1.0792782 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 85600      |
| train/episodes                 | 8560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00346   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 342400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 214         |
| stats_o/mean                   | 0.2047233   |
| stats_o/std                    | 0.077235945 |
| test/episodes                  | 2150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1102631  |
| test/Q_plus_P                  | -1.1102631  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 86000       |
| train/episodes                 | 8600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.576       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00366    |
| train/info_shaping_reward_mean | -0.0692     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 344000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 215         |
| stats_o/mean                   | 0.20472017  |
| stats_o/std                    | 0.077124715 |
| test/episodes                  | 2160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00231    |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1733427  |
| test/Q_plus_P                  | -1.1733427  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 86400       |
| train/episodes                 | 8640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.62        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00336    |
| train/info_shaping_reward_mean | -0.0642     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 345600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 216         |
| stats_o/mean                   | 0.20473915  |
| stats_o/std                    | 0.077074036 |
| test/episodes                  | 2170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00257    |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.351363   |
| test/Q_plus_P                  | -1.351363   |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 86800       |
| train/episodes                 | 8680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.502       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00459    |
| train/info_shaping_reward_mean | -0.0916     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.9       |
| train/steps                    | 347200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 217         |
| stats_o/mean                   | 0.20473216  |
| stats_o/std                    | 0.076965764 |
| test/episodes                  | 2180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00067    |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.3161957  |
| test/Q_plus_P                  | -1.3161957  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 87200       |
| train/episodes                 | 8720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.539       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0044     |
| train/info_shaping_reward_mean | -0.0728     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 348800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 218         |
| stats_o/mean                   | 0.20474419  |
| stats_o/std                    | 0.076858275 |
| test/episodes                  | 2190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00136    |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1819825  |
| test/Q_plus_P                  | -1.1819825  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 87600       |
| train/episodes                 | 8760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.574       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00465    |
| train/info_shaping_reward_mean | -0.0703     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 350400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 219        |
| stats_o/mean                   | 0.2047798  |
| stats_o/std                    | 0.07685058 |
| test/episodes                  | 2200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000474  |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2587762 |
| test/Q_plus_P                  | -1.2587762 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 88000      |
| train/episodes                 | 8800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00348   |
| train/info_shaping_reward_mean | -0.0818    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 352000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 220         |
| stats_o/mean                   | 0.20478393  |
| stats_o/std                    | 0.076768816 |
| test/episodes                  | 2210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00169    |
| test/info_shaping_reward_mean  | -0.0434     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0247673  |
| test/Q_plus_P                  | -1.0247673  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 88400       |
| train/episodes                 | 8840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.577       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.0703     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 353600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 221         |
| stats_o/mean                   | 0.20478909  |
| stats_o/std                    | 0.076719634 |
| test/episodes                  | 2220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000963   |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1612236  |
| test/Q_plus_P                  | -1.1612236  |
| test/reward_per_eps            | -9          |
| test/steps                     | 88800       |
| train/episodes                 | 8880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.0687     |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 355200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.20480585 |
| stats_o/std                    | 0.07661998 |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00316   |
| test/info_shaping_reward_mean  | -0.05      |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3145952 |
| test/Q_plus_P                  | -1.3145952 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00399   |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 223        |
| stats_o/mean                   | 0.20480959 |
| stats_o/std                    | 0.07654194 |
| test/episodes                  | 2240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00178   |
| test/info_shaping_reward_mean  | -0.0425    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0727441 |
| test/Q_plus_P                  | -1.0727441 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 89600      |
| train/episodes                 | 8960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00304   |
| train/info_shaping_reward_mean | -0.076     |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 358400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.20482112 |
| stats_o/std                    | 0.0765039  |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1798767 |
| test/Q_plus_P                  | -1.1798767 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00254   |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 225         |
| stats_o/mean                   | 0.20481648  |
| stats_o/std                    | 0.076426595 |
| test/episodes                  | 2260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00172    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.99959344 |
| test/Q_plus_P                  | -0.99959344 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 90400       |
| train/episodes                 | 9040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00383    |
| train/info_shaping_reward_mean | -0.0678     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 361600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 226        |
| stats_o/mean                   | 0.20481764 |
| stats_o/std                    | 0.07633167 |
| test/episodes                  | 2270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000946  |
| test/info_shaping_reward_mean  | -0.0427    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1607739 |
| test/Q_plus_P                  | -1.1607739 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 90800      |
| train/episodes                 | 9080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00412   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 363200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 227         |
| stats_o/mean                   | 0.2048211   |
| stats_o/std                    | 0.076221816 |
| test/episodes                  | 2280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00215    |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.9964608  |
| test/Q_plus_P                  | -0.9964608  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 91200       |
| train/episodes                 | 9120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.067      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 364800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 228        |
| stats_o/mean                   | 0.20482089 |
| stats_o/std                    | 0.07616635 |
| test/episodes                  | 2290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00317   |
| test/info_shaping_reward_mean  | -0.0457    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.0282164 |
| test/Q_plus_P                  | -1.0282164 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 91600      |
| train/episodes                 | 9160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.572      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00567   |
| train/info_shaping_reward_mean | -0.0727    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 366400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 229        |
| stats_o/mean                   | 0.20482618 |
| stats_o/std                    | 0.07610714 |
| test/episodes                  | 2300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00287   |
| test/info_shaping_reward_mean  | -0.0447    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1394775 |
| test/Q_plus_P                  | -1.1394775 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 92000      |
| train/episodes                 | 9200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.585      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00433   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 368000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 230        |
| stats_o/mean                   | 0.20482235 |
| stats_o/std                    | 0.07603214 |
| test/episodes                  | 2310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.0463    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2050761 |
| test/Q_plus_P                  | -1.2050761 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 92400      |
| train/episodes                 | 9240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00314   |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 369600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 231        |
| stats_o/mean                   | 0.20481539 |
| stats_o/std                    | 0.07595746 |
| test/episodes                  | 2320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00106   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2540603 |
| test/Q_plus_P                  | -1.2540603 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 92800      |
| train/episodes                 | 9280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.543      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00475   |
| train/info_shaping_reward_mean | -0.0756    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 371200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 232        |
| stats_o/mean                   | 0.20481172 |
| stats_o/std                    | 0.07586422 |
| test/episodes                  | 2330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00247   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1499923 |
| test/Q_plus_P                  | -1.1499923 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 93200      |
| train/episodes                 | 9320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00312   |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 372800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 233        |
| stats_o/mean                   | 0.20479919 |
| stats_o/std                    | 0.07578339 |
| test/episodes                  | 2340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00205   |
| test/info_shaping_reward_mean  | -0.0451    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.110901  |
| test/Q_plus_P                  | -1.110901  |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 93600      |
| train/episodes                 | 9360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.585      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00372   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 374400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.2047932  |
| stats_o/std                    | 0.07574117 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00146   |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0502877 |
| test/Q_plus_P                  | -1.0502877 |
| test/reward_per_eps            | -9         |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00483   |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 235        |
| stats_o/mean                   | 0.20479685 |
| stats_o/std                    | 0.07572209 |
| test/episodes                  | 2360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00164   |
| test/info_shaping_reward_mean  | -0.0737    |
| test/info_shaping_reward_min   | -0.502     |
| test/Q                         | -3.8297138 |
| test/Q_plus_P                  | -3.8297138 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 94400      |
| train/episodes                 | 9440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.517      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00331   |
| train/info_shaping_reward_mean | -0.0791    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 377600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 236         |
| stats_o/mean                   | 0.20479693  |
| stats_o/std                    | 0.07566728  |
| test/episodes                  | 2370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00613    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.99517393 |
| test/Q_plus_P                  | -0.99517393 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 94800       |
| train/episodes                 | 9480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00377    |
| train/info_shaping_reward_mean | -0.0709     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 379200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 237        |
| stats_o/mean                   | 0.20480727 |
| stats_o/std                    | 0.07567924 |
| test/episodes                  | 2380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0447    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0080972 |
| test/Q_plus_P                  | -1.0080972 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 95200      |
| train/episodes                 | 9520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.522      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00448   |
| train/info_shaping_reward_mean | -0.0814    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 380800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 238         |
| stats_o/mean                   | 0.20481265  |
| stats_o/std                    | 0.075586185 |
| test/episodes                  | 2390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0029     |
| test/info_shaping_reward_mean  | -0.0432     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0880842  |
| test/Q_plus_P                  | -1.0880842  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 95600       |
| train/episodes                 | 9560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00481    |
| train/info_shaping_reward_mean | -0.0711     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 382400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 239        |
| stats_o/mean                   | 0.20480934 |
| stats_o/std                    | 0.07549197 |
| test/episodes                  | 2400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000652  |
| test/info_shaping_reward_mean  | -0.0433    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0600306 |
| test/Q_plus_P                  | -1.0600306 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 96000      |
| train/episodes                 | 9600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00301   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 384000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 240         |
| stats_o/mean                   | 0.20479956  |
| stats_o/std                    | 0.075445235 |
| test/episodes                  | 2410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00277    |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0740007  |
| test/Q_plus_P                  | -1.0740007  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 96400       |
| train/episodes                 | 9640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.563       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00541    |
| train/info_shaping_reward_mean | -0.0747     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 385600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 241        |
| stats_o/mean                   | 0.20479865 |
| stats_o/std                    | 0.07541932 |
| test/episodes                  | 2420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000647  |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.32104   |
| test/Q_plus_P                  | -1.32104   |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 96800      |
| train/episodes                 | 9680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00413   |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 387200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 242        |
| stats_o/mean                   | 0.20479874 |
| stats_o/std                    | 0.07535712 |
| test/episodes                  | 2430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00522   |
| test/info_shaping_reward_mean  | -0.069     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -2.4596276 |
| test/Q_plus_P                  | -2.4596276 |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 97200      |
| train/episodes                 | 9720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00438   |
| train/info_shaping_reward_mean | -0.0724    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 388800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 243        |
| stats_o/mean                   | 0.20479311 |
| stats_o/std                    | 0.07535918 |
| test/episodes                  | 2440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00268   |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4999317 |
| test/Q_plus_P                  | -1.4999317 |
| test/reward_per_eps            | -11        |
| test/steps                     | 97600      |
| train/episodes                 | 9760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.458      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00497   |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.7      |
| train/steps                    | 390400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.20480126 |
| stats_o/std                    | 0.07527756 |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00409   |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4343077 |
| test/Q_plus_P                  | -1.4343077 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.563      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0044    |
| train/info_shaping_reward_mean | -0.0754    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 245        |
| stats_o/mean                   | 0.20479967 |
| stats_o/std                    | 0.07523964 |
| test/episodes                  | 2460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2402766 |
| test/Q_plus_P                  | -1.2402766 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 98400      |
| train/episodes                 | 9840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0031    |
| train/info_shaping_reward_mean | -0.0784    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 393600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 246        |
| stats_o/mean                   | 0.20479448 |
| stats_o/std                    | 0.0752172  |
| test/episodes                  | 2470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1990016 |
| test/Q_plus_P                  | -1.1990016 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 98800      |
| train/episodes                 | 9880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.582      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00435   |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 395200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 247        |
| stats_o/mean                   | 0.20481989 |
| stats_o/std                    | 0.07526711 |
| test/episodes                  | 2480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00142   |
| test/info_shaping_reward_mean  | -0.046     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2465987 |
| test/Q_plus_P                  | -1.2465987 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 99200      |
| train/episodes                 | 9920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.59       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00444   |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 396800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.20481205 |
| stats_o/std                    | 0.07522118 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00277   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9989741 |
| test/Q_plus_P                  | -0.9989741 |
| test/reward_per_eps            | -9         |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00397   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 249        |
| stats_o/mean                   | 0.20478766 |
| stats_o/std                    | 0.07521741 |
| test/episodes                  | 2500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000815  |
| test/info_shaping_reward_mean  | -0.0412    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.9814282 |
| test/Q_plus_P                  | -0.9814282 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 100000     |
| train/episodes                 | 10000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00405   |
| train/info_shaping_reward_mean | -0.0769    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 400000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 250        |
| stats_o/mean                   | 0.20479092 |
| stats_o/std                    | 0.07516259 |
| test/episodes                  | 2510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00435   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1069449 |
| test/Q_plus_P                  | -1.1069449 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 100400     |
| train/episodes                 | 10040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00416   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 401600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 251         |
| stats_o/mean                   | 0.20480737  |
| stats_o/std                    | 0.075091474 |
| test/episodes                  | 2520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00049    |
| test/info_shaping_reward_mean  | -0.0462     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1747284  |
| test/Q_plus_P                  | -1.1747284  |
| test/reward_per_eps            | -9          |
| test/steps                     | 100800      |
| train/episodes                 | 10080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0035     |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 403200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 252        |
| stats_o/mean                   | 0.20479622 |
| stats_o/std                    | 0.07502533 |
| test/episodes                  | 2530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00311   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9338814 |
| test/Q_plus_P                  | -0.9338814 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 101200     |
| train/episodes                 | 10120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00307   |
| train/info_shaping_reward_mean | -0.073     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 404800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 253        |
| stats_o/mean                   | 0.20478441 |
| stats_o/std                    | 0.07500815 |
| test/episodes                  | 2540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.0433    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0963717 |
| test/Q_plus_P                  | -1.0963717 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 101600     |
| train/episodes                 | 10160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00473   |
| train/info_shaping_reward_mean | -0.074     |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 406400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.20477235  |
| stats_o/std                    | 0.07495665  |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00195    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.96305484 |
| test/Q_plus_P                  | -0.96305484 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.602       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00448    |
| train/info_shaping_reward_mean | -0.0742     |
| train/info_shaping_reward_min  | -0.198      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 255        |
| stats_o/mean                   | 0.20477815 |
| stats_o/std                    | 0.07487759 |
| test/episodes                  | 2560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00378   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2141117 |
| test/Q_plus_P                  | -1.2141117 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 102400     |
| train/episodes                 | 10240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.524      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00522   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 409600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 256        |
| stats_o/mean                   | 0.20477958 |
| stats_o/std                    | 0.0748115  |
| test/episodes                  | 2570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000972  |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.5075316 |
| test/Q_plus_P                  | -1.5075316 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 102800     |
| train/episodes                 | 10280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00469   |
| train/info_shaping_reward_mean | -0.0799    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 411200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 257        |
| stats_o/mean                   | 0.20476873 |
| stats_o/std                    | 0.07476189 |
| test/episodes                  | 2580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00132   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4451287 |
| test/Q_plus_P                  | -1.4451287 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 103200     |
| train/episodes                 | 10320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0045    |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 412800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 258         |
| stats_o/mean                   | 0.20479019  |
| stats_o/std                    | 0.074762106 |
| test/episodes                  | 2590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000819   |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.4082333  |
| test/Q_plus_P                  | -1.4082333  |
| test/reward_per_eps            | -10         |
| test/steps                     | 103600      |
| train/episodes                 | 10360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0033     |
| train/info_shaping_reward_mean | -0.0698     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 414400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 259        |
| stats_o/mean                   | 0.2047769  |
| stats_o/std                    | 0.07476779 |
| test/episodes                  | 2600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00266   |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3059797 |
| test/Q_plus_P                  | -1.3059797 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 104000     |
| train/episodes                 | 10400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00468   |
| train/info_shaping_reward_mean | -0.0752    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 416000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 260        |
| stats_o/mean                   | 0.20479159 |
| stats_o/std                    | 0.07474953 |
| test/episodes                  | 2610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0012    |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3561519 |
| test/Q_plus_P                  | -1.3561519 |
| test/reward_per_eps            | -9         |
| test/steps                     | 104400     |
| train/episodes                 | 10440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00281   |
| train/info_shaping_reward_mean | -0.0791    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 417600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 261        |
| stats_o/mean                   | 0.20478618 |
| stats_o/std                    | 0.07467022 |
| test/episodes                  | 2620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.267856  |
| test/Q_plus_P                  | -1.267856  |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 104800     |
| train/episodes                 | 10480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00435   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 419200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 262        |
| stats_o/mean                   | 0.20478365 |
| stats_o/std                    | 0.07458829 |
| test/episodes                  | 2630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00163   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0930201 |
| test/Q_plus_P                  | -1.0930201 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 105200     |
| train/episodes                 | 10520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00499   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 420800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 263        |
| stats_o/mean                   | 0.20477541 |
| stats_o/std                    | 0.07458749 |
| test/episodes                  | 2640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00126   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.141674  |
| test/Q_plus_P                  | -1.141674  |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 105600     |
| train/episodes                 | 10560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.567      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0032    |
| train/info_shaping_reward_mean | -0.0788    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 422400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 264        |
| stats_o/mean                   | 0.20477928 |
| stats_o/std                    | 0.0745015  |
| test/episodes                  | 2650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2150016 |
| test/Q_plus_P                  | -1.2150016 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 106000     |
| train/episodes                 | 10600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00453   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 424000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 265        |
| stats_o/mean                   | 0.20477991 |
| stats_o/std                    | 0.07443792 |
| test/episodes                  | 2660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00344   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.2244865 |
| test/Q_plus_P                  | -1.2244865 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 106400     |
| train/episodes                 | 10640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0693    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 425600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.20477818  |
| stats_o/std                    | 0.074447125 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00469    |
| test/info_shaping_reward_mean  | -0.0615     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.4370139  |
| test/Q_plus_P                  | -1.4370139  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.547       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00502    |
| train/info_shaping_reward_mean | -0.0875     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 267         |
| stats_o/mean                   | 0.2047818   |
| stats_o/std                    | 0.074399926 |
| test/episodes                  | 2680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.132045   |
| test/Q_plus_P                  | -1.132045   |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 107200      |
| train/episodes                 | 10720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.0686     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 428800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.20476502  |
| stats_o/std                    | 0.074340776 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00136    |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2453076  |
| test/Q_plus_P                  | -1.2453076  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00443    |
| train/info_shaping_reward_mean | -0.0729     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 269        |
| stats_o/mean                   | 0.20476907 |
| stats_o/std                    | 0.0742819  |
| test/episodes                  | 2700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000861  |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2068372 |
| test/Q_plus_P                  | -1.2068372 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 108000     |
| train/episodes                 | 10800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00383   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 432000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 270        |
| stats_o/mean                   | 0.20476632 |
| stats_o/std                    | 0.07419463 |
| test/episodes                  | 2710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000947  |
| test/info_shaping_reward_mean  | -0.0415    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0002187 |
| test/Q_plus_P                  | -1.0002187 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 108400     |
| train/episodes                 | 10840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00273   |
| train/info_shaping_reward_mean | -0.0651    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 433600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 271        |
| stats_o/mean                   | 0.20477922 |
| stats_o/std                    | 0.07412489 |
| test/episodes                  | 2720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00233   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2104167 |
| test/Q_plus_P                  | -1.2104167 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 108800     |
| train/episodes                 | 10880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.658      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.0626    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 435200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.20477577  |
| stats_o/std                    | 0.074058086 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00338    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.0982362  |
| test/Q_plus_P                  | -1.0982362  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00553    |
| train/info_shaping_reward_mean | -0.0684     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 273         |
| stats_o/mean                   | 0.20479074  |
| stats_o/std                    | 0.074101076 |
| test/episodes                  | 2740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00181    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -1.2812883  |
| test/Q_plus_P                  | -1.2812883  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 109600      |
| train/episodes                 | 10960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.532       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00444    |
| train/info_shaping_reward_mean | -0.09       |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 438400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 274        |
| stats_o/mean                   | 0.20479067 |
| stats_o/std                    | 0.07400762 |
| test/episodes                  | 2750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1921726 |
| test/Q_plus_P                  | -1.1921726 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 110000     |
| train/episodes                 | 11000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00365   |
| train/info_shaping_reward_mean | -0.0674    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 440000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 275         |
| stats_o/mean                   | 0.20478773  |
| stats_o/std                    | 0.073961526 |
| test/episodes                  | 2760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0025     |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.191      |
| test/Q                         | -1.0214913  |
| test/Q_plus_P                  | -1.0214913  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 110400      |
| train/episodes                 | 11040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.567       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00335    |
| train/info_shaping_reward_mean | -0.0746     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 441600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 276         |
| stats_o/mean                   | 0.20479085  |
| stats_o/std                    | 0.073911935 |
| test/episodes                  | 2770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00178    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.311633   |
| test/Q_plus_P                  | -1.311633   |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 110800      |
| train/episodes                 | 11080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.004      |
| train/info_shaping_reward_mean | -0.0714     |
| train/info_shaping_reward_min  | -0.197      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 443200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 277         |
| stats_o/mean                   | 0.20479633  |
| stats_o/std                    | 0.073866636 |
| test/episodes                  | 2780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00539    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.10321    |
| test/Q_plus_P                  | -1.10321    |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 111200      |
| train/episodes                 | 11120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.067      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 444800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 278        |
| stats_o/mean                   | 0.20479159 |
| stats_o/std                    | 0.07386675 |
| test/episodes                  | 2790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0009    |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9683422 |
| test/Q_plus_P                  | -0.9683422 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 111600     |
| train/episodes                 | 11160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00377   |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 446400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 279        |
| stats_o/mean                   | 0.20479463 |
| stats_o/std                    | 0.07381478 |
| test/episodes                  | 2800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00204   |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9120205 |
| test/Q_plus_P                  | -0.9120205 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 112000     |
| train/episodes                 | 11200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00466   |
| train/info_shaping_reward_mean | -0.0713    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 448000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 280        |
| stats_o/mean                   | 0.20479919 |
| stats_o/std                    | 0.07374789 |
| test/episodes                  | 2810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00253   |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.4194478 |
| test/Q_plus_P                  | -1.4194478 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 112400     |
| train/episodes                 | 11240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00474   |
| train/info_shaping_reward_mean | -0.0747    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 449600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.20478788 |
| stats_o/std                    | 0.07369117 |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00596   |
| test/info_shaping_reward_mean  | -0.0458    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9448503 |
| test/Q_plus_P                  | -0.9448503 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00601   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.20478968 |
| stats_o/std                    | 0.07360822 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00125   |
| test/info_shaping_reward_mean  | -0.0439    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1350715 |
| test/Q_plus_P                  | -1.1350715 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00474   |
| train/info_shaping_reward_mean | -0.0686    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 283         |
| stats_o/mean                   | 0.20479637  |
| stats_o/std                    | 0.073565096 |
| test/episodes                  | 2840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00226    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1865265  |
| test/Q_plus_P                  | -1.1865265  |
| test/reward_per_eps            | -9          |
| test/steps                     | 113600      |
| train/episodes                 | 11360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00355    |
| train/info_shaping_reward_mean | -0.0731     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 454400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 284        |
| stats_o/mean                   | 0.20479739 |
| stats_o/std                    | 0.07357051 |
| test/episodes                  | 2850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00381   |
| test/info_shaping_reward_mean  | -0.0467    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0510375 |
| test/Q_plus_P                  | -1.0510375 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 114000     |
| train/episodes                 | 11400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00473   |
| train/info_shaping_reward_mean | -0.0854    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 456000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 285         |
| stats_o/mean                   | 0.20479044  |
| stats_o/std                    | 0.0735257   |
| test/episodes                  | 2860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00274    |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.90729505 |
| test/Q_plus_P                  | -0.90729505 |
| test/reward_per_eps            | -8          |
| test/steps                     | 114400      |
| train/episodes                 | 11440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00361    |
| train/info_shaping_reward_mean | -0.07       |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 457600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 286         |
| stats_o/mean                   | 0.20478617  |
| stats_o/std                    | 0.073500745 |
| test/episodes                  | 2870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00883    |
| test/info_shaping_reward_mean  | -0.0551     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.072063   |
| test/Q_plus_P                  | -1.072063   |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 114800      |
| train/episodes                 | 11480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.57        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.072      |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 459200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 287         |
| stats_o/mean                   | 0.20480825  |
| stats_o/std                    | 0.073528886 |
| test/episodes                  | 2880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00283    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0259575  |
| test/Q_plus_P                  | -1.0259575  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 115200      |
| train/episodes                 | 11520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00428    |
| train/info_shaping_reward_mean | -0.0732     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 460800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 288        |
| stats_o/mean                   | 0.20482099 |
| stats_o/std                    | 0.07355004 |
| test/episodes                  | 2890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00342   |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1294626 |
| test/Q_plus_P                  | -1.1294626 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 115600     |
| train/episodes                 | 11560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.081     |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 462400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 289        |
| stats_o/mean                   | 0.2048211  |
| stats_o/std                    | 0.07346655 |
| test/episodes                  | 2900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00625   |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2095698 |
| test/Q_plus_P                  | -1.2095698 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 116000     |
| train/episodes                 | 11600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 464000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 290         |
| stats_o/mean                   | 0.20482628  |
| stats_o/std                    | 0.073420316 |
| test/episodes                  | 2910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.012      |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0514791  |
| test/Q_plus_P                  | -1.0514791  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 116400      |
| train/episodes                 | 11640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00547    |
| train/info_shaping_reward_mean | -0.0701     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 465600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 291        |
| stats_o/mean                   | 0.20482941 |
| stats_o/std                    | 0.07335568 |
| test/episodes                  | 2920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00218   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.1517907 |
| test/Q_plus_P                  | -1.1517907 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 116800     |
| train/episodes                 | 11680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00367   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 467200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 292         |
| stats_o/mean                   | 0.204835    |
| stats_o/std                    | 0.0733129   |
| test/episodes                  | 2930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00636    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.95377785 |
| test/Q_plus_P                  | -0.95377785 |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 117200      |
| train/episodes                 | 11720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.0791     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 468800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 293        |
| stats_o/mean                   | 0.20483993 |
| stats_o/std                    | 0.07329462 |
| test/episodes                  | 2940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00482   |
| test/info_shaping_reward_mean  | -0.0454    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0810249 |
| test/Q_plus_P                  | -1.0810249 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 117600     |
| train/episodes                 | 11760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00429   |
| train/info_shaping_reward_mean | -0.076     |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 470400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.2048419   |
| stats_o/std                    | 0.07329056  |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0023     |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.96664107 |
| test/Q_plus_P                  | -0.96664107 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00416    |
| train/info_shaping_reward_mean | -0.0786     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 295         |
| stats_o/mean                   | 0.20484436  |
| stats_o/std                    | 0.073305465 |
| test/episodes                  | 2960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00251    |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.0526137  |
| test/Q_plus_P                  | -1.0526137  |
| test/reward_per_eps            | -9          |
| test/steps                     | 118400      |
| train/episodes                 | 11840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.578       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00405    |
| train/info_shaping_reward_mean | -0.0762     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 473600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 296        |
| stats_o/mean                   | 0.2048594  |
| stats_o/std                    | 0.07327244 |
| test/episodes                  | 2970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00263   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0536201 |
| test/Q_plus_P                  | -1.0536201 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 118800     |
| train/episodes                 | 11880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00596   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 475200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 297        |
| stats_o/mean                   | 0.20485988 |
| stats_o/std                    | 0.0731939  |
| test/episodes                  | 2980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00257   |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0758148 |
| test/Q_plus_P                  | -1.0758148 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 119200     |
| train/episodes                 | 11920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00352   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 476800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 298        |
| stats_o/mean                   | 0.20485768 |
| stats_o/std                    | 0.07315914 |
| test/episodes                  | 2990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0031    |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1276284 |
| test/Q_plus_P                  | -1.1276284 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 119600     |
| train/episodes                 | 11960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.563      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00458   |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 478400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 299        |
| stats_o/mean                   | 0.20485027 |
| stats_o/std                    | 0.0731908  |
| test/episodes                  | 3000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00328   |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0870333 |
| test/Q_plus_P                  | -1.0870333 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 120000     |
| train/episodes                 | 12000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00375   |
| train/info_shaping_reward_mean | -0.0791    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 480000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 300        |
| stats_o/mean                   | 0.20485468 |
| stats_o/std                    | 0.07315836 |
| test/episodes                  | 3010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00254   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0182576 |
| test/Q_plus_P                  | -1.0182576 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 120400     |
| train/episodes                 | 12040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00346   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 481600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.2048635  |
| stats_o/std                    | 0.0731404  |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00211   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0026846 |
| test/Q_plus_P                  | -1.0026846 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00374   |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 302         |
| stats_o/mean                   | 0.20486389  |
| stats_o/std                    | 0.073077306 |
| test/episodes                  | 3030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00207    |
| test/info_shaping_reward_mean  | -0.0438     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.0251833  |
| test/Q_plus_P                  | -1.0251833  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 121200      |
| train/episodes                 | 12120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00442    |
| train/info_shaping_reward_mean | -0.0761     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 484800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 303         |
| stats_o/mean                   | 0.20485838  |
| stats_o/std                    | 0.073002756 |
| test/episodes                  | 3040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00268    |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -0.9761311  |
| test/Q_plus_P                  | -0.9761311  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 121600      |
| train/episodes                 | 12160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0033     |
| train/info_shaping_reward_mean | -0.067      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 486400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 304        |
| stats_o/mean                   | 0.20485449 |
| stats_o/std                    | 0.07293786 |
| test/episodes                  | 3050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00134   |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2452202 |
| test/Q_plus_P                  | -1.2452202 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 122000     |
| train/episodes                 | 12200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00425   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 488000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 305         |
| stats_o/mean                   | 0.20484364  |
| stats_o/std                    | 0.072891235 |
| test/episodes                  | 3060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0047     |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0648707  |
| test/Q_plus_P                  | -1.0648707  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 122400      |
| train/episodes                 | 12240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.615       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00504    |
| train/info_shaping_reward_mean | -0.0676     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 489600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 306        |
| stats_o/mean                   | 0.20483358 |
| stats_o/std                    | 0.07286848 |
| test/episodes                  | 3070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00388   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0943776 |
| test/Q_plus_P                  | -1.0943776 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 122800     |
| train/episodes                 | 12280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00411   |
| train/info_shaping_reward_mean | -0.0746    |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 491200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 307         |
| stats_o/mean                   | 0.20483296  |
| stats_o/std                    | 0.07279292  |
| test/episodes                  | 3080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0116     |
| test/info_shaping_reward_mean  | -0.0488     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.95877874 |
| test/Q_plus_P                  | -0.95877874 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 123200      |
| train/episodes                 | 12320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 492800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 308        |
| stats_o/mean                   | 0.20484394 |
| stats_o/std                    | 0.07277744 |
| test/episodes                  | 3090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000429  |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -1.0992695 |
| test/Q_plus_P                  | -1.0992695 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 123600     |
| train/episodes                 | 12360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00379   |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 494400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 309        |
| stats_o/mean                   | 0.20483921 |
| stats_o/std                    | 0.07272139 |
| test/episodes                  | 3100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00353   |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.9077254 |
| test/Q_plus_P                  | -0.9077254 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 124000     |
| train/episodes                 | 12400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00536   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 496000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 310        |
| stats_o/mean                   | 0.20484157 |
| stats_o/std                    | 0.07266623 |
| test/episodes                  | 3110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00574   |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0323502 |
| test/Q_plus_P                  | -1.0323502 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 124400     |
| train/episodes                 | 12440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0047    |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 497600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 311         |
| stats_o/mean                   | 0.20485514  |
| stats_o/std                    | 0.072661705 |
| test/episodes                  | 3120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00323    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.1358898  |
| test/Q_plus_P                  | -1.1358898  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 124800      |
| train/episodes                 | 12480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.0805     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 499200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 312        |
| stats_o/mean                   | 0.20486076 |
| stats_o/std                    | 0.07263416 |
| test/episodes                  | 3130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00166   |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2326217 |
| test/Q_plus_P                  | -1.2326217 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 125200     |
| train/episodes                 | 12520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.526      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.0835    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 500800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 313        |
| stats_o/mean                   | 0.20486599 |
| stats_o/std                    | 0.07260869 |
| test/episodes                  | 3140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00447   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0524821 |
| test/Q_plus_P                  | -1.0524821 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 125600     |
| train/episodes                 | 12560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.517      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00433   |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 502400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 314         |
| stats_o/mean                   | 0.20486563  |
| stats_o/std                    | 0.072603025 |
| test/episodes                  | 3150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0129     |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.0786179  |
| test/Q_plus_P                  | -1.0786179  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 126000      |
| train/episodes                 | 12600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00451    |
| train/info_shaping_reward_mean | -0.0783     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 504000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 315        |
| stats_o/mean                   | 0.20487553 |
| stats_o/std                    | 0.07255517 |
| test/episodes                  | 3160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0038    |
| test/info_shaping_reward_mean  | -0.0565    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1529995 |
| test/Q_plus_P                  | -1.1529995 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 126400     |
| train/episodes                 | 12640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00272   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 505600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.20487866  |
| stats_o/std                    | 0.072548024 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00434    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.0073254  |
| test/Q_plus_P                  | -1.0073254  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 317         |
| stats_o/mean                   | 0.20488162  |
| stats_o/std                    | 0.07250347  |
| test/episodes                  | 3180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00444    |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -0.98978734 |
| test/Q_plus_P                  | -0.98978734 |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 127200      |
| train/episodes                 | 12720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0694     |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 508800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 318         |
| stats_o/mean                   | 0.20486857  |
| stats_o/std                    | 0.072455905 |
| test/episodes                  | 3190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00555    |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.124936   |
| test/Q_plus_P                  | -1.124936   |
| test/reward_per_eps            | -9          |
| test/steps                     | 127600      |
| train/episodes                 | 12760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00563    |
| train/info_shaping_reward_mean | -0.0692     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 510400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 319        |
| stats_o/mean                   | 0.20486996 |
| stats_o/std                    | 0.07244108 |
| test/episodes                  | 3200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0021    |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1983399 |
| test/Q_plus_P                  | -1.1983399 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 128000     |
| train/episodes                 | 12800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.08      |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 512000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 320         |
| stats_o/mean                   | 0.20487736  |
| stats_o/std                    | 0.072389066 |
| test/episodes                  | 3210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00243    |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1242511  |
| test/Q_plus_P                  | -1.1242511  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 128400      |
| train/episodes                 | 12840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00503    |
| train/info_shaping_reward_mean | -0.0691     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 513600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 321        |
| stats_o/mean                   | 0.20487812 |
| stats_o/std                    | 0.07232391 |
| test/episodes                  | 3220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00448   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1215348 |
| test/Q_plus_P                  | -1.1215348 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 128800     |
| train/episodes                 | 12880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00431   |
| train/info_shaping_reward_mean | -0.0653    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 515200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 322        |
| stats_o/mean                   | 0.20487991 |
| stats_o/std                    | 0.07228048 |
| test/episodes                  | 3230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0023    |
| test/info_shaping_reward_mean  | -0.0467    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.1212704 |
| test/Q_plus_P                  | -1.1212704 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 129200     |
| train/episodes                 | 12920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00403   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 516800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 323         |
| stats_o/mean                   | 0.20488721  |
| stats_o/std                    | 0.072255544 |
| test/episodes                  | 3240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00266    |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.378675   |
| test/Q_plus_P                  | -1.378675   |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 129600      |
| train/episodes                 | 12960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.549       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00423    |
| train/info_shaping_reward_mean | -0.0748     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 518400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 324        |
| stats_o/mean                   | 0.20488322 |
| stats_o/std                    | 0.07218615 |
| test/episodes                  | 3250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0018    |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3589349 |
| test/Q_plus_P                  | -1.3589349 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 130000     |
| train/episodes                 | 13000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00441   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 520000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 325         |
| stats_o/mean                   | 0.2048897   |
| stats_o/std                    | 0.072170846 |
| test/episodes                  | 3260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00187    |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.19       |
| test/Q                         | -1.3858713  |
| test/Q_plus_P                  | -1.3858713  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 130400      |
| train/episodes                 | 13040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00432    |
| train/info_shaping_reward_mean | -0.0693     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 521600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 326        |
| stats_o/mean                   | 0.20487814 |
| stats_o/std                    | 0.07217114 |
| test/episodes                  | 3270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2082373 |
| test/Q_plus_P                  | -1.2082373 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 130800     |
| train/episodes                 | 13080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.522      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00371   |
| train/info_shaping_reward_mean | -0.0817    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 523200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 327         |
| stats_o/mean                   | 0.20488162  |
| stats_o/std                    | 0.072127126 |
| test/episodes                  | 3280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00429    |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -1.2318836  |
| test/Q_plus_P                  | -1.2318836  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 131200      |
| train/episodes                 | 13120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0048     |
| train/info_shaping_reward_mean | -0.067      |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 524800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 328        |
| stats_o/mean                   | 0.20487978 |
| stats_o/std                    | 0.07215731 |
| test/episodes                  | 3290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00395   |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1580772 |
| test/Q_plus_P                  | -1.1580772 |
| test/reward_per_eps            | -9         |
| test/steps                     | 131600     |
| train/episodes                 | 13160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00463   |
| train/info_shaping_reward_mean | -0.0842    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 526400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 329         |
| stats_o/mean                   | 0.2048709   |
| stats_o/std                    | 0.072104275 |
| test/episodes                  | 3300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.088873   |
| test/Q_plus_P                  | -1.088873   |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 132000      |
| train/episodes                 | 13200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00472    |
| train/info_shaping_reward_mean | -0.0716     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 528000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.20487458  |
| stats_o/std                    | 0.072063014 |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00217    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1038806  |
| test/Q_plus_P                  | -1.1038806  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00435    |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 331         |
| stats_o/mean                   | 0.20488104  |
| stats_o/std                    | 0.072001874 |
| test/episodes                  | 3320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00423    |
| test/info_shaping_reward_mean  | -0.0561     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.081115   |
| test/Q_plus_P                  | -1.081115   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 132800      |
| train/episodes                 | 13280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00256    |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 531200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 332        |
| stats_o/mean                   | 0.20487848 |
| stats_o/std                    | 0.07195384 |
| test/episodes                  | 3330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9325669 |
| test/Q_plus_P                  | -0.9325669 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 133200     |
| train/episodes                 | 13320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 532800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 333        |
| stats_o/mean                   | 0.20488024 |
| stats_o/std                    | 0.07191796 |
| test/episodes                  | 3340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00462   |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0436773 |
| test/Q_plus_P                  | -1.0436773 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 133600     |
| train/episodes                 | 13360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00363   |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 534400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 334         |
| stats_o/mean                   | 0.20486455  |
| stats_o/std                    | 0.071910456 |
| test/episodes                  | 3350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00222    |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1830026  |
| test/Q_plus_P                  | -1.1830026  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 134000      |
| train/episodes                 | 13400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00426    |
| train/info_shaping_reward_mean | -0.0701     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 536000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.20486683 |
| stats_o/std                    | 0.07191013 |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00271   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0467293 |
| test/Q_plus_P                  | -1.0467293 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00493   |
| train/info_shaping_reward_mean | -0.075     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 336        |
| stats_o/mean                   | 0.20486832 |
| stats_o/std                    | 0.07185505 |
| test/episodes                  | 3370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00327   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.139278  |
| test/Q_plus_P                  | -1.139278  |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 134800     |
| train/episodes                 | 13480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00376   |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 539200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 337        |
| stats_o/mean                   | 0.20487127 |
| stats_o/std                    | 0.07181649 |
| test/episodes                  | 3380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00551   |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0245949 |
| test/Q_plus_P                  | -1.0245949 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 135200     |
| train/episodes                 | 13520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0047    |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 540800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.20486987 |
| stats_o/std                    | 0.07175424 |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00139   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.912984  |
| test/Q_plus_P                  | -0.912984  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.664      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00407   |
| train/info_shaping_reward_mean | -0.0626    |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 339        |
| stats_o/mean                   | 0.2048694  |
| stats_o/std                    | 0.0716928  |
| test/episodes                  | 3400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.0423    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9444545 |
| test/Q_plus_P                  | -0.9444545 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 136000     |
| train/episodes                 | 13600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0048    |
| train/info_shaping_reward_mean | -0.0662    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 544000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 340        |
| stats_o/mean                   | 0.20487466 |
| stats_o/std                    | 0.07167098 |
| test/episodes                  | 3410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.637      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00303   |
| test/info_shaping_reward_mean  | -0.0626    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.9082037 |
| test/Q_plus_P                  | -1.9082037 |
| test/reward_per_eps            | -14.5      |
| test/steps                     | 136400     |
| train/episodes                 | 13640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00498   |
| train/info_shaping_reward_mean | -0.0787    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 545600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 341        |
| stats_o/mean                   | 0.20486285 |
| stats_o/std                    | 0.07163592 |
| test/episodes                  | 3420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00113   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.317556  |
| test/Q_plus_P                  | -1.317556  |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 136800     |
| train/episodes                 | 13680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0044    |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 547200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 342        |
| stats_o/mean                   | 0.2048678  |
| stats_o/std                    | 0.07165921 |
| test/episodes                  | 3430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00158   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2247975 |
| test/Q_plus_P                  | -1.2247975 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 137200     |
| train/episodes                 | 13720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.547      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00366   |
| train/info_shaping_reward_mean | -0.0811    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 548800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 343        |
| stats_o/mean                   | 0.20489827 |
| stats_o/std                    | 0.07179274 |
| test/episodes                  | 3440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0057    |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.182015  |
| test/Q_plus_P                  | -1.182015  |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 137600     |
| train/episodes                 | 13760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.533      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0034    |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 550400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 344        |
| stats_o/mean                   | 0.20490263 |
| stats_o/std                    | 0.07183    |
| test/episodes                  | 3450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00173   |
| test/info_shaping_reward_mean  | -0.0439    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.1319921 |
| test/Q_plus_P                  | -1.1319921 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 138000     |
| train/episodes                 | 13800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.59       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00363   |
| train/info_shaping_reward_mean | -0.0869    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 552000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.2049021   |
| stats_o/std                    | 0.071768016 |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00193    |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0345961  |
| test/Q_plus_P                  | -1.0345961  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.0664     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 346        |
| stats_o/mean                   | 0.20490533 |
| stats_o/std                    | 0.07172832 |
| test/episodes                  | 3470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0428    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0212494 |
| test/Q_plus_P                  | -1.0212494 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 138800     |
| train/episodes                 | 13880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00444   |
| train/info_shaping_reward_mean | -0.0744    |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 555200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 347        |
| stats_o/mean                   | 0.2049078  |
| stats_o/std                    | 0.07170921 |
| test/episodes                  | 3480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00112   |
| test/info_shaping_reward_mean  | -0.0392    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0975657 |
| test/Q_plus_P                  | -1.0975657 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 139200     |
| train/episodes                 | 13920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00269   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 556800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.20490415  |
| stats_o/std                    | 0.071656406 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00188    |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.95887077 |
| test/Q_plus_P                  | -0.95887077 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0683     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 349        |
| stats_o/mean                   | 0.20490593 |
| stats_o/std                    | 0.0716013  |
| test/episodes                  | 3500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00345   |
| test/info_shaping_reward_mean  | -0.0423    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0840952 |
| test/Q_plus_P                  | -1.0840952 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 140000     |
| train/episodes                 | 14000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00384   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 560000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 350        |
| stats_o/mean                   | 0.20490104 |
| stats_o/std                    | 0.0715628  |
| test/episodes                  | 3510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00377   |
| test/info_shaping_reward_mean  | -0.0415    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.117256  |
| test/Q_plus_P                  | -1.117256  |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 140400     |
| train/episodes                 | 14040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00413   |
| train/info_shaping_reward_mean | -0.0764    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 561600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 351        |
| stats_o/mean                   | 0.20490223 |
| stats_o/std                    | 0.0715172  |
| test/episodes                  | 3520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00372   |
| test/info_shaping_reward_mean  | -0.0441    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0178772 |
| test/Q_plus_P                  | -1.0178772 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 140800     |
| train/episodes                 | 14080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00367   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 563200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 352         |
| stats_o/mean                   | 0.20490257  |
| stats_o/std                    | 0.071451865 |
| test/episodes                  | 3530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000626   |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0157002  |
| test/Q_plus_P                  | -1.0157002  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 141200      |
| train/episodes                 | 14120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00442    |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 564800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 353        |
| stats_o/mean                   | 0.20490666 |
| stats_o/std                    | 0.07141201 |
| test/episodes                  | 3540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00193   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9438079 |
| test/Q_plus_P                  | -0.9438079 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 141600     |
| train/episodes                 | 14160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0034    |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 566400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 354         |
| stats_o/mean                   | 0.20491138  |
| stats_o/std                    | 0.07135784  |
| test/episodes                  | 3550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00575    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.81839335 |
| test/Q_plus_P                  | -0.81839335 |
| test/reward_per_eps            | -8          |
| test/steps                     | 142000      |
| train/episodes                 | 14200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 568000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 355        |
| stats_o/mean                   | 0.20490457 |
| stats_o/std                    | 0.07129341 |
| test/episodes                  | 3560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000508  |
| test/info_shaping_reward_mean  | -0.0463    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0111248 |
| test/Q_plus_P                  | -1.0111248 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 142400     |
| train/episodes                 | 14240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00409   |
| train/info_shaping_reward_mean | -0.063     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 569600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 356         |
| stats_o/mean                   | 0.20489278  |
| stats_o/std                    | 0.071266696 |
| test/episodes                  | 3570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00308    |
| test/info_shaping_reward_mean  | -0.0432     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0275428  |
| test/Q_plus_P                  | -1.0275428  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 142800      |
| train/episodes                 | 14280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.6         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00473    |
| train/info_shaping_reward_mean | -0.069      |
| train/info_shaping_reward_min  | -0.201      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 571200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 357        |
| stats_o/mean                   | 0.20488673 |
| stats_o/std                    | 0.07126855 |
| test/episodes                  | 3580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0015    |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9297674 |
| test/Q_plus_P                  | -0.9297674 |
| test/reward_per_eps            | -8         |
| test/steps                     | 143200     |
| train/episodes                 | 14320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.563      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0038    |
| train/info_shaping_reward_mean | -0.0727    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 572800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 358        |
| stats_o/mean                   | 0.20488295 |
| stats_o/std                    | 0.07126964 |
| test/episodes                  | 3590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00278   |
| test/info_shaping_reward_mean  | -0.0429    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1536099 |
| test/Q_plus_P                  | -1.1536099 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 143600     |
| train/episodes                 | 14360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00401   |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 574400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 359        |
| stats_o/mean                   | 0.20488529 |
| stats_o/std                    | 0.07125084 |
| test/episodes                  | 3600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00333   |
| test/info_shaping_reward_mean  | -0.0406    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.9654408 |
| test/Q_plus_P                  | -0.9654408 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 144000     |
| train/episodes                 | 14400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00383   |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 576000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 360        |
| stats_o/mean                   | 0.20487651 |
| stats_o/std                    | 0.07125106 |
| test/episodes                  | 3610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000573  |
| test/info_shaping_reward_mean  | -0.0405    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9372281 |
| test/Q_plus_P                  | -0.9372281 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 144400     |
| train/episodes                 | 14440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00403   |
| train/info_shaping_reward_mean | -0.0681    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 577600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 361        |
| stats_o/mean                   | 0.20487347 |
| stats_o/std                    | 0.07122853 |
| test/episodes                  | 3620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0025    |
| test/info_shaping_reward_mean  | -0.0426    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.0805542 |
| test/Q_plus_P                  | -1.0805542 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 144800     |
| train/episodes                 | 14480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.645      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00259   |
| train/info_shaping_reward_mean | -0.0717    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 579200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 362         |
| stats_o/mean                   | 0.20488833  |
| stats_o/std                    | 0.07123325  |
| test/episodes                  | 3630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00368    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.76158255 |
| test/Q_plus_P                  | -0.76158255 |
| test/reward_per_eps            | -8          |
| test/steps                     | 145200      |
| train/episodes                 | 14520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.583       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00497    |
| train/info_shaping_reward_mean | -0.0822     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 580800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 363        |
| stats_o/mean                   | 0.20489416 |
| stats_o/std                    | 0.07124444 |
| test/episodes                  | 3640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0428    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9840041 |
| test/Q_plus_P                  | -0.9840041 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 145600     |
| train/episodes                 | 14560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00432   |
| train/info_shaping_reward_mean | -0.0789    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 582400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 364        |
| stats_o/mean                   | 0.20490162 |
| stats_o/std                    | 0.07119402 |
| test/episodes                  | 3650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00245   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2710428 |
| test/Q_plus_P                  | -1.2710428 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 146000     |
| train/episodes                 | 14600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00383   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 584000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.20489776  |
| stats_o/std                    | 0.071222074 |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00128    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.4007128  |
| test/Q_plus_P                  | -1.4007128  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.548       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00351    |
| train/info_shaping_reward_mean | -0.0899     |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 366        |
| stats_o/mean                   | 0.20489775 |
| stats_o/std                    | 0.07118104 |
| test/episodes                  | 3670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00222   |
| test/info_shaping_reward_mean  | -0.0418    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.036703  |
| test/Q_plus_P                  | -1.036703  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 146800     |
| train/episodes                 | 14680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.558      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00439   |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 587200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.20489551  |
| stats_o/std                    | 0.07113102  |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00322    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.95221674 |
| test/Q_plus_P                  | -0.95221674 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00422    |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 368         |
| stats_o/mean                   | 0.20489354  |
| stats_o/std                    | 0.071068466 |
| test/episodes                  | 3690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000998   |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.9794387  |
| test/Q_plus_P                  | -0.9794387  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 147600      |
| train/episodes                 | 14760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00404    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 590400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 369        |
| stats_o/mean                   | 0.20488757 |
| stats_o/std                    | 0.0710764  |
| test/episodes                  | 3700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00194   |
| test/info_shaping_reward_mean  | -0.0403    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.007024  |
| test/Q_plus_P                  | -1.007024  |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 148000     |
| train/episodes                 | 14800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00454   |
| train/info_shaping_reward_mean | -0.073     |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 592000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 370        |
| stats_o/mean                   | 0.20488887 |
| stats_o/std                    | 0.07104471 |
| test/episodes                  | 3710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00426   |
| test/info_shaping_reward_mean  | -0.0416    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9895599 |
| test/Q_plus_P                  | -0.9895599 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 148400     |
| train/episodes                 | 14840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00374   |
| train/info_shaping_reward_mean | -0.0688    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 593600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 371        |
| stats_o/mean                   | 0.2048895  |
| stats_o/std                    | 0.07102562 |
| test/episodes                  | 3720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00108   |
| test/info_shaping_reward_mean  | -0.0427    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0427296 |
| test/Q_plus_P                  | -1.0427296 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 148800     |
| train/episodes                 | 14880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00463   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 595200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 372        |
| stats_o/mean                   | 0.20489992 |
| stats_o/std                    | 0.07100766 |
| test/episodes                  | 3730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00209   |
| test/info_shaping_reward_mean  | -0.0434    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0580393 |
| test/Q_plus_P                  | -1.0580393 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 149200     |
| train/episodes                 | 14920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00381   |
| train/info_shaping_reward_mean | -0.0634    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 596800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 373        |
| stats_o/mean                   | 0.20489882 |
| stats_o/std                    | 0.07095079 |
| test/episodes                  | 3740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00356   |
| test/info_shaping_reward_mean  | -0.0416    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0033569 |
| test/Q_plus_P                  | -1.0033569 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 149600     |
| train/episodes                 | 14960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 598400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 374         |
| stats_o/mean                   | 0.20490348  |
| stats_o/std                    | 0.070914365 |
| test/episodes                  | 3750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000984   |
| test/info_shaping_reward_mean  | -0.0445     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.98477834 |
| test/Q_plus_P                  | -0.98477834 |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 150000      |
| train/episodes                 | 15000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.066      |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 600000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 375        |
| stats_o/mean                   | 0.2049023  |
| stats_o/std                    | 0.07086999 |
| test/episodes                  | 3760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00241   |
| test/info_shaping_reward_mean  | -0.0426    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9625309 |
| test/Q_plus_P                  | -0.9625309 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 150400     |
| train/episodes                 | 15040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00353   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 601600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.20490246 |
| stats_o/std                    | 0.07083793 |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00563   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2188177 |
| test/Q_plus_P                  | -1.2188177 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00453   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 377        |
| stats_o/mean                   | 0.20491308 |
| stats_o/std                    | 0.0708336  |
| test/episodes                  | 3780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0049    |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1424136 |
| test/Q_plus_P                  | -1.1424136 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 151200     |
| train/episodes                 | 15120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00653   |
| train/info_shaping_reward_mean | -0.0815    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 604800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 378        |
| stats_o/mean                   | 0.20490713 |
| stats_o/std                    | 0.07078794 |
| test/episodes                  | 3790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000666  |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.0450226 |
| test/Q_plus_P                  | -1.0450226 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 151600     |
| train/episodes                 | 15160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00492   |
| train/info_shaping_reward_mean | -0.0713    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 606400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 379        |
| stats_o/mean                   | 0.20490144 |
| stats_o/std                    | 0.07074516 |
| test/episodes                  | 3800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0014    |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1333377 |
| test/Q_plus_P                  | -1.1333377 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 152000     |
| train/episodes                 | 15200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00481   |
| train/info_shaping_reward_mean | -0.0667    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 608000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 380        |
| stats_o/mean                   | 0.20490348 |
| stats_o/std                    | 0.07067879 |
| test/episodes                  | 3810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00477   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2556471 |
| test/Q_plus_P                  | -1.2556471 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 152400     |
| train/episodes                 | 15240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00353   |
| train/info_shaping_reward_mean | -0.0632    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 609600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 381        |
| stats_o/mean                   | 0.20490925 |
| stats_o/std                    | 0.07064096 |
| test/episodes                  | 3820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00475   |
| test/info_shaping_reward_mean  | -0.0429    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.8707891 |
| test/Q_plus_P                  | -0.8707891 |
| test/reward_per_eps            | -8         |
| test/steps                     | 152800     |
| train/episodes                 | 15280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0036    |
| train/info_shaping_reward_mean | -0.0626    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 611200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.20491388 |
| stats_o/std                    | 0.07063561 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000988  |
| test/info_shaping_reward_mean  | -0.0415    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.8836572 |
| test/Q_plus_P                  | -0.8836572 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00447   |
| train/info_shaping_reward_mean | -0.072     |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.20491445  |
| stats_o/std                    | 0.07060701  |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.001      |
| test/info_shaping_reward_mean  | -0.0447     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.95251596 |
| test/Q_plus_P                  | -0.95251596 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0683     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 384        |
| stats_o/mean                   | 0.20491332 |
| stats_o/std                    | 0.07056095 |
| test/episodes                  | 3850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00101   |
| test/info_shaping_reward_mean  | -0.0399    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9022746 |
| test/Q_plus_P                  | -0.9022746 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 154000     |
| train/episodes                 | 15400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00401   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 616000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 385        |
| stats_o/mean                   | 0.20492356 |
| stats_o/std                    | 0.07053305 |
| test/episodes                  | 3860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00202   |
| test/info_shaping_reward_mean  | -0.0386    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.9256173 |
| test/Q_plus_P                  | -0.9256173 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 154400     |
| train/episodes                 | 15440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.0674    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 617600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 386        |
| stats_o/mean                   | 0.20492962 |
| stats_o/std                    | 0.070516   |
| test/episodes                  | 3870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00236   |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0033269 |
| test/Q_plus_P                  | -1.0033269 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 154800     |
| train/episodes                 | 15480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.667      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00405   |
| train/info_shaping_reward_mean | -0.0686    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 619200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.20492171  |
| stats_o/std                    | 0.070475556 |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00489    |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.9654868  |
| test/Q_plus_P                  | -0.9654868  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00528    |
| train/info_shaping_reward_mean | -0.0687     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 388         |
| stats_o/mean                   | 0.20492508  |
| stats_o/std                    | 0.07042604  |
| test/episodes                  | 3890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00467    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.97391516 |
| test/Q_plus_P                  | -0.97391516 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 155600      |
| train/episodes                 | 15560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00368    |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 622400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 389        |
| stats_o/mean                   | 0.2049199  |
| stats_o/std                    | 0.07041142 |
| test/episodes                  | 3900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0031    |
| test/info_shaping_reward_mean  | -0.0432    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0178205 |
| test/Q_plus_P                  | -1.0178205 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 156000     |
| train/episodes                 | 15600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00326   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 624000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 390         |
| stats_o/mean                   | 0.20492661  |
| stats_o/std                    | 0.07036536  |
| test/episodes                  | 3910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00646    |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.99085253 |
| test/Q_plus_P                  | -0.99085253 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 156400      |
| train/episodes                 | 15640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00371    |
| train/info_shaping_reward_mean | -0.0621     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 625600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.2049306   |
| stats_o/std                    | 0.07032137  |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00337    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.98610306 |
| test/Q_plus_P                  | -0.98610306 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00457    |
| train/info_shaping_reward_mean | -0.0693     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 392        |
| stats_o/mean                   | 0.20492372 |
| stats_o/std                    | 0.07035086 |
| test/episodes                  | 3930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00224   |
| test/info_shaping_reward_mean  | -0.0416    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9463141 |
| test/Q_plus_P                  | -0.9463141 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 157200     |
| train/episodes                 | 15720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00409   |
| train/info_shaping_reward_mean | -0.0778    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 628800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 393        |
| stats_o/mean                   | 0.20491786 |
| stats_o/std                    | 0.07031219 |
| test/episodes                  | 3940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00284   |
| test/info_shaping_reward_mean  | -0.0457    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1004049 |
| test/Q_plus_P                  | -1.1004049 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 157600     |
| train/episodes                 | 15760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00434   |
| train/info_shaping_reward_mean | -0.0635    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 630400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 394        |
| stats_o/mean                   | 0.20491493 |
| stats_o/std                    | 0.07029396 |
| test/episodes                  | 3950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000659  |
| test/info_shaping_reward_mean  | -0.0417    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.999378  |
| test/Q_plus_P                  | -0.999378  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 158000     |
| train/episodes                 | 15800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00433   |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 632000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 395        |
| stats_o/mean                   | 0.20491832 |
| stats_o/std                    | 0.07026096 |
| test/episodes                  | 3960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00204   |
| test/info_shaping_reward_mean  | -0.0399    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.012055  |
| test/Q_plus_P                  | -1.012055  |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 158400     |
| train/episodes                 | 15840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00313   |
| train/info_shaping_reward_mean | -0.0647    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 633600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 396        |
| stats_o/mean                   | 0.20491709 |
| stats_o/std                    | 0.0702302  |
| test/episodes                  | 3970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00178   |
| test/info_shaping_reward_mean  | -0.04      |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0485988 |
| test/Q_plus_P                  | -1.0485988 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 158800     |
| train/episodes                 | 15880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00379   |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 635200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.20492007  |
| stats_o/std                    | 0.07020854  |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000484   |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.91086423 |
| test/Q_plus_P                  | -0.91086423 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00446    |
| train/info_shaping_reward_mean | -0.0726     |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 398        |
| stats_o/mean                   | 0.20492515 |
| stats_o/std                    | 0.07018046 |
| test/episodes                  | 3990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000639  |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1072476 |
| test/Q_plus_P                  | -1.1072476 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 159600     |
| train/episodes                 | 15960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00396   |
| train/info_shaping_reward_mean | -0.0642    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 638400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 399        |
| stats_o/mean                   | 0.20493284 |
| stats_o/std                    | 0.07014411 |
| test/episodes                  | 4000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00342   |
| test/info_shaping_reward_mean  | -0.04      |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0548885 |
| test/Q_plus_P                  | -1.0548885 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 160000     |
| train/episodes                 | 16000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00445   |
| train/info_shaping_reward_mean | -0.0646    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 640000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 400         |
| stats_o/mean                   | 0.20494436  |
| stats_o/std                    | 0.0701442   |
| test/episodes                  | 4010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00317    |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.89361906 |
| test/Q_plus_P                  | -0.89361906 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 160400      |
| train/episodes                 | 16040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.0732     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 641600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 401        |
| stats_o/mean                   | 0.20493919 |
| stats_o/std                    | 0.07009998 |
| test/episodes                  | 4020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00278   |
| test/info_shaping_reward_mean  | -0.0434    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0768766 |
| test/Q_plus_P                  | -1.0768766 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 160800     |
| train/episodes                 | 16080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00437   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 643200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.20494285 |
| stats_o/std                    | 0.07006965 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0057    |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2567105 |
| test/Q_plus_P                  | -1.2567105 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 403        |
| stats_o/mean                   | 0.20494097 |
| stats_o/std                    | 0.07004161 |
| test/episodes                  | 4040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00278   |
| test/info_shaping_reward_mean  | -0.042     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0577123 |
| test/Q_plus_P                  | -1.0577123 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 161600     |
| train/episodes                 | 16160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00362   |
| train/info_shaping_reward_mean | -0.0656    |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 646400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 404        |
| stats_o/mean                   | 0.20494068 |
| stats_o/std                    | 0.07001244 |
| test/episodes                  | 4050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000527  |
| test/info_shaping_reward_mean  | -0.0412    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0009611 |
| test/Q_plus_P                  | -1.0009611 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 162000     |
| train/episodes                 | 16200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00318   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 648000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 405        |
| stats_o/mean                   | 0.2049496  |
| stats_o/std                    | 0.06997179 |
| test/episodes                  | 4060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0407    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9214695 |
| test/Q_plus_P                  | -0.9214695 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 162400     |
| train/episodes                 | 16240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.65       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00429   |
| train/info_shaping_reward_mean | -0.061     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 649600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 406        |
| stats_o/mean                   | 0.20494752 |
| stats_o/std                    | 0.06996644 |
| test/episodes                  | 4070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.0419    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0007378 |
| test/Q_plus_P                  | -1.0007378 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 162800     |
| train/episodes                 | 16280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00332   |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 651200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 407        |
| stats_o/mean                   | 0.20495814 |
| stats_o/std                    | 0.06996831 |
| test/episodes                  | 4080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00314   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.215     |
| test/Q                         | -1.1906971 |
| test/Q_plus_P                  | -1.1906971 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 163200     |
| train/episodes                 | 16320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.07      |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 652800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 408        |
| stats_o/mean                   | 0.2049485  |
| stats_o/std                    | 0.06995732 |
| test/episodes                  | 4090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00068   |
| test/info_shaping_reward_mean  | -0.0394    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.105346  |
| test/Q_plus_P                  | -1.105346  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 163600     |
| train/episodes                 | 16360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00338   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 654400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 409        |
| stats_o/mean                   | 0.20495304 |
| stats_o/std                    | 0.06991192 |
| test/episodes                  | 4100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00242   |
| test/info_shaping_reward_mean  | -0.0451    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1251092 |
| test/Q_plus_P                  | -1.1251092 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 164000     |
| train/episodes                 | 16400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00393   |
| train/info_shaping_reward_mean | -0.0635    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 656000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 410         |
| stats_o/mean                   | 0.20494987  |
| stats_o/std                    | 0.069870666 |
| test/episodes                  | 4110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00089    |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.9000636  |
| test/Q_plus_P                  | -0.9000636  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 164400      |
| train/episodes                 | 16440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00339    |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 657600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 411         |
| stats_o/mean                   | 0.20495161  |
| stats_o/std                    | 0.069829814 |
| test/episodes                  | 4120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000483   |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0444754  |
| test/Q_plus_P                  | -1.0444754  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 164800      |
| train/episodes                 | 16480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00398    |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 659200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 412        |
| stats_o/mean                   | 0.20495202 |
| stats_o/std                    | 0.06980693 |
| test/episodes                  | 4130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00275   |
| test/info_shaping_reward_mean  | -0.0388    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.9278464 |
| test/Q_plus_P                  | -0.9278464 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 165200     |
| train/episodes                 | 16520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.003     |
| train/info_shaping_reward_mean | -0.0775    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 660800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.20494707  |
| stats_o/std                    | 0.06977479  |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00227    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.87642914 |
| test/Q_plus_P                  | -0.87642914 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00383    |
| train/info_shaping_reward_mean | -0.0723     |
| train/info_shaping_reward_min  | -0.205      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 414        |
| stats_o/mean                   | 0.20494244 |
| stats_o/std                    | 0.0697395  |
| test/episodes                  | 4150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00601   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9981582 |
| test/Q_plus_P                  | -0.9981582 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 166000     |
| train/episodes                 | 16600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00295   |
| train/info_shaping_reward_mean | -0.0658    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 664000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 415         |
| stats_o/mean                   | 0.20494634  |
| stats_o/std                    | 0.06972656  |
| test/episodes                  | 4160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00167    |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.93207777 |
| test/Q_plus_P                  | -0.93207777 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 166400      |
| train/episodes                 | 16640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00336    |
| train/info_shaping_reward_mean | -0.0641     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 665600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 416         |
| stats_o/mean                   | 0.2049447   |
| stats_o/std                    | 0.069687426 |
| test/episodes                  | 4170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000299   |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.0107445  |
| test/Q_plus_P                  | -1.0107445  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 166800      |
| train/episodes                 | 16680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00383    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 667200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 417        |
| stats_o/mean                   | 0.20494115 |
| stats_o/std                    | 0.0696925  |
| test/episodes                  | 4180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00078   |
| test/info_shaping_reward_mean  | -0.0428    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0606052 |
| test/Q_plus_P                  | -1.0606052 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 167200     |
| train/episodes                 | 16720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00381   |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 668800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 418        |
| stats_o/mean                   | 0.2049425  |
| stats_o/std                    | 0.06969365 |
| test/episodes                  | 4190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00132   |
| test/info_shaping_reward_mean  | -0.0417    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -0.9648598 |
| test/Q_plus_P                  | -0.9648598 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 167600     |
| train/episodes                 | 16760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00379   |
| train/info_shaping_reward_mean | -0.0755    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 670400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 419        |
| stats_o/mean                   | 0.2049502  |
| stats_o/std                    | 0.06965247 |
| test/episodes                  | 4200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00351   |
| test/info_shaping_reward_mean  | -0.0439    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0445435 |
| test/Q_plus_P                  | -1.0445435 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 168000     |
| train/episodes                 | 16800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00284   |
| train/info_shaping_reward_mean | -0.061     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 672000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 420         |
| stats_o/mean                   | 0.20495264  |
| stats_o/std                    | 0.069635704 |
| test/episodes                  | 4210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.0335898  |
| test/Q_plus_P                  | -1.0335898  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 168400      |
| train/episodes                 | 16840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0042     |
| train/info_shaping_reward_mean | -0.067      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 673600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 421        |
| stats_o/mean                   | 0.2049551  |
| stats_o/std                    | 0.06961937 |
| test/episodes                  | 4220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00372   |
| test/info_shaping_reward_mean  | -0.0409    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0035712 |
| test/Q_plus_P                  | -1.0035712 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 168800     |
| train/episodes                 | 16880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.668      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 675200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 422        |
| stats_o/mean                   | 0.20495842 |
| stats_o/std                    | 0.0696161  |
| test/episodes                  | 4230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00174   |
| test/info_shaping_reward_mean  | -0.0382    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9243203 |
| test/Q_plus_P                  | -0.9243203 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 169200     |
| train/episodes                 | 16920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 676800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.20497517  |
| stats_o/std                    | 0.06968437  |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00105    |
| test/info_shaping_reward_mean  | -0.042      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.82133573 |
| test/Q_plus_P                  | -0.82133573 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0048     |
| train/info_shaping_reward_mean | -0.0739     |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 424        |
| stats_o/mean                   | 0.2049748  |
| stats_o/std                    | 0.06969708 |
| test/episodes                  | 4250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9641451 |
| test/Q_plus_P                  | -0.9641451 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 170000     |
| train/episodes                 | 17000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00508   |
| train/info_shaping_reward_mean | -0.0798    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 680000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.20497909  |
| stats_o/std                    | 0.069656536 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0018     |
| test/info_shaping_reward_mean  | -0.0432     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.98300993 |
| test/Q_plus_P                  | -0.98300993 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00349    |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.20498063  |
| stats_o/std                    | 0.06960905  |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000853   |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.82629186 |
| test/Q_plus_P                  | -0.82629186 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00338    |
| train/info_shaping_reward_mean | -0.0642     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 427         |
| stats_o/mean                   | 0.20498516  |
| stats_o/std                    | 0.069583006 |
| test/episodes                  | 4280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00211    |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.95899755 |
| test/Q_plus_P                  | -0.95899755 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 171200      |
| train/episodes                 | 17120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00433    |
| train/info_shaping_reward_mean | -0.0724     |
| train/info_shaping_reward_min  | -0.211      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 684800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.2049806   |
| stats_o/std                    | 0.06963418  |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00219    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.91509277 |
| test/Q_plus_P                  | -0.91509277 |
| test/reward_per_eps            | -8          |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00406    |
| train/info_shaping_reward_mean | -0.0881     |
| train/info_shaping_reward_min  | -0.288      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 429         |
| stats_o/mean                   | 0.20497501  |
| stats_o/std                    | 0.069598116 |
| test/episodes                  | 4300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00121    |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.8548927  |
| test/Q_plus_P                  | -0.8548927  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 172000      |
| train/episodes                 | 17200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.635       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 688000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 430        |
| stats_o/mean                   | 0.20497519 |
| stats_o/std                    | 0.06960456 |
| test/episodes                  | 4310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00123   |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.055465  |
| test/Q_plus_P                  | -1.055465  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 172400     |
| train/episodes                 | 17240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00453   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 689600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 431         |
| stats_o/mean                   | 0.20497131  |
| stats_o/std                    | 0.069572546 |
| test/episodes                  | 4320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00136    |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.876927   |
| test/Q_plus_P                  | -0.876927   |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 172800      |
| train/episodes                 | 17280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00408    |
| train/info_shaping_reward_mean | -0.0678     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 691200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 432         |
| stats_o/mean                   | 0.20497902  |
| stats_o/std                    | 0.069558695 |
| test/episodes                  | 4330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0422047  |
| test/Q_plus_P                  | -1.0422047  |
| test/reward_per_eps            | -8          |
| test/steps                     | 173200      |
| train/episodes                 | 17320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.0671     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 692800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 433        |
| stats_o/mean                   | 0.20498216 |
| stats_o/std                    | 0.06953807 |
| test/episodes                  | 4340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.0405    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0314293 |
| test/Q_plus_P                  | -1.0314293 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 173600     |
| train/episodes                 | 17360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00489   |
| train/info_shaping_reward_mean | -0.0664    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 694400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 434        |
| stats_o/mean                   | 0.20497744 |
| stats_o/std                    | 0.06959102 |
| test/episodes                  | 4350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00468   |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.194511  |
| test/Q_plus_P                  | -1.194511  |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 174000     |
| train/episodes                 | 17400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00445   |
| train/info_shaping_reward_mean | -0.0869    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 696000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 435        |
| stats_o/mean                   | 0.20497352 |
| stats_o/std                    | 0.0695628  |
| test/episodes                  | 4360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2382011 |
| test/Q_plus_P                  | -1.2382011 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 174400     |
| train/episodes                 | 17440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00391   |
| train/info_shaping_reward_mean | -0.0733    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 697600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 436        |
| stats_o/mean                   | 0.20497316 |
| stats_o/std                    | 0.06957663 |
| test/episodes                  | 4370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000681  |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.45007   |
| test/Q_plus_P                  | -1.45007   |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 174800     |
| train/episodes                 | 17480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0796    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 699200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 437        |
| stats_o/mean                   | 0.20497467 |
| stats_o/std                    | 0.06955934 |
| test/episodes                  | 4380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.0472    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3360938 |
| test/Q_plus_P                  | -1.3360938 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 175200     |
| train/episodes                 | 17520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00408   |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 700800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 438        |
| stats_o/mean                   | 0.20497267 |
| stats_o/std                    | 0.06953175 |
| test/episodes                  | 4390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.0443    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -1.2266866 |
| test/Q_plus_P                  | -1.2266866 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 175600     |
| train/episodes                 | 17560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00313   |
| train/info_shaping_reward_mean | -0.0744    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 702400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 439        |
| stats_o/mean                   | 0.20496531 |
| stats_o/std                    | 0.06949349 |
| test/episodes                  | 4400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0924816 |
| test/Q_plus_P                  | -1.0924816 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 176000     |
| train/episodes                 | 17600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00464   |
| train/info_shaping_reward_mean | -0.07      |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 704000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 440        |
| stats_o/mean                   | 0.20496859 |
| stats_o/std                    | 0.06950994 |
| test/episodes                  | 4410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000528  |
| test/info_shaping_reward_mean  | -0.0403    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.011874  |
| test/Q_plus_P                  | -1.011874  |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 176400     |
| train/episodes                 | 17640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00396   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 705600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 441        |
| stats_o/mean                   | 0.20496646 |
| stats_o/std                    | 0.06947181 |
| test/episodes                  | 4420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00106   |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.114806  |
| test/Q_plus_P                  | -1.114806  |
| test/reward_per_eps            | -9         |
| test/steps                     | 176800     |
| train/episodes                 | 17680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00363   |
| train/info_shaping_reward_mean | -0.0642    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 707200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 442        |
| stats_o/mean                   | 0.20496854 |
| stats_o/std                    | 0.06943114 |
| test/episodes                  | 4430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000904  |
| test/info_shaping_reward_mean  | -0.0404    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0133452 |
| test/Q_plus_P                  | -1.0133452 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 177200     |
| train/episodes                 | 17720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00527   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 708800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 443        |
| stats_o/mean                   | 0.20497252 |
| stats_o/std                    | 0.06940505 |
| test/episodes                  | 4440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000491  |
| test/info_shaping_reward_mean  | -0.0397    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9119762 |
| test/Q_plus_P                  | -0.9119762 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 177600     |
| train/episodes                 | 17760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00369   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 710400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 444         |
| stats_o/mean                   | 0.20496844  |
| stats_o/std                    | 0.069379285 |
| test/episodes                  | 4450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000648   |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0355855  |
| test/Q_plus_P                  | -1.0355855  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 178000      |
| train/episodes                 | 17800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 712000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.20496444  |
| stats_o/std                    | 0.069388606 |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00406    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.0167148  |
| test/Q_plus_P                  | -1.0167148  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.615       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00478    |
| train/info_shaping_reward_mean | -0.0717     |
| train/info_shaping_reward_min  | -0.208      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 446        |
| stats_o/mean                   | 0.20496717 |
| stats_o/std                    | 0.06936587 |
| test/episodes                  | 4470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0376    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.8041532 |
| test/Q_plus_P                  | -0.8041532 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 178800     |
| train/episodes                 | 17880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00472   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 715200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 447         |
| stats_o/mean                   | 0.20496833  |
| stats_o/std                    | 0.06931438  |
| test/episodes                  | 4480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000773   |
| test/info_shaping_reward_mean  | -0.0389     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.92706734 |
| test/Q_plus_P                  | -0.92706734 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 179200      |
| train/episodes                 | 17920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00572    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 716800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 448        |
| stats_o/mean                   | 0.20497048 |
| stats_o/std                    | 0.06930188 |
| test/episodes                  | 4490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.001     |
| test/info_shaping_reward_mean  | -0.0407    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0313214 |
| test/Q_plus_P                  | -1.0313214 |
| test/reward_per_eps            | -9         |
| test/steps                     | 179600     |
| train/episodes                 | 17960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00498   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 718400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 449        |
| stats_o/mean                   | 0.20497091 |
| stats_o/std                    | 0.06925586 |
| test/episodes                  | 4500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00106   |
| test/info_shaping_reward_mean  | -0.0431    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0308499 |
| test/Q_plus_P                  | -1.0308499 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 180000     |
| train/episodes                 | 18000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.666      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00297   |
| train/info_shaping_reward_mean | -0.0594    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 720000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.20497341 |
| stats_o/std                    | 0.06924678 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000531  |
| test/info_shaping_reward_mean  | -0.0422    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1334573 |
| test/Q_plus_P                  | -1.1334573 |
| test/reward_per_eps            | -9         |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00512   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 451        |
| stats_o/mean                   | 0.2049637  |
| stats_o/std                    | 0.06925502 |
| test/episodes                  | 4520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00288   |
| test/info_shaping_reward_mean  | -0.0414    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0286213 |
| test/Q_plus_P                  | -1.0286213 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 180800     |
| train/episodes                 | 18080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00453   |
| train/info_shaping_reward_mean | -0.0749    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 723200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 452        |
| stats_o/mean                   | 0.20496611 |
| stats_o/std                    | 0.06926015 |
| test/episodes                  | 4530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00138   |
| test/info_shaping_reward_mean  | -0.0402    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0205998 |
| test/Q_plus_P                  | -1.0205998 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 181200     |
| train/episodes                 | 18120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0041    |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 724800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 453        |
| stats_o/mean                   | 0.20497416 |
| stats_o/std                    | 0.06925141 |
| test/episodes                  | 4540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00207   |
| test/info_shaping_reward_mean  | -0.0402    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0482364 |
| test/Q_plus_P                  | -1.0482364 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 181600     |
| train/episodes                 | 18160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00403   |
| train/info_shaping_reward_mean | -0.0739    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 726400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.20497188  |
| stats_o/std                    | 0.069210626 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00177    |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0105406  |
| test/Q_plus_P                  | -1.0105406  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00384    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 455        |
| stats_o/mean                   | 0.20496969 |
| stats_o/std                    | 0.06920317 |
| test/episodes                  | 4560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00208   |
| test/info_shaping_reward_mean  | -0.0408    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0224047 |
| test/Q_plus_P                  | -1.0224047 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 182400     |
| train/episodes                 | 18240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00372   |
| train/info_shaping_reward_mean | -0.0779    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 729600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 456        |
| stats_o/mean                   | 0.20498012 |
| stats_o/std                    | 0.06919888 |
| test/episodes                  | 4570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00147   |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.227     |
| test/Q                         | -1.3712182 |
| test/Q_plus_P                  | -1.3712182 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 182800     |
| train/episodes                 | 18280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.652      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00363   |
| train/info_shaping_reward_mean | -0.0615    |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 731200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.20497607  |
| stats_o/std                    | 0.06922527  |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00139    |
| test/info_shaping_reward_mean  | -0.0432     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.94111276 |
| test/Q_plus_P                  | -0.94111276 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00516    |
| train/info_shaping_reward_mean | -0.0744     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.20498046  |
| stats_o/std                    | 0.069207914 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00139    |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.9804585  |
| test/Q_plus_P                  | -0.9804585  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00426    |
| train/info_shaping_reward_mean | -0.0703     |
| train/info_shaping_reward_min  | -0.207      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 459        |
| stats_o/mean                   | 0.20499302 |
| stats_o/std                    | 0.0692556  |
| test/episodes                  | 4600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.812      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00104   |
| test/info_shaping_reward_mean  | -0.0397    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.8696971 |
| test/Q_plus_P                  | -0.8696971 |
| test/reward_per_eps            | -7.5       |
| test/steps                     | 184000     |
| train/episodes                 | 18400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00488   |
| train/info_shaping_reward_mean | -0.0837    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 736000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.20499244  |
| stats_o/std                    | 0.06923018  |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00188    |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.96931195 |
| test/Q_plus_P                  | -0.96931195 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00396    |
| train/info_shaping_reward_mean | -0.0725     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 461        |
| stats_o/mean                   | 0.20499459 |
| stats_o/std                    | 0.0692205  |
| test/episodes                  | 4620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0025    |
| test/info_shaping_reward_mean  | -0.0389    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9268016 |
| test/Q_plus_P                  | -0.9268016 |
| test/reward_per_eps            | -8         |
| test/steps                     | 184800     |
| train/episodes                 | 18480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00321   |
| train/info_shaping_reward_mean | -0.0748    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 739200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 462        |
| stats_o/mean                   | 0.20498857 |
| stats_o/std                    | 0.06922933 |
| test/episodes                  | 4630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00489   |
| test/info_shaping_reward_mean  | -0.0414    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9076448 |
| test/Q_plus_P                  | -0.9076448 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 185200     |
| train/episodes                 | 18520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00292   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 740800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 463        |
| stats_o/mean                   | 0.2049953  |
| stats_o/std                    | 0.06926723 |
| test/episodes                  | 4640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.224     |
| test/Q                         | -1.0726205 |
| test/Q_plus_P                  | -1.0726205 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 185600     |
| train/episodes                 | 18560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00479   |
| train/info_shaping_reward_mean | -0.0826    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 742400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 464         |
| stats_o/mean                   | 0.20498846  |
| stats_o/std                    | 0.069235526 |
| test/episodes                  | 4650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00181    |
| test/info_shaping_reward_mean  | -0.0408     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.89823455 |
| test/Q_plus_P                  | -0.89823455 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 186000      |
| train/episodes                 | 18600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0049     |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 744000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 465         |
| stats_o/mean                   | 0.20499112  |
| stats_o/std                    | 0.06919939  |
| test/episodes                  | 4660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00444    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.97187454 |
| test/Q_plus_P                  | -0.97187454 |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 186400      |
| train/episodes                 | 18640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00473    |
| train/info_shaping_reward_mean | -0.0631     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 745600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.20498925  |
| stats_o/std                    | 0.06916522  |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00204    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.93391246 |
| test/Q_plus_P                  | -0.93391246 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00384    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 467        |
| stats_o/mean                   | 0.20499024 |
| stats_o/std                    | 0.06913729 |
| test/episodes                  | 4680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0015    |
| test/info_shaping_reward_mean  | -0.0383    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.8577233 |
| test/Q_plus_P                  | -0.8577233 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 187200     |
| train/episodes                 | 18720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00323   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 748800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 468        |
| stats_o/mean                   | 0.2049942  |
| stats_o/std                    | 0.06913852 |
| test/episodes                  | 4690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00207   |
| test/info_shaping_reward_mean  | -0.0433    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9618282 |
| test/Q_plus_P                  | -0.9618282 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 187600     |
| train/episodes                 | 18760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.541      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0037    |
| train/info_shaping_reward_mean | -0.0803    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 750400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 469        |
| stats_o/mean                   | 0.20499384 |
| stats_o/std                    | 0.06914402 |
| test/episodes                  | 4700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00193   |
| test/info_shaping_reward_mean  | -0.0413    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0891896 |
| test/Q_plus_P                  | -1.0891896 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 188000     |
| train/episodes                 | 18800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0049    |
| train/info_shaping_reward_mean | -0.0828    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 752000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.20499127 |
| stats_o/std                    | 0.06911858 |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.675      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00653   |
| test/info_shaping_reward_mean  | -0.0603    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.1764524 |
| test/Q_plus_P                  | -2.1764524 |
| test/reward_per_eps            | -13        |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00439   |
| train/info_shaping_reward_mean | -0.0645    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.20499052 |
| stats_o/std                    | 0.06911706 |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0022    |
| test/info_shaping_reward_mean  | -0.041     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9964597 |
| test/Q_plus_P                  | -0.9964597 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00273   |
| train/info_shaping_reward_mean | -0.0764    |
| train/info_shaping_reward_min  | -0.267     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 472         |
| stats_o/mean                   | 0.20499524  |
| stats_o/std                    | 0.069103286 |
| test/episodes                  | 4730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00229    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.0240226  |
| test/Q_plus_P                  | -1.0240226  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 189200      |
| train/episodes                 | 18920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00418    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.211      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 756800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 473        |
| stats_o/mean                   | 0.20498417 |
| stats_o/std                    | 0.06910498 |
| test/episodes                  | 4740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00211   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0662069 |
| test/Q_plus_P                  | -1.0662069 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 189600     |
| train/episodes                 | 18960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.0741    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 758400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 474        |
| stats_o/mean                   | 0.2049756  |
| stats_o/std                    | 0.06907917 |
| test/episodes                  | 4750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00348   |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0995724 |
| test/Q_plus_P                  | -1.0995724 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 190000     |
| train/episodes                 | 19000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0038    |
| train/info_shaping_reward_mean | -0.0669    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 760000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 475        |
| stats_o/mean                   | 0.20496786 |
| stats_o/std                    | 0.06906184 |
| test/episodes                  | 4760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00268   |
| test/info_shaping_reward_mean  | -0.0426    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0541681 |
| test/Q_plus_P                  | -1.0541681 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 190400     |
| train/episodes                 | 19040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00313   |
| train/info_shaping_reward_mean | -0.0673    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 761600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 476        |
| stats_o/mean                   | 0.20497192 |
| stats_o/std                    | 0.06904421 |
| test/episodes                  | 4770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00425   |
| test/info_shaping_reward_mean  | -0.0418    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -1.0038075 |
| test/Q_plus_P                  | -1.0038075 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 190800     |
| train/episodes                 | 19080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00442   |
| train/info_shaping_reward_mean | -0.073     |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 763200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 477        |
| stats_o/mean                   | 0.20497432 |
| stats_o/std                    | 0.0690695  |
| test/episodes                  | 4780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00177   |
| test/info_shaping_reward_mean  | -0.042     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9640959 |
| test/Q_plus_P                  | -0.9640959 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 191200     |
| train/episodes                 | 19120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00445   |
| train/info_shaping_reward_mean | -0.0838    |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 764800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 478         |
| stats_o/mean                   | 0.20497242  |
| stats_o/std                    | 0.06902937  |
| test/episodes                  | 4790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00221    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.99973154 |
| test/Q_plus_P                  | -0.99973154 |
| test/reward_per_eps            | -9          |
| test/steps                     | 191600      |
| train/episodes                 | 19160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00399    |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 766400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 479        |
| stats_o/mean                   | 0.20496729 |
| stats_o/std                    | 0.06901962 |
| test/episodes                  | 4800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00419   |
| test/info_shaping_reward_mean  | -0.047     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.04839   |
| test/Q_plus_P                  | -1.04839   |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 192000     |
| train/episodes                 | 19200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00431   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 768000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 480         |
| stats_o/mean                   | 0.20497198  |
| stats_o/std                    | 0.069022804 |
| test/episodes                  | 4810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00313    |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.0500443  |
| test/Q_plus_P                  | -1.0500443  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 192400      |
| train/episodes                 | 19240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0051     |
| train/info_shaping_reward_mean | -0.0664     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 769600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 481        |
| stats_o/mean                   | 0.20496841 |
| stats_o/std                    | 0.06907067 |
| test/episodes                  | 4820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00429   |
| test/info_shaping_reward_mean  | -0.0431    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9936387 |
| test/Q_plus_P                  | -0.9936387 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 192800     |
| train/episodes                 | 19280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00384   |
| train/info_shaping_reward_mean | -0.0772    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 771200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.20496486  |
| stats_o/std                    | 0.069035746 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00309    |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.94431025 |
| test/Q_plus_P                  | -0.94431025 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00453    |
| train/info_shaping_reward_mean | -0.0664     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 483        |
| stats_o/mean                   | 0.2049664  |
| stats_o/std                    | 0.06902089 |
| test/episodes                  | 4840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00162   |
| test/info_shaping_reward_mean  | -0.0441    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1400359 |
| test/Q_plus_P                  | -1.1400359 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 193600     |
| train/episodes                 | 19360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0051    |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 774400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 484        |
| stats_o/mean                   | 0.20497425 |
| stats_o/std                    | 0.06902253 |
| test/episodes                  | 4850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00245   |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1240722 |
| test/Q_plus_P                  | -1.1240722 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 194000     |
| train/episodes                 | 19400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0734    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 776000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 485        |
| stats_o/mean                   | 0.20498358 |
| stats_o/std                    | 0.06904936 |
| test/episodes                  | 4860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00077   |
| test/info_shaping_reward_mean  | -0.0434    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0620476 |
| test/Q_plus_P                  | -1.0620476 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 194400     |
| train/episodes                 | 19440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00492   |
| train/info_shaping_reward_mean | -0.0844    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 777600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 486        |
| stats_o/mean                   | 0.20498392 |
| stats_o/std                    | 0.06908855 |
| test/episodes                  | 4870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000489  |
| test/info_shaping_reward_mean  | -0.0439    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1395954 |
| test/Q_plus_P                  | -1.1395954 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 194800     |
| train/episodes                 | 19480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.086     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 779200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 487         |
| stats_o/mean                   | 0.20497769  |
| stats_o/std                    | 0.069076516 |
| test/episodes                  | 4880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00112    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.9975638  |
| test/Q_plus_P                  | -0.9975638  |
| test/reward_per_eps            | -9          |
| test/steps                     | 195200      |
| train/episodes                 | 19520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00435    |
| train/info_shaping_reward_mean | -0.0677     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 780800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 488        |
| stats_o/mean                   | 0.20497492 |
| stats_o/std                    | 0.0690963  |
| test/episodes                  | 4890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00243   |
| test/info_shaping_reward_mean  | -0.0437    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0437657 |
| test/Q_plus_P                  | -1.0437657 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 195600     |
| train/episodes                 | 19560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.534      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00423   |
| train/info_shaping_reward_mean | -0.0889    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 782400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 489        |
| stats_o/mean                   | 0.20498687 |
| stats_o/std                    | 0.0691041  |
| test/episodes                  | 4900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00203   |
| test/info_shaping_reward_mean  | -0.0446    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1919464 |
| test/Q_plus_P                  | -1.1919464 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 196000     |
| train/episodes                 | 19600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0035    |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 784000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 490        |
| stats_o/mean                   | 0.20499387 |
| stats_o/std                    | 0.06914722 |
| test/episodes                  | 4910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00112   |
| test/info_shaping_reward_mean  | -0.0441    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0476844 |
| test/Q_plus_P                  | -1.0476844 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 196400     |
| train/episodes                 | 19640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00351   |
| train/info_shaping_reward_mean | -0.0871    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 785600     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 491       |
| stats_o/mean                   | 0.2049946 |
| stats_o/std                    | 0.0691231 |
| test/episodes                  | 4920      |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.752     |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.00117  |
| test/info_shaping_reward_mean  | -0.048    |
| test/info_shaping_reward_min   | -0.173    |
| test/Q                         | -1.230847 |
| test/Q_plus_P                  | -1.230847 |
| test/reward_per_eps            | -9.9      |
| test/steps                     | 196800    |
| train/episodes                 | 19680     |
| train/info_is_success_max      | 1         |
| train/info_is_success_mean     | 0.604     |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.0041   |
| train/info_shaping_reward_mean | -0.069    |
| train/info_shaping_reward_min  | -0.188    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -15.8     |
| train/steps                    | 787200    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 492        |
| stats_o/mean                   | 0.20499681 |
| stats_o/std                    | 0.06915362 |
| test/episodes                  | 4930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1693784 |
| test/Q_plus_P                  | -1.1693784 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 197200     |
| train/episodes                 | 19720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00609   |
| train/info_shaping_reward_mean | -0.0693    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 788800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.20499776 |
| stats_o/std                    | 0.0691429  |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00125   |
| test/info_shaping_reward_mean  | -0.0423    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0399573 |
| test/Q_plus_P                  | -1.0399573 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00498   |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.20499401  |
| stats_o/std                    | 0.069103904 |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0034     |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0848187  |
| test/Q_plus_P                  | -1.0848187  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00444    |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 495         |
| stats_o/mean                   | 0.20499241  |
| stats_o/std                    | 0.069072835 |
| test/episodes                  | 4960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00186    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.0624328  |
| test/Q_plus_P                  | -1.0624328  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 198400      |
| train/episodes                 | 19840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0044     |
| train/info_shaping_reward_mean | -0.0652     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 793600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 496        |
| stats_o/mean                   | 0.20498821 |
| stats_o/std                    | 0.0690413  |
| test/episodes                  | 4970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.042     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.931354  |
| test/Q_plus_P                  | -0.931354  |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 198800     |
| train/episodes                 | 19880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00359   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 795200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 497        |
| stats_o/mean                   | 0.20499189 |
| stats_o/std                    | 0.06904946 |
| test/episodes                  | 4980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00077   |
| test/info_shaping_reward_mean  | -0.0394    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0311124 |
| test/Q_plus_P                  | -1.0311124 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 199200     |
| train/episodes                 | 19920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.587      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00543   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 796800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 498        |
| stats_o/mean                   | 0.2049914  |
| stats_o/std                    | 0.06906233 |
| test/episodes                  | 4990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00318   |
| test/info_shaping_reward_mean  | -0.043     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0882109 |
| test/Q_plus_P                  | -1.0882109 |
| test/reward_per_eps            | -9         |
| test/steps                     | 199600     |
| train/episodes                 | 19960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00438   |
| train/info_shaping_reward_mean | -0.08      |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 798400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.2049964   |
| stats_o/std                    | 0.06903361  |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.99712557 |
| test/Q_plus_P                  | -0.99712557 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00537    |
| train/info_shaping_reward_mean | -0.0667     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 500        |
| stats_o/mean                   | 0.20499265 |
| stats_o/std                    | 0.0690044  |
| test/episodes                  | 5010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000409  |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.089755  |
| test/Q_plus_P                  | -1.089755  |
| test/reward_per_eps            | -9         |
| test/steps                     | 200400     |
| train/episodes                 | 20040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00444   |
| train/info_shaping_reward_mean | -0.0614    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 801600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 501         |
| stats_o/mean                   | 0.20499584  |
| stats_o/std                    | 0.06896798  |
| test/episodes                  | 5020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00268    |
| test/info_shaping_reward_mean  | -0.043      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.96730804 |
| test/Q_plus_P                  | -0.96730804 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 200800      |
| train/episodes                 | 20080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00445    |
| train/info_shaping_reward_mean | -0.0661     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 803200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 502        |
| stats_o/mean                   | 0.20499685 |
| stats_o/std                    | 0.06895229 |
| test/episodes                  | 5030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000747  |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.227613  |
| test/Q_plus_P                  | -1.227613  |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 201200     |
| train/episodes                 | 20120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00488   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 804800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 503        |
| stats_o/mean                   | 0.20499317 |
| stats_o/std                    | 0.06893727 |
| test/episodes                  | 5040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00165   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1871276 |
| test/Q_plus_P                  | -1.1871276 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 201600     |
| train/episodes                 | 20160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 806400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 504        |
| stats_o/mean                   | 0.20499569 |
| stats_o/std                    | 0.06893703 |
| test/episodes                  | 5050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000327  |
| test/info_shaping_reward_mean  | -0.0427    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0750293 |
| test/Q_plus_P                  | -1.0750293 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 202000     |
| train/episodes                 | 20200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.575      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00454   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 808000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 505         |
| stats_o/mean                   | 0.20500265  |
| stats_o/std                    | 0.068921916 |
| test/episodes                  | 5060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000796   |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.9065906  |
| test/Q_plus_P                  | -0.9065906  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 202400      |
| train/episodes                 | 20240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00438    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 809600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.20499466  |
| stats_o/std                    | 0.068931006 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00245    |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.9198452  |
| test/Q_plus_P                  | -0.9198452  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00482    |
| train/info_shaping_reward_mean | -0.0735     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 507        |
| stats_o/mean                   | 0.20499317 |
| stats_o/std                    | 0.06894231 |
| test/episodes                  | 5080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000377  |
| test/info_shaping_reward_mean  | -0.0423    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.037292  |
| test/Q_plus_P                  | -1.037292  |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 203200     |
| train/episodes                 | 20320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.59       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00348   |
| train/info_shaping_reward_mean | -0.0691    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 812800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 508         |
| stats_o/mean                   | 0.20499936  |
| stats_o/std                    | 0.068925634 |
| test/episodes                  | 5090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000592   |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.1344584  |
| test/Q_plus_P                  | -1.1344584  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 203600      |
| train/episodes                 | 20360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.589       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00465    |
| train/info_shaping_reward_mean | -0.0704     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 814400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 509         |
| stats_o/mean                   | 0.2050013   |
| stats_o/std                    | 0.068915136 |
| test/episodes                  | 5100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1973715  |
| test/Q_plus_P                  | -1.1973715  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 204000      |
| train/episodes                 | 20400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00376    |
| train/info_shaping_reward_mean | -0.0699     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 816000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 510        |
| stats_o/mean                   | 0.20499586 |
| stats_o/std                    | 0.06888268 |
| test/episodes                  | 5110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00339   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3500223 |
| test/Q_plus_P                  | -1.3500223 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 204400     |
| train/episodes                 | 20440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00363   |
| train/info_shaping_reward_mean | -0.0663    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 817600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 511        |
| stats_o/mean                   | 0.2049925  |
| stats_o/std                    | 0.06887777 |
| test/episodes                  | 5120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00313   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1575286 |
| test/Q_plus_P                  | -1.1575286 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 204800     |
| train/episodes                 | 20480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.635      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00371   |
| train/info_shaping_reward_mean | -0.0654    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 819200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 512        |
| stats_o/mean                   | 0.2049951  |
| stats_o/std                    | 0.06884634 |
| test/episodes                  | 5130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.0444    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1529257 |
| test/Q_plus_P                  | -1.1529257 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 205200     |
| train/episodes                 | 20520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.661      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00473   |
| train/info_shaping_reward_mean | -0.0617    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 820800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 513        |
| stats_o/mean                   | 0.2049951  |
| stats_o/std                    | 0.06882111 |
| test/episodes                  | 5140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00113   |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0588053 |
| test/Q_plus_P                  | -1.0588053 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 205600     |
| train/episodes                 | 20560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00421   |
| train/info_shaping_reward_mean | -0.0665    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 822400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 514        |
| stats_o/mean                   | 0.20499161 |
| stats_o/std                    | 0.06880505 |
| test/episodes                  | 5150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00184   |
| test/info_shaping_reward_mean  | -0.0402    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.985129  |
| test/Q_plus_P                  | -0.985129  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 206000     |
| train/episodes                 | 20600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00295   |
| train/info_shaping_reward_mean | -0.0673    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 824000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 515         |
| stats_o/mean                   | 0.20498261  |
| stats_o/std                    | 0.068804495 |
| test/episodes                  | 5160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000797   |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1282185  |
| test/Q_plus_P                  | -1.1282185  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 206400      |
| train/episodes                 | 20640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00379    |
| train/info_shaping_reward_mean | -0.0709     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 825600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 516        |
| stats_o/mean                   | 0.20498963 |
| stats_o/std                    | 0.06882543 |
| test/episodes                  | 5170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0012    |
| test/info_shaping_reward_mean  | -0.0393    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9728029 |
| test/Q_plus_P                  | -0.9728029 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 206800     |
| train/episodes                 | 20680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.0669    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 827200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 517        |
| stats_o/mean                   | 0.20499706 |
| stats_o/std                    | 0.06880083 |
| test/episodes                  | 5180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00144   |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.1453322 |
| test/Q_plus_P                  | -1.1453322 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 207200     |
| train/episodes                 | 20720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00439   |
| train/info_shaping_reward_mean | -0.0681    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 828800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 518         |
| stats_o/mean                   | 0.20499071  |
| stats_o/std                    | 0.068793826 |
| test/episodes                  | 5190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00186    |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -0.9768545  |
| test/Q_plus_P                  | -0.9768545  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 207600      |
| train/episodes                 | 20760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00426    |
| train/info_shaping_reward_mean | -0.0714     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 830400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 519        |
| stats_o/mean                   | 0.20498516 |
| stats_o/std                    | 0.06881202 |
| test/episodes                  | 5200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00209   |
| test/info_shaping_reward_mean  | -0.0429    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.0432734 |
| test/Q_plus_P                  | -1.0432734 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 208000     |
| train/episodes                 | 20800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00516   |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 832000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.20498821  |
| stats_o/std                    | 0.068783514 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00306    |
| test/info_shaping_reward_mean  | -0.041      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0849081  |
| test/Q_plus_P                  | -1.0849081  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00302    |
| train/info_shaping_reward_mean | -0.0662     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 521         |
| stats_o/mean                   | 0.20499054  |
| stats_o/std                    | 0.06878878  |
| test/episodes                  | 5220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00248    |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.95975715 |
| test/Q_plus_P                  | -0.95975715 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 208800      |
| train/episodes                 | 20880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00438    |
| train/info_shaping_reward_mean | -0.0706     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 835200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 522         |
| stats_o/mean                   | 0.2049961   |
| stats_o/std                    | 0.068789184 |
| test/episodes                  | 5230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000665   |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1096072  |
| test/Q_plus_P                  | -1.1096072  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 209200      |
| train/episodes                 | 20920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00371    |
| train/info_shaping_reward_mean | -0.0704     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 836800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 523        |
| stats_o/mean                   | 0.20499058 |
| stats_o/std                    | 0.06880072 |
| test/episodes                  | 5240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00372   |
| test/info_shaping_reward_mean  | -0.0413    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.943318  |
| test/Q_plus_P                  | -0.943318  |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 209600     |
| train/episodes                 | 20960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00464   |
| train/info_shaping_reward_mean | -0.0796    |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 838400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 524        |
| stats_o/mean                   | 0.20499542 |
| stats_o/std                    | 0.06882192 |
| test/episodes                  | 5250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00134   |
| test/info_shaping_reward_mean  | -0.0395    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0086683 |
| test/Q_plus_P                  | -1.0086683 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 210000     |
| train/episodes                 | 21000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00426   |
| train/info_shaping_reward_mean | -0.0744    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 840000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 525         |
| stats_o/mean                   | 0.20499398  |
| stats_o/std                    | 0.068792455 |
| test/episodes                  | 5260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00163    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.028511   |
| test/Q_plus_P                  | -1.028511   |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 210400      |
| train/episodes                 | 21040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00411    |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 841600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 526        |
| stats_o/mean                   | 0.20499821 |
| stats_o/std                    | 0.06875329 |
| test/episodes                  | 5270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00159   |
| test/info_shaping_reward_mean  | -0.042     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.8972993 |
| test/Q_plus_P                  | -0.8972993 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 210800     |
| train/episodes                 | 21080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00493   |
| train/info_shaping_reward_mean | -0.0655    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 843200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 527        |
| stats_o/mean                   | 0.2049979  |
| stats_o/std                    | 0.06873192 |
| test/episodes                  | 5280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0429    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0419521 |
| test/Q_plus_P                  | -1.0419521 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 211200     |
| train/episodes                 | 21120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.652      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00424   |
| train/info_shaping_reward_mean | -0.0636    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 844800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 528        |
| stats_o/mean                   | 0.20500213 |
| stats_o/std                    | 0.06875521 |
| test/episodes                  | 5290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.0415    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1137547 |
| test/Q_plus_P                  | -1.1137547 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 211600     |
| train/episodes                 | 21160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00521   |
| train/info_shaping_reward_mean | -0.0834    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 846400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.2049982   |
| stats_o/std                    | 0.068730555 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0022     |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0126086  |
| test/Q_plus_P                  | -1.0126086  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00441    |
| train/info_shaping_reward_mean | -0.0652     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 530        |
| stats_o/mean                   | 0.20499668 |
| stats_o/std                    | 0.06870329 |
| test/episodes                  | 5310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00198   |
| test/info_shaping_reward_mean  | -0.0397    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9748164 |
| test/Q_plus_P                  | -0.9748164 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 212400     |
| train/episodes                 | 21240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00473   |
| train/info_shaping_reward_mean | -0.0643    |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 849600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 531         |
| stats_o/mean                   | 0.20499988  |
| stats_o/std                    | 0.068700835 |
| test/episodes                  | 5320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000979   |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.9883453  |
| test/Q_plus_P                  | -0.9883453  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 212800      |
| train/episodes                 | 21280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00526    |
| train/info_shaping_reward_mean | -0.0688     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 851200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 532        |
| stats_o/mean                   | 0.20500216 |
| stats_o/std                    | 0.06867371 |
| test/episodes                  | 5330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000217  |
| test/info_shaping_reward_mean  | -0.0406    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.873361  |
| test/Q_plus_P                  | -0.873361  |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 213200     |
| train/episodes                 | 21320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00399   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 852800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.20500992  |
| stats_o/std                    | 0.068690665 |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.1062646  |
| test/Q_plus_P                  | -1.1062646  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00343    |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 534         |
| stats_o/mean                   | 0.20500433  |
| stats_o/std                    | 0.068671115 |
| test/episodes                  | 5350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00112    |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1519356  |
| test/Q_plus_P                  | -1.1519356  |
| test/reward_per_eps            | -9          |
| test/steps                     | 214000      |
| train/episodes                 | 21400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00496    |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 856000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 535        |
| stats_o/mean                   | 0.20500702 |
| stats_o/std                    | 0.06866453 |
| test/episodes                  | 5360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00126   |
| test/info_shaping_reward_mean  | -0.0405    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0061562 |
| test/Q_plus_P                  | -1.0061562 |
| test/reward_per_eps            | -8         |
| test/steps                     | 214400     |
| train/episodes                 | 21440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00417   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 857600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 536        |
| stats_o/mean                   | 0.20500511 |
| stats_o/std                    | 0.06864587 |
| test/episodes                  | 5370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00475   |
| test/info_shaping_reward_mean  | -0.0411    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0484968 |
| test/Q_plus_P                  | -1.0484968 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 214800     |
| train/episodes                 | 21480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00514   |
| train/info_shaping_reward_mean | -0.068     |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 859200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 537        |
| stats_o/mean                   | 0.20500469 |
| stats_o/std                    | 0.06862703 |
| test/episodes                  | 5380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00441   |
| test/info_shaping_reward_mean  | -0.0446    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.0437841 |
| test/Q_plus_P                  | -1.0437841 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 215200     |
| train/episodes                 | 21520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00451   |
| train/info_shaping_reward_mean | -0.066     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 860800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 538        |
| stats_o/mean                   | 0.20499912 |
| stats_o/std                    | 0.06860974 |
| test/episodes                  | 5390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00159   |
| test/info_shaping_reward_mean  | -0.0426    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0552691 |
| test/Q_plus_P                  | -1.0552691 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 215600     |
| train/episodes                 | 21560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.595      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00431   |
| train/info_shaping_reward_mean | -0.0708    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 862400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 539        |
| stats_o/mean                   | 0.20499884 |
| stats_o/std                    | 0.06858748 |
| test/episodes                  | 5400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00156   |
| test/info_shaping_reward_mean  | -0.0415    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0912907 |
| test/Q_plus_P                  | -1.0912907 |
| test/reward_per_eps            | -9         |
| test/steps                     | 216000     |
| train/episodes                 | 21600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00396   |
| train/info_shaping_reward_mean | -0.0645    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 864000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 540         |
| stats_o/mean                   | 0.20500474  |
| stats_o/std                    | 0.068585336 |
| test/episodes                  | 5410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00204    |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0084388  |
| test/Q_plus_P                  | -1.0084388  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 216400      |
| train/episodes                 | 21640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0044     |
| train/info_shaping_reward_mean | -0.0727     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 865600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.20500433  |
| stats_o/std                    | 0.06859798  |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.0396     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.98095465 |
| test/Q_plus_P                  | -0.98095465 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00418    |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 542        |
| stats_o/mean                   | 0.20499474 |
| stats_o/std                    | 0.06862134 |
| test/episodes                  | 5430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00413   |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0443515 |
| test/Q_plus_P                  | -1.0443515 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 217200     |
| train/episodes                 | 21720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00347   |
| train/info_shaping_reward_mean | -0.0768    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 868800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 543         |
| stats_o/mean                   | 0.20499732  |
| stats_o/std                    | 0.068597265 |
| test/episodes                  | 5440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0419     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0144434  |
| test/Q_plus_P                  | -1.0144434  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 217600      |
| train/episodes                 | 21760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00448    |
| train/info_shaping_reward_mean | -0.0656     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 870400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 544        |
| stats_o/mean                   | 0.20499599 |
| stats_o/std                    | 0.06861011 |
| test/episodes                  | 5450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00244   |
| test/info_shaping_reward_mean  | -0.0408    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0598937 |
| test/Q_plus_P                  | -1.0598937 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 218000     |
| train/episodes                 | 21800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00408   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 872000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 545        |
| stats_o/mean                   | 0.20499425 |
| stats_o/std                    | 0.06857456 |
| test/episodes                  | 5460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00114   |
| test/info_shaping_reward_mean  | -0.0401    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0534732 |
| test/Q_plus_P                  | -1.0534732 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 218400     |
| train/episodes                 | 21840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0041    |
| train/info_shaping_reward_mean | -0.0615    |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 873600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 546        |
| stats_o/mean                   | 0.20498644 |
| stats_o/std                    | 0.06857415 |
| test/episodes                  | 5470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00172   |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -0.9972384 |
| test/Q_plus_P                  | -0.9972384 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 218800     |
| train/episodes                 | 21880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00569   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 875200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 547        |
| stats_o/mean                   | 0.20498478 |
| stats_o/std                    | 0.0685561  |
| test/episodes                  | 5480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.0421    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1539605 |
| test/Q_plus_P                  | -1.1539605 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 219200     |
| train/episodes                 | 21920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00354   |
| train/info_shaping_reward_mean | -0.0778    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 876800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 548        |
| stats_o/mean                   | 0.20497951 |
| stats_o/std                    | 0.06852759 |
| test/episodes                  | 5490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00289   |
| test/info_shaping_reward_mean  | -0.0412    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9995085 |
| test/Q_plus_P                  | -0.9995085 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 219600     |
| train/episodes                 | 21960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.643      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00369   |
| train/info_shaping_reward_mean | -0.0631    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 878400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 549        |
| stats_o/mean                   | 0.20498411 |
| stats_o/std                    | 0.06850199 |
| test/episodes                  | 5500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00137   |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0018102 |
| test/Q_plus_P                  | -1.0018102 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 220000     |
| train/episodes                 | 22000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00444   |
| train/info_shaping_reward_mean | -0.065     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 880000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 550         |
| stats_o/mean                   | 0.20498534  |
| stats_o/std                    | 0.068485446 |
| test/episodes                  | 5510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00405    |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0337123  |
| test/Q_plus_P                  | -1.0337123  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 220400      |
| train/episodes                 | 22040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00442    |
| train/info_shaping_reward_mean | -0.0698     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 881600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 551        |
| stats_o/mean                   | 0.20498896 |
| stats_o/std                    | 0.06849618 |
| test/episodes                  | 5520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000496  |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.173021  |
| test/Q_plus_P                  | -1.173021  |
| test/reward_per_eps            | -10        |
| test/steps                     | 220800     |
| train/episodes                 | 22080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00354   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 883200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 552         |
| stats_o/mean                   | 0.20498358  |
| stats_o/std                    | 0.06848049  |
| test/episodes                  | 5530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00305    |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.96671754 |
| test/Q_plus_P                  | -0.96671754 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 221200      |
| train/episodes                 | 22120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0735     |
| train/info_shaping_reward_min  | -0.21       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 884800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 553        |
| stats_o/mean                   | 0.2049861  |
| stats_o/std                    | 0.06850436 |
| test/episodes                  | 5540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00155   |
| test/info_shaping_reward_mean  | -0.0396    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -0.9705648 |
| test/Q_plus_P                  | -0.9705648 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 221600     |
| train/episodes                 | 22160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00308   |
| train/info_shaping_reward_mean | -0.0786    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 886400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 554         |
| stats_o/mean                   | 0.20498037  |
| stats_o/std                    | 0.06849477  |
| test/episodes                  | 5550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00326    |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.98999023 |
| test/Q_plus_P                  | -0.98999023 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 222000      |
| train/episodes                 | 22200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00349    |
| train/info_shaping_reward_mean | -0.0665     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 888000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 555         |
| stats_o/mean                   | 0.20497964  |
| stats_o/std                    | 0.068456076 |
| test/episodes                  | 5560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00322    |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.993985   |
| test/Q_plus_P                  | -0.993985   |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 222400      |
| train/episodes                 | 22240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00368    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 889600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 556        |
| stats_o/mean                   | 0.20497583 |
| stats_o/std                    | 0.06843881 |
| test/episodes                  | 5570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00224   |
| test/info_shaping_reward_mean  | -0.041     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0294799 |
| test/Q_plus_P                  | -1.0294799 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 222800     |
| train/episodes                 | 22280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.653      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00339   |
| train/info_shaping_reward_mean | -0.062     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 891200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 557        |
| stats_o/mean                   | 0.20496711 |
| stats_o/std                    | 0.0684386  |
| test/episodes                  | 5580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00306   |
| test/info_shaping_reward_mean  | -0.0417    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0140219 |
| test/Q_plus_P                  | -1.0140219 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 223200     |
| train/episodes                 | 22320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00403   |
| train/info_shaping_reward_mean | -0.0666    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 892800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 558        |
| stats_o/mean                   | 0.20496118 |
| stats_o/std                    | 0.06842548 |
| test/episodes                  | 5590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000552  |
| test/info_shaping_reward_mean  | -0.0407    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0742352 |
| test/Q_plus_P                  | -1.0742352 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 223600     |
| train/episodes                 | 22360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0053    |
| train/info_shaping_reward_mean | -0.0734    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 894400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 559        |
| stats_o/mean                   | 0.20496605 |
| stats_o/std                    | 0.0684099  |
| test/episodes                  | 5600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00166   |
| test/info_shaping_reward_mean  | -0.0432    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.938179  |
| test/Q_plus_P                  | -0.938179  |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 224000     |
| train/episodes                 | 22400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00316   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 896000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 560        |
| stats_o/mean                   | 0.20496178 |
| stats_o/std                    | 0.06838266 |
| test/episodes                  | 5610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0019    |
| test/info_shaping_reward_mean  | -0.0431    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0184251 |
| test/Q_plus_P                  | -1.0184251 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 224400     |
| train/episodes                 | 22440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0045    |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 897600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 561         |
| stats_o/mean                   | 0.20496005  |
| stats_o/std                    | 0.068368174 |
| test/episodes                  | 5620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00558    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0775781  |
| test/Q_plus_P                  | -1.0775781  |
| test/reward_per_eps            | -9          |
| test/steps                     | 224800      |
| train/episodes                 | 22480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00445    |
| train/info_shaping_reward_mean | -0.0689     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 899200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.20495522  |
| stats_o/std                    | 0.068364285 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00358    |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0115714  |
| test/Q_plus_P                  | -1.0115714  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.583       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00468    |
| train/info_shaping_reward_mean | -0.0699     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.20494808  |
| stats_o/std                    | 0.06837143  |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00247    |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.97597796 |
| test/Q_plus_P                  | -0.97597796 |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00454    |
| train/info_shaping_reward_mean | -0.0691     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 564         |
| stats_o/mean                   | 0.20494653  |
| stats_o/std                    | 0.06834705  |
| test/episodes                  | 5650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00474    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.91567415 |
| test/Q_plus_P                  | -0.91567415 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 226000      |
| train/episodes                 | 22600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00435    |
| train/info_shaping_reward_mean | -0.0678     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 904000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 565         |
| stats_o/mean                   | 0.20494837  |
| stats_o/std                    | 0.06832018  |
| test/episodes                  | 5660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000692   |
| test/info_shaping_reward_mean  | -0.0438     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.99312454 |
| test/Q_plus_P                  | -0.99312454 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 226400      |
| train/episodes                 | 22640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00404    |
| train/info_shaping_reward_mean | -0.0641     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 905600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.2049476   |
| stats_o/std                    | 0.068325356 |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00247    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.0507792  |
| test/Q_plus_P                  | -1.0507792  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00512    |
| train/info_shaping_reward_mean | -0.0669     |
| train/info_shaping_reward_min  | -0.194      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 567        |
| stats_o/mean                   | 0.20494667 |
| stats_o/std                    | 0.06831247 |
| test/episodes                  | 5680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00205   |
| test/info_shaping_reward_mean  | -0.0387    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0127878 |
| test/Q_plus_P                  | -1.0127878 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 227200     |
| train/episodes                 | 22720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 908800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.20494272  |
| stats_o/std                    | 0.068280034 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0021     |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0932364  |
| test/Q_plus_P                  | -1.0932364  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00373    |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.20494878  |
| stats_o/std                    | 0.068268284 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00203    |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.9285183  |
| test/Q_plus_P                  | -0.9285183  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00543    |
| train/info_shaping_reward_mean | -0.0751     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 570        |
| stats_o/mean                   | 0.20494613 |
| stats_o/std                    | 0.06825964 |
| test/episodes                  | 5710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0025    |
| test/info_shaping_reward_mean  | -0.0419    |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -0.9970736 |
| test/Q_plus_P                  | -0.9970736 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 228400     |
| train/episodes                 | 22840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00449   |
| train/info_shaping_reward_mean | -0.0686    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 913600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 571        |
| stats_o/mean                   | 0.2049386  |
| stats_o/std                    | 0.06829074 |
| test/episodes                  | 5720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00214   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0119938 |
| test/Q_plus_P                  | -1.0119938 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 228800     |
| train/episodes                 | 22880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.567      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.0794    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 915200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 572        |
| stats_o/mean                   | 0.20493135 |
| stats_o/std                    | 0.06828563 |
| test/episodes                  | 5730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0698082 |
| test/Q_plus_P                  | -1.0698082 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 229200     |
| train/episodes                 | 22920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00516   |
| train/info_shaping_reward_mean | -0.0667    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 916800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 573         |
| stats_o/mean                   | 0.20493242  |
| stats_o/std                    | 0.068268456 |
| test/episodes                  | 5740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000961   |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0053778  |
| test/Q_plus_P                  | -1.0053778  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 229600      |
| train/episodes                 | 22960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0639     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 918400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.20493864  |
| stats_o/std                    | 0.06824286  |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00329    |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -0.94618547 |
| test/Q_plus_P                  | -0.94618547 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00493    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.20494586  |
| stats_o/std                    | 0.06822968  |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00234    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.95693827 |
| test/Q_plus_P                  | -0.95693827 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 576        |
| stats_o/mean                   | 0.20494631 |
| stats_o/std                    | 0.06820748 |
| test/episodes                  | 5770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00214   |
| test/info_shaping_reward_mean  | -0.0407    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9641821 |
| test/Q_plus_P                  | -0.9641821 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 230800     |
| train/episodes                 | 23080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.647      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00479   |
| train/info_shaping_reward_mean | -0.0641    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 923200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 577        |
| stats_o/mean                   | 0.20495419 |
| stats_o/std                    | 0.06819588 |
| test/episodes                  | 5780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00274   |
| test/info_shaping_reward_mean  | -0.0417    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0054004 |
| test/Q_plus_P                  | -1.0054004 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 231200     |
| train/episodes                 | 23120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00494   |
| train/info_shaping_reward_mean | -0.0618    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 924800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.20494871 |
| stats_o/std                    | 0.06820696 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00265   |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0969353 |
| test/Q_plus_P                  | -1.0969353 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00511   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 579         |
| stats_o/mean                   | 0.20495088  |
| stats_o/std                    | 0.06820021  |
| test/episodes                  | 5800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00285    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.96800345 |
| test/Q_plus_P                  | -0.96800345 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 232000      |
| train/episodes                 | 23200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00548    |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 928000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 580        |
| stats_o/mean                   | 0.20494616 |
| stats_o/std                    | 0.06818841 |
| test/episodes                  | 5810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00126   |
| test/info_shaping_reward_mean  | -0.0405    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.8683614 |
| test/Q_plus_P                  | -0.8683614 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 232400     |
| train/episodes                 | 23240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.664      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00354   |
| train/info_shaping_reward_mean | -0.0623    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 929600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 581         |
| stats_o/mean                   | 0.20495197  |
| stats_o/std                    | 0.068190224 |
| test/episodes                  | 5820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00625    |
| test/info_shaping_reward_mean  | -0.0445     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.02002    |
| test/Q_plus_P                  | -1.02002    |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 232800      |
| train/episodes                 | 23280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00449    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 931200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 582        |
| stats_o/mean                   | 0.20495167 |
| stats_o/std                    | 0.06817394 |
| test/episodes                  | 5830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00135   |
| test/info_shaping_reward_mean  | -0.0432    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0125633 |
| test/Q_plus_P                  | -1.0125633 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 233200     |
| train/episodes                 | 23320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0046    |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 932800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 583         |
| stats_o/mean                   | 0.2049588   |
| stats_o/std                    | 0.068167284 |
| test/episodes                  | 5840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00327    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1007068  |
| test/Q_plus_P                  | -1.1007068  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 233600      |
| train/episodes                 | 23360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00479    |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 934400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 584        |
| stats_o/mean                   | 0.2049633  |
| stats_o/std                    | 0.06815033 |
| test/episodes                  | 5850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0022    |
| test/info_shaping_reward_mean  | -0.0424    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0922492 |
| test/Q_plus_P                  | -1.0922492 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 234000     |
| train/episodes                 | 23400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.65       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00359   |
| train/info_shaping_reward_mean | -0.0615    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 936000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 585        |
| stats_o/mean                   | 0.2049683  |
| stats_o/std                    | 0.06815445 |
| test/episodes                  | 5860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00237   |
| test/info_shaping_reward_mean  | -0.0416    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9033996 |
| test/Q_plus_P                  | -0.9033996 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 234400     |
| train/episodes                 | 23440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00522   |
| train/info_shaping_reward_mean | -0.0753    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 937600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.20497303  |
| stats_o/std                    | 0.06814144  |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00392    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.95114136 |
| test/Q_plus_P                  | -0.95114136 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00317    |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 587        |
| stats_o/mean                   | 0.20497125 |
| stats_o/std                    | 0.06811904 |
| test/episodes                  | 5880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000893  |
| test/info_shaping_reward_mean  | -0.0426    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9979882 |
| test/Q_plus_P                  | -0.9979882 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 235200     |
| train/episodes                 | 23520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00443   |
| train/info_shaping_reward_mean | -0.0633    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 940800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 588         |
| stats_o/mean                   | 0.20497186  |
| stats_o/std                    | 0.068085305 |
| test/episodes                  | 5890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00243    |
| test/info_shaping_reward_mean  | -0.043      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1192224  |
| test/Q_plus_P                  | -1.1192224  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 235600      |
| train/episodes                 | 23560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00471    |
| train/info_shaping_reward_mean | -0.0656     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 942400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 589         |
| stats_o/mean                   | 0.20496987  |
| stats_o/std                    | 0.068051495 |
| test/episodes                  | 5900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00465    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0707037  |
| test/Q_plus_P                  | -1.0707037  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 236000      |
| train/episodes                 | 23600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00385    |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 944000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 590        |
| stats_o/mean                   | 0.20496921 |
| stats_o/std                    | 0.06802957 |
| test/episodes                  | 5910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00181   |
| test/info_shaping_reward_mean  | -0.0385    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9392829 |
| test/Q_plus_P                  | -0.9392829 |
| test/reward_per_eps            | -8         |
| test/steps                     | 236400     |
| train/episodes                 | 23640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.653      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00443   |
| train/info_shaping_reward_mean | -0.0633    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 945600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 591        |
| stats_o/mean                   | 0.2049636  |
| stats_o/std                    | 0.06802497 |
| test/episodes                  | 5920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00333   |
| test/info_shaping_reward_mean  | -0.0431    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0375541 |
| test/Q_plus_P                  | -1.0375541 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 236800     |
| train/episodes                 | 23680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00568   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 947200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 592         |
| stats_o/mean                   | 0.20496732  |
| stats_o/std                    | 0.06800545  |
| test/episodes                  | 5930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.041      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.95370257 |
| test/Q_plus_P                  | -0.95370257 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 237200      |
| train/episodes                 | 23720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.62        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00385    |
| train/info_shaping_reward_mean | -0.0663     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 948800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.2049671   |
| stats_o/std                    | 0.067977756 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00214    |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.98098326 |
| test/Q_plus_P                  | -0.98098326 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 594         |
| stats_o/mean                   | 0.20496434  |
| stats_o/std                    | 0.068021595 |
| test/episodes                  | 5950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00179    |
| test/info_shaping_reward_mean  | -0.041      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.9358167  |
| test/Q_plus_P                  | -0.9358167  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 238000      |
| train/episodes                 | 23800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.576       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.079      |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 952000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 595         |
| stats_o/mean                   | 0.20496294  |
| stats_o/std                    | 0.06805635  |
| test/episodes                  | 5960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00206    |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.97784114 |
| test/Q_plus_P                  | -0.97784114 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 238400      |
| train/episodes                 | 23840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.559       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00608    |
| train/info_shaping_reward_mean | -0.0774     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 953600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 596        |
| stats_o/mean                   | 0.2049606  |
| stats_o/std                    | 0.06806799 |
| test/episodes                  | 5970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00314   |
| test/info_shaping_reward_mean  | -0.0446    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1295139 |
| test/Q_plus_P                  | -1.1295139 |
| test/reward_per_eps            | -9         |
| test/steps                     | 238800     |
| train/episodes                 | 23880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00339   |
| train/info_shaping_reward_mean | -0.0768    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 955200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 597        |
| stats_o/mean                   | 0.20496139 |
| stats_o/std                    | 0.06805954 |
| test/episodes                  | 5980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00349   |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -0.9913628 |
| test/Q_plus_P                  | -0.9913628 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 239200     |
| train/episodes                 | 23920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0726    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 956800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.20496431  |
| stats_o/std                    | 0.068086185 |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00158    |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -0.9659923  |
| test/Q_plus_P                  | -0.9659923  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.605       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0729     |
| train/info_shaping_reward_min  | -0.211      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 599         |
| stats_o/mean                   | 0.20496117  |
| stats_o/std                    | 0.068066016 |
| test/episodes                  | 6000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00199    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0959477  |
| test/Q_plus_P                  | -1.0959477  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 240000      |
| train/episodes                 | 24000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00426    |
| train/info_shaping_reward_mean | -0.0673     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 960000      |
------------------------------------------------
Saving latest policy.
