Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPickAndPlace/config_TD3_BC_QFilter/q_filter_True/prm_loss_weight_0.0001/seed_0
-----------------------------------------------
| epoch                          | 0          |
| stats_o/mean                   | 0.20405333 |
| stats_o/std                    | 0.06920753 |
| test/episodes                  | 10         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.116     |
| test/info_shaping_reward_mean  | -0.177     |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -1.5273788 |
| test/Q_plus_P                  | -1.5273788 |
| test/reward_per_eps            | -40        |
| test/steps                     | 400        |
| train/episodes                 | 40         |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00313    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.289     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 1600       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 1          |
| stats_o/mean                   | 0.20292376 |
| stats_o/std                    | 0.07071435 |
| test/episodes                  | 20         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.005      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0398    |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -1.9042718 |
| test/Q_plus_P                  | -1.9042718 |
| test/reward_per_eps            | -39.8      |
| test/steps                     | 800        |
| train/episodes                 | 80         |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 3200       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 2          |
| stats_o/mean                   | 0.20310241 |
| stats_o/std                    | 0.07606036 |
| test/episodes                  | 30         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.141     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -2.2595906 |
| test/Q_plus_P                  | -2.2595906 |
| test/reward_per_eps            | -40        |
| test/steps                     | 1200       |
| train/episodes                 | 120        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 4800       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.20294586 |
| stats_o/std                    | 0.0737665  |
| test/episodes                  | 40         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.114     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.309     |
| test/Q                         | -2.5678797 |
| test/Q_plus_P                  | -2.5678797 |
| test/reward_per_eps            | -40        |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 4          |
| stats_o/mean                   | 0.20152391 |
| stats_o/std                    | 0.07654043 |
| test/episodes                  | 50         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.179     |
| test/info_shaping_reward_min   | -0.224     |
| test/Q                         | -2.9845045 |
| test/Q_plus_P                  | -2.9845045 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2000       |
| train/episodes                 | 200        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 8000       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 5          |
| stats_o/mean                   | 0.20250043 |
| stats_o/std                    | 0.07654659 |
| test/episodes                  | 60         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.372235  |
| test/Q_plus_P                  | -3.372235  |
| test/reward_per_eps            | -40        |
| test/steps                     | 2400       |
| train/episodes                 | 240        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 9600       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 6          |
| stats_o/mean                   | 0.20211051 |
| stats_o/std                    | 0.07812866 |
| test/episodes                  | 70         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.118     |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.228     |
| test/Q                         | -3.7152357 |
| test/Q_plus_P                  | -3.7152357 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2800       |
| train/episodes                 | 280        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 11200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 7          |
| stats_o/mean                   | 0.20213425 |
| stats_o/std                    | 0.07852671 |
| test/episodes                  | 80         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00192   |
| test/info_shaping_reward_mean  | -0.146     |
| test/info_shaping_reward_min   | -0.217     |
| test/Q                         | -4.0087934 |
| test/Q_plus_P                  | -4.0087934 |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 3200       |
| train/episodes                 | 320        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 12800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 8          |
| stats_o/mean                   | 0.20215338 |
| stats_o/std                    | 0.08208622 |
| test/episodes                  | 90         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.111     |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -4.577502  |
| test/Q_plus_P                  | -4.577502  |
| test/reward_per_eps            | -40        |
| test/steps                     | 3600       |
| train/episodes                 | 360        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 14400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 9          |
| stats_o/mean                   | 0.20233428 |
| stats_o/std                    | 0.0823523  |
| test/episodes                  | 100        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0175     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.032     |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -4.9537883 |
| test/Q_plus_P                  | -4.9537883 |
| test/reward_per_eps            | -39.3      |
| test/steps                     | 4000       |
| train/episodes                 | 400        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0075     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 16000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 10         |
| stats_o/mean                   | 0.20273134 |
| stats_o/std                    | 0.08377167 |
| test/episodes                  | 110        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.126     |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.212     |
| test/Q                         | -5.4576483 |
| test/Q_plus_P                  | -5.4576483 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4400       |
| train/episodes                 | 440        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 17600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 11         |
| stats_o/mean                   | 0.20240448 |
| stats_o/std                    | 0.08345142 |
| test/episodes                  | 120        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.02       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0319    |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.332     |
| test/Q                         | -5.828474  |
| test/Q_plus_P                  | -5.828474  |
| test/reward_per_eps            | -39.2      |
| test/steps                     | 4800       |
| train/episodes                 | 480        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 19200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.20274338  |
| stats_o/std                    | 0.085566804 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.138       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0233     |
| test/info_shaping_reward_mean  | -0.144      |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -5.947723   |
| test/Q_plus_P                  | -5.947723   |
| test/reward_per_eps            | -34.5       |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.186      |
| train/info_shaping_reward_min  | -0.272      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 13         |
| stats_o/mean                   | 0.2027555  |
| stats_o/std                    | 0.08624382 |
| test/episodes                  | 140        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0575     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0155    |
| test/info_shaping_reward_mean  | -0.158     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -6.513125  |
| test/Q_plus_P                  | -6.513125  |
| test/reward_per_eps            | -37.7      |
| test/steps                     | 5600       |
| train/episodes                 | 560        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.114     |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 22400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 14         |
| stats_o/mean                   | 0.20239326 |
| stats_o/std                    | 0.08774883 |
| test/episodes                  | 150        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.13      |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -6.9419775 |
| test/Q_plus_P                  | -6.9419775 |
| test/reward_per_eps            | -40        |
| test/steps                     | 6000       |
| train/episodes                 | 600        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.205     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 24000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 15         |
| stats_o/mean                   | 0.20182224 |
| stats_o/std                    | 0.09196097 |
| test/episodes                  | 160        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0575     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00244   |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -7.0815372 |
| test/Q_plus_P                  | -7.0815372 |
| test/reward_per_eps            | -37.7      |
| test/steps                     | 6400       |
| train/episodes                 | 640        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00688    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.118     |
| train/info_shaping_reward_mean | -0.202     |
| train/info_shaping_reward_min  | -0.373     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 25600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 16         |
| stats_o/mean                   | 0.2017909  |
| stats_o/std                    | 0.09263158 |
| test/episodes                  | 170        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.06       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0267    |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.624     |
| test/Q                         | -7.7866554 |
| test/Q_plus_P                  | -7.7866554 |
| test/reward_per_eps            | -37.6      |
| test/steps                     | 6800       |
| train/episodes                 | 680        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0025     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.197     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 27200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 17         |
| stats_o/mean                   | 0.2017701  |
| stats_o/std                    | 0.09331428 |
| test/episodes                  | 180        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.18       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00657   |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.643     |
| test/Q                         | -7.653009  |
| test/Q_plus_P                  | -7.653009  |
| test/reward_per_eps            | -32.8      |
| test/steps                     | 7200       |
| train/episodes                 | 720        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 28800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 18         |
| stats_o/mean                   | 0.20199747 |
| stats_o/std                    | 0.09302034 |
| test/episodes                  | 190        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0113    |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -8.243499  |
| test/Q_plus_P                  | -8.243499  |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 7600       |
| train/episodes                 | 760        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00562    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.291     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 30400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.20175149 |
| stats_o/std                    | 0.09564699 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.115      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0259    |
| test/info_shaping_reward_mean  | -0.149     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -8.345863  |
| test/Q_plus_P                  | -8.345863  |
| test/reward_per_eps            | -35.4      |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0112     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.207     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 20         |
| stats_o/mean                   | 0.20180029 |
| stats_o/std                    | 0.09545176 |
| test/episodes                  | 210        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0675     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0129    |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -8.918556  |
| test/Q_plus_P                  | -8.918556  |
| test/reward_per_eps            | -37.3      |
| test/steps                     | 8400       |
| train/episodes                 | 840        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 33600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 21         |
| stats_o/mean                   | 0.20186453 |
| stats_o/std                    | 0.09520351 |
| test/episodes                  | 220        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.152      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00348   |
| test/info_shaping_reward_mean  | -0.14      |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -8.949571  |
| test/Q_plus_P                  | -8.949571  |
| test/reward_per_eps            | -33.9      |
| test/steps                     | 8800       |
| train/episodes                 | 880        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00125    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.293     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 35200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 22         |
| stats_o/mean                   | 0.20177427 |
| stats_o/std                    | 0.09639877 |
| test/episodes                  | 230        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0575     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0135    |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.509     |
| test/Q                         | -9.542499  |
| test/Q_plus_P                  | -9.542499  |
| test/reward_per_eps            | -37.7      |
| test/steps                     | 9200       |
| train/episodes                 | 920        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.37      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 36800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 23         |
| stats_o/mean                   | 0.20186166 |
| stats_o/std                    | 0.09768864 |
| test/episodes                  | 240        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.235      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00203   |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.507     |
| test/Q                         | -9.412021  |
| test/Q_plus_P                  | -9.412021  |
| test/reward_per_eps            | -30.6      |
| test/steps                     | 9600       |
| train/episodes                 | 960        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00562    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 38400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.20181438  |
| stats_o/std                    | 0.098772615 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.177       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0143     |
| test/info_shaping_reward_mean  | -0.147      |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -9.692029   |
| test/Q_plus_P                  | -9.692029   |
| test/reward_per_eps            | -32.9       |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.00937     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.197      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 25         |
| stats_o/mean                   | 0.20193261 |
| stats_o/std                    | 0.09929675 |
| test/episodes                  | 260        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.188      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00541   |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.574     |
| test/Q                         | -9.913034  |
| test/Q_plus_P                  | -9.913034  |
| test/reward_per_eps            | -32.5      |
| test/steps                     | 10400      |
| train/episodes                 | 1040       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0106     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.265     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 41600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 26         |
| stats_o/mean                   | 0.2019587  |
| stats_o/std                    | 0.09952578 |
| test/episodes                  | 270        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.34       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00775   |
| test/info_shaping_reward_mean  | -0.105     |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -9.453477  |
| test/Q_plus_P                  | -9.453477  |
| test/reward_per_eps            | -26.4      |
| test/steps                     | 10800      |
| train/episodes                 | 1080       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0256     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.114     |
| train/info_shaping_reward_mean | -0.217     |
| train/info_shaping_reward_min  | -0.423     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 43200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 27         |
| stats_o/mean                   | 0.20215507 |
| stats_o/std                    | 0.10014612 |
| test/episodes                  | 280        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.24       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00273   |
| test/info_shaping_reward_mean  | -0.122     |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -9.98346   |
| test/Q_plus_P                  | -9.98346   |
| test/reward_per_eps            | -30.4      |
| test/steps                     | 11200      |
| train/episodes                 | 1120       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0125     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 44800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.201891   |
| stats_o/std                    | 0.10045559 |
| test/episodes                  | 290        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.223      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00385   |
| test/info_shaping_reward_mean  | -0.148     |
| test/info_shaping_reward_min   | -0.333     |
| test/Q                         | -10.592324 |
| test/Q_plus_P                  | -10.592324 |
| test/reward_per_eps            | -31.1      |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.203     |
| train/info_shaping_reward_min  | -0.362     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.20180981 |
| stats_o/std                    | 0.10054719 |
| test/episodes                  | 300        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.297      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00386   |
| test/info_shaping_reward_mean  | -0.122     |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -10.180717 |
| test/Q_plus_P                  | -10.180717 |
| test/reward_per_eps            | -28.1      |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00813    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.122     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 30         |
| stats_o/mean                   | 0.2019573  |
| stats_o/std                    | 0.10144535 |
| test/episodes                  | 310        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.217      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00166   |
| test/info_shaping_reward_mean  | -0.133     |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -10.848911 |
| test/Q_plus_P                  | -10.848911 |
| test/reward_per_eps            | -31.3      |
| test/steps                     | 12400      |
| train/episodes                 | 1240       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00937    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.307     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 49600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 31         |
| stats_o/mean                   | 0.20197445 |
| stats_o/std                    | 0.10217671 |
| test/episodes                  | 320        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.25       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00192   |
| test/info_shaping_reward_mean  | -0.128     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -10.689663 |
| test/Q_plus_P                  | -10.689663 |
| test/reward_per_eps            | -30        |
| test/steps                     | 12800      |
| train/episodes                 | 1280       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00437    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.214     |
| train/info_shaping_reward_min  | -0.37      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 51200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 32         |
| stats_o/mean                   | 0.2021151  |
| stats_o/std                    | 0.10267805 |
| test/episodes                  | 330        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.388      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00363   |
| test/info_shaping_reward_mean  | -0.11      |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -10.31896  |
| test/Q_plus_P                  | -10.31896  |
| test/reward_per_eps            | -24.5      |
| test/steps                     | 13200      |
| train/episodes                 | 1320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.213     |
| train/info_shaping_reward_min  | -0.521     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 52800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 33          |
| stats_o/mean                   | 0.20204721  |
| stats_o/std                    | 0.102672234 |
| test/episodes                  | 340         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.22        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00996    |
| test/info_shaping_reward_mean  | -0.139      |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -11.563096  |
| test/Q_plus_P                  | -11.563096  |
| test/reward_per_eps            | -31.2       |
| test/steps                     | 13600       |
| train/episodes                 | 1360        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0181      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.198      |
| train/info_shaping_reward_min  | -0.29       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 54400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 34          |
| stats_o/mean                   | 0.20192447  |
| stats_o/std                    | 0.103134945 |
| test/episodes                  | 350         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.47        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0036     |
| test/info_shaping_reward_mean  | -0.0909     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -9.41188    |
| test/Q_plus_P                  | -9.41188    |
| test/reward_per_eps            | -21.2       |
| test/steps                     | 14000       |
| train/episodes                 | 1400        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0419      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.116      |
| train/info_shaping_reward_mean | -0.197      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.3       |
| train/steps                    | 56000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.20193142 |
| stats_o/std                    | 0.10442658 |
| test/episodes                  | 360        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.34       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00161   |
| test/info_shaping_reward_mean  | -0.114     |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -10.783621 |
| test/Q_plus_P                  | -10.783621 |
| test/reward_per_eps            | -26.4      |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0275     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0894    |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.489     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 36         |
| stats_o/mean                   | 0.20178163 |
| stats_o/std                    | 0.10576893 |
| test/episodes                  | 370        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.282      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.118     |
| test/info_shaping_reward_min   | -0.213     |
| test/Q                         | -11.503705 |
| test/Q_plus_P                  | -11.503705 |
| test/reward_per_eps            | -28.7      |
| test/steps                     | 14800      |
| train/episodes                 | 1480       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0212     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.087     |
| train/info_shaping_reward_mean | -0.223     |
| train/info_shaping_reward_min  | -0.495     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 59200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 37         |
| stats_o/mean                   | 0.20189345 |
| stats_o/std                    | 0.10671138 |
| test/episodes                  | 380        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.28       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00494   |
| test/info_shaping_reward_mean  | -0.15      |
| test/info_shaping_reward_min   | -0.606     |
| test/Q                         | -11.855477 |
| test/Q_plus_P                  | -11.855477 |
| test/reward_per_eps            | -28.8      |
| test/steps                     | 15200      |
| train/episodes                 | 1520       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0337     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0736    |
| train/info_shaping_reward_mean | -0.222     |
| train/info_shaping_reward_min  | -0.495     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 60800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 38         |
| stats_o/mean                   | 0.2018094  |
| stats_o/std                    | 0.1072632  |
| test/episodes                  | 390        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.158      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00609   |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.555     |
| test/Q                         | -12.877293 |
| test/Q_plus_P                  | -12.877293 |
| test/reward_per_eps            | -33.7      |
| test/steps                     | 15600      |
| train/episodes                 | 1560       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0375     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0945    |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.534     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 62400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 39         |
| stats_o/mean                   | 0.20197631 |
| stats_o/std                    | 0.10777536 |
| test/episodes                  | 400        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.398      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00389   |
| test/info_shaping_reward_mean  | -0.0964    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -10.288831 |
| test/Q_plus_P                  | -10.288831 |
| test/reward_per_eps            | -24.1      |
| test/steps                     | 16000      |
| train/episodes                 | 1600       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.108     |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 64000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 40         |
| stats_o/mean                   | 0.20212013 |
| stats_o/std                    | 0.10849451 |
| test/episodes                  | 410        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.507      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00132   |
| test/info_shaping_reward_mean  | -0.0881    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -9.34466   |
| test/Q_plus_P                  | -9.34466   |
| test/reward_per_eps            | -19.7      |
| test/steps                     | 16400      |
| train/episodes                 | 1640       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.035      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.1       |
| train/info_shaping_reward_mean | -0.194     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 65600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 41         |
| stats_o/mean                   | 0.20219451 |
| stats_o/std                    | 0.10874374 |
| test/episodes                  | 420        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.343      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.106     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -11.777531 |
| test/Q_plus_P                  | -11.777531 |
| test/reward_per_eps            | -26.3      |
| test/steps                     | 16800      |
| train/episodes                 | 1680       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0312     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0904    |
| train/info_shaping_reward_mean | -0.207     |
| train/info_shaping_reward_min  | -0.42      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 67200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 42         |
| stats_o/mean                   | 0.20241384 |
| stats_o/std                    | 0.10944171 |
| test/episodes                  | 430        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.47       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00317   |
| test/info_shaping_reward_mean  | -0.0836    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -9.625002  |
| test/Q_plus_P                  | -9.625002  |
| test/reward_per_eps            | -21.2      |
| test/steps                     | 17200      |
| train/episodes                 | 1720       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0556     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0644    |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 68800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 43         |
| stats_o/mean                   | 0.20248169 |
| stats_o/std                    | 0.10996696 |
| test/episodes                  | 440        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.398      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00181   |
| test/info_shaping_reward_mean  | -0.105     |
| test/info_shaping_reward_min   | -0.206     |
| test/Q                         | -11.324285 |
| test/Q_plus_P                  | -11.324285 |
| test/reward_per_eps            | -24.1      |
| test/steps                     | 17600      |
| train/episodes                 | 1760       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0231     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.106     |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.484     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 70400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 44         |
| stats_o/mean                   | 0.2024193  |
| stats_o/std                    | 0.11034786 |
| test/episodes                  | 450        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.385      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00122   |
| test/info_shaping_reward_mean  | -0.108     |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -11.896782 |
| test/Q_plus_P                  | -11.896782 |
| test/reward_per_eps            | -24.6      |
| test/steps                     | 18000      |
| train/episodes                 | 1800       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0356     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0959    |
| train/info_shaping_reward_mean | -0.201     |
| train/info_shaping_reward_min  | -0.372     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 72000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 45         |
| stats_o/mean                   | 0.20214063 |
| stats_o/std                    | 0.11130109 |
| test/episodes                  | 460        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.547      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0018    |
| test/info_shaping_reward_mean  | -0.0782    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -9.096685  |
| test/Q_plus_P                  | -9.096685  |
| test/reward_per_eps            | -18.1      |
| test/steps                     | 18400      |
| train/episodes                 | 1840       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0375     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0826    |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 73600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 46         |
| stats_o/mean                   | 0.20218638 |
| stats_o/std                    | 0.11254627 |
| test/episodes                  | 470        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.292      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00233   |
| test/info_shaping_reward_mean  | -0.144     |
| test/info_shaping_reward_min   | -0.578     |
| test/Q                         | -12.936368 |
| test/Q_plus_P                  | -12.936368 |
| test/reward_per_eps            | -28.3      |
| test/steps                     | 18800      |
| train/episodes                 | 1880       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0231     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0906    |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.521     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 75200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 47         |
| stats_o/mean                   | 0.20209461 |
| stats_o/std                    | 0.11238415 |
| test/episodes                  | 480        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.297      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00127   |
| test/info_shaping_reward_mean  | -0.135     |
| test/info_shaping_reward_min   | -0.504     |
| test/Q                         | -12.757739 |
| test/Q_plus_P                  | -12.757739 |
| test/reward_per_eps            | -28.1      |
| test/steps                     | 19200      |
| train/episodes                 | 1920       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0325     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.104     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 76800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 48         |
| stats_o/mean                   | 0.20207652 |
| stats_o/std                    | 0.11238357 |
| test/episodes                  | 490        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.458      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000519  |
| test/info_shaping_reward_mean  | -0.0901    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -10.385015 |
| test/Q_plus_P                  | -10.385015 |
| test/reward_per_eps            | -21.7      |
| test/steps                     | 19600      |
| train/episodes                 | 1960       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0363     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.111     |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.406     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 78400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 49          |
| stats_o/mean                   | 0.20203455  |
| stats_o/std                    | 0.112940356 |
| test/episodes                  | 500         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.542       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00306    |
| test/info_shaping_reward_mean  | -0.0799     |
| test/info_shaping_reward_min   | -0.191      |
| test/Q                         | -9.451256   |
| test/Q_plus_P                  | -9.451256   |
| test/reward_per_eps            | -18.3       |
| test/steps                     | 20000       |
| train/episodes                 | 2000        |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.00812     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0956     |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.275      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 80000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 50          |
| stats_o/mean                   | 0.20214689  |
| stats_o/std                    | 0.113433905 |
| test/episodes                  | 510         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.407       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00305    |
| test/info_shaping_reward_mean  | -0.103      |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -10.446643  |
| test/Q_plus_P                  | -10.446643  |
| test/reward_per_eps            | -23.7       |
| test/steps                     | 20400       |
| train/episodes                 | 2040        |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0569      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0767     |
| train/info_shaping_reward_mean | -0.191      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.7       |
| train/steps                    | 81600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.20214196 |
| stats_o/std                    | 0.11387324 |
| test/episodes                  | 520        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.39       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00185   |
| test/info_shaping_reward_mean  | -0.0964    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -11.588022 |
| test/Q_plus_P                  | -11.588022 |
| test/reward_per_eps            | -24.4      |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0575     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.084     |
| train/info_shaping_reward_mean | -0.199     |
| train/info_shaping_reward_min  | -0.464     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.7      |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.20218843 |
| stats_o/std                    | 0.11432436 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.542      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00139   |
| test/info_shaping_reward_mean  | -0.0777    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -8.926128  |
| test/Q_plus_P                  | -8.926128  |
| test/reward_per_eps            | -18.3      |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0856     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0436    |
| train/info_shaping_reward_mean | -0.202     |
| train/info_shaping_reward_min  | -0.459     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.6      |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.20222133  |
| stats_o/std                    | 0.114599854 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.53        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00711    |
| test/info_shaping_reward_mean  | -0.0852     |
| test/info_shaping_reward_min   | -0.2        |
| test/Q                         | -8.731407   |
| test/Q_plus_P                  | -8.731407   |
| test/reward_per_eps            | -18.8       |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0525      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0621     |
| train/info_shaping_reward_mean | -0.187      |
| train/info_shaping_reward_min  | -0.408      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.9       |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 54          |
| stats_o/mean                   | 0.20221472  |
| stats_o/std                    | 0.114867255 |
| test/episodes                  | 550         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.335       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00335    |
| test/info_shaping_reward_mean  | -0.11       |
| test/info_shaping_reward_min   | -0.195      |
| test/Q                         | -12.242124  |
| test/Q_plus_P                  | -12.242124  |
| test/reward_per_eps            | -26.6       |
| test/steps                     | 22000       |
| train/episodes                 | 2200        |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0319      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0714     |
| train/info_shaping_reward_mean | -0.193      |
| train/info_shaping_reward_min  | -0.416      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.7       |
| train/steps                    | 88000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 55         |
| stats_o/mean                   | 0.20225635 |
| stats_o/std                    | 0.11516454 |
| test/episodes                  | 560        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.48       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00332   |
| test/info_shaping_reward_mean  | -0.087     |
| test/info_shaping_reward_min   | -0.209     |
| test/Q                         | -10.790016 |
| test/Q_plus_P                  | -10.790016 |
| test/reward_per_eps            | -20.8      |
| test/steps                     | 22400      |
| train/episodes                 | 2240       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0612     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0674    |
| train/info_shaping_reward_mean | -0.194     |
| train/info_shaping_reward_min  | -0.382     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 89600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 56          |
| stats_o/mean                   | 0.2022041   |
| stats_o/std                    | 0.115614526 |
| test/episodes                  | 570         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.295       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0085     |
| test/info_shaping_reward_mean  | -0.118      |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -12.639449  |
| test/Q_plus_P                  | -12.639449  |
| test/reward_per_eps            | -28.2       |
| test/steps                     | 22800       |
| train/episodes                 | 2280        |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0225      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0675     |
| train/info_shaping_reward_mean | -0.205      |
| train/info_shaping_reward_min  | -0.451      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.1       |
| train/steps                    | 91200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 57         |
| stats_o/mean                   | 0.20226867 |
| stats_o/std                    | 0.11571319 |
| test/episodes                  | 580        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.47       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00134   |
| test/info_shaping_reward_mean  | -0.0964    |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -10.165043 |
| test/Q_plus_P                  | -10.165043 |
| test/reward_per_eps            | -21.2      |
| test/steps                     | 23200      |
| train/episodes                 | 2320       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0606     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0737    |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.561     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.6      |
| train/steps                    | 92800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.2022272   |
| stats_o/std                    | 0.116303235 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.36        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00307    |
| test/info_shaping_reward_mean  | -0.108      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -12.487521  |
| test/Q_plus_P                  | -12.487521  |
| test/reward_per_eps            | -25.6       |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0469      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0715     |
| train/info_shaping_reward_mean | -0.21       |
| train/info_shaping_reward_min  | -0.464      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.1       |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 59         |
| stats_o/mean                   | 0.20191933 |
| stats_o/std                    | 0.11676036 |
| test/episodes                  | 600        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.365      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00221   |
| test/info_shaping_reward_mean  | -0.105     |
| test/info_shaping_reward_min   | -0.206     |
| test/Q                         | -11.965835 |
| test/Q_plus_P                  | -11.965835 |
| test/reward_per_eps            | -25.4      |
| test/steps                     | 24000      |
| train/episodes                 | 2400       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0356     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0671    |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 96000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 60          |
| stats_o/mean                   | 0.20183386  |
| stats_o/std                    | 0.117049664 |
| test/episodes                  | 610         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.482       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0031     |
| test/info_shaping_reward_mean  | -0.0932     |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -10.596469  |
| test/Q_plus_P                  | -10.596469  |
| test/reward_per_eps            | -20.7       |
| test/steps                     | 24400       |
| train/episodes                 | 2440        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0444      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0679     |
| train/info_shaping_reward_mean | -0.189      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.2       |
| train/steps                    | 97600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 61          |
| stats_o/mean                   | 0.20192261  |
| stats_o/std                    | 0.117213205 |
| test/episodes                  | 620         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.407       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00705    |
| test/info_shaping_reward_mean  | -0.0955     |
| test/info_shaping_reward_min   | -0.195      |
| test/Q                         | -10.841348  |
| test/Q_plus_P                  | -10.841348  |
| test/reward_per_eps            | -23.7       |
| test/steps                     | 24800       |
| train/episodes                 | 2480        |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0181      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0805     |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.371      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 99200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 62         |
| stats_o/mean                   | 0.2019352  |
| stats_o/std                    | 0.11735078 |
| test/episodes                  | 630        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.53       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00257   |
| test/info_shaping_reward_mean  | -0.0889    |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -8.542696  |
| test/Q_plus_P                  | -8.542696  |
| test/reward_per_eps            | -18.8      |
| test/steps                     | 25200      |
| train/episodes                 | 2520       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.045      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0661    |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.2      |
| train/steps                    | 100800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 63         |
| stats_o/mean                   | 0.2019605  |
| stats_o/std                    | 0.11759741 |
| test/episodes                  | 640        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.343      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0034    |
| test/info_shaping_reward_mean  | -0.101     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -12.749299 |
| test/Q_plus_P                  | -12.749299 |
| test/reward_per_eps            | -26.3      |
| test/steps                     | 25600      |
| train/episodes                 | 2560       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0331     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0712    |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.312     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 102400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.20197578 |
| stats_o/std                    | 0.11767168 |
| test/episodes                  | 650        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.36       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0018    |
| test/info_shaping_reward_mean  | -0.11      |
| test/info_shaping_reward_min   | -0.223     |
| test/Q                         | -12.314928 |
| test/Q_plus_P                  | -12.314928 |
| test/reward_per_eps            | -25.6      |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0519     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0624    |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.9      |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 65         |
| stats_o/mean                   | 0.20195618 |
| stats_o/std                    | 0.11809657 |
| test/episodes                  | 660        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.468      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00442   |
| test/info_shaping_reward_mean  | -0.0961    |
| test/info_shaping_reward_min   | -0.228     |
| test/Q                         | -10.200643 |
| test/Q_plus_P                  | -10.200643 |
| test/reward_per_eps            | -21.3      |
| test/steps                     | 26400      |
| train/episodes                 | 2640       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0481     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0545    |
| train/info_shaping_reward_mean | -0.212     |
| train/info_shaping_reward_min  | -0.514     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.1      |
| train/steps                    | 105600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 66         |
| stats_o/mean                   | 0.2019815  |
| stats_o/std                    | 0.11852965 |
| test/episodes                  | 670        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.412      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00325   |
| test/info_shaping_reward_mean  | -0.103     |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -10.920005 |
| test/Q_plus_P                  | -10.920005 |
| test/reward_per_eps            | -23.5      |
| test/steps                     | 26800      |
| train/episodes                 | 2680       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00375    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0965    |
| train/info_shaping_reward_mean | -0.206     |
| train/info_shaping_reward_min  | -0.443     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 107200     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 67        |
| stats_o/mean                   | 0.2019978 |
| stats_o/std                    | 0.1187778 |
| test/episodes                  | 680       |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.445     |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.00159  |
| test/info_shaping_reward_mean  | -0.0901   |
| test/info_shaping_reward_min   | -0.232    |
| test/Q                         | -9.964326 |
| test/Q_plus_P                  | -9.964326 |
| test/reward_per_eps            | -22.2     |
| test/steps                     | 27200     |
| train/episodes                 | 2720      |
| train/info_is_success_max      | 0.7       |
| train/info_is_success_mean     | 0.0581    |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.0422   |
| train/info_shaping_reward_mean | -0.171    |
| train/info_shaping_reward_min  | -0.325    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -37.7     |
| train/steps                    | 108800    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 68         |
| stats_o/mean                   | 0.20207635 |
| stats_o/std                    | 0.1192431  |
| test/episodes                  | 690        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.44       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00495   |
| test/info_shaping_reward_mean  | -0.0996    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -9.971209  |
| test/Q_plus_P                  | -9.971209  |
| test/reward_per_eps            | -22.4      |
| test/steps                     | 27600      |
| train/episodes                 | 2760       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0681     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.071     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.3       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.3      |
| train/steps                    | 110400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 69          |
| stats_o/mean                   | 0.20210138  |
| stats_o/std                    | 0.119847305 |
| test/episodes                  | 700         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.42        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0985     |
| test/info_shaping_reward_min   | -0.214      |
| test/Q                         | -10.406247  |
| test/Q_plus_P                  | -10.406247  |
| test/reward_per_eps            | -23.2       |
| test/steps                     | 28000       |
| train/episodes                 | 2800        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.0544      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0404     |
| train/info_shaping_reward_mean | -0.208      |
| train/info_shaping_reward_min  | -0.573      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.8       |
| train/steps                    | 112000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.20205967 |
| stats_o/std                    | 0.11989984 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.542      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00183   |
| test/info_shaping_reward_mean  | -0.0819    |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -7.629261  |
| test/Q_plus_P                  | -7.629261  |
| test/reward_per_eps            | -18.3      |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0462     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0682    |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.299     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.1      |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 71          |
| stats_o/mean                   | 0.20212585  |
| stats_o/std                    | 0.120401576 |
| test/episodes                  | 720         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.39        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00188    |
| test/info_shaping_reward_mean  | -0.11       |
| test/info_shaping_reward_min   | -0.594      |
| test/Q                         | -12.419949  |
| test/Q_plus_P                  | -12.419949  |
| test/reward_per_eps            | -24.4       |
| test/steps                     | 28800       |
| train/episodes                 | 2880        |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0219      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0788     |
| train/info_shaping_reward_mean | -0.2        |
| train/info_shaping_reward_min  | -0.39       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.1       |
| train/steps                    | 115200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 72         |
| stats_o/mean                   | 0.20211595 |
| stats_o/std                    | 0.12034666 |
| test/episodes                  | 730        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.415      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000842  |
| test/info_shaping_reward_mean  | -0.0949    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -10.226856 |
| test/Q_plus_P                  | -10.226856 |
| test/reward_per_eps            | -23.4      |
| test/steps                     | 29200      |
| train/episodes                 | 2920       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0556     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0497    |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 116800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.2021148  |
| stats_o/std                    | 0.12026312 |
| test/episodes                  | 740        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.492      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00121   |
| test/info_shaping_reward_mean  | -0.0932    |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -8.516956  |
| test/Q_plus_P                  | -8.516956  |
| test/reward_per_eps            | -20.3      |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0406     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0516    |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 74         |
| stats_o/mean                   | 0.20216572 |
| stats_o/std                    | 0.12005565 |
| test/episodes                  | 750        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.458      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00241   |
| test/info_shaping_reward_mean  | -0.0926    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -9.336998  |
| test/Q_plus_P                  | -9.336998  |
| test/reward_per_eps            | -21.7      |
| test/steps                     | 30000      |
| train/episodes                 | 3000       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0412     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.104     |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.354     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 120000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 75         |
| stats_o/mean                   | 0.20212506 |
| stats_o/std                    | 0.1200025  |
| test/episodes                  | 760        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.375      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.103     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -11.024632 |
| test/Q_plus_P                  | -11.024632 |
| test/reward_per_eps            | -25        |
| test/steps                     | 30400      |
| train/episodes                 | 3040       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00625    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.113     |
| train/info_shaping_reward_mean | -0.199     |
| train/info_shaping_reward_min  | -0.385     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 121600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 76         |
| stats_o/mean                   | 0.20212941 |
| stats_o/std                    | 0.11996105 |
| test/episodes                  | 770        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.477      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000709  |
| test/info_shaping_reward_mean  | -0.0907    |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -7.9211535 |
| test/Q_plus_P                  | -7.9211535 |
| test/reward_per_eps            | -20.9      |
| test/steps                     | 30800      |
| train/episodes                 | 3080       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0175     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0683    |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 123200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 77         |
| stats_o/mean                   | 0.20210245 |
| stats_o/std                    | 0.1199443  |
| test/episodes                  | 780        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.448      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000531  |
| test/info_shaping_reward_mean  | -0.0903    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -8.050279  |
| test/Q_plus_P                  | -8.050279  |
| test/reward_per_eps            | -22.1      |
| test/steps                     | 31200      |
| train/episodes                 | 3120       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.108     |
| train/info_shaping_reward_mean | -0.205     |
| train/info_shaping_reward_min  | -0.473     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 124800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 78         |
| stats_o/mean                   | 0.20211023 |
| stats_o/std                    | 0.11988089 |
| test/episodes                  | 790        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.253      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.117     |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -13.92656  |
| test/Q_plus_P                  | -13.92656  |
| test/reward_per_eps            | -29.9      |
| test/steps                     | 31600      |
| train/episodes                 | 3160       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0269     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0602    |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.372     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 126400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 79          |
| stats_o/mean                   | 0.20207733  |
| stats_o/std                    | 0.119912416 |
| test/episodes                  | 800         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.378       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00469    |
| test/info_shaping_reward_mean  | -0.104      |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -8.589128   |
| test/Q_plus_P                  | -8.589128   |
| test/reward_per_eps            | -24.9       |
| test/steps                     | 32000       |
| train/episodes                 | 3200        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0294      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0572     |
| train/info_shaping_reward_mean | -0.18       |
| train/info_shaping_reward_min  | -0.407      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.8       |
| train/steps                    | 128000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.20206328  |
| stats_o/std                    | 0.119908854 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.445       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000894   |
| test/info_shaping_reward_mean  | -0.0915     |
| test/info_shaping_reward_min   | -0.199      |
| test/Q                         | -8.409086   |
| test/Q_plus_P                  | -8.409086   |
| test/reward_per_eps            | -22.2       |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.02        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.063      |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.372      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 81         |
| stats_o/mean                   | 0.20208313 |
| stats_o/std                    | 0.11971832 |
| test/episodes                  | 820        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.43       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.0937    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -8.058958  |
| test/Q_plus_P                  | -8.058958  |
| test/reward_per_eps            | -22.8      |
| test/steps                     | 32800      |
| train/episodes                 | 3280       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0281     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0521    |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.415     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 131200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.20219614  |
| stats_o/std                    | 0.119886875 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.545       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.0809     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -6.319612   |
| test/Q_plus_P                  | -6.319612   |
| test/reward_per_eps            | -18.2       |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.0312      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0413     |
| train/info_shaping_reward_mean | -0.188      |
| train/info_shaping_reward_min  | -0.408      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.8       |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 83         |
| stats_o/mean                   | 0.20230635 |
| stats_o/std                    | 0.11990548 |
| test/episodes                  | 840        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.453      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00142   |
| test/info_shaping_reward_mean  | -0.0838    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -6.974867  |
| test/Q_plus_P                  | -6.974867  |
| test/reward_per_eps            | -21.9      |
| test/steps                     | 33600      |
| train/episodes                 | 3360       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0531     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0536    |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.282     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.9      |
| train/steps                    | 134400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 84         |
| stats_o/mean                   | 0.2023526  |
| stats_o/std                    | 0.11978769 |
| test/episodes                  | 850        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.393      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0021    |
| test/info_shaping_reward_mean  | -0.0966    |
| test/info_shaping_reward_min   | -0.217     |
| test/Q                         | -9.380649  |
| test/Q_plus_P                  | -9.380649  |
| test/reward_per_eps            | -24.3      |
| test/steps                     | 34000      |
| train/episodes                 | 3400       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0462     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0529    |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.378     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.1      |
| train/steps                    | 136000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 85         |
| stats_o/mean                   | 0.20244375 |
| stats_o/std                    | 0.11979188 |
| test/episodes                  | 860        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.522      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00201   |
| test/info_shaping_reward_mean  | -0.0812    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -6.3841567 |
| test/Q_plus_P                  | -6.3841567 |
| test/reward_per_eps            | -19.1      |
| test/steps                     | 34400      |
| train/episodes                 | 3440       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0563     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0403    |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.287     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 137600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 86          |
| stats_o/mean                   | 0.20256184  |
| stats_o/std                    | 0.120054424 |
| test/episodes                  | 870         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.4         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00131    |
| test/info_shaping_reward_mean  | -0.103      |
| test/info_shaping_reward_min   | -0.202      |
| test/Q                         | -9.390658   |
| test/Q_plus_P                  | -9.390658   |
| test/reward_per_eps            | -24         |
| test/steps                     | 34800       |
| train/episodes                 | 3480        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0269      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0644     |
| train/info_shaping_reward_mean | -0.192      |
| train/info_shaping_reward_min  | -0.414      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.9       |
| train/steps                    | 139200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 87         |
| stats_o/mean                   | 0.20269202 |
| stats_o/std                    | 0.12002138 |
| test/episodes                  | 880        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.515      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00142   |
| test/info_shaping_reward_mean  | -0.0824    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -6.2967663 |
| test/Q_plus_P                  | -6.2967663 |
| test/reward_per_eps            | -19.4      |
| test/steps                     | 35200      |
| train/episodes                 | 3520       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.0569     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0349    |
| train/info_shaping_reward_mean | -0.162     |
| train/info_shaping_reward_min  | -0.297     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.7      |
| train/steps                    | 140800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 88         |
| stats_o/mean                   | 0.20273657 |
| stats_o/std                    | 0.12013331 |
| test/episodes                  | 890        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.325      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00295   |
| test/info_shaping_reward_mean  | -0.103     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -9.268039  |
| test/Q_plus_P                  | -9.268039  |
| test/reward_per_eps            | -27        |
| test/steps                     | 35600      |
| train/episodes                 | 3560       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0656     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0378    |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.412     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.4      |
| train/steps                    | 142400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 89          |
| stats_o/mean                   | 0.20275806  |
| stats_o/std                    | 0.120133735 |
| test/episodes                  | 900         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.385       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00439    |
| test/info_shaping_reward_mean  | -0.0985     |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -7.5484767  |
| test/Q_plus_P                  | -7.5484767  |
| test/reward_per_eps            | -24.6       |
| test/steps                     | 36000       |
| train/episodes                 | 3600        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.0794      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0349     |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.408      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.8       |
| train/steps                    | 144000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 90         |
| stats_o/mean                   | 0.2028152  |
| stats_o/std                    | 0.12006282 |
| test/episodes                  | 910        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.495      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000626  |
| test/info_shaping_reward_mean  | -0.0866    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -6.2598033 |
| test/Q_plus_P                  | -6.2598033 |
| test/reward_per_eps            | -20.2      |
| test/steps                     | 36400      |
| train/episodes                 | 3640       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0919     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0334    |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.4       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.3      |
| train/steps                    | 145600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 91         |
| stats_o/mean                   | 0.20277023 |
| stats_o/std                    | 0.12009065 |
| test/episodes                  | 920        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.56       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00216   |
| test/info_shaping_reward_mean  | -0.075     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -5.5709143 |
| test/Q_plus_P                  | -5.5709143 |
| test/reward_per_eps            | -17.6      |
| test/steps                     | 36800      |
| train/episodes                 | 3680       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.0731     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0369    |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.374     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.1      |
| train/steps                    | 147200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 92         |
| stats_o/mean                   | 0.20284757 |
| stats_o/std                    | 0.12006699 |
| test/episodes                  | 930        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.547      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0026    |
| test/info_shaping_reward_mean  | -0.0802    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -6.022447  |
| test/Q_plus_P                  | -6.022447  |
| test/reward_per_eps            | -18.1      |
| test/steps                     | 37200      |
| train/episodes                 | 3720       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.0744     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0307    |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37        |
| train/steps                    | 148800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.20279428  |
| stats_o/std                    | 0.119883604 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.52        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000842   |
| test/info_shaping_reward_mean  | -0.0779     |
| test/info_shaping_reward_min   | -0.19       |
| test/Q                         | -6.108003   |
| test/Q_plus_P                  | -6.108003   |
| test/reward_per_eps            | -19.2       |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.135       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.022      |
| train/info_shaping_reward_mean | -0.137      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.6       |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 94         |
| stats_o/mean                   | 0.20281485 |
| stats_o/std                    | 0.11984471 |
| test/episodes                  | 950        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.552      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00132   |
| test/info_shaping_reward_mean  | -0.0777    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -5.6633306 |
| test/Q_plus_P                  | -5.6633306 |
| test/reward_per_eps            | -17.9      |
| test/steps                     | 38000      |
| train/episodes                 | 3800       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.095      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0307    |
| train/info_shaping_reward_mean | -0.164     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.2      |
| train/steps                    | 152000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 95         |
| stats_o/mean                   | 0.20285988 |
| stats_o/std                    | 0.11959805 |
| test/episodes                  | 960        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.535      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00242   |
| test/info_shaping_reward_mean  | -0.0795    |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -5.2063484 |
| test/Q_plus_P                  | -5.2063484 |
| test/reward_per_eps            | -18.6      |
| test/steps                     | 38400      |
| train/episodes                 | 3840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.148      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.012     |
| train/info_shaping_reward_mean | -0.146     |
| train/info_shaping_reward_min  | -0.292     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.1      |
| train/steps                    | 153600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 96         |
| stats_o/mean                   | 0.20295997 |
| stats_o/std                    | 0.11956341 |
| test/episodes                  | 970        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.58       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000722  |
| test/info_shaping_reward_mean  | -0.0734    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -4.597526  |
| test/Q_plus_P                  | -4.597526  |
| test/reward_per_eps            | -16.8      |
| test/steps                     | 38800      |
| train/episodes                 | 3880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.161      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0112    |
| train/info_shaping_reward_mean | -0.144     |
| train/info_shaping_reward_min  | -0.386     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.5      |
| train/steps                    | 155200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 97          |
| stats_o/mean                   | 0.20292494  |
| stats_o/std                    | 0.119497105 |
| test/episodes                  | 980         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.443       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00201    |
| test/info_shaping_reward_mean  | -0.088      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -7.216923   |
| test/Q_plus_P                  | -7.216923   |
| test/reward_per_eps            | -22.3       |
| test/steps                     | 39200       |
| train/episodes                 | 3920        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.133       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0192     |
| train/info_shaping_reward_mean | -0.146      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.7       |
| train/steps                    | 156800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 98         |
| stats_o/mean                   | 0.2030118  |
| stats_o/std                    | 0.11937496 |
| test/episodes                  | 990        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.562      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000572  |
| test/info_shaping_reward_mean  | -0.0694    |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -4.8352118 |
| test/Q_plus_P                  | -4.8352118 |
| test/reward_per_eps            | -17.5      |
| test/steps                     | 39600      |
| train/episodes                 | 3960       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.133      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0189    |
| train/info_shaping_reward_mean | -0.146     |
| train/info_shaping_reward_min  | -0.283     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.7      |
| train/steps                    | 158400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.2031024  |
| stats_o/std                    | 0.11925251 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.482      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000914  |
| test/info_shaping_reward_mean  | -0.0855    |
| test/info_shaping_reward_min   | -0.217     |
| test/Q                         | -5.761136  |
| test/Q_plus_P                  | -5.761136  |
| test/reward_per_eps            | -20.7      |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.113      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0198    |
| train/info_shaping_reward_mean | -0.152     |
| train/info_shaping_reward_min  | -0.288     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.5      |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 100         |
| stats_o/mean                   | 0.2031872   |
| stats_o/std                    | 0.119193524 |
| test/episodes                  | 1010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.57        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0014     |
| test/info_shaping_reward_mean  | -0.074      |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -4.960737   |
| test/Q_plus_P                  | -4.960737   |
| test/reward_per_eps            | -17.2       |
| test/steps                     | 40400       |
| train/episodes                 | 4040        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.137       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0239     |
| train/info_shaping_reward_mean | -0.157      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.5       |
| train/steps                    | 161600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 101        |
| stats_o/mean                   | 0.20316184 |
| stats_o/std                    | 0.11918436 |
| test/episodes                  | 1020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.57       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00114   |
| test/info_shaping_reward_mean  | -0.0713    |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -4.652423  |
| test/Q_plus_P                  | -4.652423  |
| test/reward_per_eps            | -17.2      |
| test/steps                     | 40800      |
| train/episodes                 | 4080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.151      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0169    |
| train/info_shaping_reward_mean | -0.16      |
| train/info_shaping_reward_min  | -0.381     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34        |
| train/steps                    | 163200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 102        |
| stats_o/mean                   | 0.20313735 |
| stats_o/std                    | 0.11907095 |
| test/episodes                  | 1030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.547      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000652  |
| test/info_shaping_reward_mean  | -0.0715    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -4.7810616 |
| test/Q_plus_P                  | -4.7810616 |
| test/reward_per_eps            | -18.1      |
| test/steps                     | 41200      |
| train/episodes                 | 4120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.158      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0156    |
| train/info_shaping_reward_mean | -0.144     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.7      |
| train/steps                    | 164800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 103         |
| stats_o/mean                   | 0.2030992   |
| stats_o/std                    | 0.118904956 |
| test/episodes                  | 1040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.6         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00286    |
| test/info_shaping_reward_mean  | -0.0756     |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -4.38594    |
| test/Q_plus_P                  | -4.38594    |
| test/reward_per_eps            | -16         |
| test/steps                     | 41600       |
| train/episodes                 | 4160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.154       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0146     |
| train/info_shaping_reward_mean | -0.146      |
| train/info_shaping_reward_min  | -0.38       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.8       |
| train/steps                    | 166400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 104        |
| stats_o/mean                   | 0.20322786 |
| stats_o/std                    | 0.11869607 |
| test/episodes                  | 1050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.56       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00303   |
| test/info_shaping_reward_mean  | -0.0795    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -4.5283427 |
| test/Q_plus_P                  | -4.5283427 |
| test/reward_per_eps            | -17.6      |
| test/steps                     | 42000      |
| train/episodes                 | 4200       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.183      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0194    |
| train/info_shaping_reward_mean | -0.133     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.7      |
| train/steps                    | 168000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 105        |
| stats_o/mean                   | 0.2033021  |
| stats_o/std                    | 0.1186157  |
| test/episodes                  | 1060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.59       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00419   |
| test/info_shaping_reward_mean  | -0.0743    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -4.9948645 |
| test/Q_plus_P                  | -4.9948645 |
| test/reward_per_eps            | -16.4      |
| test/steps                     | 42400      |
| train/episodes                 | 4240       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0762     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0336    |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.397     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37        |
| train/steps                    | 169600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 106         |
| stats_o/mean                   | 0.20329838  |
| stats_o/std                    | 0.118597984 |
| test/episodes                  | 1070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.552       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00245    |
| test/info_shaping_reward_mean  | -0.0805     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -5.2658787  |
| test/Q_plus_P                  | -5.2658787  |
| test/reward_per_eps            | -17.9       |
| test/steps                     | 42800       |
| train/episodes                 | 4280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.116       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0165     |
| train/info_shaping_reward_mean | -0.167      |
| train/info_shaping_reward_min  | -0.395      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.4       |
| train/steps                    | 171200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 107        |
| stats_o/mean                   | 0.20335007 |
| stats_o/std                    | 0.11854625 |
| test/episodes                  | 1080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.535      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00163   |
| test/info_shaping_reward_mean  | -0.0828    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -5.649087  |
| test/Q_plus_P                  | -5.649087  |
| test/reward_per_eps            | -18.6      |
| test/steps                     | 43200      |
| train/episodes                 | 4320       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.193      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0163    |
| train/info_shaping_reward_mean | -0.129     |
| train/info_shaping_reward_min  | -0.277     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.3      |
| train/steps                    | 172800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 108        |
| stats_o/mean                   | 0.20332025 |
| stats_o/std                    | 0.11854341 |
| test/episodes                  | 1090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.615      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00096   |
| test/info_shaping_reward_mean  | -0.0713    |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -5.4061427 |
| test/Q_plus_P                  | -5.4061427 |
| test/reward_per_eps            | -15.4      |
| test/steps                     | 43600      |
| train/episodes                 | 4360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.129      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0184    |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.415     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.8      |
| train/steps                    | 174400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 109        |
| stats_o/mean                   | 0.20334144 |
| stats_o/std                    | 0.1185052  |
| test/episodes                  | 1100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.487      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00153   |
| test/info_shaping_reward_mean  | -0.0933    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -5.483023  |
| test/Q_plus_P                  | -5.483023  |
| test/reward_per_eps            | -20.5      |
| test/steps                     | 44000      |
| train/episodes                 | 4400       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.128      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0269    |
| train/info_shaping_reward_mean | -0.139     |
| train/info_shaping_reward_min  | -0.301     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.9      |
| train/steps                    | 176000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 110         |
| stats_o/mean                   | 0.2033483   |
| stats_o/std                    | 0.118372485 |
| test/episodes                  | 1110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.598       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00306    |
| test/info_shaping_reward_mean  | -0.0735     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -4.36526    |
| test/Q_plus_P                  | -4.36526    |
| test/reward_per_eps            | -16.1       |
| test/steps                     | 44400       |
| train/episodes                 | 4440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.185       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0158     |
| train/info_shaping_reward_mean | -0.141      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.6       |
| train/steps                    | 177600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 111        |
| stats_o/mean                   | 0.20336574 |
| stats_o/std                    | 0.11832888 |
| test/episodes                  | 1120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.613      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00293   |
| test/info_shaping_reward_mean  | -0.0711    |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -4.176281  |
| test/Q_plus_P                  | -4.176281  |
| test/reward_per_eps            | -15.5      |
| test/steps                     | 44800      |
| train/episodes                 | 4480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.165      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.015     |
| train/info_shaping_reward_mean | -0.15      |
| train/info_shaping_reward_min  | -0.38      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.4      |
| train/steps                    | 179200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 112        |
| stats_o/mean                   | 0.20338795 |
| stats_o/std                    | 0.1182593  |
| test/episodes                  | 1130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.557      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000953  |
| test/info_shaping_reward_mean  | -0.0806    |
| test/info_shaping_reward_min   | -0.228     |
| test/Q                         | -5.0453687 |
| test/Q_plus_P                  | -5.0453687 |
| test/reward_per_eps            | -17.7      |
| test/steps                     | 45200      |
| train/episodes                 | 4520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.162      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0112    |
| train/info_shaping_reward_mean | -0.144     |
| train/info_shaping_reward_min  | -0.302     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.5      |
| train/steps                    | 180800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 113         |
| stats_o/mean                   | 0.20342475  |
| stats_o/std                    | 0.118253544 |
| test/episodes                  | 1140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.598       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00518    |
| test/info_shaping_reward_mean  | -0.0775     |
| test/info_shaping_reward_min   | -0.2        |
| test/Q                         | -4.7918577  |
| test/Q_plus_P                  | -4.7918577  |
| test/reward_per_eps            | -16.1       |
| test/steps                     | 45600       |
| train/episodes                 | 4560        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.139       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0256     |
| train/info_shaping_reward_mean | -0.147      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.4       |
| train/steps                    | 182400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 114        |
| stats_o/mean                   | 0.20344667 |
| stats_o/std                    | 0.11816537 |
| test/episodes                  | 1150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.61       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.0702    |
| test/info_shaping_reward_min   | -0.217     |
| test/Q                         | -3.8569915 |
| test/Q_plus_P                  | -3.8569915 |
| test/reward_per_eps            | -15.6      |
| test/steps                     | 46000      |
| train/episodes                 | 4600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.153      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0152    |
| train/info_shaping_reward_mean | -0.146     |
| train/info_shaping_reward_min  | -0.294     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.9      |
| train/steps                    | 184000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 115        |
| stats_o/mean                   | 0.2034855  |
| stats_o/std                    | 0.11806502 |
| test/episodes                  | 1160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.562      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00656   |
| test/info_shaping_reward_mean  | -0.0844    |
| test/info_shaping_reward_min   | -0.216     |
| test/Q                         | -4.68698   |
| test/Q_plus_P                  | -4.68698   |
| test/reward_per_eps            | -17.5      |
| test/steps                     | 46400      |
| train/episodes                 | 4640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.212      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00949   |
| train/info_shaping_reward_mean | -0.153     |
| train/info_shaping_reward_min  | -0.429     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.5      |
| train/steps                    | 185600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 116        |
| stats_o/mean                   | 0.2034981  |
| stats_o/std                    | 0.11786642 |
| test/episodes                  | 1170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.527      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00173   |
| test/info_shaping_reward_mean  | -0.0768    |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -5.1095114 |
| test/Q_plus_P                  | -5.1095114 |
| test/reward_per_eps            | -18.9      |
| test/steps                     | 46800      |
| train/episodes                 | 4680       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.191      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.015     |
| train/info_shaping_reward_mean | -0.135     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.4      |
| train/steps                    | 187200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 117         |
| stats_o/mean                   | 0.20355125  |
| stats_o/std                    | 0.117875814 |
| test/episodes                  | 1180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.545       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00208    |
| test/info_shaping_reward_mean  | -0.0807     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -5.011294   |
| test/Q_plus_P                  | -5.011294   |
| test/reward_per_eps            | -18.2       |
| test/steps                     | 47200       |
| train/episodes                 | 4720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.217       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0109     |
| train/info_shaping_reward_mean | -0.142      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.3       |
| train/steps                    | 188800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 118        |
| stats_o/mean                   | 0.20355926 |
| stats_o/std                    | 0.11761057 |
| test/episodes                  | 1190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.603      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00134   |
| test/info_shaping_reward_mean  | -0.0716    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -3.6686714 |
| test/Q_plus_P                  | -3.6686714 |
| test/reward_per_eps            | -15.9      |
| test/steps                     | 47600      |
| train/episodes                 | 4760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.176      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0119    |
| train/info_shaping_reward_mean | -0.122     |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33        |
| train/steps                    | 190400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 119         |
| stats_o/mean                   | 0.20360747  |
| stats_o/std                    | 0.117517956 |
| test/episodes                  | 1200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.632       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0701     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -3.9083016  |
| test/Q_plus_P                  | -3.9083016  |
| test/reward_per_eps            | -14.7       |
| test/steps                     | 48000       |
| train/episodes                 | 4800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.21        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0102     |
| train/info_shaping_reward_mean | -0.123      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.6       |
| train/steps                    | 192000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 120        |
| stats_o/mean                   | 0.20360237 |
| stats_o/std                    | 0.11724898 |
| test/episodes                  | 1210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.623      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00173   |
| test/info_shaping_reward_mean  | -0.0702    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -3.6067822 |
| test/Q_plus_P                  | -3.6067822 |
| test/reward_per_eps            | -15.1      |
| test/steps                     | 48400      |
| train/episodes                 | 4840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.237      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00665   |
| train/info_shaping_reward_mean | -0.113     |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.5      |
| train/steps                    | 193600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.20359781 |
| stats_o/std                    | 0.11712721 |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.535      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00228   |
| test/info_shaping_reward_mean  | -0.109     |
| test/info_shaping_reward_min   | -0.526     |
| test/Q                         | -6.934231  |
| test/Q_plus_P                  | -6.934231  |
| test/reward_per_eps            | -18.6      |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.197      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0123    |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.1      |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 122        |
| stats_o/mean                   | 0.20363288 |
| stats_o/std                    | 0.11685616 |
| test/episodes                  | 1230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00184   |
| test/info_shaping_reward_mean  | -0.07      |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -3.8939764 |
| test/Q_plus_P                  | -3.8939764 |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 49200      |
| train/episodes                 | 4920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.216      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0116    |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.4      |
| train/steps                    | 196800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 123        |
| stats_o/mean                   | 0.20361684 |
| stats_o/std                    | 0.11661517 |
| test/episodes                  | 1240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.613      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000654  |
| test/info_shaping_reward_mean  | -0.0694    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -4.186033  |
| test/Q_plus_P                  | -4.186033  |
| test/reward_per_eps            | -15.5      |
| test/steps                     | 49600      |
| train/episodes                 | 4960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.228      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00929   |
| train/info_shaping_reward_mean | -0.118     |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.9      |
| train/steps                    | 198400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 124        |
| stats_o/mean                   | 0.20369862 |
| stats_o/std                    | 0.11640324 |
| test/episodes                  | 1250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.52       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0041    |
| test/info_shaping_reward_mean  | -0.0816    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -5.0212593 |
| test/Q_plus_P                  | -5.0212593 |
| test/reward_per_eps            | -19.2      |
| test/steps                     | 50000      |
| train/episodes                 | 5000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.231      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0141    |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.8      |
| train/steps                    | 200000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 125        |
| stats_o/mean                   | 0.20371239 |
| stats_o/std                    | 0.11615679 |
| test/episodes                  | 1260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00266   |
| test/info_shaping_reward_mean  | -0.0661    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -4.018506  |
| test/Q_plus_P                  | -4.018506  |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 50400      |
| train/episodes                 | 5040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.272      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0123    |
| train/info_shaping_reward_mean | -0.117     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.1      |
| train/steps                    | 201600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 126        |
| stats_o/mean                   | 0.20367722 |
| stats_o/std                    | 0.116016   |
| test/episodes                  | 1270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00192   |
| test/info_shaping_reward_mean  | -0.0681    |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -3.1841772 |
| test/Q_plus_P                  | -3.1841772 |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 50800      |
| train/episodes                 | 5080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.239      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00857   |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.4      |
| train/steps                    | 203200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 127        |
| stats_o/mean                   | 0.20368713 |
| stats_o/std                    | 0.11584492 |
| test/episodes                  | 1280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00112   |
| test/info_shaping_reward_mean  | -0.0698    |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -3.9676313 |
| test/Q_plus_P                  | -3.9676313 |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 51200      |
| train/episodes                 | 5120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.25       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0121    |
| train/info_shaping_reward_mean | -0.124     |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30        |
| train/steps                    | 204800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 128         |
| stats_o/mean                   | 0.20368516  |
| stats_o/std                    | 0.115700796 |
| test/episodes                  | 1290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.62        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000364   |
| test/info_shaping_reward_mean  | -0.0687     |
| test/info_shaping_reward_min   | -0.19       |
| test/Q                         | -3.8492022  |
| test/Q_plus_P                  | -3.8492022  |
| test/reward_per_eps            | -15.2       |
| test/steps                     | 51600       |
| train/episodes                 | 5160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.239       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00924    |
| train/info_shaping_reward_mean | -0.128      |
| train/info_shaping_reward_min  | -0.287      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.4       |
| train/steps                    | 206400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 129        |
| stats_o/mean                   | 0.20368715 |
| stats_o/std                    | 0.11564736 |
| test/episodes                  | 1300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.627      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00224   |
| test/info_shaping_reward_mean  | -0.0689    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -2.9598584 |
| test/Q_plus_P                  | -2.9598584 |
| test/reward_per_eps            | -14.9      |
| test/steps                     | 52000      |
| train/episodes                 | 5200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.266      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.011     |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.4      |
| train/steps                    | 208000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 130         |
| stats_o/mean                   | 0.20368238  |
| stats_o/std                    | 0.115477145 |
| test/episodes                  | 1310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.64        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.065      |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -3.216665   |
| test/Q_plus_P                  | -3.216665   |
| test/reward_per_eps            | -14.4       |
| test/steps                     | 52400       |
| train/episodes                 | 5240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.331       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00726    |
| train/info_shaping_reward_mean | -0.1        |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.8       |
| train/steps                    | 209600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 131         |
| stats_o/mean                   | 0.20365576  |
| stats_o/std                    | 0.115185566 |
| test/episodes                  | 1320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.605       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000657   |
| test/info_shaping_reward_mean  | -0.0715     |
| test/info_shaping_reward_min   | -0.201      |
| test/Q                         | -3.3576496  |
| test/Q_plus_P                  | -3.3576496  |
| test/reward_per_eps            | -15.8       |
| test/steps                     | 52800       |
| train/episodes                 | 5280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.258       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00923    |
| train/info_shaping_reward_mean | -0.11       |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.7       |
| train/steps                    | 211200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 132        |
| stats_o/mean                   | 0.20366421 |
| stats_o/std                    | 0.11504551 |
| test/episodes                  | 1330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.625      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00141   |
| test/info_shaping_reward_mean  | -0.0676    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -3.4627028 |
| test/Q_plus_P                  | -3.4627028 |
| test/reward_per_eps            | -15        |
| test/steps                     | 53200      |
| train/episodes                 | 5320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.236      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0133    |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.6      |
| train/steps                    | 212800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 133        |
| stats_o/mean                   | 0.20365591 |
| stats_o/std                    | 0.11497877 |
| test/episodes                  | 1340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.623      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.0705    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.3735998 |
| test/Q_plus_P                  | -3.3735998 |
| test/reward_per_eps            | -15.1      |
| test/steps                     | 53600      |
| train/episodes                 | 5360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.235      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0102    |
| train/info_shaping_reward_mean | -0.138     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.6      |
| train/steps                    | 214400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 134        |
| stats_o/mean                   | 0.20366426 |
| stats_o/std                    | 0.11483575 |
| test/episodes                  | 1350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.593      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.0732    |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -3.837534  |
| test/Q_plus_P                  | -3.837534  |
| test/reward_per_eps            | -16.3      |
| test/steps                     | 54000      |
| train/episodes                 | 5400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.212      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0114    |
| train/info_shaping_reward_mean | -0.139     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.5      |
| train/steps                    | 216000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 135         |
| stats_o/mean                   | 0.2036598   |
| stats_o/std                    | 0.114613265 |
| test/episodes                  | 1360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.645       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00222    |
| test/info_shaping_reward_mean  | -0.0662     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -3.12553    |
| test/Q_plus_P                  | -3.12553    |
| test/reward_per_eps            | -14.2       |
| test/steps                     | 54400       |
| train/episodes                 | 5440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.236       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0117     |
| train/info_shaping_reward_mean | -0.115      |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.6       |
| train/steps                    | 217600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 136        |
| stats_o/mean                   | 0.20366326 |
| stats_o/std                    | 0.114499   |
| test/episodes                  | 1370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.061     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -2.7363787 |
| test/Q_plus_P                  | -2.7363787 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 54800      |
| train/episodes                 | 5480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.293      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00906   |
| train/info_shaping_reward_mean | -0.12      |
| train/info_shaping_reward_min  | -0.3       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.3      |
| train/steps                    | 219200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 137        |
| stats_o/mean                   | 0.20366585 |
| stats_o/std                    | 0.11431649 |
| test/episodes                  | 1380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.57       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00308   |
| test/info_shaping_reward_mean  | -0.0803    |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -4.9068327 |
| test/Q_plus_P                  | -4.9068327 |
| test/reward_per_eps            | -17.2      |
| test/steps                     | 55200      |
| train/episodes                 | 5520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.25       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00898   |
| train/info_shaping_reward_mean | -0.121     |
| train/info_shaping_reward_min  | -0.276     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30        |
| train/steps                    | 220800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.20368654 |
| stats_o/std                    | 0.11413843 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.64       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00242   |
| test/info_shaping_reward_mean  | -0.0664    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.1365964 |
| test/Q_plus_P                  | -3.1365964 |
| test/reward_per_eps            | -14.4      |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.306      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00884   |
| train/info_shaping_reward_mean | -0.111     |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.8      |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.20368516 |
| stats_o/std                    | 0.11387655 |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.61       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00283   |
| test/info_shaping_reward_mean  | -0.0716    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -3.4814315 |
| test/Q_plus_P                  | -3.4814315 |
| test/reward_per_eps            | -15.6      |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.35       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00765   |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26        |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 140        |
| stats_o/mean                   | 0.20370893 |
| stats_o/std                    | 0.11364971 |
| test/episodes                  | 1410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.64       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00144   |
| test/info_shaping_reward_mean  | -0.0667    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.9891279 |
| test/Q_plus_P                  | -2.9891279 |
| test/reward_per_eps            | -14.4      |
| test/steps                     | 56400      |
| train/episodes                 | 5640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.335      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00854   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.6      |
| train/steps                    | 225600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.20370679 |
| stats_o/std                    | 0.11340599 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000537  |
| test/info_shaping_reward_mean  | -0.0606    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -2.7093933 |
| test/Q_plus_P                  | -2.7093933 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.297      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00938   |
| train/info_shaping_reward_mean | -0.113     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.1      |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 142        |
| stats_o/mean                   | 0.20367317 |
| stats_o/std                    | 0.11326081 |
| test/episodes                  | 1430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.603      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00208   |
| test/info_shaping_reward_mean  | -0.0734    |
| test/info_shaping_reward_min   | -0.211     |
| test/Q                         | -4.874181  |
| test/Q_plus_P                  | -4.874181  |
| test/reward_per_eps            | -15.9      |
| test/steps                     | 57200      |
| train/episodes                 | 5720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.322      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00964   |
| train/info_shaping_reward_mean | -0.106     |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.1      |
| train/steps                    | 228800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 143        |
| stats_o/mean                   | 0.20365115 |
| stats_o/std                    | 0.11310508 |
| test/episodes                  | 1440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00332   |
| test/info_shaping_reward_mean  | -0.0622    |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -3.0693874 |
| test/Q_plus_P                  | -3.0693874 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 57600      |
| train/episodes                 | 5760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.321      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00908   |
| train/info_shaping_reward_mean | -0.119     |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.2      |
| train/steps                    | 230400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 144        |
| stats_o/mean                   | 0.20367268 |
| stats_o/std                    | 0.11286344 |
| test/episodes                  | 1450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.67       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.0615    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.8192806 |
| test/Q_plus_P                  | -2.8192806 |
| test/reward_per_eps            | -13.2      |
| test/steps                     | 58000      |
| train/episodes                 | 5800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.358      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00741   |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.7      |
| train/steps                    | 232000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 145        |
| stats_o/mean                   | 0.20369259 |
| stats_o/std                    | 0.11263714 |
| test/episodes                  | 1460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.004     |
| test/info_shaping_reward_mean  | -0.0635    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.7492476 |
| test/Q_plus_P                  | -2.7492476 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 58400      |
| train/episodes                 | 5840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.301      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0121    |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.9      |
| train/steps                    | 233600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 146        |
| stats_o/mean                   | 0.20367528 |
| stats_o/std                    | 0.11252494 |
| test/episodes                  | 1470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.625      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00176   |
| test/info_shaping_reward_mean  | -0.0684    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -3.294839  |
| test/Q_plus_P                  | -3.294839  |
| test/reward_per_eps            | -15        |
| test/steps                     | 58800      |
| train/episodes                 | 5880       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.223      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0216    |
| train/info_shaping_reward_mean | -0.132     |
| train/info_shaping_reward_min  | -0.291     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.1      |
| train/steps                    | 235200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 147        |
| stats_o/mean                   | 0.20366156 |
| stats_o/std                    | 0.11238826 |
| test/episodes                  | 1480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.63       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00431   |
| test/info_shaping_reward_mean  | -0.0676    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -2.8493052 |
| test/Q_plus_P                  | -2.8493052 |
| test/reward_per_eps            | -14.8      |
| test/steps                     | 59200      |
| train/episodes                 | 5920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.239      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0109    |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.298     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.4      |
| train/steps                    | 236800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 148        |
| stats_o/mean                   | 0.20366429 |
| stats_o/std                    | 0.11221467 |
| test/episodes                  | 1490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.618      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00153   |
| test/info_shaping_reward_mean  | -0.069     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -3.0637784 |
| test/Q_plus_P                  | -3.0637784 |
| test/reward_per_eps            | -15.3      |
| test/steps                     | 59600      |
| train/episodes                 | 5960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.284      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00931   |
| train/info_shaping_reward_mean | -0.117     |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.6      |
| train/steps                    | 238400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 149        |
| stats_o/mean                   | 0.20364541 |
| stats_o/std                    | 0.1120226  |
| test/episodes                  | 1500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.585      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000614  |
| test/info_shaping_reward_mean  | -0.0724    |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -3.9017138 |
| test/Q_plus_P                  | -3.9017138 |
| test/reward_per_eps            | -16.6      |
| test/steps                     | 60000      |
| train/episodes                 | 6000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.267      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00934   |
| train/info_shaping_reward_mean | -0.115     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.3      |
| train/steps                    | 240000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 150         |
| stats_o/mean                   | 0.20356914  |
| stats_o/std                    | 0.111924484 |
| test/episodes                  | 1510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.608       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00197    |
| test/info_shaping_reward_mean  | -0.0703     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -3.146872   |
| test/Q_plus_P                  | -3.146872   |
| test/reward_per_eps            | -15.7       |
| test/steps                     | 60400       |
| train/episodes                 | 6040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.316       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00715    |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.4       |
| train/steps                    | 241600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 151        |
| stats_o/mean                   | 0.20352143 |
| stats_o/std                    | 0.11177152 |
| test/episodes                  | 1520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.618      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00318   |
| test/info_shaping_reward_mean  | -0.0714    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -3.3211296 |
| test/Q_plus_P                  | -3.3211296 |
| test/reward_per_eps            | -15.3      |
| test/steps                     | 60800      |
| train/episodes                 | 6080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.271      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0119    |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.1      |
| train/steps                    | 243200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 152        |
| stats_o/mean                   | 0.2035415  |
| stats_o/std                    | 0.11153463 |
| test/episodes                  | 1530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.657      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00415   |
| test/info_shaping_reward_mean  | -0.0634    |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -2.9771087 |
| test/Q_plus_P                  | -2.9771087 |
| test/reward_per_eps            | -13.7      |
| test/steps                     | 61200      |
| train/episodes                 | 6120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.352      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00875   |
| train/info_shaping_reward_mean | -0.0999    |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.9      |
| train/steps                    | 244800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 153         |
| stats_o/mean                   | 0.20352584  |
| stats_o/std                    | 0.111377865 |
| test/episodes                  | 1540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.64        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00306    |
| test/info_shaping_reward_mean  | -0.0669     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -3.325861   |
| test/Q_plus_P                  | -3.325861   |
| test/reward_per_eps            | -14.4       |
| test/steps                     | 61600       |
| train/episodes                 | 6160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.389       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00824    |
| train/info_shaping_reward_mean | -0.102      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.4       |
| train/steps                    | 246400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 154         |
| stats_o/mean                   | 0.20354216  |
| stats_o/std                    | 0.111195095 |
| test/episodes                  | 1550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.627       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000603   |
| test/info_shaping_reward_mean  | -0.0676     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -3.0265343  |
| test/Q_plus_P                  | -3.0265343  |
| test/reward_per_eps            | -14.9       |
| test/steps                     | 62000       |
| train/episodes                 | 6200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.349       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00681    |
| train/info_shaping_reward_mean | -0.108      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26         |
| train/steps                    | 248000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.20356876 |
| stats_o/std                    | 0.11104658 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.59       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00178   |
| test/info_shaping_reward_mean  | -0.0942    |
| test/info_shaping_reward_min   | -0.6       |
| test/Q                         | -4.901137  |
| test/Q_plus_P                  | -4.901137  |
| test/reward_per_eps            | -16.4      |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.317      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00816   |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.3      |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 156         |
| stats_o/mean                   | 0.20355749  |
| stats_o/std                    | 0.110832855 |
| test/episodes                  | 1570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.657       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00348    |
| test/info_shaping_reward_mean  | -0.0629     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -2.6569     |
| test/Q_plus_P                  | -2.6569     |
| test/reward_per_eps            | -13.7       |
| test/steps                     | 62800       |
| train/episodes                 | 6280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.334       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00702    |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.6       |
| train/steps                    | 251200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 157        |
| stats_o/mean                   | 0.20355667 |
| stats_o/std                    | 0.11063481 |
| test/episodes                  | 1580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.63       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000875  |
| test/info_shaping_reward_mean  | -0.0692    |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -3.2554307 |
| test/Q_plus_P                  | -3.2554307 |
| test/reward_per_eps            | -14.8      |
| test/steps                     | 63200      |
| train/episodes                 | 6320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.331      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00709   |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.8      |
| train/steps                    | 252800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.20361745  |
| stats_o/std                    | 0.110508874 |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.662       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0625     |
| test/info_shaping_reward_min   | -0.198      |
| test/Q                         | -2.8646152  |
| test/Q_plus_P                  | -2.8646152  |
| test/reward_per_eps            | -13.5       |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.333       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00994    |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.205      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.7       |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.2036106   |
| stats_o/std                    | 0.110320285 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.645       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0028     |
| test/info_shaping_reward_mean  | -0.0666     |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -2.8460891  |
| test/Q_plus_P                  | -2.8460891  |
| test/reward_per_eps            | -14.2       |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.371       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00521    |
| train/info_shaping_reward_mean | -0.102      |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.1       |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 160         |
| stats_o/mean                   | 0.20360893  |
| stats_o/std                    | 0.110155575 |
| test/episodes                  | 1610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.647       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00244    |
| test/info_shaping_reward_mean  | -0.0671     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -2.6008565  |
| test/Q_plus_P                  | -2.6008565  |
| test/reward_per_eps            | -14.1       |
| test/steps                     | 64400       |
| train/episodes                 | 6440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.333       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00838    |
| train/info_shaping_reward_mean | -0.1        |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.7       |
| train/steps                    | 257600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.20365067  |
| stats_o/std                    | 0.110001214 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.688       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00437    |
| test/info_shaping_reward_mean  | -0.0602     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -2.2549736  |
| test/Q_plus_P                  | -2.2549736  |
| test/reward_per_eps            | -12.5       |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.386       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00638    |
| train/info_shaping_reward_mean | -0.0941     |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.6       |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 162         |
| stats_o/mean                   | 0.20363346  |
| stats_o/std                    | 0.109857224 |
| test/episodes                  | 1630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.67        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00327    |
| test/info_shaping_reward_mean  | -0.0624     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -2.4869623  |
| test/Q_plus_P                  | -2.4869623  |
| test/reward_per_eps            | -13.2       |
| test/steps                     | 65200       |
| train/episodes                 | 6520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.343       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00734    |
| train/info_shaping_reward_mean | -0.1        |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.3       |
| train/steps                    | 260800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 163        |
| stats_o/mean                   | 0.20363088 |
| stats_o/std                    | 0.10972748 |
| test/episodes                  | 1640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.657      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00154   |
| test/info_shaping_reward_mean  | -0.0653    |
| test/info_shaping_reward_min   | -0.222     |
| test/Q                         | -2.7904158 |
| test/Q_plus_P                  | -2.7904158 |
| test/reward_per_eps            | -13.7      |
| test/steps                     | 65600      |
| train/episodes                 | 6560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.352      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00608   |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.9      |
| train/steps                    | 262400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 164         |
| stats_o/mean                   | 0.20364517  |
| stats_o/std                    | 0.109504156 |
| test/episodes                  | 1650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00189    |
| test/info_shaping_reward_mean  | -0.0609     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -2.4544344  |
| test/Q_plus_P                  | -2.4544344  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 66000       |
| train/episodes                 | 6600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.416       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00709    |
| train/info_shaping_reward_mean | -0.0905     |
| train/info_shaping_reward_min  | -0.199      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.4       |
| train/steps                    | 264000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 165         |
| stats_o/mean                   | 0.20364296  |
| stats_o/std                    | 0.109321706 |
| test/episodes                  | 1660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.65        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00322    |
| test/info_shaping_reward_mean  | -0.0657     |
| test/info_shaping_reward_min   | -0.194      |
| test/Q                         | -2.6729026  |
| test/Q_plus_P                  | -2.6729026  |
| test/reward_per_eps            | -14         |
| test/steps                     | 66400       |
| train/episodes                 | 6640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.362       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00633    |
| train/info_shaping_reward_mean | -0.109      |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.5       |
| train/steps                    | 265600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 166         |
| stats_o/mean                   | 0.20362872  |
| stats_o/std                    | 0.109281495 |
| test/episodes                  | 1670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.662       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000827   |
| test/info_shaping_reward_mean  | -0.0612     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -2.2649393  |
| test/Q_plus_P                  | -2.2649393  |
| test/reward_per_eps            | -13.5       |
| test/steps                     | 66800       |
| train/episodes                 | 6680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.364       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00662    |
| train/info_shaping_reward_mean | -0.127      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.4       |
| train/steps                    | 267200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 167         |
| stats_o/mean                   | 0.2036233   |
| stats_o/std                    | 0.109226495 |
| test/episodes                  | 1680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.613       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00381    |
| test/info_shaping_reward_mean  | -0.0743     |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -3.1051865  |
| test/Q_plus_P                  | -3.1051865  |
| test/reward_per_eps            | -15.5       |
| test/steps                     | 67200       |
| train/episodes                 | 6720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.365       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00624    |
| train/info_shaping_reward_mean | -0.113      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.4       |
| train/steps                    | 268800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 168        |
| stats_o/mean                   | 0.20364012 |
| stats_o/std                    | 0.10916483 |
| test/episodes                  | 1690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.608      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00218   |
| test/info_shaping_reward_mean  | -0.069     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.0544937 |
| test/Q_plus_P                  | -3.0544937 |
| test/reward_per_eps            | -15.7      |
| test/steps                     | 67600      |
| train/episodes                 | 6760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.337      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00694   |
| train/info_shaping_reward_mean | -0.114     |
| train/info_shaping_reward_min  | -0.265     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.5      |
| train/steps                    | 270400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.20364355 |
| stats_o/std                    | 0.10897415 |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.58       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0016    |
| test/info_shaping_reward_mean  | -0.0749    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -4.4296765 |
| test/Q_plus_P                  | -4.4296765 |
| test/reward_per_eps            | -16.8      |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.309      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00744   |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.6      |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 170         |
| stats_o/mean                   | 0.20363899  |
| stats_o/std                    | 0.108855665 |
| test/episodes                  | 1710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.632       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00121    |
| test/info_shaping_reward_mean  | -0.0678     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -2.7273688  |
| test/Q_plus_P                  | -2.7273688  |
| test/reward_per_eps            | -14.7       |
| test/steps                     | 68400       |
| train/episodes                 | 6840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.378       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00738    |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.9       |
| train/steps                    | 273600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 171        |
| stats_o/mean                   | 0.20364542 |
| stats_o/std                    | 0.10868661 |
| test/episodes                  | 1720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.605      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0701    |
| test/info_shaping_reward_min   | -0.209     |
| test/Q                         | -3.0936072 |
| test/Q_plus_P                  | -3.0936072 |
| test/reward_per_eps            | -15.8      |
| test/steps                     | 68800      |
| train/episodes                 | 6880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.432      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00461   |
| train/info_shaping_reward_mean | -0.0932    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.7      |
| train/steps                    | 275200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 172         |
| stats_o/mean                   | 0.20364037  |
| stats_o/std                    | 0.108577244 |
| test/episodes                  | 1730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.55        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.0839     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -4.343483   |
| test/Q_plus_P                  | -4.343483   |
| test/reward_per_eps            | -18         |
| test/steps                     | 69200       |
| train/episodes                 | 6920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.383       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00619    |
| train/info_shaping_reward_mean | -0.108      |
| train/info_shaping_reward_min  | -0.276      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.7       |
| train/steps                    | 276800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 173        |
| stats_o/mean                   | 0.20362356 |
| stats_o/std                    | 0.10858756 |
| test/episodes                  | 1740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.63       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00146   |
| test/info_shaping_reward_mean  | -0.0663    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -2.809451  |
| test/Q_plus_P                  | -2.809451  |
| test/reward_per_eps            | -14.8      |
| test/steps                     | 69600      |
| train/episodes                 | 6960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.332      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00673   |
| train/info_shaping_reward_mean | -0.127     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.7      |
| train/steps                    | 278400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 174        |
| stats_o/mean                   | 0.20362343 |
| stats_o/std                    | 0.10846268 |
| test/episodes                  | 1750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.655      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00126   |
| test/info_shaping_reward_mean  | -0.0669    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.6780481 |
| test/Q_plus_P                  | -2.6780481 |
| test/reward_per_eps            | -13.8      |
| test/steps                     | 70000      |
| train/episodes                 | 7000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.396      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0068    |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.2      |
| train/steps                    | 280000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 175         |
| stats_o/mean                   | 0.20360844  |
| stats_o/std                    | 0.108412065 |
| test/episodes                  | 1760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.637       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00203    |
| test/info_shaping_reward_mean  | -0.0657     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -3.336217   |
| test/Q_plus_P                  | -3.336217   |
| test/reward_per_eps            | -14.5       |
| test/steps                     | 70400       |
| train/episodes                 | 7040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.361       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0061     |
| train/info_shaping_reward_mean | -0.109      |
| train/info_shaping_reward_min  | -0.275      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.6       |
| train/steps                    | 281600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 176        |
| stats_o/mean                   | 0.20360449 |
| stats_o/std                    | 0.1083105  |
| test/episodes                  | 1770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00196   |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -2.4142456 |
| test/Q_plus_P                  | -2.4142456 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 70800      |
| train/episodes                 | 7080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.379      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00585   |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.9      |
| train/steps                    | 283200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 177        |
| stats_o/mean                   | 0.20359296 |
| stats_o/std                    | 0.10818067 |
| test/episodes                  | 1780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00229   |
| test/info_shaping_reward_mean  | -0.0615    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.5206816 |
| test/Q_plus_P                  | -2.5206816 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 71200      |
| train/episodes                 | 7120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.401      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00511   |
| train/info_shaping_reward_mean | -0.0984    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.9      |
| train/steps                    | 284800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.20357874 |
| stats_o/std                    | 0.10808464 |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00238   |
| test/info_shaping_reward_mean  | -0.0589    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -2.2629616 |
| test/Q_plus_P                  | -2.2629616 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.401      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00538   |
| train/info_shaping_reward_mean | -0.0959    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.9      |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 179         |
| stats_o/mean                   | 0.2035733   |
| stats_o/std                    | 0.108029164 |
| test/episodes                  | 1800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.655       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00167    |
| test/info_shaping_reward_mean  | -0.0616     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -2.5942504  |
| test/Q_plus_P                  | -2.5942504  |
| test/reward_per_eps            | -13.8       |
| test/steps                     | 72000       |
| train/episodes                 | 7200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.389       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00621    |
| train/info_shaping_reward_mean | -0.0989     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.4       |
| train/steps                    | 288000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 180        |
| stats_o/mean                   | 0.20353754 |
| stats_o/std                    | 0.10786929 |
| test/episodes                  | 1810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00295   |
| test/info_shaping_reward_mean  | -0.0635    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.4162426 |
| test/Q_plus_P                  | -2.4162426 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 72400      |
| train/episodes                 | 7240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.424      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00426   |
| train/info_shaping_reward_mean | -0.0884    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.1      |
| train/steps                    | 289600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 181        |
| stats_o/mean                   | 0.20352797 |
| stats_o/std                    | 0.10778229 |
| test/episodes                  | 1820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.665      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00162   |
| test/info_shaping_reward_mean  | -0.0615    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.3872788 |
| test/Q_plus_P                  | -2.3872788 |
| test/reward_per_eps            | -13.4      |
| test/steps                     | 72800      |
| train/episodes                 | 7280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.413      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00702   |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.5      |
| train/steps                    | 291200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 182        |
| stats_o/mean                   | 0.20353189 |
| stats_o/std                    | 0.1076665  |
| test/episodes                  | 1830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00163   |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -1.8738388 |
| test/Q_plus_P                  | -1.8738388 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 73200      |
| train/episodes                 | 7320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.437      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00588   |
| train/info_shaping_reward_mean | -0.0858    |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.5      |
| train/steps                    | 292800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 183         |
| stats_o/mean                   | 0.20352621  |
| stats_o/std                    | 0.107584335 |
| test/episodes                  | 1840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00254    |
| test/info_shaping_reward_mean  | -0.0583     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -2.175555   |
| test/Q_plus_P                  | -2.175555   |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 73600       |
| train/episodes                 | 7360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.459       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00365    |
| train/info_shaping_reward_mean | -0.0892     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.6       |
| train/steps                    | 294400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 184         |
| stats_o/mean                   | 0.20347573  |
| stats_o/std                    | 0.107461445 |
| test/episodes                  | 1850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.688       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000772   |
| test/info_shaping_reward_mean  | -0.0575     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -2.496065   |
| test/Q_plus_P                  | -2.496065   |
| test/reward_per_eps            | -12.5       |
| test/steps                     | 74000       |
| train/episodes                 | 7400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.45        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00608    |
| train/info_shaping_reward_mean | -0.0863     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22         |
| train/steps                    | 296000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 185         |
| stats_o/mean                   | 0.20348908  |
| stats_o/std                    | 0.107368685 |
| test/episodes                  | 1860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.627       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.0918     |
| test/info_shaping_reward_min   | -0.506      |
| test/Q                         | -4.8605366  |
| test/Q_plus_P                  | -4.8605366  |
| test/reward_per_eps            | -14.9       |
| test/steps                     | 74400       |
| train/episodes                 | 7440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.429       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0054     |
| train/info_shaping_reward_mean | -0.103      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.9       |
| train/steps                    | 297600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 186         |
| stats_o/mean                   | 0.20347641  |
| stats_o/std                    | 0.107235424 |
| test/episodes                  | 1870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.667       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00178    |
| test/info_shaping_reward_mean  | -0.0644     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -2.5665123  |
| test/Q_plus_P                  | -2.5665123  |
| test/reward_per_eps            | -13.3       |
| test/steps                     | 74800       |
| train/episodes                 | 7480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.422       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00541    |
| train/info_shaping_reward_mean | -0.0883     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.1       |
| train/steps                    | 299200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.20343538 |
| stats_o/std                    | 0.10715276 |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0016    |
| test/info_shaping_reward_mean  | -0.0587    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.1789005 |
| test/Q_plus_P                  | -2.1789005 |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.389      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00567   |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.4      |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 188        |
| stats_o/mean                   | 0.20344543 |
| stats_o/std                    | 0.10699196 |
| test/episodes                  | 1890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00307   |
| test/info_shaping_reward_mean  | -0.058     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.9517735 |
| test/Q_plus_P                  | -1.9517735 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 75600      |
| train/episodes                 | 7560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.476      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00522   |
| train/info_shaping_reward_mean | -0.0862    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21        |
| train/steps                    | 302400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 189        |
| stats_o/mean                   | 0.20343663 |
| stats_o/std                    | 0.10682031 |
| test/episodes                  | 1900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00351   |
| test/info_shaping_reward_mean  | -0.0596    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.0847826 |
| test/Q_plus_P                  | -2.0847826 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 76000      |
| train/episodes                 | 7600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.418      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.093     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.3      |
| train/steps                    | 304000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 190        |
| stats_o/mean                   | 0.20345832 |
| stats_o/std                    | 0.10664915 |
| test/episodes                  | 1910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.0589    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -2.151768  |
| test/Q_plus_P                  | -2.151768  |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 76400      |
| train/episodes                 | 7640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.443      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00662   |
| train/info_shaping_reward_mean | -0.0924    |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.3      |
| train/steps                    | 305600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 191        |
| stats_o/mean                   | 0.20347668 |
| stats_o/std                    | 0.10654559 |
| test/episodes                  | 1920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.65       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00523   |
| test/info_shaping_reward_mean  | -0.0607    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.0888934 |
| test/Q_plus_P                  | -2.0888934 |
| test/reward_per_eps            | -14        |
| test/steps                     | 76800      |
| train/episodes                 | 7680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.449      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00574   |
| train/info_shaping_reward_mean | -0.091     |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.1      |
| train/steps                    | 307200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 192        |
| stats_o/mean                   | 0.20347077 |
| stats_o/std                    | 0.10641711 |
| test/episodes                  | 1930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.65       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00189   |
| test/info_shaping_reward_mean  | -0.063     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -2.319137  |
| test/Q_plus_P                  | -2.319137  |
| test/reward_per_eps            | -14        |
| test/steps                     | 77200      |
| train/episodes                 | 7720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.441      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00496   |
| train/info_shaping_reward_mean | -0.0958    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.4      |
| train/steps                    | 308800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 193        |
| stats_o/mean                   | 0.20348185 |
| stats_o/std                    | 0.10631754 |
| test/episodes                  | 1940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00402   |
| test/info_shaping_reward_mean  | -0.0618    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -2.078037  |
| test/Q_plus_P                  | -2.078037  |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 77600      |
| train/episodes                 | 7760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.379      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00488   |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.9      |
| train/steps                    | 310400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 194        |
| stats_o/mean                   | 0.20351714 |
| stats_o/std                    | 0.10618376 |
| test/episodes                  | 1950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0032    |
| test/info_shaping_reward_mean  | -0.0597    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.9079103 |
| test/Q_plus_P                  | -1.9079103 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 78000      |
| train/episodes                 | 7800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.434      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00562   |
| train/info_shaping_reward_mean | -0.0949    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.6      |
| train/steps                    | 312000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 195        |
| stats_o/mean                   | 0.20351839 |
| stats_o/std                    | 0.10603776 |
| test/episodes                  | 1960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00192   |
| test/info_shaping_reward_mean  | -0.0595    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.1533868 |
| test/Q_plus_P                  | -2.1533868 |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 78400      |
| train/episodes                 | 7840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.461      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00426   |
| train/info_shaping_reward_mean | -0.0855    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 313600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 196         |
| stats_o/mean                   | 0.20355436  |
| stats_o/std                    | 0.105900146 |
| test/episodes                  | 1970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00247    |
| test/info_shaping_reward_mean  | -0.0569     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.0236773  |
| test/Q_plus_P                  | -2.0236773  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 78800       |
| train/episodes                 | 7880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.446       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00348    |
| train/info_shaping_reward_mean | -0.0897     |
| train/info_shaping_reward_min  | -0.207      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.2       |
| train/steps                    | 315200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 197        |
| stats_o/mean                   | 0.20358182 |
| stats_o/std                    | 0.1057379  |
| test/episodes                  | 1980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00162   |
| test/info_shaping_reward_mean  | -0.0615    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.3331573 |
| test/Q_plus_P                  | -2.3331573 |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 79200      |
| train/episodes                 | 7920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.463      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00547   |
| train/info_shaping_reward_mean | -0.0823    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.5      |
| train/steps                    | 316800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.20358592  |
| stats_o/std                    | 0.105540134 |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.688       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000851   |
| test/info_shaping_reward_mean  | -0.0591     |
| test/info_shaping_reward_min   | -0.2        |
| test/Q                         | -2.1424644  |
| test/Q_plus_P                  | -2.1424644  |
| test/reward_per_eps            | -12.5       |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.472       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0041     |
| train/info_shaping_reward_mean | -0.0828     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.1       |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.20356776 |
| stats_o/std                    | 0.10539093 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000701  |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.8599179 |
| test/Q_plus_P                  | -1.8599179 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.489      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00506   |
| train/info_shaping_reward_mean | -0.0891    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 200         |
| stats_o/mean                   | 0.20360033  |
| stats_o/std                    | 0.105303325 |
| test/episodes                  | 2010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000233   |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -2.0150633  |
| test/Q_plus_P                  | -2.0150633  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 80400       |
| train/episodes                 | 8040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.484       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00416    |
| train/info_shaping_reward_mean | -0.0828     |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.6       |
| train/steps                    | 321600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 201        |
| stats_o/mean                   | 0.20360461 |
| stats_o/std                    | 0.10518514 |
| test/episodes                  | 2020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00206   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.215     |
| test/Q                         | -1.9140463 |
| test/Q_plus_P                  | -1.9140463 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 80800      |
| train/episodes                 | 8080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.469      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0042    |
| train/info_shaping_reward_mean | -0.0872    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 323200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 202        |
| stats_o/mean                   | 0.20359665 |
| stats_o/std                    | 0.10506354 |
| test/episodes                  | 2030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00249   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.7366351 |
| test/Q_plus_P                  | -1.7366351 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 81200      |
| train/episodes                 | 8120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.465      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00495   |
| train/info_shaping_reward_mean | -0.089     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 324800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 203         |
| stats_o/mean                   | 0.20361167  |
| stats_o/std                    | 0.104965776 |
| test/episodes                  | 2040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0028     |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -1.7290987  |
| test/Q_plus_P                  | -1.7290987  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 81600       |
| train/episodes                 | 8160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.503       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00456    |
| train/info_shaping_reward_mean | -0.079      |
| train/info_shaping_reward_min  | -0.194      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.9       |
| train/steps                    | 326400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 204        |
| stats_o/mean                   | 0.2036173  |
| stats_o/std                    | 0.10480875 |
| test/episodes                  | 2050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00416   |
| test/info_shaping_reward_mean  | -0.057     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.9022317 |
| test/Q_plus_P                  | -1.9022317 |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 82000      |
| train/episodes                 | 8200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.468      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00382   |
| train/info_shaping_reward_mean | -0.0857    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.3      |
| train/steps                    | 328000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 205        |
| stats_o/mean                   | 0.20363925 |
| stats_o/std                    | 0.10473456 |
| test/episodes                  | 2060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00104   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.7185358 |
| test/Q_plus_P                  | -1.7185358 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 82400      |
| train/episodes                 | 8240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.499      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00406   |
| train/info_shaping_reward_mean | -0.0881    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 329600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 206        |
| stats_o/mean                   | 0.20364845 |
| stats_o/std                    | 0.10454147 |
| test/episodes                  | 2070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000825  |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.7762927 |
| test/Q_plus_P                  | -1.7762927 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 82800      |
| train/episodes                 | 8280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00366   |
| train/info_shaping_reward_mean | -0.0764    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 331200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.20368788 |
| stats_o/std                    | 0.10439736 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0029    |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5195396 |
| test/Q_plus_P                  | -1.5195396 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.505      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00445   |
| train/info_shaping_reward_mean | -0.0815    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 208        |
| stats_o/mean                   | 0.20370056 |
| stats_o/std                    | 0.1042938  |
| test/episodes                  | 2090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.649464  |
| test/Q_plus_P                  | -1.649464  |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 83600      |
| train/episodes                 | 8360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.483      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00367   |
| train/info_shaping_reward_mean | -0.0827    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.7      |
| train/steps                    | 334400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 209        |
| stats_o/mean                   | 0.20369531 |
| stats_o/std                    | 0.10412988 |
| test/episodes                  | 2100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00308   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.9056373 |
| test/Q_plus_P                  | -1.9056373 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 84000      |
| train/episodes                 | 8400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.502      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00434   |
| train/info_shaping_reward_mean | -0.0799    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 336000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 210        |
| stats_o/mean                   | 0.20370284 |
| stats_o/std                    | 0.10400851 |
| test/episodes                  | 2110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00235   |
| test/info_shaping_reward_mean  | -0.0582    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.0410306 |
| test/Q_plus_P                  | -2.0410306 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 84400      |
| train/episodes                 | 8440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.484      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00516   |
| train/info_shaping_reward_mean | -0.0886    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 337600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 211        |
| stats_o/mean                   | 0.2037304  |
| stats_o/std                    | 0.10387485 |
| test/episodes                  | 2120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00302   |
| test/info_shaping_reward_mean  | -0.0654    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.2217903 |
| test/Q_plus_P                  | -2.2217903 |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 84800      |
| train/episodes                 | 8480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.476      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00554   |
| train/info_shaping_reward_mean | -0.0829    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21        |
| train/steps                    | 339200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 212        |
| stats_o/mean                   | 0.2037504  |
| stats_o/std                    | 0.10376944 |
| test/episodes                  | 2130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0601    |
| test/info_shaping_reward_min   | -0.211     |
| test/Q                         | -2.1126645 |
| test/Q_plus_P                  | -2.1126645 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 85200      |
| train/episodes                 | 8520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.406      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00525   |
| train/info_shaping_reward_mean | -0.0995    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.8      |
| train/steps                    | 340800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 213        |
| stats_o/mean                   | 0.20376812 |
| stats_o/std                    | 0.10360252 |
| test/episodes                  | 2140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000739  |
| test/info_shaping_reward_mean  | -0.0621    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.197363  |
| test/Q_plus_P                  | -2.197363  |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 85600      |
| train/episodes                 | 8560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.461      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00481   |
| train/info_shaping_reward_mean | -0.084     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 342400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 214        |
| stats_o/mean                   | 0.20378456 |
| stats_o/std                    | 0.10355291 |
| test/episodes                  | 2150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00298   |
| test/info_shaping_reward_mean  | -0.0614    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.9401734 |
| test/Q_plus_P                  | -1.9401734 |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 86000      |
| train/episodes                 | 8600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.444      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00587   |
| train/info_shaping_reward_mean | -0.0986    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.2      |
| train/steps                    | 344000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 215        |
| stats_o/mean                   | 0.20379059 |
| stats_o/std                    | 0.1033861  |
| test/episodes                  | 2160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.63       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00104   |
| test/info_shaping_reward_mean  | -0.0815    |
| test/info_shaping_reward_min   | -0.53      |
| test/Q                         | -3.628409  |
| test/Q_plus_P                  | -3.628409  |
| test/reward_per_eps            | -14.8      |
| test/steps                     | 86400      |
| train/episodes                 | 8640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.476      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0046    |
| train/info_shaping_reward_mean | -0.0828    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 345600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 216        |
| stats_o/mean                   | 0.20379426 |
| stats_o/std                    | 0.10322848 |
| test/episodes                  | 2170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00145   |
| test/info_shaping_reward_mean  | -0.0592    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -2.1003351 |
| test/Q_plus_P                  | -2.1003351 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 86800      |
| train/episodes                 | 8680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.482      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00433   |
| train/info_shaping_reward_mean | -0.0873    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.7      |
| train/steps                    | 347200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 217        |
| stats_o/mean                   | 0.20380995 |
| stats_o/std                    | 0.10312717 |
| test/episodes                  | 2180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000923  |
| test/info_shaping_reward_mean  | -0.0602    |
| test/info_shaping_reward_min   | -0.212     |
| test/Q                         | -1.9333457 |
| test/Q_plus_P                  | -1.9333457 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 87200      |
| train/episodes                 | 8720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.474      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00555   |
| train/info_shaping_reward_mean | -0.0919    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 348800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 218        |
| stats_o/mean                   | 0.20380817 |
| stats_o/std                    | 0.10307877 |
| test/episodes                  | 2190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00122   |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -1.8929799 |
| test/Q_plus_P                  | -1.8929799 |
| test/reward_per_eps            | -12        |
| test/steps                     | 87600      |
| train/episodes                 | 8760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.443      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00379   |
| train/info_shaping_reward_mean | -0.0885    |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.3      |
| train/steps                    | 350400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 219        |
| stats_o/mean                   | 0.20380907 |
| stats_o/std                    | 0.10297969 |
| test/episodes                  | 2200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00143   |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.213     |
| test/Q                         | -3.0307112 |
| test/Q_plus_P                  | -3.0307112 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 88000      |
| train/episodes                 | 8800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.453      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00565   |
| train/info_shaping_reward_mean | -0.086     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.9      |
| train/steps                    | 352000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 220        |
| stats_o/mean                   | 0.20379645 |
| stats_o/std                    | 0.10288052 |
| test/episodes                  | 2210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000741  |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.6745573 |
| test/Q_plus_P                  | -1.6745573 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 88400      |
| train/episodes                 | 8840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.553      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00451   |
| train/info_shaping_reward_mean | -0.0789    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 353600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 221        |
| stats_o/mean                   | 0.20380716 |
| stats_o/std                    | 0.10276208 |
| test/episodes                  | 2220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00135   |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.206     |
| test/Q                         | -1.738551  |
| test/Q_plus_P                  | -1.738551  |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 88800      |
| train/episodes                 | 8880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.429      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00505   |
| train/info_shaping_reward_mean | -0.0962    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.8      |
| train/steps                    | 355200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.20383383 |
| stats_o/std                    | 0.10265132 |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4869435 |
| test/Q_plus_P                  | -1.4869435 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.471      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00367   |
| train/info_shaping_reward_mean | -0.0853    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 223        |
| stats_o/mean                   | 0.20384143 |
| stats_o/std                    | 0.10251948 |
| test/episodes                  | 2240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00135   |
| test/info_shaping_reward_mean  | -0.0547    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.7610981 |
| test/Q_plus_P                  | -1.7610981 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 89600      |
| train/episodes                 | 8960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00383   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 358400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.2038348  |
| stats_o/std                    | 0.10248436 |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00262   |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.7667336 |
| test/Q_plus_P                  | -1.7667336 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.467      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00506   |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.3      |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 225        |
| stats_o/mean                   | 0.20384508 |
| stats_o/std                    | 0.1023964  |
| test/episodes                  | 2260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.623      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00202   |
| test/info_shaping_reward_mean  | -0.09      |
| test/info_shaping_reward_min   | -0.458     |
| test/Q                         | -4.309011  |
| test/Q_plus_P                  | -4.309011  |
| test/reward_per_eps            | -15.1      |
| test/steps                     | 90400      |
| train/episodes                 | 9040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.427      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00552   |
| train/info_shaping_reward_mean | -0.0914    |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.9      |
| train/steps                    | 361600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 226        |
| stats_o/mean                   | 0.20386548 |
| stats_o/std                    | 0.10227405 |
| test/episodes                  | 2270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00281   |
| test/info_shaping_reward_mean  | -0.0623    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.1159933 |
| test/Q_plus_P                  | -2.1159933 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 90800      |
| train/episodes                 | 9080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.444      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00547   |
| train/info_shaping_reward_mean | -0.092     |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.2      |
| train/steps                    | 363200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 227        |
| stats_o/mean                   | 0.20387913 |
| stats_o/std                    | 0.1021575  |
| test/episodes                  | 2280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00141   |
| test/info_shaping_reward_mean  | -0.0563    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.8513674 |
| test/Q_plus_P                  | -1.8513674 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 91200      |
| train/episodes                 | 9120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.458      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00396   |
| train/info_shaping_reward_mean | -0.085     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.7      |
| train/steps                    | 364800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 228        |
| stats_o/mean                   | 0.20393318 |
| stats_o/std                    | 0.10204935 |
| test/episodes                  | 2290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00103   |
| test/info_shaping_reward_mean  | -0.0569    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.8868654 |
| test/Q_plus_P                  | -1.8868654 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 91600      |
| train/episodes                 | 9160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.369      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00657   |
| train/info_shaping_reward_mean | -0.0923    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.2      |
| train/steps                    | 366400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 229         |
| stats_o/mean                   | 0.20394005  |
| stats_o/std                    | 0.101974055 |
| test/episodes                  | 2300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00273    |
| test/info_shaping_reward_mean  | -0.0564     |
| test/info_shaping_reward_min   | -0.191      |
| test/Q                         | -1.8186046  |
| test/Q_plus_P                  | -1.8186046  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 92000       |
| train/episodes                 | 9200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.435       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00441    |
| train/info_shaping_reward_mean | -0.085      |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.6       |
| train/steps                    | 368000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 230         |
| stats_o/mean                   | 0.20399651  |
| stats_o/std                    | 0.101943776 |
| test/episodes                  | 2310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00265    |
| test/info_shaping_reward_mean  | -0.056      |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -1.9855697  |
| test/Q_plus_P                  | -1.9855697  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 92400       |
| train/episodes                 | 9240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.463       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00548    |
| train/info_shaping_reward_mean | -0.092      |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.5       |
| train/steps                    | 369600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 231        |
| stats_o/mean                   | 0.20401348 |
| stats_o/std                    | 0.10184503 |
| test/episodes                  | 2320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00252   |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.9485707 |
| test/Q_plus_P                  | -1.9485707 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 92800      |
| train/episodes                 | 9280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.465      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00583   |
| train/info_shaping_reward_mean | -0.0843    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 371200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 232        |
| stats_o/mean                   | 0.20402315 |
| stats_o/std                    | 0.10171519 |
| test/episodes                  | 2330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000153  |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.6347977 |
| test/Q_plus_P                  | -1.6347977 |
| test/reward_per_eps            | -11        |
| test/steps                     | 93200      |
| train/episodes                 | 9320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.484      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00381   |
| train/info_shaping_reward_mean | -0.0908    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 372800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 233        |
| stats_o/mean                   | 0.20403686 |
| stats_o/std                    | 0.10157875 |
| test/episodes                  | 2340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00186   |
| test/info_shaping_reward_mean  | -0.0575    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.9147172 |
| test/Q_plus_P                  | -1.9147172 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 93600      |
| train/episodes                 | 9360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.468      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0042    |
| train/info_shaping_reward_mean | -0.0857    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.3      |
| train/steps                    | 374400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.20402083 |
| stats_o/std                    | 0.10146315 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.657      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00201   |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -2.6121633 |
| test/Q_plus_P                  | -2.6121633 |
| test/reward_per_eps            | -13.7      |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.47       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00497   |
| train/info_shaping_reward_mean | -0.0848    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 235        |
| stats_o/mean                   | 0.20402433 |
| stats_o/std                    | 0.10136898 |
| test/episodes                  | 2360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000728  |
| test/info_shaping_reward_mean  | -0.0585    |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -2.2643142 |
| test/Q_plus_P                  | -2.2643142 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 94400      |
| train/episodes                 | 9440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.435      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00428   |
| train/info_shaping_reward_mean | -0.0914    |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.6      |
| train/steps                    | 377600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 236        |
| stats_o/mean                   | 0.20404501 |
| stats_o/std                    | 0.10126814 |
| test/episodes                  | 2370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00242   |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.9850798 |
| test/Q_plus_P                  | -1.9850798 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 94800      |
| train/episodes                 | 9480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.441      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00559   |
| train/info_shaping_reward_mean | -0.0853    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.4      |
| train/steps                    | 379200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 237        |
| stats_o/mean                   | 0.2040526  |
| stats_o/std                    | 0.10121603 |
| test/episodes                  | 2380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.665      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00454   |
| test/info_shaping_reward_mean  | -0.061     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -2.3106573 |
| test/Q_plus_P                  | -2.3106573 |
| test/reward_per_eps            | -13.4      |
| test/steps                     | 95200      |
| train/episodes                 | 9520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.454      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00436   |
| train/info_shaping_reward_mean | -0.0887    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 380800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 238        |
| stats_o/mean                   | 0.2040606  |
| stats_o/std                    | 0.10109844 |
| test/episodes                  | 2390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0027    |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.8740503 |
| test/Q_plus_P                  | -1.8740503 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 95600      |
| train/episodes                 | 9560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.464      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00465   |
| train/info_shaping_reward_mean | -0.0893    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 382400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 239        |
| stats_o/mean                   | 0.20406711 |
| stats_o/std                    | 0.10098328 |
| test/episodes                  | 2400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00178   |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.6700217 |
| test/Q_plus_P                  | -1.6700217 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 96000      |
| train/episodes                 | 9600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00445   |
| train/info_shaping_reward_mean | -0.0826    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 384000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 240        |
| stats_o/mean                   | 0.2040683  |
| stats_o/std                    | 0.10087329 |
| test/episodes                  | 2410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.0584    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.0654387 |
| test/Q_plus_P                  | -2.0654387 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 96400      |
| train/episodes                 | 9640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.47       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0877    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 385600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 241        |
| stats_o/mean                   | 0.2040765  |
| stats_o/std                    | 0.10079031 |
| test/episodes                  | 2420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00303   |
| test/info_shaping_reward_mean  | -0.0564    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.8469512 |
| test/Q_plus_P                  | -1.8469512 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 96800      |
| train/episodes                 | 9680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.471      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00499   |
| train/info_shaping_reward_mean | -0.0869    |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 387200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 242        |
| stats_o/mean                   | 0.2040807  |
| stats_o/std                    | 0.10071742 |
| test/episodes                  | 2430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00121   |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5980935 |
| test/Q_plus_P                  | -1.5980935 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 97200      |
| train/episodes                 | 9720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.443      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00471   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.3      |
| train/steps                    | 388800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 243         |
| stats_o/mean                   | 0.20408943  |
| stats_o/std                    | 0.100625604 |
| test/episodes                  | 2440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00231    |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.8608458  |
| test/Q_plus_P                  | -1.8608458  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 97600       |
| train/episodes                 | 9760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.496       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00344    |
| train/info_shaping_reward_mean | -0.0824     |
| train/info_shaping_reward_min  | -0.203      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 390400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 244         |
| stats_o/mean                   | 0.2040995   |
| stats_o/std                    | 0.100536354 |
| test/episodes                  | 2450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -1.7854187  |
| test/Q_plus_P                  | -1.7854187  |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 98000       |
| train/episodes                 | 9800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.47        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00622    |
| train/info_shaping_reward_mean | -0.0869     |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.2       |
| train/steps                    | 392000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 245        |
| stats_o/mean                   | 0.20408732 |
| stats_o/std                    | 0.10040505 |
| test/episodes                  | 2460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00336   |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6189055 |
| test/Q_plus_P                  | -1.6189055 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 98400      |
| train/episodes                 | 9840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.464      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00551   |
| train/info_shaping_reward_mean | -0.0868    |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 393600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 246        |
| stats_o/mean                   | 0.20406881 |
| stats_o/std                    | 0.10031571 |
| test/episodes                  | 2470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00112   |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.5669584 |
| test/Q_plus_P                  | -1.5669584 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 98800      |
| train/episodes                 | 9880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.459      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00472   |
| train/info_shaping_reward_mean | -0.0878    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 395200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 247        |
| stats_o/mean                   | 0.20406616 |
| stats_o/std                    | 0.10020974 |
| test/episodes                  | 2480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00344   |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.622217  |
| test/Q_plus_P                  | -1.622217  |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 99200      |
| train/episodes                 | 9920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.497      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00513   |
| train/info_shaping_reward_mean | -0.081     |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 396800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.2040604  |
| stats_o/std                    | 0.10013924 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00119   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -1.469033  |
| test/Q_plus_P                  | -1.469033  |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.463      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00529   |
| train/info_shaping_reward_mean | -0.0906    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.5      |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 249        |
| stats_o/mean                   | 0.20406643 |
| stats_o/std                    | 0.10003334 |
| test/episodes                  | 2500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.7914854 |
| test/Q_plus_P                  | -1.7914854 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 100000     |
| train/episodes                 | 10000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.511      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.0777    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 400000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 250        |
| stats_o/mean                   | 0.20409651 |
| stats_o/std                    | 0.09993592 |
| test/episodes                  | 2510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00215   |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.8029927 |
| test/Q_plus_P                  | -1.8029927 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 100400     |
| train/episodes                 | 10040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.467      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00405   |
| train/info_shaping_reward_mean | -0.0837    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.3      |
| train/steps                    | 401600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 251        |
| stats_o/mean                   | 0.20411521 |
| stats_o/std                    | 0.0999127  |
| test/episodes                  | 2520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0026    |
| test/info_shaping_reward_mean  | -0.0593    |
| test/info_shaping_reward_min   | -0.206     |
| test/Q                         | -1.8650689 |
| test/Q_plus_P                  | -1.8650689 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 100800     |
| train/episodes                 | 10080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.437      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00504   |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.283     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.5      |
| train/steps                    | 403200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 252        |
| stats_o/mean                   | 0.2041367  |
| stats_o/std                    | 0.09980933 |
| test/episodes                  | 2530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00139   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.4897068 |
| test/Q_plus_P                  | -1.4897068 |
| test/reward_per_eps            | -10        |
| test/steps                     | 101200     |
| train/episodes                 | 10120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.491      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00349   |
| train/info_shaping_reward_mean | -0.0864    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 404800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.20416293  |
| stats_o/std                    | 0.099692896 |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.001      |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.7951267  |
| test/Q_plus_P                  | -1.7951267  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.557       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00459    |
| train/info_shaping_reward_mean | -0.0735     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 254        |
| stats_o/mean                   | 0.20415382 |
| stats_o/std                    | 0.09965672 |
| test/episodes                  | 2550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000842  |
| test/info_shaping_reward_mean  | -0.0515    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.7252481 |
| test/Q_plus_P                  | -1.7252481 |
| test/reward_per_eps            | -11        |
| test/steps                     | 102000     |
| train/episodes                 | 10200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.448      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.1       |
| train/info_shaping_reward_min  | -0.278     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.1      |
| train/steps                    | 408000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 255        |
| stats_o/mean                   | 0.20415367 |
| stats_o/std                    | 0.09953758 |
| test/episodes                  | 2560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0034    |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -1.8093125 |
| test/Q_plus_P                  | -1.8093125 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 102400     |
| train/episodes                 | 10240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.476      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.0811    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 409600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 256        |
| stats_o/mean                   | 0.20416729 |
| stats_o/std                    | 0.0994287  |
| test/episodes                  | 2570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000952  |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5859349 |
| test/Q_plus_P                  | -1.5859349 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 102800     |
| train/episodes                 | 10280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00475   |
| train/info_shaping_reward_mean | -0.0752    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 411200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 257        |
| stats_o/mean                   | 0.20417795 |
| stats_o/std                    | 0.09937605 |
| test/episodes                  | 2580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00392   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -1.688487  |
| test/Q_plus_P                  | -1.688487  |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 103200     |
| train/episodes                 | 10320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.478      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00417   |
| train/info_shaping_reward_mean | -0.0899    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 412800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 258        |
| stats_o/mean                   | 0.20415969 |
| stats_o/std                    | 0.09931473 |
| test/episodes                  | 2590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.655      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0018    |
| test/info_shaping_reward_mean  | -0.0631    |
| test/info_shaping_reward_min   | -0.202     |
| test/Q                         | -2.2321494 |
| test/Q_plus_P                  | -2.2321494 |
| test/reward_per_eps            | -13.8      |
| test/steps                     | 103600     |
| train/episodes                 | 10360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.448      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00513   |
| train/info_shaping_reward_mean | -0.0966    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.1      |
| train/steps                    | 414400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 259         |
| stats_o/mean                   | 0.20416203  |
| stats_o/std                    | 0.099220514 |
| test/episodes                  | 2600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.665       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -5.22e-05   |
| test/info_shaping_reward_mean  | -0.0623     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -1.8942801  |
| test/Q_plus_P                  | -1.8942801  |
| test/reward_per_eps            | -13.4       |
| test/steps                     | 104000      |
| train/episodes                 | 10400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.426       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0057     |
| train/info_shaping_reward_mean | -0.0918     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23         |
| train/steps                    | 416000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 260        |
| stats_o/mean                   | 0.20416765 |
| stats_o/std                    | 0.09914082 |
| test/episodes                  | 2610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00329   |
| test/info_shaping_reward_mean  | -0.0591    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.9092662 |
| test/Q_plus_P                  | -1.9092662 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 104400     |
| train/episodes                 | 10440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.402      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00467   |
| train/info_shaping_reward_mean | -0.0959    |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.9      |
| train/steps                    | 417600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 261        |
| stats_o/mean                   | 0.20417517 |
| stats_o/std                    | 0.09905562 |
| test/episodes                  | 2620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00255   |
| test/info_shaping_reward_mean  | -0.0563    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.0599349 |
| test/Q_plus_P                  | -2.0599349 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 104800     |
| train/episodes                 | 10480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.437      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00494   |
| train/info_shaping_reward_mean | -0.085     |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.5      |
| train/steps                    | 419200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.2041814   |
| stats_o/std                    | 0.098985635 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.672       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00255    |
| test/info_shaping_reward_mean  | -0.0612     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -2.1858926  |
| test/Q_plus_P                  | -2.1858926  |
| test/reward_per_eps            | -13.1       |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.503       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00418    |
| train/info_shaping_reward_mean | -0.0944     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.9       |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.20415817  |
| stats_o/std                    | 0.098947525 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0036     |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -1.7520783  |
| test/Q_plus_P                  | -1.7520783  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.503       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00524    |
| train/info_shaping_reward_mean | -0.0908     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.9       |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 264        |
| stats_o/mean                   | 0.20416056 |
| stats_o/std                    | 0.09883924 |
| test/episodes                  | 2650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00252   |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.8063654 |
| test/Q_plus_P                  | -1.8063654 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 106000     |
| train/episodes                 | 10600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.443      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00525   |
| train/info_shaping_reward_mean | -0.0966    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.3      |
| train/steps                    | 424000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 265        |
| stats_o/mean                   | 0.20415197 |
| stats_o/std                    | 0.09872804 |
| test/episodes                  | 2660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00219   |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.8682318 |
| test/Q_plus_P                  | -1.8682318 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 106400     |
| train/episodes                 | 10640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.503      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00636   |
| train/info_shaping_reward_mean | -0.0776    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 425600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 266        |
| stats_o/mean                   | 0.2041573  |
| stats_o/std                    | 0.09860536 |
| test/episodes                  | 2670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00104   |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.8264976 |
| test/Q_plus_P                  | -1.8264976 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 106800     |
| train/episodes                 | 10680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.509      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00437   |
| train/info_shaping_reward_mean | -0.0801    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 427200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 267        |
| stats_o/mean                   | 0.20416732 |
| stats_o/std                    | 0.09852899 |
| test/episodes                  | 2680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.547      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00439   |
| test/info_shaping_reward_mean  | -0.067     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -2.3481655 |
| test/Q_plus_P                  | -2.3481655 |
| test/reward_per_eps            | -18.1      |
| test/steps                     | 107200     |
| train/episodes                 | 10720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.484      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00652   |
| train/info_shaping_reward_mean | -0.0816    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 428800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.20416969  |
| stats_o/std                    | 0.098464146 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.7622037  |
| test/Q_plus_P                  | -1.7622037  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.401       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00518    |
| train/info_shaping_reward_mean | -0.101      |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.9       |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 269        |
| stats_o/mean                   | 0.20416458 |
| stats_o/std                    | 0.09843256 |
| test/episodes                  | 2700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00314   |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -1.833066  |
| test/Q_plus_P                  | -1.833066  |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 108000     |
| train/episodes                 | 10800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.45       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00491   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.286     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22        |
| train/steps                    | 432000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.20415981  |
| stats_o/std                    | 0.098372914 |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00218    |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -1.687299   |
| test/Q_plus_P                  | -1.687299   |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.491       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00547    |
| train/info_shaping_reward_mean | -0.102      |
| train/info_shaping_reward_min  | -0.288      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.4       |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 271        |
| stats_o/mean                   | 0.20417424 |
| stats_o/std                    | 0.09831446 |
| test/episodes                  | 2720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000797  |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.7182819 |
| test/Q_plus_P                  | -1.7182819 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 108800     |
| train/episodes                 | 10880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.473      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00472   |
| train/info_shaping_reward_mean | -0.0882    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 435200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.20415397  |
| stats_o/std                    | 0.098290205 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.196      |
| test/Q                         | -1.5489104  |
| test/Q_plus_P                  | -1.5489104  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.476       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00518    |
| train/info_shaping_reward_mean | -0.0977     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.9       |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 273        |
| stats_o/mean                   | 0.20417547 |
| stats_o/std                    | 0.09820137 |
| test/episodes                  | 2740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00169   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.8787063 |
| test/Q_plus_P                  | -1.8787063 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 109600     |
| train/episodes                 | 10960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.544      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00423   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 438400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 274        |
| stats_o/mean                   | 0.20418862 |
| stats_o/std                    | 0.09812235 |
| test/episodes                  | 2750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0018    |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.7477721 |
| test/Q_plus_P                  | -1.7477721 |
| test/reward_per_eps            | -12        |
| test/steps                     | 110000     |
| train/episodes                 | 11000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.479      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00401   |
| train/info_shaping_reward_mean | -0.0819    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 440000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 275        |
| stats_o/mean                   | 0.20418079 |
| stats_o/std                    | 0.09806156 |
| test/episodes                  | 2760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000692  |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6925576 |
| test/Q_plus_P                  | -1.6925576 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 110400     |
| train/episodes                 | 11040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.509      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00533   |
| train/info_shaping_reward_mean | -0.0894    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 441600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 276        |
| stats_o/mean                   | 0.20419432 |
| stats_o/std                    | 0.09797978 |
| test/episodes                  | 2770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00147   |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.9071889 |
| test/Q_plus_P                  | -1.9071889 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 110800     |
| train/episodes                 | 11080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.455      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00367   |
| train/info_shaping_reward_mean | -0.0896    |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 443200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 277        |
| stats_o/mean                   | 0.20419896 |
| stats_o/std                    | 0.09788535 |
| test/episodes                  | 2780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00169   |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.7783505 |
| test/Q_plus_P                  | -1.7783505 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 111200     |
| train/episodes                 | 11120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.506      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00502   |
| train/info_shaping_reward_mean | -0.0863    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 444800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 278         |
| stats_o/mean                   | 0.20418991  |
| stats_o/std                    | 0.097816356 |
| test/episodes                  | 2790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00194    |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.6425024  |
| test/Q_plus_P                  | -1.6425024  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 111600      |
| train/episodes                 | 11160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.512       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00526    |
| train/info_shaping_reward_mean | -0.0829     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.5       |
| train/steps                    | 446400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 279        |
| stats_o/mean                   | 0.20418823 |
| stats_o/std                    | 0.09775803 |
| test/episodes                  | 2800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000651  |
| test/info_shaping_reward_mean  | -0.0468    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4024047 |
| test/Q_plus_P                  | -1.4024047 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 112000     |
| train/episodes                 | 11200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00383   |
| train/info_shaping_reward_mean | -0.083     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 448000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 280        |
| stats_o/mean                   | 0.2041694  |
| stats_o/std                    | 0.09765247 |
| test/episodes                  | 2810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000945  |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.8017468 |
| test/Q_plus_P                  | -1.8017468 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 112400     |
| train/episodes                 | 11240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00327   |
| train/info_shaping_reward_mean | -0.0746    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 449600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.20416415 |
| stats_o/std                    | 0.097557   |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00376   |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6119424 |
| test/Q_plus_P                  | -1.6119424 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00427   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.20416503 |
| stats_o/std                    | 0.09752577 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.6365488 |
| test/Q_plus_P                  | -1.6365488 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.477      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00467   |
| train/info_shaping_reward_mean | -0.0933    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.20415714 |
| stats_o/std                    | 0.09747969 |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000626  |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.841712  |
| test/Q_plus_P                  | -1.841712  |
| test/reward_per_eps            | -12        |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.489      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00484   |
| train/info_shaping_reward_mean | -0.0909    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 284        |
| stats_o/mean                   | 0.20416641 |
| stats_o/std                    | 0.0973959  |
| test/episodes                  | 2850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000836  |
| test/info_shaping_reward_mean  | -0.0547    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.7323936 |
| test/Q_plus_P                  | -1.7323936 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 114000     |
| train/episodes                 | 11400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00451   |
| train/info_shaping_reward_mean | -0.0801    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 456000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 285         |
| stats_o/mean                   | 0.20416316  |
| stats_o/std                    | 0.097310975 |
| test/episodes                  | 2860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00228    |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.6443447  |
| test/Q_plus_P                  | -1.6443447  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 114400      |
| train/episodes                 | 11440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.479       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00579    |
| train/info_shaping_reward_mean | -0.0855     |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.8       |
| train/steps                    | 457600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 286        |
| stats_o/mean                   | 0.20416348 |
| stats_o/std                    | 0.09724873 |
| test/episodes                  | 2870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00058   |
| test/info_shaping_reward_mean  | -0.0564    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -1.947228  |
| test/Q_plus_P                  | -1.947228  |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 114800     |
| train/episodes                 | 11480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.505      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00374   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 459200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.20416507 |
| stats_o/std                    | 0.09716262 |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00163   |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.5891477 |
| test/Q_plus_P                  | -1.5891477 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.499      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0037    |
| train/info_shaping_reward_mean | -0.083     |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 288        |
| stats_o/mean                   | 0.20417699 |
| stats_o/std                    | 0.09710708 |
| test/episodes                  | 2890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00119   |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.5685303 |
| test/Q_plus_P                  | -1.5685303 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 115600     |
| train/episodes                 | 11560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.488      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00528   |
| train/info_shaping_reward_mean | -0.0877    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.5      |
| train/steps                    | 462400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 289        |
| stats_o/mean                   | 0.20416634 |
| stats_o/std                    | 0.09701784 |
| test/episodes                  | 2900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00384   |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -2.0257404 |
| test/Q_plus_P                  | -2.0257404 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 116000     |
| train/episodes                 | 11600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.486      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00423   |
| train/info_shaping_reward_mean | -0.0838    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 464000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 290        |
| stats_o/mean                   | 0.20418093 |
| stats_o/std                    | 0.09691572 |
| test/episodes                  | 2910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.595457  |
| test/Q_plus_P                  | -1.595457  |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 116400     |
| train/episodes                 | 11640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 465600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.20419039  |
| stats_o/std                    | 0.096827105 |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00313    |
| test/info_shaping_reward_mean  | -0.0578     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.7543268  |
| test/Q_plus_P                  | -1.7543268  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.459       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00561    |
| train/info_shaping_reward_mean | -0.0829     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.6       |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 292        |
| stats_o/mean                   | 0.2041964  |
| stats_o/std                    | 0.09673265 |
| test/episodes                  | 2930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00121   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.7211571 |
| test/Q_plus_P                  | -1.7211571 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 117200     |
| train/episodes                 | 11720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.523      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0039    |
| train/info_shaping_reward_mean | -0.0869    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 468800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 293        |
| stats_o/mean                   | 0.20419174 |
| stats_o/std                    | 0.09665877 |
| test/episodes                  | 2940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00206   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5790496 |
| test/Q_plus_P                  | -1.5790496 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 117600     |
| train/episodes                 | 11760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.479      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0047    |
| train/info_shaping_reward_mean | -0.0919    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 470400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 294        |
| stats_o/mean                   | 0.20419766 |
| stats_o/std                    | 0.09655672 |
| test/episodes                  | 2950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0023    |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6966003 |
| test/Q_plus_P                  | -1.6966003 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 118000     |
| train/episodes                 | 11800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00423   |
| train/info_shaping_reward_mean | -0.0746    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 472000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 295         |
| stats_o/mean                   | 0.20420235  |
| stats_o/std                    | 0.096473046 |
| test/episodes                  | 2960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00205    |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.762314   |
| test/Q_plus_P                  | -1.762314   |
| test/reward_per_eps            | -11         |
| test/steps                     | 118400      |
| train/episodes                 | 11840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.497       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0881     |
| train/info_shaping_reward_min  | -0.226      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 473600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 296        |
| stats_o/mean                   | 0.20419101 |
| stats_o/std                    | 0.09637566 |
| test/episodes                  | 2970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00176   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.5212014 |
| test/Q_plus_P                  | -1.5212014 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 118800     |
| train/episodes                 | 11880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00487   |
| train/info_shaping_reward_mean | -0.0801    |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 475200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 297        |
| stats_o/mean                   | 0.20419632 |
| stats_o/std                    | 0.09627054 |
| test/episodes                  | 2980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00174   |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.212     |
| test/Q                         | -1.6433064 |
| test/Q_plus_P                  | -1.6433064 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 119200     |
| train/episodes                 | 11920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.516      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00417   |
| train/info_shaping_reward_mean | -0.0771    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 476800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 298        |
| stats_o/mean                   | 0.20420025 |
| stats_o/std                    | 0.09622246 |
| test/episodes                  | 2990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00188   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6284475 |
| test/Q_plus_P                  | -1.6284475 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 119600     |
| train/episodes                 | 11960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.499      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00477   |
| train/info_shaping_reward_mean | -0.0872    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 478400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 299        |
| stats_o/mean                   | 0.20422117 |
| stats_o/std                    | 0.09615854 |
| test/episodes                  | 3000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2308989 |
| test/Q_plus_P                  | -1.2308989 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 120000     |
| train/episodes                 | 12000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00358   |
| train/info_shaping_reward_mean | -0.083     |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 480000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 300        |
| stats_o/mean                   | 0.20421654 |
| stats_o/std                    | 0.09606528 |
| test/episodes                  | 3010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00108   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5143018 |
| test/Q_plus_P                  | -1.5143018 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 120400     |
| train/episodes                 | 12040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.525      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00364   |
| train/info_shaping_reward_mean | -0.0788    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 481600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.20422763 |
| stats_o/std                    | 0.09597641 |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5342369 |
| test/Q_plus_P                  | -1.5342369 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.515      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.0805    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 302        |
| stats_o/mean                   | 0.20424938 |
| stats_o/std                    | 0.09594926 |
| test/episodes                  | 3030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.5878932 |
| test/Q_plus_P                  | -1.5878932 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 121200     |
| train/episodes                 | 12120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00541   |
| train/info_shaping_reward_mean | -0.0831    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 484800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 303        |
| stats_o/mean                   | 0.20424813 |
| stats_o/std                    | 0.09588549 |
| test/episodes                  | 3040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.665      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00319   |
| test/info_shaping_reward_mean  | -0.0659    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -2.46733   |
| test/Q_plus_P                  | -2.46733   |
| test/reward_per_eps            | -13.4      |
| test/steps                     | 121600     |
| train/episodes                 | 12160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.505      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.082     |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 486400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.20424883  |
| stats_o/std                    | 0.095870264 |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00105    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.575446   |
| test/Q_plus_P                  | -1.575446   |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.522       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00341    |
| train/info_shaping_reward_mean | -0.0738     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.2042336  |
| stats_o/std                    | 0.09581349 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00229   |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.4144876 |
| test/Q_plus_P                  | -1.4144876 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.49       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00377   |
| train/info_shaping_reward_mean | -0.0851    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 306        |
| stats_o/mean                   | 0.20424141 |
| stats_o/std                    | 0.09577138 |
| test/episodes                  | 3070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.5192513 |
| test/Q_plus_P                  | -1.5192513 |
| test/reward_per_eps            | -11        |
| test/steps                     | 122800     |
| train/episodes                 | 12280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.538      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00499   |
| train/info_shaping_reward_mean | -0.0764    |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 491200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.204231   |
| stats_o/std                    | 0.09571582 |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000936  |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.4630172 |
| test/Q_plus_P                  | -1.4630172 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.52       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00454   |
| train/info_shaping_reward_mean | -0.075     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 308         |
| stats_o/mean                   | 0.2042284   |
| stats_o/std                    | 0.095687255 |
| test/episodes                  | 3090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00337    |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.217      |
| test/Q                         | -1.5822636  |
| test/Q_plus_P                  | -1.5822636  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 123600      |
| train/episodes                 | 12360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.55        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00399    |
| train/info_shaping_reward_mean | -0.0832     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18         |
| train/steps                    | 494400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 309         |
| stats_o/mean                   | 0.20422502  |
| stats_o/std                    | 0.095673315 |
| test/episodes                  | 3100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -1.3798867  |
| test/Q_plus_P                  | -1.3798867  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 124000      |
| train/episodes                 | 12400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.513       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00422    |
| train/info_shaping_reward_mean | -0.0871     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.5       |
| train/steps                    | 496000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 310        |
| stats_o/mean                   | 0.20423335 |
| stats_o/std                    | 0.09560257 |
| test/episodes                  | 3110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00215   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.3919117 |
| test/Q_plus_P                  | -1.3919117 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 124400     |
| train/episodes                 | 12440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00439   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 497600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.2042183  |
| stats_o/std                    | 0.09553167 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00252   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.488421  |
| test/Q_plus_P                  | -1.488421  |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00309   |
| train/info_shaping_reward_mean | -0.0738    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 312        |
| stats_o/mean                   | 0.20421848 |
| stats_o/std                    | 0.09544009 |
| test/episodes                  | 3130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00173   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.40503   |
| test/Q_plus_P                  | -1.40503   |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 125200     |
| train/episodes                 | 12520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.504      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00472   |
| train/info_shaping_reward_mean | -0.0804    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 500800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 313        |
| stats_o/mean                   | 0.20421547 |
| stats_o/std                    | 0.0953821  |
| test/episodes                  | 3140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00121   |
| test/info_shaping_reward_mean  | -0.047     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.3289833 |
| test/Q_plus_P                  | -1.3289833 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 125600     |
| train/episodes                 | 12560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.48       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00599   |
| train/info_shaping_reward_mean | -0.0794    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 502400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 314        |
| stats_o/mean                   | 0.20419787 |
| stats_o/std                    | 0.09529166 |
| test/episodes                  | 3150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.637      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.0629    |
| test/info_shaping_reward_min   | -0.215     |
| test/Q                         | -3.4570277 |
| test/Q_plus_P                  | -3.4570277 |
| test/reward_per_eps            | -14.5      |
| test/steps                     | 126000     |
| train/episodes                 | 12600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.505      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00371   |
| train/info_shaping_reward_mean | -0.0778    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 504000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 315        |
| stats_o/mean                   | 0.20421213 |
| stats_o/std                    | 0.09524726 |
| test/episodes                  | 3160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000981  |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.5837811 |
| test/Q_plus_P                  | -1.5837811 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 126400     |
| train/episodes                 | 12640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.491      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00355   |
| train/info_shaping_reward_mean | -0.0838    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 505600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 316        |
| stats_o/mean                   | 0.20419988 |
| stats_o/std                    | 0.09517125 |
| test/episodes                  | 3170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00194   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3499627 |
| test/Q_plus_P                  | -1.3499627 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 126800     |
| train/episodes                 | 12680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.498      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00595   |
| train/info_shaping_reward_mean | -0.0823    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 507200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 317        |
| stats_o/mean                   | 0.2042077  |
| stats_o/std                    | 0.09507918 |
| test/episodes                  | 3180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00127   |
| test/info_shaping_reward_mean  | -0.0565    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.8094689 |
| test/Q_plus_P                  | -1.8094689 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 127200     |
| train/episodes                 | 12720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0757    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 508800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 318         |
| stats_o/mean                   | 0.20421593  |
| stats_o/std                    | 0.095049664 |
| test/episodes                  | 3190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.4343699  |
| test/Q_plus_P                  | -1.4343699  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 127600      |
| train/episodes                 | 12760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.49        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00481    |
| train/info_shaping_reward_mean | -0.087      |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.4       |
| train/steps                    | 510400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 319        |
| stats_o/mean                   | 0.20422956 |
| stats_o/std                    | 0.09495067 |
| test/episodes                  | 3200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0013    |
| test/info_shaping_reward_mean  | -0.054     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.4959661 |
| test/Q_plus_P                  | -1.4959661 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 128000     |
| train/episodes                 | 12800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.452      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00491   |
| train/info_shaping_reward_mean | -0.0865    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.9      |
| train/steps                    | 512000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 320         |
| stats_o/mean                   | 0.20423174  |
| stats_o/std                    | 0.094861776 |
| test/episodes                  | 3210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00208    |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.5210922  |
| test/Q_plus_P                  | -1.5210922  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 128400      |
| train/episodes                 | 12840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.524       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00373    |
| train/info_shaping_reward_mean | -0.0755     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19         |
| train/steps                    | 513600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 321        |
| stats_o/mean                   | 0.20422055 |
| stats_o/std                    | 0.09478274 |
| test/episodes                  | 3220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00228   |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5717233 |
| test/Q_plus_P                  | -1.5717233 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 128800     |
| train/episodes                 | 12880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.486      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00538   |
| train/info_shaping_reward_mean | -0.0801    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 515200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 322        |
| stats_o/mean                   | 0.20422246 |
| stats_o/std                    | 0.09468274 |
| test/episodes                  | 3230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00235   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.47718   |
| test/Q_plus_P                  | -1.47718   |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 129200     |
| train/episodes                 | 12920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.538      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00267   |
| train/info_shaping_reward_mean | -0.0743    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 516800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 323        |
| stats_o/mean                   | 0.2042368  |
| stats_o/std                    | 0.09463785 |
| test/episodes                  | 3240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00233   |
| test/info_shaping_reward_mean  | -0.0567    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -1.8133005 |
| test/Q_plus_P                  | -1.8133005 |
| test/reward_per_eps            | -12        |
| test/steps                     | 129600     |
| train/episodes                 | 12960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.491      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00431   |
| train/info_shaping_reward_mean | -0.0827    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 518400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.20422783  |
| stats_o/std                    | 0.094571054 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00345    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.4084986  |
| test/Q_plus_P                  | -1.4084986  |
| test/reward_per_eps            | -10         |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.533       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00416    |
| train/info_shaping_reward_mean | -0.0763     |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 325         |
| stats_o/mean                   | 0.204237    |
| stats_o/std                    | 0.094528325 |
| test/episodes                  | 3260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000801   |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.4409895  |
| test/Q_plus_P                  | -1.4409895  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 130400      |
| train/episodes                 | 13040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.5         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00374    |
| train/info_shaping_reward_mean | -0.08       |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20         |
| train/steps                    | 521600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.20425797  |
| stats_o/std                    | 0.094452955 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00382    |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5026506  |
| test/Q_plus_P                  | -1.5026506  |
| test/reward_per_eps            | -10         |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.503       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00449    |
| train/info_shaping_reward_mean | -0.0822     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.9       |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 327        |
| stats_o/mean                   | 0.20424728 |
| stats_o/std                    | 0.09435573 |
| test/episodes                  | 3280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0016    |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.3251649 |
| test/Q_plus_P                  | -1.3251649 |
| test/reward_per_eps            | -10        |
| test/steps                     | 131200     |
| train/episodes                 | 13120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00368   |
| train/info_shaping_reward_mean | -0.0692    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 524800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 328        |
| stats_o/mean                   | 0.20425588 |
| stats_o/std                    | 0.09431505 |
| test/episodes                  | 3290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00281   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4371616 |
| test/Q_plus_P                  | -1.4371616 |
| test/reward_per_eps            | -10        |
| test/steps                     | 131600     |
| train/episodes                 | 13160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00328   |
| train/info_shaping_reward_mean | -0.0741    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 526400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 329         |
| stats_o/mean                   | 0.20425083  |
| stats_o/std                    | 0.094263345 |
| test/episodes                  | 3300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000457   |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.3867642  |
| test/Q_plus_P                  | -1.3867642  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 132000      |
| train/episodes                 | 13200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.532       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0038     |
| train/info_shaping_reward_mean | -0.077      |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 528000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 330        |
| stats_o/mean                   | 0.20425811 |
| stats_o/std                    | 0.09420046 |
| test/episodes                  | 3310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000961  |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5020273 |
| test/Q_plus_P                  | -1.5020273 |
| test/reward_per_eps            | -10        |
| test/steps                     | 132400     |
| train/episodes                 | 13240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.533      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00435   |
| train/info_shaping_reward_mean | -0.0796    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 529600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 331        |
| stats_o/mean                   | 0.20426308 |
| stats_o/std                    | 0.09415349 |
| test/episodes                  | 3320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.557425  |
| test/Q_plus_P                  | -1.557425  |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 132800     |
| train/episodes                 | 13280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.492      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0048    |
| train/info_shaping_reward_mean | -0.0799    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.3      |
| train/steps                    | 531200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 332        |
| stats_o/mean                   | 0.2042684  |
| stats_o/std                    | 0.09407329 |
| test/episodes                  | 3330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00177   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.373843  |
| test/Q_plus_P                  | -1.373843  |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 133200     |
| train/episodes                 | 13320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.52       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00492   |
| train/info_shaping_reward_mean | -0.0771    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 532800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 333         |
| stats_o/mean                   | 0.20426717  |
| stats_o/std                    | 0.093992166 |
| test/episodes                  | 3340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00144    |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.2966288  |
| test/Q_plus_P                  | -1.2966288  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 133600      |
| train/episodes                 | 13360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.514       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00397    |
| train/info_shaping_reward_mean | -0.0766     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.4       |
| train/steps                    | 534400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 334         |
| stats_o/mean                   | 0.20426859  |
| stats_o/std                    | 0.093913786 |
| test/episodes                  | 3350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00397    |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.3697078  |
| test/Q_plus_P                  | -1.3697078  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 134000      |
| train/episodes                 | 13400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.555       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00532    |
| train/info_shaping_reward_mean | -0.0754     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 536000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.2042609  |
| stats_o/std                    | 0.09387889 |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.7476132 |
| test/Q_plus_P                  | -1.7476132 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.526      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.0837    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 336        |
| stats_o/mean                   | 0.20427948 |
| stats_o/std                    | 0.09383767 |
| test/episodes                  | 3370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00127   |
| test/info_shaping_reward_mean  | -0.0585    |
| test/info_shaping_reward_min   | -0.214     |
| test/Q                         | -1.6934988 |
| test/Q_plus_P                  | -1.6934988 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 134800     |
| train/episodes                 | 13480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.523      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00352   |
| train/info_shaping_reward_mean | -0.0775    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 539200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 337        |
| stats_o/mean                   | 0.20429939 |
| stats_o/std                    | 0.09380941 |
| test/episodes                  | 3380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00198   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -1.3340883 |
| test/Q_plus_P                  | -1.3340883 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 135200     |
| train/episodes                 | 13520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.449      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00407   |
| train/info_shaping_reward_mean | -0.0885    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.1      |
| train/steps                    | 540800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.2043025  |
| stats_o/std                    | 0.0937499  |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00147   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.6607586 |
| test/Q_plus_P                  | -1.6607586 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00463   |
| train/info_shaping_reward_mean | -0.0837    |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.20430285  |
| stats_o/std                    | 0.093685664 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00158    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.2392777  |
| test/Q_plus_P                  | -1.2392777  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.517       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0794     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.3       |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 340        |
| stats_o/mean                   | 0.20429806 |
| stats_o/std                    | 0.09362342 |
| test/episodes                  | 3410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3409624 |
| test/Q_plus_P                  | -1.3409624 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 136400     |
| train/episodes                 | 13640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0029    |
| train/info_shaping_reward_mean | -0.0743    |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 545600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 341        |
| stats_o/mean                   | 0.20429331 |
| stats_o/std                    | 0.09353654 |
| test/episodes                  | 3420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00394   |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3953508 |
| test/Q_plus_P                  | -1.3953508 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 136800     |
| train/episodes                 | 13680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00371   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 547200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 342        |
| stats_o/mean                   | 0.20429152 |
| stats_o/std                    | 0.09346795 |
| test/episodes                  | 3430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00257   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.2563434 |
| test/Q_plus_P                  | -1.2563434 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 137200     |
| train/episodes                 | 13720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.493      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.0872    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.3      |
| train/steps                    | 548800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 343         |
| stats_o/mean                   | 0.2043047   |
| stats_o/std                    | 0.093442395 |
| test/episodes                  | 3440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00173    |
| test/info_shaping_reward_mean  | -0.0579     |
| test/info_shaping_reward_min   | -0.205      |
| test/Q                         | -2.1148264  |
| test/Q_plus_P                  | -2.1148264  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 137600      |
| train/episodes                 | 13760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.498       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.0857     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 550400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 344        |
| stats_o/mean                   | 0.20430522 |
| stats_o/std                    | 0.09339397 |
| test/episodes                  | 3450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00284   |
| test/info_shaping_reward_mean  | -0.0454    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2846805 |
| test/Q_plus_P                  | -1.2846805 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 138000     |
| train/episodes                 | 13800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.498      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0048    |
| train/info_shaping_reward_mean | -0.0834    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 552000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 345        |
| stats_o/mean                   | 0.20431496 |
| stats_o/std                    | 0.09330955 |
| test/episodes                  | 3460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00193   |
| test/info_shaping_reward_mean  | -0.0437    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.139769  |
| test/Q_plus_P                  | -1.139769  |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 138400     |
| train/episodes                 | 13840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00421   |
| train/info_shaping_reward_mean | -0.0708    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 553600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 346         |
| stats_o/mean                   | 0.20431477  |
| stats_o/std                    | 0.093224265 |
| test/episodes                  | 3470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.204      |
| test/Q                         | -1.5033891  |
| test/Q_plus_P                  | -1.5033891  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 138800      |
| train/episodes                 | 13880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.566       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00382    |
| train/info_shaping_reward_mean | -0.0732     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 555200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 347         |
| stats_o/mean                   | 0.20430349  |
| stats_o/std                    | 0.093192376 |
| test/episodes                  | 3480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00233    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3537792  |
| test/Q_plus_P                  | -1.3537792  |
| test/reward_per_eps            | -10         |
| test/steps                     | 139200      |
| train/episodes                 | 13920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.519       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.0864     |
| train/info_shaping_reward_min  | -0.225      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.2       |
| train/steps                    | 556800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.2043042   |
| stats_o/std                    | 0.093194306 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.327502   |
| test/Q_plus_P                  | -1.327502   |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.491       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0037     |
| train/info_shaping_reward_mean | -0.0915     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.4       |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 349        |
| stats_o/mean                   | 0.20431459 |
| stats_o/std                    | 0.09311607 |
| test/episodes                  | 3500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00255   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2818336 |
| test/Q_plus_P                  | -1.2818336 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 140000     |
| train/episodes                 | 14000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.488      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00355   |
| train/info_shaping_reward_mean | -0.0892    |
| train/info_shaping_reward_min  | -0.267     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.5      |
| train/steps                    | 560000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 350        |
| stats_o/mean                   | 0.20433335 |
| stats_o/std                    | 0.09306942 |
| test/episodes                  | 3510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00185   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -1.9172523 |
| test/Q_plus_P                  | -1.9172523 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 140400     |
| train/episodes                 | 14040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0046    |
| train/info_shaping_reward_mean | -0.0786    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 561600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 351        |
| stats_o/mean                   | 0.20432696 |
| stats_o/std                    | 0.0929893  |
| test/episodes                  | 3520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00219   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.1889834 |
| test/Q_plus_P                  | -1.1889834 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 140800     |
| train/episodes                 | 14080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 563200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 352        |
| stats_o/mean                   | 0.20431146 |
| stats_o/std                    | 0.09290907 |
| test/episodes                  | 3530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0017    |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3345389 |
| test/Q_plus_P                  | -1.3345389 |
| test/reward_per_eps            | -10        |
| test/steps                     | 141200     |
| train/episodes                 | 14120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.555      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00374   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 564800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 353        |
| stats_o/mean                   | 0.20429876 |
| stats_o/std                    | 0.09284265 |
| test/episodes                  | 3540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000755  |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.4753566 |
| test/Q_plus_P                  | -1.4753566 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 141600     |
| train/episodes                 | 14160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00344   |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 566400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 354         |
| stats_o/mean                   | 0.20430382  |
| stats_o/std                    | 0.092760846 |
| test/episodes                  | 3550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000892   |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.2124019  |
| test/Q_plus_P                  | -1.2124019  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 142000      |
| train/episodes                 | 14200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.532       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.0767     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 568000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 355        |
| stats_o/mean                   | 0.20430101 |
| stats_o/std                    | 0.09267573 |
| test/episodes                  | 3560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000594  |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2691908 |
| test/Q_plus_P                  | -1.2691908 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 142400     |
| train/episodes                 | 14240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00484   |
| train/info_shaping_reward_mean | -0.0724    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 569600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 356        |
| stats_o/mean                   | 0.20430106 |
| stats_o/std                    | 0.09263049 |
| test/episodes                  | 3570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.5328858 |
| test/Q_plus_P                  | -1.5328858 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 142800     |
| train/episodes                 | 14280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00369   |
| train/info_shaping_reward_mean | -0.0903    |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 571200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 357         |
| stats_o/mean                   | 0.20430982  |
| stats_o/std                    | 0.092569314 |
| test/episodes                  | 3580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0027     |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.194      |
| test/Q                         | -1.5930514  |
| test/Q_plus_P                  | -1.5930514  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 143200      |
| train/episodes                 | 14320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00436    |
| train/info_shaping_reward_mean | -0.0747     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 572800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 358         |
| stats_o/mean                   | 0.20430614  |
| stats_o/std                    | 0.092484854 |
| test/episodes                  | 3590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00212    |
| test/info_shaping_reward_mean  | -0.0496     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.3285956  |
| test/Q_plus_P                  | -1.3285956  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 143600      |
| train/episodes                 | 14360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00408    |
| train/info_shaping_reward_mean | -0.0733     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 574400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 359        |
| stats_o/mean                   | 0.20432608 |
| stats_o/std                    | 0.09252646 |
| test/episodes                  | 3600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00164   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.4509702 |
| test/Q_plus_P                  | -1.4509702 |
| test/reward_per_eps            | -10        |
| test/steps                     | 144000     |
| train/episodes                 | 14400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.514      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00502   |
| train/info_shaping_reward_mean | -0.0953    |
| train/info_shaping_reward_min  | -0.285     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 576000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 360        |
| stats_o/mean                   | 0.20433012 |
| stats_o/std                    | 0.09246837 |
| test/episodes                  | 3610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00133   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1949643 |
| test/Q_plus_P                  | -1.1949643 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 144400     |
| train/episodes                 | 14440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00492   |
| train/info_shaping_reward_mean | -0.0756    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 577600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 361        |
| stats_o/mean                   | 0.20432767 |
| stats_o/std                    | 0.09240325 |
| test/episodes                  | 3620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00126   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4741145 |
| test/Q_plus_P                  | -1.4741145 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 144800     |
| train/episodes                 | 14480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.526      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00495   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 579200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 362         |
| stats_o/mean                   | 0.20432988  |
| stats_o/std                    | 0.092325725 |
| test/episodes                  | 3630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.3283772  |
| test/Q_plus_P                  | -1.3283772  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 145200      |
| train/episodes                 | 14520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00532    |
| train/info_shaping_reward_mean | -0.0686     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 580800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 363        |
| stats_o/mean                   | 0.20433037 |
| stats_o/std                    | 0.09228593 |
| test/episodes                  | 3640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00145   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.257455  |
| test/Q_plus_P                  | -1.257455  |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 145600     |
| train/episodes                 | 14560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00498   |
| train/info_shaping_reward_mean | -0.0764    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 582400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 364        |
| stats_o/mean                   | 0.2043277  |
| stats_o/std                    | 0.09219666 |
| test/episodes                  | 3650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0019    |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4797618 |
| test/Q_plus_P                  | -1.4797618 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 146000     |
| train/episodes                 | 14600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00345   |
| train/info_shaping_reward_mean | -0.068     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 584000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 365        |
| stats_o/mean                   | 0.20433342 |
| stats_o/std                    | 0.09212287 |
| test/episodes                  | 3660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00114   |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3813487 |
| test/Q_plus_P                  | -1.3813487 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 146400     |
| train/episodes                 | 14640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00354   |
| train/info_shaping_reward_mean | -0.0756    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 585600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 366        |
| stats_o/mean                   | 0.20434156 |
| stats_o/std                    | 0.09205265 |
| test/episodes                  | 3670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000644  |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.6984836 |
| test/Q_plus_P                  | -1.6984836 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 146800     |
| train/episodes                 | 14680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00439   |
| train/info_shaping_reward_mean | -0.0768    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 587200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.20433001  |
| stats_o/std                    | 0.091963865 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00256    |
| test/info_shaping_reward_mean  | -0.0563     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.5599312  |
| test/Q_plus_P                  | -1.5599312  |
| test/reward_per_eps            | -11         |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.559       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.0744     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 368        |
| stats_o/mean                   | 0.20432979 |
| stats_o/std                    | 0.09190332 |
| test/episodes                  | 3690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00323   |
| test/info_shaping_reward_mean  | -0.0515    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.3176559 |
| test/Q_plus_P                  | -1.3176559 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 147600     |
| train/episodes                 | 14760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.55       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00517   |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 590400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 369         |
| stats_o/mean                   | 0.20432998  |
| stats_o/std                    | 0.091833495 |
| test/episodes                  | 3700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000797   |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.7084038  |
| test/Q_plus_P                  | -1.7084038  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 148000      |
| train/episodes                 | 14800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.528       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00446    |
| train/info_shaping_reward_mean | -0.0773     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 592000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 370        |
| stats_o/mean                   | 0.20432813 |
| stats_o/std                    | 0.09176895 |
| test/episodes                  | 3710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00159   |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -1.4686933 |
| test/Q_plus_P                  | -1.4686933 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 148400     |
| train/episodes                 | 14840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00407   |
| train/info_shaping_reward_mean | -0.0719    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 593600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 371        |
| stats_o/mean                   | 0.20432511 |
| stats_o/std                    | 0.09170498 |
| test/episodes                  | 3720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00124   |
| test/info_shaping_reward_mean  | -0.05      |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.512249  |
| test/Q_plus_P                  | -1.512249  |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 148800     |
| train/episodes                 | 14880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00505   |
| train/info_shaping_reward_mean | -0.0772    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 595200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 372        |
| stats_o/mean                   | 0.20433202 |
| stats_o/std                    | 0.09166823 |
| test/episodes                  | 3730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00067   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3694283 |
| test/Q_plus_P                  | -1.3694283 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 149200     |
| train/episodes                 | 14920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00425   |
| train/info_shaping_reward_mean | -0.08      |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 596800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 373         |
| stats_o/mean                   | 0.20434484  |
| stats_o/std                    | 0.091624126 |
| test/episodes                  | 3740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00257    |
| test/info_shaping_reward_mean  | -0.0497     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.4092599  |
| test/Q_plus_P                  | -1.4092599  |
| test/reward_per_eps            | -10         |
| test/steps                     | 149600      |
| train/episodes                 | 14960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.497       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00359    |
| train/info_shaping_reward_mean | -0.0865     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 598400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.20435277 |
| stats_o/std                    | 0.09157079 |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00162   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1478361 |
| test/Q_plus_P                  | -1.1478361 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.512      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00355   |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.5      |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.2043669   |
| stats_o/std                    | 0.091551274 |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00252    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.229935   |
| test/Q_plus_P                  | -1.229935   |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.523       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00399    |
| train/info_shaping_reward_mean | -0.0748     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.20437154 |
| stats_o/std                    | 0.09150349 |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00187   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.3240412 |
| test/Q_plus_P                  | -1.3240412 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.547      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0042    |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 377        |
| stats_o/mean                   | 0.2043707  |
| stats_o/std                    | 0.09149794 |
| test/episodes                  | 3780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00144   |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5050942 |
| test/Q_plus_P                  | -1.5050942 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 151200     |
| train/episodes                 | 15120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.49       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00336   |
| train/info_shaping_reward_mean | -0.0938    |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 604800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 378        |
| stats_o/mean                   | 0.20436975 |
| stats_o/std                    | 0.09148988 |
| test/episodes                  | 3790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00264   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5292256 |
| test/Q_plus_P                  | -1.5292256 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 151600     |
| train/episodes                 | 15160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.555      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00317   |
| train/info_shaping_reward_mean | -0.0805    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 606400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 379         |
| stats_o/mean                   | 0.20437942  |
| stats_o/std                    | 0.091452464 |
| test/episodes                  | 3800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00202    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.318378   |
| test/Q_plus_P                  | -1.318378   |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 152000      |
| train/episodes                 | 15200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.521       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0026     |
| train/info_shaping_reward_mean | -0.0826     |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 608000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 380        |
| stats_o/mean                   | 0.20438842 |
| stats_o/std                    | 0.09139899 |
| test/episodes                  | 3810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00337   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3487319 |
| test/Q_plus_P                  | -1.3487319 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 152400     |
| train/episodes                 | 15240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.51       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00555   |
| train/info_shaping_reward_mean | -0.0793    |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 609600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 381        |
| stats_o/mean                   | 0.20437643 |
| stats_o/std                    | 0.09134084 |
| test/episodes                  | 3820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000899  |
| test/info_shaping_reward_mean  | -0.0472    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3313458 |
| test/Q_plus_P                  | -1.3313458 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 152800     |
| train/episodes                 | 15280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.558      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00425   |
| train/info_shaping_reward_mean | -0.0824    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 611200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.20439453 |
| stats_o/std                    | 0.09128774 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00149   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.430288  |
| test/Q_plus_P                  | -1.430288  |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.525      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0041    |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.20439786  |
| stats_o/std                    | 0.091219574 |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000895   |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.193      |
| test/Q                         | -1.5609665  |
| test/Q_plus_P                  | -1.5609665  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.525       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00421    |
| train/info_shaping_reward_mean | -0.0763     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19         |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 384        |
| stats_o/mean                   | 0.20440815 |
| stats_o/std                    | 0.0911465  |
| test/episodes                  | 3850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000843  |
| test/info_shaping_reward_mean  | -0.0447    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3227684 |
| test/Q_plus_P                  | -1.3227684 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 154000     |
| train/episodes                 | 15400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0044    |
| train/info_shaping_reward_mean | -0.071     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 616000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 385         |
| stats_o/mean                   | 0.20442076  |
| stats_o/std                    | 0.091110446 |
| test/episodes                  | 3860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00134    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.3353648  |
| test/Q_plus_P                  | -1.3353648  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 154400      |
| train/episodes                 | 15440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.519       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00417    |
| train/info_shaping_reward_mean | -0.0793     |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.2       |
| train/steps                    | 617600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 386        |
| stats_o/mean                   | 0.20441784 |
| stats_o/std                    | 0.09106402 |
| test/episodes                  | 3870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00157   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -1.3515695 |
| test/Q_plus_P                  | -1.3515695 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 154800     |
| train/episodes                 | 15480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0036    |
| train/info_shaping_reward_mean | -0.0717    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 619200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 387        |
| stats_o/mean                   | 0.20442194 |
| stats_o/std                    | 0.09100189 |
| test/episodes                  | 3880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000955  |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.3890884 |
| test/Q_plus_P                  | -1.3890884 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 155200     |
| train/episodes                 | 15520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00413   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 620800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 388        |
| stats_o/mean                   | 0.20442754 |
| stats_o/std                    | 0.0909349  |
| test/episodes                  | 3890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00109   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2816805 |
| test/Q_plus_P                  | -1.2816805 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 155600     |
| train/episodes                 | 15560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00452   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 622400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 389        |
| stats_o/mean                   | 0.20442554 |
| stats_o/std                    | 0.09086657 |
| test/episodes                  | 3900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00183   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.3935959 |
| test/Q_plus_P                  | -1.3935959 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 156000     |
| train/episodes                 | 15600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00425   |
| train/info_shaping_reward_mean | -0.0749    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 624000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 390        |
| stats_o/mean                   | 0.20441838 |
| stats_o/std                    | 0.09080653 |
| test/episodes                  | 3910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00263   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -1.3509917 |
| test/Q_plus_P                  | -1.3509917 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 156400     |
| train/episodes                 | 15640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0049    |
| train/info_shaping_reward_mean | -0.0754    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 625600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.20441519  |
| stats_o/std                    | 0.090755224 |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00243    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -1.2638927  |
| test/Q_plus_P                  | -1.2638927  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00363    |
| train/info_shaping_reward_mean | -0.0723     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 392         |
| stats_o/mean                   | 0.2044133   |
| stats_o/std                    | 0.090701245 |
| test/episodes                  | 3930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00308    |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -1.4595319  |
| test/Q_plus_P                  | -1.4595319  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 157200      |
| train/episodes                 | 15720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.535       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0031     |
| train/info_shaping_reward_mean | -0.0746     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 628800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 393         |
| stats_o/mean                   | 0.20440257  |
| stats_o/std                    | 0.090677574 |
| test/episodes                  | 3940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.2520328  |
| test/Q_plus_P                  | -1.2520328  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 157600      |
| train/episodes                 | 15760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.506       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00491    |
| train/info_shaping_reward_mean | -0.0886     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.8       |
| train/steps                    | 630400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 394        |
| stats_o/mean                   | 0.20440055 |
| stats_o/std                    | 0.09061988 |
| test/episodes                  | 3950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00143   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -1.2781676 |
| test/Q_plus_P                  | -1.2781676 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 158000     |
| train/episodes                 | 15800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0781    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 632000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 395        |
| stats_o/mean                   | 0.20439783 |
| stats_o/std                    | 0.09057877 |
| test/episodes                  | 3960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000415  |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.212     |
| test/Q                         | -1.4361103 |
| test/Q_plus_P                  | -1.4361103 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 158400     |
| train/episodes                 | 15840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00477   |
| train/info_shaping_reward_mean | -0.0741    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 633600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 396        |
| stats_o/mean                   | 0.20440052 |
| stats_o/std                    | 0.09057056 |
| test/episodes                  | 3970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00141   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -1.3521367 |
| test/Q_plus_P                  | -1.3521367 |
| test/reward_per_eps            | -10        |
| test/steps                     | 158800     |
| train/episodes                 | 15880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.522      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00399   |
| train/info_shaping_reward_mean | -0.0857    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 635200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.20441301  |
| stats_o/std                    | 0.090560205 |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00263    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.4148989  |
| test/Q_plus_P                  | -1.4148989  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.514       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00395    |
| train/info_shaping_reward_mean | -0.0815     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.4       |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 398        |
| stats_o/mean                   | 0.204432   |
| stats_o/std                    | 0.09053757 |
| test/episodes                  | 3990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5026001 |
| test/Q_plus_P                  | -1.5026001 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 159600     |
| train/episodes                 | 15960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00415   |
| train/info_shaping_reward_mean | -0.0853    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 638400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 399         |
| stats_o/mean                   | 0.20443353  |
| stats_o/std                    | 0.090491496 |
| test/episodes                  | 4000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00179    |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2078687  |
| test/Q_plus_P                  | -1.2078687  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 160000      |
| train/episodes                 | 16000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.485       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0048     |
| train/info_shaping_reward_mean | -0.082      |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.6       |
| train/steps                    | 640000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 400         |
| stats_o/mean                   | 0.20442599  |
| stats_o/std                    | 0.090427704 |
| test/episodes                  | 4010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00275    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.3687767  |
| test/Q_plus_P                  | -1.3687767  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 160400      |
| train/episodes                 | 16040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.571       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00538    |
| train/info_shaping_reward_mean | -0.0717     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 641600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 401        |
| stats_o/mean                   | 0.20443603 |
| stats_o/std                    | 0.0903528  |
| test/episodes                  | 4020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00174   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2398708 |
| test/Q_plus_P                  | -1.2398708 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 160800     |
| train/episodes                 | 16080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00421   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 643200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.20444071 |
| stats_o/std                    | 0.09032607 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00402   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4263015 |
| test/Q_plus_P                  | -1.4263015 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.487      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00373   |
| train/info_shaping_reward_mean | -0.0856    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.5      |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 403        |
| stats_o/mean                   | 0.20445083 |
| stats_o/std                    | 0.09025919 |
| test/episodes                  | 4040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00233   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.6266648 |
| test/Q_plus_P                  | -1.6266648 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 161600     |
| train/episodes                 | 16160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.585      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00317   |
| train/info_shaping_reward_mean | -0.0687    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 646400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.20445399  |
| stats_o/std                    | 0.090208225 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00203    |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.3859375  |
| test/Q_plus_P                  | -1.3859375  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.556       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00558    |
| train/info_shaping_reward_mean | -0.0722     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.20445791  |
| stats_o/std                    | 0.090191066 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00212    |
| test/info_shaping_reward_mean  | -0.0447     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1303439  |
| test/Q_plus_P                  | -1.1303439  |
| test/reward_per_eps            | -9          |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.493       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00538    |
| train/info_shaping_reward_mean | -0.104      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.3       |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 406        |
| stats_o/mean                   | 0.20445502 |
| stats_o/std                    | 0.09015962 |
| test/episodes                  | 4070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00189   |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.7494175 |
| test/Q_plus_P                  | -1.7494175 |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 162800     |
| train/episodes                 | 16280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.509      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00405   |
| train/info_shaping_reward_mean | -0.0777    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 651200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 407        |
| stats_o/mean                   | 0.20446286 |
| stats_o/std                    | 0.09010965 |
| test/episodes                  | 4080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00206   |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1297224 |
| test/Q_plus_P                  | -1.1297224 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 163200     |
| train/episodes                 | 16320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0033    |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 652800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 408        |
| stats_o/mean                   | 0.20446534 |
| stats_o/std                    | 0.09007174 |
| test/episodes                  | 4090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00325   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.2407141 |
| test/Q_plus_P                  | -1.2407141 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 163600     |
| train/episodes                 | 16360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0037    |
| train/info_shaping_reward_mean | -0.0757    |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 654400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 409        |
| stats_o/mean                   | 0.20447649 |
| stats_o/std                    | 0.09004538 |
| test/episodes                  | 4100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00135   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1140785 |
| test/Q_plus_P                  | -1.1140785 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 164000     |
| train/episodes                 | 16400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00503   |
| train/info_shaping_reward_mean | -0.0767    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 656000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 410        |
| stats_o/mean                   | 0.20448437 |
| stats_o/std                    | 0.0900269  |
| test/episodes                  | 4110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00239   |
| test/info_shaping_reward_mean  | -0.0457    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3736969 |
| test/Q_plus_P                  | -1.3736969 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 164400     |
| train/episodes                 | 16440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.492      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00386   |
| train/info_shaping_reward_mean | -0.0886    |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.3      |
| train/steps                    | 657600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 411        |
| stats_o/mean                   | 0.20448476 |
| stats_o/std                    | 0.0899618  |
| test/episodes                  | 4120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00129   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.3631748 |
| test/Q_plus_P                  | -1.3631748 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 164800     |
| train/episodes                 | 16480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.541      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00346   |
| train/info_shaping_reward_mean | -0.0732    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 659200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 412        |
| stats_o/mean                   | 0.20449375 |
| stats_o/std                    | 0.0899367  |
| test/episodes                  | 4130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2098112 |
| test/Q_plus_P                  | -1.2098112 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 165200     |
| train/episodes                 | 16520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00325   |
| train/info_shaping_reward_mean | -0.0772    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 660800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 413        |
| stats_o/mean                   | 0.20449477 |
| stats_o/std                    | 0.08987795 |
| test/episodes                  | 4140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.0468    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3152243 |
| test/Q_plus_P                  | -1.3152243 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 165600     |
| train/episodes                 | 16560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0724    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 662400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 414         |
| stats_o/mean                   | 0.2044852   |
| stats_o/std                    | 0.089861795 |
| test/episodes                  | 4150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00183    |
| test/info_shaping_reward_mean  | -0.0533     |
| test/info_shaping_reward_min   | -0.198      |
| test/Q                         | -1.5723468  |
| test/Q_plus_P                  | -1.5723468  |
| test/reward_per_eps            | -11         |
| test/steps                     | 166000      |
| train/episodes                 | 16600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.516       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00469    |
| train/info_shaping_reward_mean | -0.0792     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.4       |
| train/steps                    | 664000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 415         |
| stats_o/mean                   | 0.2045075   |
| stats_o/std                    | 0.089831516 |
| test/episodes                  | 4160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000245   |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.3976278  |
| test/Q_plus_P                  | -1.3976278  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 166400      |
| train/episodes                 | 16640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.519       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00363    |
| train/info_shaping_reward_mean | -0.0778     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.2       |
| train/steps                    | 665600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 416         |
| stats_o/mean                   | 0.20449713  |
| stats_o/std                    | 0.089780934 |
| test/episodes                  | 4170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00191    |
| test/info_shaping_reward_mean  | -0.0527     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.6190509  |
| test/Q_plus_P                  | -1.6190509  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 166800      |
| train/episodes                 | 16680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00404    |
| train/info_shaping_reward_mean | -0.074      |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 667200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 417        |
| stats_o/mean                   | 0.20450667 |
| stats_o/std                    | 0.08972981 |
| test/episodes                  | 4180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00238   |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.3512666 |
| test/Q_plus_P                  | -1.3512666 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 167200     |
| train/episodes                 | 16720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.509      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0053    |
| train/info_shaping_reward_mean | -0.0787    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 668800     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 418       |
| stats_o/mean                   | 0.2045066 |
| stats_o/std                    | 0.089693  |
| test/episodes                  | 4190      |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.75      |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.00202  |
| test/info_shaping_reward_mean  | -0.0502   |
| test/info_shaping_reward_min   | -0.18     |
| test/Q                         | -1.432003 |
| test/Q_plus_P                  | -1.432003 |
| test/reward_per_eps            | -10       |
| test/steps                     | 167600    |
| train/episodes                 | 16760     |
| train/info_is_success_max      | 1         |
| train/info_is_success_mean     | 0.539     |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.00388  |
| train/info_shaping_reward_mean | -0.0797   |
| train/info_shaping_reward_min  | -0.217    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -18.4     |
| train/steps                    | 670400    |
----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 419         |
| stats_o/mean                   | 0.20450667  |
| stats_o/std                    | 0.089624554 |
| test/episodes                  | 4200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00204    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.3723664  |
| test/Q_plus_P                  | -1.3723664  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 168000      |
| train/episodes                 | 16800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.59        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00345    |
| train/info_shaping_reward_mean | -0.0686     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 672000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 420        |
| stats_o/mean                   | 0.20451213 |
| stats_o/std                    | 0.08958558 |
| test/episodes                  | 4210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000862  |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2571383 |
| test/Q_plus_P                  | -1.2571383 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 168400     |
| train/episodes                 | 16840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00336   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 673600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 421        |
| stats_o/mean                   | 0.20448644 |
| stats_o/std                    | 0.08955556 |
| test/episodes                  | 4220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00134   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -1.3342975 |
| test/Q_plus_P                  | -1.3342975 |
| test/reward_per_eps            | -10        |
| test/steps                     | 168800     |
| train/episodes                 | 16880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.56       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00437   |
| train/info_shaping_reward_mean | -0.0771    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 675200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 422        |
| stats_o/mean                   | 0.20449658 |
| stats_o/std                    | 0.08953636 |
| test/episodes                  | 4230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000869  |
| test/info_shaping_reward_mean  | -0.0463    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3404758 |
| test/Q_plus_P                  | -1.3404758 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 169200     |
| train/episodes                 | 16920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.505      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0036    |
| train/info_shaping_reward_mean | -0.0842    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 676800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 423        |
| stats_o/mean                   | 0.20450626 |
| stats_o/std                    | 0.08947006 |
| test/episodes                  | 4240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00263   |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.2769196 |
| test/Q_plus_P                  | -1.2769196 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 169600     |
| train/episodes                 | 16960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00447   |
| train/info_shaping_reward_mean | -0.07      |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 678400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 424        |
| stats_o/mean                   | 0.20450176 |
| stats_o/std                    | 0.08945207 |
| test/episodes                  | 4250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.3728904 |
| test/Q_plus_P                  | -1.3728904 |
| test/reward_per_eps            | -10        |
| test/steps                     | 170000     |
| train/episodes                 | 17000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.526      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00434   |
| train/info_shaping_reward_mean | -0.0871    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 680000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.20449844  |
| stats_o/std                    | 0.089424714 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00369    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.4866035  |
| test/Q_plus_P                  | -1.4866035  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.56        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00353    |
| train/info_shaping_reward_mean | -0.0802     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 426        |
| stats_o/mean                   | 0.20451124 |
| stats_o/std                    | 0.08940649 |
| test/episodes                  | 4270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00338   |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4361408 |
| test/Q_plus_P                  | -1.4361408 |
| test/reward_per_eps            | -10        |
| test/steps                     | 170800     |
| train/episodes                 | 17080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00469   |
| train/info_shaping_reward_mean | -0.0863    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 683200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 427        |
| stats_o/mean                   | 0.2045034  |
| stats_o/std                    | 0.08935091 |
| test/episodes                  | 4280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000573  |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3765912 |
| test/Q_plus_P                  | -1.3765912 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 171200     |
| train/episodes                 | 17120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00413   |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 684800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 428        |
| stats_o/mean                   | 0.20450062 |
| stats_o/std                    | 0.08929044 |
| test/episodes                  | 4290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1938176 |
| test/Q_plus_P                  | -1.1938176 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 171600     |
| train/episodes                 | 17160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00447   |
| train/info_shaping_reward_mean | -0.0797    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 686400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 429        |
| stats_o/mean                   | 0.20450482 |
| stats_o/std                    | 0.08922028 |
| test/episodes                  | 4300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00147   |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2826499 |
| test/Q_plus_P                  | -1.2826499 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 172000     |
| train/episodes                 | 17200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00368   |
| train/info_shaping_reward_mean | -0.0679    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 688000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 430        |
| stats_o/mean                   | 0.20449461 |
| stats_o/std                    | 0.08925684 |
| test/episodes                  | 4310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00289   |
| test/info_shaping_reward_mean  | -0.0443    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1626629 |
| test/Q_plus_P                  | -1.1626629 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 172400     |
| train/episodes                 | 17240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.502      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00611   |
| train/info_shaping_reward_mean | -0.0843    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 689600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 431        |
| stats_o/mean                   | 0.2044974  |
| stats_o/std                    | 0.08921055 |
| test/episodes                  | 4320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00275   |
| test/info_shaping_reward_mean  | -0.0467    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2605938 |
| test/Q_plus_P                  | -1.2605938 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 172800     |
| train/episodes                 | 17280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00339   |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 691200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 432        |
| stats_o/mean                   | 0.2045004  |
| stats_o/std                    | 0.08915979 |
| test/episodes                  | 4330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00124   |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4175858 |
| test/Q_plus_P                  | -1.4175858 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 173200     |
| train/episodes                 | 17320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00562   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 692800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 433        |
| stats_o/mean                   | 0.20451231 |
| stats_o/std                    | 0.0891085  |
| test/episodes                  | 4340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00194   |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.4871477 |
| test/Q_plus_P                  | -1.4871477 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 173600     |
| train/episodes                 | 17360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00338   |
| train/info_shaping_reward_mean | -0.0819    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 694400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 434        |
| stats_o/mean                   | 0.20451152 |
| stats_o/std                    | 0.08905998 |
| test/episodes                  | 4350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000905  |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3182907 |
| test/Q_plus_P                  | -1.3182907 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 174000     |
| train/episodes                 | 17400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.527      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00346   |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 696000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 435        |
| stats_o/mean                   | 0.20451674 |
| stats_o/std                    | 0.08901232 |
| test/episodes                  | 4360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00065   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6301595 |
| test/Q_plus_P                  | -1.6301595 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 174400     |
| train/episodes                 | 17440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00357   |
| train/info_shaping_reward_mean | -0.0793    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 697600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 436         |
| stats_o/mean                   | 0.20450853  |
| stats_o/std                    | 0.088984124 |
| test/episodes                  | 4370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000271   |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.5447526  |
| test/Q_plus_P                  | -1.5447526  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 174800      |
| train/episodes                 | 17480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.509       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00544    |
| train/info_shaping_reward_mean | -0.0796     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.6       |
| train/steps                    | 699200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 437        |
| stats_o/mean                   | 0.20451702 |
| stats_o/std                    | 0.08895144 |
| test/episodes                  | 4380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00297   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.4590619 |
| test/Q_plus_P                  | -1.4590619 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 175200     |
| train/episodes                 | 17520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.526      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 700800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 438        |
| stats_o/mean                   | 0.2045167  |
| stats_o/std                    | 0.0889259  |
| test/episodes                  | 4390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00185   |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4578738 |
| test/Q_plus_P                  | -1.4578738 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 175600     |
| train/episodes                 | 17560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.502      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00379   |
| train/info_shaping_reward_mean | -0.0816    |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 702400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 439        |
| stats_o/mean                   | 0.20451775 |
| stats_o/std                    | 0.0889227  |
| test/episodes                  | 4400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00186   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.465732  |
| test/Q_plus_P                  | -1.465732  |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 176000     |
| train/episodes                 | 17600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00431   |
| train/info_shaping_reward_mean | -0.0843    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 704000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 440        |
| stats_o/mean                   | 0.20451076 |
| stats_o/std                    | 0.08894017 |
| test/episodes                  | 4410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00415   |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.7056084 |
| test/Q_plus_P                  | -1.7056084 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 176400     |
| train/episodes                 | 17640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.472      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0883    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 705600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 441        |
| stats_o/mean                   | 0.2045134  |
| stats_o/std                    | 0.08893051 |
| test/episodes                  | 4420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00178   |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.7480618 |
| test/Q_plus_P                  | -1.7480618 |
| test/reward_per_eps            | -11        |
| test/steps                     | 176800     |
| train/episodes                 | 17680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.518      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00412   |
| train/info_shaping_reward_mean | -0.0868    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 707200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 442         |
| stats_o/mean                   | 0.20451036  |
| stats_o/std                    | 0.088913955 |
| test/episodes                  | 4430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00199    |
| test/info_shaping_reward_mean  | -0.0578     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.9776212  |
| test/Q_plus_P                  | -1.9776212  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 177200      |
| train/episodes                 | 17720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.532       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00349    |
| train/info_shaping_reward_mean | -0.092      |
| train/info_shaping_reward_min  | -0.305      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 708800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 443        |
| stats_o/mean                   | 0.2045175  |
| stats_o/std                    | 0.08889264 |
| test/episodes                  | 4440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00211   |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.670306  |
| test/Q_plus_P                  | -1.670306  |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 177600     |
| train/episodes                 | 17760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.513      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00428   |
| train/info_shaping_reward_mean | -0.0863    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.5      |
| train/steps                    | 710400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 444        |
| stats_o/mean                   | 0.2045175  |
| stats_o/std                    | 0.08885791 |
| test/episodes                  | 4450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00405   |
| test/info_shaping_reward_mean  | -0.0598    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.9530188 |
| test/Q_plus_P                  | -1.9530188 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 178000     |
| train/episodes                 | 17800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00475   |
| train/info_shaping_reward_mean | -0.0797    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 712000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.20452634  |
| stats_o/std                    | 0.088803634 |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00239    |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.5047863  |
| test/Q_plus_P                  | -1.5047863  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.541       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00421    |
| train/info_shaping_reward_mean | -0.0754     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 446        |
| stats_o/mean                   | 0.20452192 |
| stats_o/std                    | 0.08876143 |
| test/episodes                  | 4470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00155   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.3339524 |
| test/Q_plus_P                  | -1.3339524 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 178800     |
| train/episodes                 | 17880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00526   |
| train/info_shaping_reward_mean | -0.0732    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 715200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 447        |
| stats_o/mean                   | 0.20452283 |
| stats_o/std                    | 0.08873987 |
| test/episodes                  | 4480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00166   |
| test/info_shaping_reward_mean  | -0.0541    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.6024694 |
| test/Q_plus_P                  | -1.6024694 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 179200     |
| train/episodes                 | 17920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.54       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00413   |
| train/info_shaping_reward_mean | -0.0819    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 716800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 448        |
| stats_o/mean                   | 0.20452984 |
| stats_o/std                    | 0.08870779 |
| test/episodes                  | 4490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0011    |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3901808 |
| test/Q_plus_P                  | -1.3901808 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 179600     |
| train/episodes                 | 17960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.543      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00446   |
| train/info_shaping_reward_mean | -0.0827    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 718400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 449        |
| stats_o/mean                   | 0.20453478 |
| stats_o/std                    | 0.08865938 |
| test/episodes                  | 4500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00169   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.4006937 |
| test/Q_plus_P                  | -1.4006937 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 180000     |
| train/episodes                 | 18000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.541      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00373   |
| train/info_shaping_reward_mean | -0.0764    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 720000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.2045294  |
| stats_o/std                    | 0.08860969 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00266   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.209     |
| test/Q                         | -1.4926001 |
| test/Q_plus_P                  | -1.4926001 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.553      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0037    |
| train/info_shaping_reward_mean | -0.0738    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 451        |
| stats_o/mean                   | 0.20452209 |
| stats_o/std                    | 0.0885494  |
| test/episodes                  | 4520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00158   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4770747 |
| test/Q_plus_P                  | -1.4770747 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 180800     |
| train/episodes                 | 18080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 723200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 452        |
| stats_o/mean                   | 0.20453413 |
| stats_o/std                    | 0.0885072  |
| test/episodes                  | 4530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000865  |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.228     |
| test/Q                         | -1.2902964 |
| test/Q_plus_P                  | -1.2902964 |
| test/reward_per_eps            | -10        |
| test/steps                     | 181200     |
| train/episodes                 | 18120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.585      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00369   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 724800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 453         |
| stats_o/mean                   | 0.20454043  |
| stats_o/std                    | 0.088469714 |
| test/episodes                  | 4540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00241    |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.4548459  |
| test/Q_plus_P                  | -1.4548459  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 181600      |
| train/episodes                 | 18160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.543       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00481    |
| train/info_shaping_reward_mean | -0.0747     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.3       |
| train/steps                    | 726400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 454        |
| stats_o/mean                   | 0.2045396  |
| stats_o/std                    | 0.08843477 |
| test/episodes                  | 4550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3097527 |
| test/Q_plus_P                  | -1.3097527 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 182000     |
| train/episodes                 | 18200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.0772    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 728000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.20455362  |
| stats_o/std                    | 0.088402346 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00121    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.411848   |
| test/Q_plus_P                  | -1.411848   |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00488    |
| train/info_shaping_reward_mean | -0.0757     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 456        |
| stats_o/mean                   | 0.20455606 |
| stats_o/std                    | 0.08835502 |
| test/episodes                  | 4570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00267   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.4973079 |
| test/Q_plus_P                  | -1.4973079 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 182800     |
| train/episodes                 | 18280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.541      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00374   |
| train/info_shaping_reward_mean | -0.074     |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 731200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 457        |
| stats_o/mean                   | 0.2045589  |
| stats_o/std                    | 0.08835388 |
| test/episodes                  | 4580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00327   |
| test/info_shaping_reward_mean  | -0.0552    |
| test/info_shaping_reward_min   | -0.209     |
| test/Q                         | -1.7000352 |
| test/Q_plus_P                  | -1.7000352 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 183200     |
| train/episodes                 | 18320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00449   |
| train/info_shaping_reward_mean | -0.0832    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 732800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.2045598   |
| stats_o/std                    | 0.088325255 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00161    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -1.3089857  |
| test/Q_plus_P                  | -1.3089857  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.524       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00533    |
| train/info_shaping_reward_mean | -0.088      |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19         |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 459         |
| stats_o/mean                   | 0.20455542  |
| stats_o/std                    | 0.088270836 |
| test/episodes                  | 4600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00143    |
| test/info_shaping_reward_mean  | -0.0578     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -2.0017338  |
| test/Q_plus_P                  | -2.0017338  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 184000      |
| train/episodes                 | 18400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.528       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00472    |
| train/info_shaping_reward_mean | -0.0781     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 736000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 460        |
| stats_o/mean                   | 0.204556   |
| stats_o/std                    | 0.08826357 |
| test/episodes                  | 4610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00221   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.413773  |
| test/Q_plus_P                  | -1.413773  |
| test/reward_per_eps            | -10        |
| test/steps                     | 184400     |
| train/episodes                 | 18440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.503      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00474   |
| train/info_shaping_reward_mean | -0.085     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 737600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 461        |
| stats_o/mean                   | 0.20455654 |
| stats_o/std                    | 0.08822794 |
| test/episodes                  | 4620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00219   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3718718 |
| test/Q_plus_P                  | -1.3718718 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 184800     |
| train/episodes                 | 18480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00488   |
| train/info_shaping_reward_mean | -0.0837    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 739200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 462         |
| stats_o/mean                   | 0.20456085  |
| stats_o/std                    | 0.088185236 |
| test/episodes                  | 4630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00063    |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.3857785  |
| test/Q_plus_P                  | -1.3857785  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 185200      |
| train/episodes                 | 18520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0059     |
| train/info_shaping_reward_mean | -0.072      |
| train/info_shaping_reward_min  | -0.197      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 740800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 463        |
| stats_o/mean                   | 0.20456022 |
| stats_o/std                    | 0.08812336 |
| test/episodes                  | 4640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00181   |
| test/info_shaping_reward_mean  | -0.0549    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.8381892 |
| test/Q_plus_P                  | -1.8381892 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 185600     |
| train/episodes                 | 18560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 742400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 464        |
| stats_o/mean                   | 0.20457883 |
| stats_o/std                    | 0.08810375 |
| test/episodes                  | 4650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00559   |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.6317673 |
| test/Q_plus_P                  | -1.6317673 |
| test/reward_per_eps            | -11        |
| test/steps                     | 186000     |
| train/episodes                 | 18600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.518      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.0792    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 744000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 465        |
| stats_o/mean                   | 0.20458145 |
| stats_o/std                    | 0.08809414 |
| test/episodes                  | 4660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0022    |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.9022493 |
| test/Q_plus_P                  | -1.9022493 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 186400     |
| train/episodes                 | 18640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.567      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00331   |
| train/info_shaping_reward_mean | -0.0734    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 745600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 466        |
| stats_o/mean                   | 0.20457621 |
| stats_o/std                    | 0.08810748 |
| test/episodes                  | 4670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00137   |
| test/info_shaping_reward_mean  | -0.0444    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.1746509 |
| test/Q_plus_P                  | -1.1746509 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 186800     |
| train/episodes                 | 18680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00407   |
| train/info_shaping_reward_mean | -0.0895    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 747200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 467        |
| stats_o/mean                   | 0.20458217 |
| stats_o/std                    | 0.08806722 |
| test/episodes                  | 4680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00336   |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2718618 |
| test/Q_plus_P                  | -1.2718618 |
| test/reward_per_eps            | -10        |
| test/steps                     | 187200     |
| train/episodes                 | 18720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.582      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00384   |
| train/info_shaping_reward_mean | -0.0693    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 748800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 468         |
| stats_o/mean                   | 0.20458317  |
| stats_o/std                    | 0.088053085 |
| test/episodes                  | 4690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000565   |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -1.4724417  |
| test/Q_plus_P                  | -1.4724417  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 187600      |
| train/episodes                 | 18760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.47        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00451    |
| train/info_shaping_reward_mean | -0.095      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.2       |
| train/steps                    | 750400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 469        |
| stats_o/mean                   | 0.20459051 |
| stats_o/std                    | 0.08805588 |
| test/episodes                  | 4700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00106   |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2901115 |
| test/Q_plus_P                  | -1.2901115 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 188000     |
| train/episodes                 | 18800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.524      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0044    |
| train/info_shaping_reward_mean | -0.0941    |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 752000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.2045967  |
| stats_o/std                    | 0.08800548 |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00138   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.3835541 |
| test/Q_plus_P                  | -1.3835541 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00453   |
| train/info_shaping_reward_mean | -0.0696    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 471         |
| stats_o/mean                   | 0.20458847  |
| stats_o/std                    | 0.087979406 |
| test/episodes                  | 4720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000987   |
| test/info_shaping_reward_mean  | -0.0447     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2213142  |
| test/Q_plus_P                  | -1.2213142  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 188800      |
| train/episodes                 | 18880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.557       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0786     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 755200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.20458716 |
| stats_o/std                    | 0.08792992 |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00164   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4258214 |
| test/Q_plus_P                  | -1.4258214 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00436   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 473        |
| stats_o/mean                   | 0.20458624 |
| stats_o/std                    | 0.08786978 |
| test/episodes                  | 4740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00282   |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3851264 |
| test/Q_plus_P                  | -1.3851264 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 189600     |
| train/episodes                 | 18960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00395   |
| train/info_shaping_reward_mean | -0.0693    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 758400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 474        |
| stats_o/mean                   | 0.20458017 |
| stats_o/std                    | 0.08782064 |
| test/episodes                  | 4750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4373374 |
| test/Q_plus_P                  | -1.4373374 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 190000     |
| train/episodes                 | 19000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00405   |
| train/info_shaping_reward_mean | -0.0719    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 760000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 475        |
| stats_o/mean                   | 0.20457937 |
| stats_o/std                    | 0.08777282 |
| test/episodes                  | 4760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00309   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2136755 |
| test/Q_plus_P                  | -1.2136755 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 190400     |
| train/episodes                 | 19040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00331   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 761600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 476        |
| stats_o/mean                   | 0.20457068 |
| stats_o/std                    | 0.08773535 |
| test/episodes                  | 4770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4568586 |
| test/Q_plus_P                  | -1.4568586 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 190800     |
| train/episodes                 | 19080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.575      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00384   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 763200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.20458329  |
| stats_o/std                    | 0.087736584 |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0488     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.4640547  |
| test/Q_plus_P                  | -1.4640547  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.536       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00419    |
| train/info_shaping_reward_mean | -0.081      |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 478        |
| stats_o/mean                   | 0.20458506 |
| stats_o/std                    | 0.08771966 |
| test/episodes                  | 4790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00132   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4628284 |
| test/Q_plus_P                  | -1.4628284 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 191600     |
| train/episodes                 | 19160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00313   |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 766400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 479        |
| stats_o/mean                   | 0.20459118 |
| stats_o/std                    | 0.08765814 |
| test/episodes                  | 4800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00336   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3168014 |
| test/Q_plus_P                  | -1.3168014 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 192000     |
| train/episodes                 | 19200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00338   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 768000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 480        |
| stats_o/mean                   | 0.20458932 |
| stats_o/std                    | 0.0876102  |
| test/episodes                  | 4810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00103   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.304873  |
| test/Q_plus_P                  | -1.304873  |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 192400     |
| train/episodes                 | 19240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.533      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0037    |
| train/info_shaping_reward_mean | -0.075     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 769600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 481         |
| stats_o/mean                   | 0.20459773  |
| stats_o/std                    | 0.087582216 |
| test/episodes                  | 4820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0501     |
| test/info_shaping_reward_min   | -0.207      |
| test/Q                         | -1.2543461  |
| test/Q_plus_P                  | -1.2543461  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 192800      |
| train/episodes                 | 19280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.531       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00607    |
| train/info_shaping_reward_mean | -0.0762     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.8       |
| train/steps                    | 771200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.20460239  |
| stats_o/std                    | 0.087537095 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00222    |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.191      |
| test/Q                         | -1.1358181  |
| test/Q_plus_P                  | -1.1358181  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.592       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00313    |
| train/info_shaping_reward_mean | -0.0726     |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 483        |
| stats_o/mean                   | 0.20460662 |
| stats_o/std                    | 0.08749856 |
| test/episodes                  | 4840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.675      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.0705    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -3.0465546 |
| test/Q_plus_P                  | -3.0465546 |
| test/reward_per_eps            | -13        |
| test/steps                     | 193600     |
| train/episodes                 | 19360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00471   |
| train/info_shaping_reward_mean | -0.0799    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 774400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 484        |
| stats_o/mean                   | 0.20460461 |
| stats_o/std                    | 0.08746787 |
| test/episodes                  | 4850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00186   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1503282 |
| test/Q_plus_P                  | -1.1503282 |
| test/reward_per_eps            | -9         |
| test/steps                     | 194000     |
| train/episodes                 | 19400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.558      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 776000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.20461343  |
| stats_o/std                    | 0.087457396 |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000745   |
| test/info_shaping_reward_mean  | -0.046      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3026417  |
| test/Q_plus_P                  | -1.3026417  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.562       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00417    |
| train/info_shaping_reward_mean | -0.0851     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 486         |
| stats_o/mean                   | 0.2046197   |
| stats_o/std                    | 0.087423116 |
| test/episodes                  | 4870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00227    |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.4454395  |
| test/Q_plus_P                  | -1.4454395  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 194800      |
| train/episodes                 | 19480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0038     |
| train/info_shaping_reward_mean | -0.0709     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 779200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 487        |
| stats_o/mean                   | 0.20462665 |
| stats_o/std                    | 0.08739525 |
| test/episodes                  | 4880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00261   |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2980168 |
| test/Q_plus_P                  | -1.2980168 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 195200     |
| train/episodes                 | 19520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00431   |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 780800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 488        |
| stats_o/mean                   | 0.2046389  |
| stats_o/std                    | 0.08737155 |
| test/episodes                  | 4890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00269   |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1428915 |
| test/Q_plus_P                  | -1.1428915 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 195600     |
| train/episodes                 | 19560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00451   |
| train/info_shaping_reward_mean | -0.0808    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 782400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 489        |
| stats_o/mean                   | 0.2046472  |
| stats_o/std                    | 0.08733626 |
| test/episodes                  | 4900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00299   |
| test/info_shaping_reward_mean  | -0.0436    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.273533  |
| test/Q_plus_P                  | -1.273533  |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 196000     |
| train/episodes                 | 19600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00493   |
| train/info_shaping_reward_mean | -0.0726    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 784000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 490        |
| stats_o/mean                   | 0.20463751 |
| stats_o/std                    | 0.0873007  |
| test/episodes                  | 4910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00386   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.3296826 |
| test/Q_plus_P                  | -1.3296826 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 196400     |
| train/episodes                 | 19640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.541      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00535   |
| train/info_shaping_reward_mean | -0.0798    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 785600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.2046542   |
| stats_o/std                    | 0.087322064 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00321    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.4536365  |
| test/Q_plus_P                  | -1.4536365  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.524       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00349    |
| train/info_shaping_reward_mean | -0.0924     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19         |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 492        |
| stats_o/mean                   | 0.20466472 |
| stats_o/std                    | 0.08731698 |
| test/episodes                  | 4930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00206   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4980545 |
| test/Q_plus_P                  | -1.4980545 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 197200     |
| train/episodes                 | 19720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.499      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00439   |
| train/info_shaping_reward_mean | -0.0958    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 788800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.20466237 |
| stats_o/std                    | 0.08727371 |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00276   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.1932431 |
| test/Q_plus_P                  | -1.1932431 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00527   |
| train/info_shaping_reward_mean | -0.0727    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.2046622   |
| stats_o/std                    | 0.087226495 |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00131    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.326539   |
| test/Q_plus_P                  | -1.326539   |
| test/reward_per_eps            | -10         |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.54        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00515    |
| train/info_shaping_reward_mean | -0.0806     |
| train/info_shaping_reward_min  | -0.219      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 495        |
| stats_o/mean                   | 0.20466177 |
| stats_o/std                    | 0.08718735 |
| test/episodes                  | 4960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3102313 |
| test/Q_plus_P                  | -1.3102313 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 198400     |
| train/episodes                 | 19840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00439   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 793600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 496        |
| stats_o/mean                   | 0.20466766 |
| stats_o/std                    | 0.08713453 |
| test/episodes                  | 4970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00237   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.247141  |
| test/Q_plus_P                  | -1.247141  |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 198800     |
| train/episodes                 | 19880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00467   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 795200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 497        |
| stats_o/mean                   | 0.20466588 |
| stats_o/std                    | 0.08709686 |
| test/episodes                  | 4980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00377   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3469636 |
| test/Q_plus_P                  | -1.3469636 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 199200     |
| train/episodes                 | 19920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00358   |
| train/info_shaping_reward_mean | -0.0748    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 796800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 498         |
| stats_o/mean                   | 0.20466228  |
| stats_o/std                    | 0.087061174 |
| test/episodes                  | 4990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00134    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.2870175  |
| test/Q_plus_P                  | -1.2870175  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 199600      |
| train/episodes                 | 19960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.573       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00446    |
| train/info_shaping_reward_mean | -0.0706     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 798400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 499        |
| stats_o/mean                   | 0.20466755 |
| stats_o/std                    | 0.08701744 |
| test/episodes                  | 5000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00481   |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0966923 |
| test/Q_plus_P                  | -1.0966923 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 200000     |
| train/episodes                 | 20000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.075     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 800000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 500         |
| stats_o/mean                   | 0.20467198  |
| stats_o/std                    | 0.086969204 |
| test/episodes                  | 5010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00262    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.2054046  |
| test/Q_plus_P                  | -1.2054046  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 200400      |
| train/episodes                 | 20040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00418    |
| train/info_shaping_reward_mean | -0.0674     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 801600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 501         |
| stats_o/mean                   | 0.20467146  |
| stats_o/std                    | 0.086912505 |
| test/episodes                  | 5020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00293    |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2043802  |
| test/Q_plus_P                  | -1.2043802  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 200800      |
| train/episodes                 | 20080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00459    |
| train/info_shaping_reward_mean | -0.0724     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 803200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 502        |
| stats_o/mean                   | 0.20468554 |
| stats_o/std                    | 0.08690174 |
| test/episodes                  | 5030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00153   |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5268549 |
| test/Q_plus_P                  | -1.5268549 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 201200     |
| train/episodes                 | 20120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0047    |
| train/info_shaping_reward_mean | -0.0844    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 804800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 503         |
| stats_o/mean                   | 0.20468624  |
| stats_o/std                    | 0.086865805 |
| test/episodes                  | 5040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00254    |
| test/info_shaping_reward_mean  | -0.0518     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.3229665  |
| test/Q_plus_P                  | -1.3229665  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 201600      |
| train/episodes                 | 20160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.538       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00355    |
| train/info_shaping_reward_mean | -0.0751     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.5       |
| train/steps                    | 806400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 504        |
| stats_o/mean                   | 0.20469652 |
| stats_o/std                    | 0.08682948 |
| test/episodes                  | 5050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00204   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2618424 |
| test/Q_plus_P                  | -1.2618424 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 202000     |
| train/episodes                 | 20200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00333   |
| train/info_shaping_reward_mean | -0.0722    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 808000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 505        |
| stats_o/mean                   | 0.20470162 |
| stats_o/std                    | 0.08677537 |
| test/episodes                  | 5060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00286   |
| test/info_shaping_reward_mean  | -0.0468    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.2610924 |
| test/Q_plus_P                  | -1.2610924 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 202400     |
| train/episodes                 | 20240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00363   |
| train/info_shaping_reward_mean | -0.0713    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 809600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.20470375  |
| stats_o/std                    | 0.086757466 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00209    |
| test/info_shaping_reward_mean  | -0.046      |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.2399675  |
| test/Q_plus_P                  | -1.2399675  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00434    |
| train/info_shaping_reward_mean | -0.0793     |
| train/info_shaping_reward_min  | -0.226      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 507        |
| stats_o/mean                   | 0.2047162  |
| stats_o/std                    | 0.08673212 |
| test/episodes                  | 5080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00194   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.563299  |
| test/Q_plus_P                  | -1.563299  |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 203200     |
| train/episodes                 | 20320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00387   |
| train/info_shaping_reward_mean | -0.0813    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 812800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 508        |
| stats_o/mean                   | 0.20471014 |
| stats_o/std                    | 0.08673327 |
| test/episodes                  | 5090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00275   |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4221119 |
| test/Q_plus_P                  | -1.4221119 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 203600     |
| train/episodes                 | 20360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00357   |
| train/info_shaping_reward_mean | -0.0767    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 814400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 509        |
| stats_o/mean                   | 0.20470607 |
| stats_o/std                    | 0.08671989 |
| test/episodes                  | 5100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00158   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3319883 |
| test/Q_plus_P                  | -1.3319883 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 204000     |
| train/episodes                 | 20400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.525      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.0834    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 816000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 510         |
| stats_o/mean                   | 0.20471819  |
| stats_o/std                    | 0.086723775 |
| test/episodes                  | 5110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00162    |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.308008   |
| test/Q_plus_P                  | -1.308008   |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 204400      |
| train/episodes                 | 20440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.542       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00573    |
| train/info_shaping_reward_mean | -0.0738     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.3       |
| train/steps                    | 817600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 511        |
| stats_o/mean                   | 0.20472634 |
| stats_o/std                    | 0.08668725 |
| test/episodes                  | 5120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00266   |
| test/info_shaping_reward_mean  | -0.047     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.3444128 |
| test/Q_plus_P                  | -1.3444128 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 204800     |
| train/episodes                 | 20480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00566   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 819200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 512         |
| stats_o/mean                   | 0.20472123  |
| stats_o/std                    | 0.086666934 |
| test/episodes                  | 5130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00171    |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.5565585  |
| test/Q_plus_P                  | -1.5565585  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 205200      |
| train/episodes                 | 20520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00527    |
| train/info_shaping_reward_mean | -0.0778     |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 820800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 513        |
| stats_o/mean                   | 0.20472279 |
| stats_o/std                    | 0.08663897 |
| test/episodes                  | 5140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00184   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.4865067 |
| test/Q_plus_P                  | -1.4865067 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 205600     |
| train/episodes                 | 20560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00391   |
| train/info_shaping_reward_mean | -0.0753    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 822400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 514        |
| stats_o/mean                   | 0.20472153 |
| stats_o/std                    | 0.08662002 |
| test/episodes                  | 5150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5314525 |
| test/Q_plus_P                  | -1.5314525 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 206000     |
| train/episodes                 | 20600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.517      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.0848    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 824000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 515        |
| stats_o/mean                   | 0.20471609 |
| stats_o/std                    | 0.08663624 |
| test/episodes                  | 5160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000742  |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3735203 |
| test/Q_plus_P                  | -1.3735203 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 206400     |
| train/episodes                 | 20640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.522      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00455   |
| train/info_shaping_reward_mean | -0.0901    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 825600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 516         |
| stats_o/mean                   | 0.20471296  |
| stats_o/std                    | 0.086605765 |
| test/episodes                  | 5170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.377406   |
| test/Q_plus_P                  | -1.377406   |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 206800      |
| train/episodes                 | 20680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00448    |
| train/info_shaping_reward_mean | -0.0723     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 827200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 517         |
| stats_o/mean                   | 0.20471483  |
| stats_o/std                    | 0.086558245 |
| test/episodes                  | 5180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000526   |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2693561  |
| test/Q_plus_P                  | -1.2693561  |
| test/reward_per_eps            | -10         |
| test/steps                     | 207200      |
| train/episodes                 | 20720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.561       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00275    |
| train/info_shaping_reward_mean | -0.074      |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 828800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 518        |
| stats_o/mean                   | 0.2047182  |
| stats_o/std                    | 0.08652926 |
| test/episodes                  | 5190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00173   |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.3678354 |
| test/Q_plus_P                  | -1.3678354 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 207600     |
| train/episodes                 | 20760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.567      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00396   |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 830400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 519         |
| stats_o/mean                   | 0.20471402  |
| stats_o/std                    | 0.086488456 |
| test/episodes                  | 5200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00334    |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.253616   |
| test/Q_plus_P                  | -1.253616   |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 208000      |
| train/episodes                 | 20800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.57        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00414    |
| train/info_shaping_reward_mean | -0.071      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 832000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 520        |
| stats_o/mean                   | 0.20471641 |
| stats_o/std                    | 0.08645465 |
| test/episodes                  | 5210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00353   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2766554 |
| test/Q_plus_P                  | -1.2766554 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 208400     |
| train/episodes                 | 20840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0662    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 833600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 521        |
| stats_o/mean                   | 0.20471871 |
| stats_o/std                    | 0.08643283 |
| test/episodes                  | 5220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00303   |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.3346741 |
| test/Q_plus_P                  | -1.3346741 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 208800     |
| train/episodes                 | 20880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0749    |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 835200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 522        |
| stats_o/mean                   | 0.20471723 |
| stats_o/std                    | 0.0863848  |
| test/episodes                  | 5230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00157   |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2438803 |
| test/Q_plus_P                  | -1.2438803 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 209200     |
| train/episodes                 | 20920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00396   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 836800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 523        |
| stats_o/mean                   | 0.20472218 |
| stats_o/std                    | 0.08635511 |
| test/episodes                  | 5240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000817  |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.3010298 |
| test/Q_plus_P                  | -1.3010298 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 209600     |
| train/episodes                 | 20960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00349   |
| train/info_shaping_reward_mean | -0.0818    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 838400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 524        |
| stats_o/mean                   | 0.20472708 |
| stats_o/std                    | 0.08633773 |
| test/episodes                  | 5250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000893  |
| test/info_shaping_reward_mean  | -0.0439    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1705488 |
| test/Q_plus_P                  | -1.1705488 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 210000     |
| train/episodes                 | 21000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.541      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00435   |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 840000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 525         |
| stats_o/mean                   | 0.20472997  |
| stats_o/std                    | 0.086290725 |
| test/episodes                  | 5260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00236    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.2509272  |
| test/Q_plus_P                  | -1.2509272  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 210400      |
| train/episodes                 | 21040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.58        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0705     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 841600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 526        |
| stats_o/mean                   | 0.20472467 |
| stats_o/std                    | 0.0862575  |
| test/episodes                  | 5270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00128   |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -1.3444116 |
| test/Q_plus_P                  | -1.3444116 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 210800     |
| train/episodes                 | 21080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00382   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 843200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 527         |
| stats_o/mean                   | 0.20471966  |
| stats_o/std                    | 0.086215325 |
| test/episodes                  | 5280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00248    |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0558485  |
| test/Q_plus_P                  | -1.0558485  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 211200      |
| train/episodes                 | 21120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00477    |
| train/info_shaping_reward_mean | -0.0712     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 844800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 528        |
| stats_o/mean                   | 0.20471889 |
| stats_o/std                    | 0.08616588 |
| test/episodes                  | 5290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000903  |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.2608432 |
| test/Q_plus_P                  | -1.2608432 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 211600     |
| train/episodes                 | 21160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00344   |
| train/info_shaping_reward_mean | -0.0676    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 846400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.20472845  |
| stats_o/std                    | 0.086155064 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00292    |
| test/info_shaping_reward_mean  | -0.0434     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.2787108  |
| test/Q_plus_P                  | -1.2787108  |
| test/reward_per_eps            | -9          |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00369    |
| train/info_shaping_reward_mean | -0.0762     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 530        |
| stats_o/mean                   | 0.2047344  |
| stats_o/std                    | 0.08612381 |
| test/episodes                  | 5310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00311   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.3231349 |
| test/Q_plus_P                  | -1.3231349 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 212400     |
| train/episodes                 | 21240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.572      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 849600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 531        |
| stats_o/mean                   | 0.20472185 |
| stats_o/std                    | 0.08609993 |
| test/episodes                  | 5320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00211   |
| test/info_shaping_reward_mean  | -0.0468    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3069808 |
| test/Q_plus_P                  | -1.3069808 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 212800     |
| train/episodes                 | 21280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00346   |
| train/info_shaping_reward_mean | -0.0799    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 851200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 532        |
| stats_o/mean                   | 0.20472181 |
| stats_o/std                    | 0.08606226 |
| test/episodes                  | 5330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000985  |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2401841 |
| test/Q_plus_P                  | -1.2401841 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 213200     |
| train/episodes                 | 21320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00472   |
| train/info_shaping_reward_mean | -0.0784    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 852800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.20472276  |
| stats_o/std                    | 0.086027525 |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00303    |
| test/info_shaping_reward_mean  | -0.0438     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1395605  |
| test/Q_plus_P                  | -1.1395605  |
| test/reward_per_eps            | -9          |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0031     |
| train/info_shaping_reward_mean | -0.0689     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 534        |
| stats_o/mean                   | 0.20472318 |
| stats_o/std                    | 0.08599179 |
| test/episodes                  | 5350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000994  |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.332097  |
| test/Q_plus_P                  | -1.332097  |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 214000     |
| train/episodes                 | 21400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.572      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 856000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 535        |
| stats_o/mean                   | 0.20472287 |
| stats_o/std                    | 0.08597453 |
| test/episodes                  | 5360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00145   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2641664 |
| test/Q_plus_P                  | -1.2641664 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 214400     |
| train/episodes                 | 21440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00427   |
| train/info_shaping_reward_mean | -0.0786    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 857600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 536        |
| stats_o/mean                   | 0.20472276 |
| stats_o/std                    | 0.08595259 |
| test/episodes                  | 5370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00321   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.261483  |
| test/Q_plus_P                  | -1.261483  |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 214800     |
| train/episodes                 | 21480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00376   |
| train/info_shaping_reward_mean | -0.0696    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 859200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 537        |
| stats_o/mean                   | 0.20472613 |
| stats_o/std                    | 0.08593315 |
| test/episodes                  | 5380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000325  |
| test/info_shaping_reward_mean  | -0.0441    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1296285 |
| test/Q_plus_P                  | -1.1296285 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 215200     |
| train/episodes                 | 21520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00416   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 860800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 538         |
| stats_o/mean                   | 0.20471741  |
| stats_o/std                    | 0.085903876 |
| test/episodes                  | 5390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.415973   |
| test/Q_plus_P                  | -1.415973   |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 215600      |
| train/episodes                 | 21560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.571       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00507    |
| train/info_shaping_reward_mean | -0.0705     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 862400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 539         |
| stats_o/mean                   | 0.20471813  |
| stats_o/std                    | 0.085870646 |
| test/episodes                  | 5400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0025     |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.2638845  |
| test/Q_plus_P                  | -1.2638845  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 216000      |
| train/episodes                 | 21600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.533       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00508    |
| train/info_shaping_reward_mean | -0.0869     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 864000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 540        |
| stats_o/mean                   | 0.20472193 |
| stats_o/std                    | 0.08584599 |
| test/episodes                  | 5410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00101   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5484532 |
| test/Q_plus_P                  | -1.5484532 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 216400     |
| train/episodes                 | 21640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.55       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.0743    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 865600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.20472096  |
| stats_o/std                    | 0.085813686 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00277    |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.4157403  |
| test/Q_plus_P                  | -1.4157403  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.572       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00444    |
| train/info_shaping_reward_mean | -0.0746     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 542         |
| stats_o/mean                   | 0.20470999  |
| stats_o/std                    | 0.085803166 |
| test/episodes                  | 5430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00099    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1516875  |
| test/Q_plus_P                  | -1.1516875  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 217200      |
| train/episodes                 | 21720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00394    |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 868800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 543        |
| stats_o/mean                   | 0.20470783 |
| stats_o/std                    | 0.08576229 |
| test/episodes                  | 5440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00113   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.3383112 |
| test/Q_plus_P                  | -1.3383112 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 217600     |
| train/episodes                 | 21760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00342   |
| train/info_shaping_reward_mean | -0.0788    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 870400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.20470034  |
| stats_o/std                    | 0.085745536 |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00283    |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -1.1350671  |
| test/Q_plus_P                  | -1.1350671  |
| test/reward_per_eps            | -9          |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00406    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.203      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 545        |
| stats_o/mean                   | 0.20470776 |
| stats_o/std                    | 0.08574541 |
| test/episodes                  | 5460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00275   |
| test/info_shaping_reward_mean  | -0.042     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0661784 |
| test/Q_plus_P                  | -1.0661784 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 218400     |
| train/episodes                 | 21840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00308   |
| train/info_shaping_reward_mean | -0.0833    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 873600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.20470859  |
| stats_o/std                    | 0.085741684 |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00287    |
| test/info_shaping_reward_mean  | -0.043      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.1879156  |
| test/Q_plus_P                  | -1.1879156  |
| test/reward_per_eps            | -9          |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.526       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0057     |
| train/info_shaping_reward_mean | -0.0772     |
| train/info_shaping_reward_min  | -0.206      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 547         |
| stats_o/mean                   | 0.20471135  |
| stats_o/std                    | 0.085711114 |
| test/episodes                  | 5480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000836   |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.191      |
| test/Q                         | -1.3053672  |
| test/Q_plus_P                  | -1.3053672  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 219200      |
| train/episodes                 | 21920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00377    |
| train/info_shaping_reward_mean | -0.0766     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 876800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 548        |
| stats_o/mean                   | 0.20471177 |
| stats_o/std                    | 0.08567538 |
| test/episodes                  | 5490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00158   |
| test/info_shaping_reward_mean  | -0.0454    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2038838 |
| test/Q_plus_P                  | -1.2038838 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 219600     |
| train/episodes                 | 21960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00407   |
| train/info_shaping_reward_mean | -0.0722    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 878400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 549        |
| stats_o/mean                   | 0.20471267 |
| stats_o/std                    | 0.08564278 |
| test/episodes                  | 5500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00315   |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1445379 |
| test/Q_plus_P                  | -1.1445379 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 220000     |
| train/episodes                 | 22000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00285   |
| train/info_shaping_reward_mean | -0.072     |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 880000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 550        |
| stats_o/mean                   | 0.20470949 |
| stats_o/std                    | 0.08560805 |
| test/episodes                  | 5510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00261   |
| test/info_shaping_reward_mean  | -0.043     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.051386  |
| test/Q_plus_P                  | -1.051386  |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 220400     |
| train/episodes                 | 22040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00354   |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 881600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 551        |
| stats_o/mean                   | 0.20471078 |
| stats_o/std                    | 0.08556188 |
| test/episodes                  | 5520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00252   |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1938607 |
| test/Q_plus_P                  | -1.1938607 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 220800     |
| train/episodes                 | 22080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 883200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 552        |
| stats_o/mean                   | 0.20470268 |
| stats_o/std                    | 0.08555233 |
| test/episodes                  | 5530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00206   |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.1857986 |
| test/Q_plus_P                  | -1.1857986 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 221200     |
| train/episodes                 | 22120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00346   |
| train/info_shaping_reward_mean | -0.0703    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 884800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 553        |
| stats_o/mean                   | 0.20470573 |
| stats_o/std                    | 0.08552421 |
| test/episodes                  | 5540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00145   |
| test/info_shaping_reward_mean  | -0.0457    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3319294 |
| test/Q_plus_P                  | -1.3319294 |
| test/reward_per_eps            | -10        |
| test/steps                     | 221600     |
| train/episodes                 | 22160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00421   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 886400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 554        |
| stats_o/mean                   | 0.20469818 |
| stats_o/std                    | 0.08548576 |
| test/episodes                  | 5550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00114   |
| test/info_shaping_reward_mean  | -0.0774    |
| test/info_shaping_reward_min   | -0.503     |
| test/Q                         | -4.2478957 |
| test/Q_plus_P                  | -4.2478957 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 222000     |
| train/episodes                 | 22200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00471   |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 888000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 555         |
| stats_o/mean                   | 0.20470724  |
| stats_o/std                    | 0.085473835 |
| test/episodes                  | 5560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1460613  |
| test/Q_plus_P                  | -1.1460613  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 222400      |
| train/episodes                 | 22240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.512       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00302    |
| train/info_shaping_reward_mean | -0.0845     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.5       |
| train/steps                    | 889600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 556        |
| stats_o/mean                   | 0.20470539 |
| stats_o/std                    | 0.08545521 |
| test/episodes                  | 5570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00154   |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2794341 |
| test/Q_plus_P                  | -1.2794341 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 222800     |
| train/episodes                 | 22280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00393   |
| train/info_shaping_reward_mean | -0.0747    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 891200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 557         |
| stats_o/mean                   | 0.20470382  |
| stats_o/std                    | 0.085432775 |
| test/episodes                  | 5580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -1.2165467  |
| test/Q_plus_P                  | -1.2165467  |
| test/reward_per_eps            | -9          |
| test/steps                     | 223200      |
| train/episodes                 | 22320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.526       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00383    |
| train/info_shaping_reward_mean | -0.0749     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19         |
| train/steps                    | 892800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 558        |
| stats_o/mean                   | 0.2047132  |
| stats_o/std                    | 0.0854253  |
| test/episodes                  | 5590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000563  |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2599933 |
| test/Q_plus_P                  | -1.2599933 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 223600     |
| train/episodes                 | 22360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00412   |
| train/info_shaping_reward_mean | -0.0774    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 894400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 559        |
| stats_o/mean                   | 0.20471214 |
| stats_o/std                    | 0.08540621 |
| test/episodes                  | 5600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00214   |
| test/info_shaping_reward_mean  | -0.0439    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1412827 |
| test/Q_plus_P                  | -1.1412827 |
| test/reward_per_eps            | -9         |
| test/steps                     | 224000     |
| train/episodes                 | 22400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00398   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 896000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 560         |
| stats_o/mean                   | 0.20471     |
| stats_o/std                    | 0.085385315 |
| test/episodes                  | 5610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00238    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.253802   |
| test/Q_plus_P                  | -1.253802   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 224400      |
| train/episodes                 | 22440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.578       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00506    |
| train/info_shaping_reward_mean | -0.0733     |
| train/info_shaping_reward_min  | -0.21       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 897600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 561        |
| stats_o/mean                   | 0.20470573 |
| stats_o/std                    | 0.08534296 |
| test/episodes                  | 5620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00243   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.4863484 |
| test/Q_plus_P                  | -1.4863484 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 224800     |
| train/episodes                 | 22480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0038    |
| train/info_shaping_reward_mean | -0.072     |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 899200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 562        |
| stats_o/mean                   | 0.2046994  |
| stats_o/std                    | 0.08533535 |
| test/episodes                  | 5630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00263   |
| test/info_shaping_reward_mean  | -0.046     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3746328 |
| test/Q_plus_P                  | -1.3746328 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 225200     |
| train/episodes                 | 22520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00291   |
| train/info_shaping_reward_mean | -0.0832    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 900800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 563        |
| stats_o/mean                   | 0.20468529 |
| stats_o/std                    | 0.08533016 |
| test/episodes                  | 5640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0013    |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1719772 |
| test/Q_plus_P                  | -1.1719772 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 225600     |
| train/episodes                 | 22560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0038    |
| train/info_shaping_reward_mean | -0.0715    |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 902400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 564        |
| stats_o/mean                   | 0.20468111 |
| stats_o/std                    | 0.08531213 |
| test/episodes                  | 5650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00178   |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2875786 |
| test/Q_plus_P                  | -1.2875786 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 226000     |
| train/episodes                 | 22600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00385   |
| train/info_shaping_reward_mean | -0.0734    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 904000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 565        |
| stats_o/mean                   | 0.20468254 |
| stats_o/std                    | 0.08530619 |
| test/episodes                  | 5660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0456    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3247476 |
| test/Q_plus_P                  | -1.3247476 |
| test/reward_per_eps            | -9         |
| test/steps                     | 226400     |
| train/episodes                 | 22640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.587      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00299   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 905600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 566        |
| stats_o/mean                   | 0.20468338 |
| stats_o/std                    | 0.08526542 |
| test/episodes                  | 5670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00142   |
| test/info_shaping_reward_mean  | -0.0418    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0926464 |
| test/Q_plus_P                  | -1.0926464 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 226800     |
| train/episodes                 | 22680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.575      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00378   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 907200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 567        |
| stats_o/mean                   | 0.20468755 |
| stats_o/std                    | 0.08524615 |
| test/episodes                  | 5680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00169   |
| test/info_shaping_reward_mean  | -0.0451    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1952456 |
| test/Q_plus_P                  | -1.1952456 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 227200     |
| train/episodes                 | 22720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.538      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00371   |
| train/info_shaping_reward_mean | -0.0882    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 908800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 568        |
| stats_o/mean                   | 0.20467663 |
| stats_o/std                    | 0.08521195 |
| test/episodes                  | 5690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00325   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2829229 |
| test/Q_plus_P                  | -1.2829229 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 227600     |
| train/episodes                 | 22760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00311   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 910400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 569        |
| stats_o/mean                   | 0.20468363 |
| stats_o/std                    | 0.08519213 |
| test/episodes                  | 5700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0444    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.1809316 |
| test/Q_plus_P                  | -1.1809316 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 228000     |
| train/episodes                 | 22800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00466   |
| train/info_shaping_reward_mean | -0.0741    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 912000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 570        |
| stats_o/mean                   | 0.20468323 |
| stats_o/std                    | 0.08517034 |
| test/episodes                  | 5710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000736  |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0720961 |
| test/Q_plus_P                  | -1.0720961 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 228400     |
| train/episodes                 | 22840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.595      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0038    |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 913600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 571        |
| stats_o/mean                   | 0.20469119 |
| stats_o/std                    | 0.0851557  |
| test/episodes                  | 5720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1450416 |
| test/Q_plus_P                  | -1.1450416 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 228800     |
| train/episodes                 | 22880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00393   |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 915200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.20469514  |
| stats_o/std                    | 0.085127994 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00189    |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.2156063  |
| test/Q_plus_P                  | -1.2156063  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.557       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00473    |
| train/info_shaping_reward_mean | -0.0748     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 573         |
| stats_o/mean                   | 0.20469566  |
| stats_o/std                    | 0.085126914 |
| test/episodes                  | 5740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000682   |
| test/info_shaping_reward_mean  | -0.0472     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.3447766  |
| test/Q_plus_P                  | -1.3447766  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 229600      |
| train/episodes                 | 22960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.567       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00543    |
| train/info_shaping_reward_mean | -0.0832     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 918400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 574        |
| stats_o/mean                   | 0.20469679 |
| stats_o/std                    | 0.08512634 |
| test/episodes                  | 5750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00206   |
| test/info_shaping_reward_mean  | -0.0447    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2700993 |
| test/Q_plus_P                  | -1.2700993 |
| test/reward_per_eps            | -9         |
| test/steps                     | 230000     |
| train/episodes                 | 23000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.513      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00463   |
| train/info_shaping_reward_mean | -0.0878    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.5      |
| train/steps                    | 920000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 575        |
| stats_o/mean                   | 0.20468785 |
| stats_o/std                    | 0.08510764 |
| test/episodes                  | 5760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00262   |
| test/info_shaping_reward_mean  | -0.0433    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.2161345 |
| test/Q_plus_P                  | -1.2161345 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 230400     |
| train/episodes                 | 23040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00373   |
| train/info_shaping_reward_mean | -0.0686    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 921600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.20469195  |
| stats_o/std                    | 0.085090846 |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00264    |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.165852   |
| test/Q_plus_P                  | -1.165852   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.541       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00484    |
| train/info_shaping_reward_mean | -0.0771     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 577        |
| stats_o/mean                   | 0.20469923 |
| stats_o/std                    | 0.08506195 |
| test/episodes                  | 5780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00301   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2922801 |
| test/Q_plus_P                  | -1.2922801 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 231200     |
| train/episodes                 | 23120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00342   |
| train/info_shaping_reward_mean | -0.076     |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 924800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.20470299 |
| stats_o/std                    | 0.08502409 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00159   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.3432612 |
| test/Q_plus_P                  | -1.3432612 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00374   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 579        |
| stats_o/mean                   | 0.20470135 |
| stats_o/std                    | 0.08500044 |
| test/episodes                  | 5800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000443  |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.131675  |
| test/Q_plus_P                  | -1.131675  |
| test/reward_per_eps            | -9         |
| test/steps                     | 232000     |
| train/episodes                 | 23200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00296   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 928000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 580        |
| stats_o/mean                   | 0.20470673 |
| stats_o/std                    | 0.08497429 |
| test/episodes                  | 5810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00135   |
| test/info_shaping_reward_mean  | -0.0437    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2363727 |
| test/Q_plus_P                  | -1.2363727 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 232400     |
| train/episodes                 | 23240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00272   |
| train/info_shaping_reward_mean | -0.0772    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 929600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 581        |
| stats_o/mean                   | 0.20471026 |
| stats_o/std                    | 0.08493068 |
| test/episodes                  | 5820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0417    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1944087 |
| test/Q_plus_P                  | -1.1944087 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 232800     |
| train/episodes                 | 23280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00282   |
| train/info_shaping_reward_mean | -0.0747    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 931200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 582        |
| stats_o/mean                   | 0.2047062  |
| stats_o/std                    | 0.0849048  |
| test/episodes                  | 5830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00147   |
| test/info_shaping_reward_mean  | -0.0444    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2025621 |
| test/Q_plus_P                  | -1.2025621 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 233200     |
| train/episodes                 | 23320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00437   |
| train/info_shaping_reward_mean | -0.0787    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 932800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 583        |
| stats_o/mean                   | 0.20470767 |
| stats_o/std                    | 0.08488055 |
| test/episodes                  | 5840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00308   |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2643563 |
| test/Q_plus_P                  | -1.2643563 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 233600     |
| train/episodes                 | 23360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.558      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00473   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 934400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 584         |
| stats_o/mean                   | 0.20470436  |
| stats_o/std                    | 0.084838666 |
| test/episodes                  | 5850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.042      |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.0579411  |
| test/Q_plus_P                  | -1.0579411  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 234000      |
| train/episodes                 | 23400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00379    |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 936000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 585        |
| stats_o/mean                   | 0.20470877 |
| stats_o/std                    | 0.08480429 |
| test/episodes                  | 5860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.2214319 |
| test/Q_plus_P                  | -1.2214319 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 234400     |
| train/episodes                 | 23440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00388   |
| train/info_shaping_reward_mean | -0.0688    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 937600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 586        |
| stats_o/mean                   | 0.20470715 |
| stats_o/std                    | 0.08478362 |
| test/episodes                  | 5870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00142   |
| test/info_shaping_reward_mean  | -0.0425    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1655256 |
| test/Q_plus_P                  | -1.1655256 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 234800     |
| train/episodes                 | 23480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00341   |
| train/info_shaping_reward_mean | -0.0667    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 939200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 587        |
| stats_o/mean                   | 0.2047027  |
| stats_o/std                    | 0.08475954 |
| test/episodes                  | 5880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00166   |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.1608936 |
| test/Q_plus_P                  | -1.1608936 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 235200     |
| train/episodes                 | 23520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00423   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 940800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 588        |
| stats_o/mean                   | 0.20470303 |
| stats_o/std                    | 0.08474936 |
| test/episodes                  | 5890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000661  |
| test/info_shaping_reward_mean  | -0.0412    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1587777 |
| test/Q_plus_P                  | -1.1587777 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 235600     |
| train/episodes                 | 23560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.005     |
| train/info_shaping_reward_mean | -0.0811    |
| train/info_shaping_reward_min  | -0.272     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 942400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 589         |
| stats_o/mean                   | 0.20470332  |
| stats_o/std                    | 0.084762335 |
| test/episodes                  | 5900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00289    |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.473786   |
| test/Q_plus_P                  | -1.473786   |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 236000      |
| train/episodes                 | 23600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.536       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00439    |
| train/info_shaping_reward_mean | -0.0922     |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 944000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.20470147  |
| stats_o/std                    | 0.084738426 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000775   |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2292081  |
| test/Q_plus_P                  | -1.2292081  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00414    |
| train/info_shaping_reward_mean | -0.074      |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 591        |
| stats_o/mean                   | 0.20470053 |
| stats_o/std                    | 0.0847188  |
| test/episodes                  | 5920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00208   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2423397 |
| test/Q_plus_P                  | -1.2423397 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 236800     |
| train/episodes                 | 23680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.533      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00349   |
| train/info_shaping_reward_mean | -0.0801    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 947200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 592         |
| stats_o/mean                   | 0.20470645  |
| stats_o/std                    | 0.084691115 |
| test/episodes                  | 5930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.3293566  |
| test/Q_plus_P                  | -1.3293566  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 237200      |
| train/episodes                 | 23720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.59        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0679     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 948800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.20470686  |
| stats_o/std                    | 0.084658585 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00222    |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.0801136  |
| test/Q_plus_P                  | -1.0801136  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.539       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00529    |
| train/info_shaping_reward_mean | -0.077      |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 594        |
| stats_o/mean                   | 0.20470423 |
| stats_o/std                    | 0.08466826 |
| test/episodes                  | 5950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000633  |
| test/info_shaping_reward_mean  | -0.0431    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1748621 |
| test/Q_plus_P                  | -1.1748621 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 238000     |
| train/episodes                 | 23800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00352   |
| train/info_shaping_reward_mean | -0.0884    |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 952000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 595        |
| stats_o/mean                   | 0.20469633 |
| stats_o/std                    | 0.08465679 |
| test/episodes                  | 5960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00238   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1578807 |
| test/Q_plus_P                  | -1.1578807 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 238400     |
| train/episodes                 | 23840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00401   |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 953600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 596        |
| stats_o/mean                   | 0.20468678 |
| stats_o/std                    | 0.08464472 |
| test/episodes                  | 5970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000806  |
| test/info_shaping_reward_mean  | -0.0434    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2552463 |
| test/Q_plus_P                  | -1.2552463 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 238800     |
| train/episodes                 | 23880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.526      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00404   |
| train/info_shaping_reward_mean | -0.0815    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 955200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 597        |
| stats_o/mean                   | 0.20469193 |
| stats_o/std                    | 0.08463053 |
| test/episodes                  | 5980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000965  |
| test/info_shaping_reward_mean  | -0.0434    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2297605 |
| test/Q_plus_P                  | -1.2297605 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 239200     |
| train/episodes                 | 23920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00388   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 956800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 598        |
| stats_o/mean                   | 0.20468979 |
| stats_o/std                    | 0.08462103 |
| test/episodes                  | 5990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00255   |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.127538  |
| test/Q_plus_P                  | -1.127538  |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 239600     |
| train/episodes                 | 23960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00413   |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 958400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.2046915  |
| stats_o/std                    | 0.08458165 |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00203   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2455602 |
| test/Q_plus_P                  | -1.2455602 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00411   |
| train/info_shaping_reward_mean | -0.0688    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
