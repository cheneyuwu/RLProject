Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPickAndPlace/config_TD3_BC_QFilter/q_filter_True/prm_loss_weight_0.0001/seed_1
-----------------------------------------------
| epoch                          | 0          |
| stats_o/mean                   | 0.20251909 |
| stats_o/std                    | 0.07325245 |
| test/episodes                  | 10         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.196     |
| test/info_shaping_reward_min   | -0.852     |
| test/Q                         | -1.3501035 |
| test/Q_plus_P                  | -1.3501035 |
| test/reward_per_eps            | -40        |
| test/steps                     | 400        |
| train/episodes                 | 40         |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 1600       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 1          |
| stats_o/mean                   | 0.20210095 |
| stats_o/std                    | 0.08020585 |
| test/episodes                  | 20         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.01       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0356    |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.576     |
| test/Q                         | -1.7031723 |
| test/Q_plus_P                  | -1.7031723 |
| test/reward_per_eps            | -39.6      |
| test/steps                     | 800        |
| train/episodes                 | 80         |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 3200       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 2          |
| stats_o/mean                   | 0.2002148  |
| stats_o/std                    | 0.08748066 |
| test/episodes                  | 30         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.193     |
| test/info_shaping_reward_min   | -0.414     |
| test/Q                         | -2.16122   |
| test/Q_plus_P                  | -2.16122   |
| test/reward_per_eps            | -40        |
| test/steps                     | 1200       |
| train/episodes                 | 120        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 4800       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.20017242 |
| stats_o/std                    | 0.09122139 |
| test/episodes                  | 40         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -2.3658316 |
| test/Q_plus_P                  | -2.3658316 |
| test/reward_per_eps            | -40        |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.201     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 4          |
| stats_o/mean                   | 0.2005766  |
| stats_o/std                    | 0.09190879 |
| test/episodes                  | 50         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.113      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00492   |
| test/info_shaping_reward_mean  | -0.138     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.5919213 |
| test/Q_plus_P                  | -2.5919213 |
| test/reward_per_eps            | -35.5      |
| test/steps                     | 2000       |
| train/episodes                 | 200        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0112     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.278     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 8000       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 5          |
| stats_o/mean                   | 0.20190436 |
| stats_o/std                    | 0.09420035 |
| test/episodes                  | 60         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.095      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00834   |
| test/info_shaping_reward_mean  | -0.148     |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -3.0159292 |
| test/Q_plus_P                  | -3.0159292 |
| test/reward_per_eps            | -36.2      |
| test/steps                     | 2400       |
| train/episodes                 | 240        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.372     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 9600       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 6           |
| stats_o/mean                   | 0.20129438  |
| stats_o/std                    | 0.097327195 |
| test/episodes                  | 70          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.135      |
| test/info_shaping_reward_mean  | -0.179      |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -3.5449612  |
| test/Q_plus_P                  | -3.5449612  |
| test/reward_per_eps            | -40         |
| test/steps                     | 2800        |
| train/episodes                 | 280         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0175      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.115      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.275      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 11200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 7          |
| stats_o/mean                   | 0.20076603 |
| stats_o/std                    | 0.09875681 |
| test/episodes                  | 80         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0877    |
| test/info_shaping_reward_mean  | -0.155     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -3.9013007 |
| test/Q_plus_P                  | -3.9013007 |
| test/reward_per_eps            | -40        |
| test/steps                     | 3200       |
| train/episodes                 | 320        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0075     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.192     |
| train/info_shaping_reward_min  | -0.308     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 12800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 8          |
| stats_o/mean                   | 0.20030068 |
| stats_o/std                    | 0.09752705 |
| test/episodes                  | 90         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.116     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.22      |
| test/Q                         | -4.4373927 |
| test/Q_plus_P                  | -4.4373927 |
| test/reward_per_eps            | -40        |
| test/steps                     | 3600       |
| train/episodes                 | 360        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0144     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 14400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 9          |
| stats_o/mean                   | 0.20064135 |
| stats_o/std                    | 0.09924933 |
| test/episodes                  | 100        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0941    |
| test/info_shaping_reward_mean  | -0.157     |
| test/info_shaping_reward_min   | -0.222     |
| test/Q                         | -4.7171125 |
| test/Q_plus_P                  | -4.7171125 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4000       |
| train/episodes                 | 400        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00375    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 16000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 10         |
| stats_o/mean                   | 0.20078564 |
| stats_o/std                    | 0.09945739 |
| test/episodes                  | 110        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.123     |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -5.20875   |
| test/Q_plus_P                  | -5.20875   |
| test/reward_per_eps            | -40        |
| test/steps                     | 4400       |
| train/episodes                 | 440        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00125    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.194     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 17600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 11         |
| stats_o/mean                   | 0.20081474 |
| stats_o/std                    | 0.09971588 |
| test/episodes                  | 120        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.122      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0155    |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -5.066161  |
| test/Q_plus_P                  | -5.066161  |
| test/reward_per_eps            | -35.1      |
| test/steps                     | 4800       |
| train/episodes                 | 480        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0244     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 19200      |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 12        |
| stats_o/mean                   | 0.2005932 |
| stats_o/std                    | 0.1012106 |
| test/episodes                  | 130       |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.0975    |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.0248   |
| test/info_shaping_reward_mean  | -0.126    |
| test/info_shaping_reward_min   | -0.182    |
| test/Q                         | -5.481162 |
| test/Q_plus_P                  | -5.481162 |
| test/reward_per_eps            | -36.1     |
| test/steps                     | 5200      |
| train/episodes                 | 520       |
| train/info_is_success_max      | 0.2       |
| train/info_is_success_mean     | 0.00562   |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.11     |
| train/info_shaping_reward_mean | -0.185    |
| train/info_shaping_reward_min  | -0.267    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -39.8     |
| train/steps                    | 20800     |
----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 13        |
| stats_o/mean                   | 0.2006427 |
| stats_o/std                    | 0.100667  |
| test/episodes                  | 140       |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.0575    |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.0149   |
| test/info_shaping_reward_mean  | -0.156    |
| test/info_shaping_reward_min   | -0.195    |
| test/Q                         | -6.154121 |
| test/Q_plus_P                  | -6.154121 |
| test/reward_per_eps            | -37.7     |
| test/steps                     | 5600      |
| train/episodes                 | 560       |
| train/info_is_success_max      | 0.1       |
| train/info_is_success_mean     | 0.000625  |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.118    |
| train/info_shaping_reward_mean | -0.17     |
| train/info_shaping_reward_min  | -0.205    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -40       |
| train/steps                    | 22400     |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 14         |
| stats_o/mean                   | 0.20030674 |
| stats_o/std                    | 0.1020857  |
| test/episodes                  | 150        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0125     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.043     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.312     |
| test/Q                         | -6.813645  |
| test/Q_plus_P                  | -6.813645  |
| test/reward_per_eps            | -39.5      |
| test/steps                     | 6000       |
| train/episodes                 | 600        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.025      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.194     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 24000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 15         |
| stats_o/mean                   | 0.2003836  |
| stats_o/std                    | 0.1037188  |
| test/episodes                  | 160        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.06       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0143    |
| test/info_shaping_reward_mean  | -0.138     |
| test/info_shaping_reward_min   | -0.202     |
| test/Q                         | -6.5383983 |
| test/Q_plus_P                  | -6.5383983 |
| test/reward_per_eps            | -37.6      |
| test/steps                     | 6400       |
| train/episodes                 | 640        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.194     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 25600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 16         |
| stats_o/mean                   | 0.20033012 |
| stats_o/std                    | 0.10335694 |
| test/episodes                  | 170        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0425     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0343    |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -7.2778773 |
| test/Q_plus_P                  | -7.2778773 |
| test/reward_per_eps            | -38.3      |
| test/steps                     | 6800       |
| train/episodes                 | 680        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0175     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 27200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 17          |
| stats_o/mean                   | 0.20023863  |
| stats_o/std                    | 0.102539375 |
| test/episodes                  | 180         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0375      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0094     |
| test/info_shaping_reward_mean  | -0.152      |
| test/info_shaping_reward_min   | -0.197      |
| test/Q                         | -7.5786896  |
| test/Q_plus_P                  | -7.5786896  |
| test/reward_per_eps            | -38.5       |
| test/steps                     | 7200        |
| train/episodes                 | 720         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.184      |
| train/info_shaping_reward_min  | -0.28       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 28800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 18         |
| stats_o/mean                   | 0.20006423 |
| stats_o/std                    | 0.102586   |
| test/episodes                  | 190        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00761   |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.277     |
| test/Q                         | -7.947045  |
| test/Q_plus_P                  | -7.947045  |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 7600       |
| train/episodes                 | 760        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0025     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.3       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 30400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.20026201 |
| stats_o/std                    | 0.10331579 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.24       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0118    |
| test/info_shaping_reward_mean  | -0.121     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -6.832653  |
| test/Q_plus_P                  | -6.832653  |
| test/reward_per_eps            | -30.4      |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0075     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.121     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 20          |
| stats_o/mean                   | 0.20010132  |
| stats_o/std                    | 0.104286015 |
| test/episodes                  | 210         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.089      |
| test/info_shaping_reward_mean  | -0.143      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -8.7674675  |
| test/Q_plus_P                  | -8.7674675  |
| test/reward_per_eps            | -40         |
| test/steps                     | 8400        |
| train/episodes                 | 840         |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0131      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.111      |
| train/info_shaping_reward_mean | -0.209      |
| train/info_shaping_reward_min  | -0.475      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 33600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 21         |
| stats_o/mean                   | 0.20005916 |
| stats_o/std                    | 0.10542392 |
| test/episodes                  | 220        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.215      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00528   |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.209     |
| test/Q                         | -7.626534  |
| test/Q_plus_P                  | -7.626534  |
| test/reward_per_eps            | -31.4      |
| test/steps                     | 8800       |
| train/episodes                 | 880        |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.03       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0965    |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 35200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 22         |
| stats_o/mean                   | 0.19983266 |
| stats_o/std                    | 0.10734388 |
| test/episodes                  | 230        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.242      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0107    |
| test/info_shaping_reward_mean  | -0.115     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -7.036656  |
| test/Q_plus_P                  | -7.036656  |
| test/reward_per_eps            | -30.3      |
| test/steps                     | 9200       |
| train/episodes                 | 920        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.01       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.101     |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.468     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 36800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 23         |
| stats_o/mean                   | 0.19996375 |
| stats_o/std                    | 0.10676455 |
| test/episodes                  | 240        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.168      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00966   |
| test/info_shaping_reward_mean  | -0.149     |
| test/info_shaping_reward_min   | -0.497     |
| test/Q                         | -8.705776  |
| test/Q_plus_P                  | -8.705776  |
| test/reward_per_eps            | -33.3      |
| test/steps                     | 9600       |
| train/episodes                 | 960        |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0075     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.11      |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 38400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 24         |
| stats_o/mean                   | 0.1998316  |
| stats_o/std                    | 0.10746105 |
| test/episodes                  | 250        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.357      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00749   |
| test/info_shaping_reward_mean  | -0.1       |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -7.0443115 |
| test/Q_plus_P                  | -7.0443115 |
| test/reward_per_eps            | -25.7      |
| test/steps                     | 10000      |
| train/episodes                 | 1000       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0125     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.109     |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.284     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 40000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 25         |
| stats_o/mean                   | 0.19997863 |
| stats_o/std                    | 0.10816721 |
| test/episodes                  | 260        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.145      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00247   |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.236     |
| test/Q                         | -8.708119  |
| test/Q_plus_P                  | -8.708119  |
| test/reward_per_eps            | -34.2      |
| test/steps                     | 10400      |
| train/episodes                 | 1040       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0138     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 41600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 26         |
| stats_o/mean                   | 0.19991308 |
| stats_o/std                    | 0.10785341 |
| test/episodes                  | 270        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.32       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00773   |
| test/info_shaping_reward_mean  | -0.115     |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -7.899149  |
| test/Q_plus_P                  | -7.899149  |
| test/reward_per_eps            | -27.2      |
| test/steps                     | 10800      |
| train/episodes                 | 1080       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0238     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.103     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 43200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 27         |
| stats_o/mean                   | 0.20000237 |
| stats_o/std                    | 0.10793748 |
| test/episodes                  | 280        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.165      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00763   |
| test/info_shaping_reward_mean  | -0.126     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -8.8683195 |
| test/Q_plus_P                  | -8.8683195 |
| test/reward_per_eps            | -33.4      |
| test/steps                     | 11200      |
| train/episodes                 | 1120       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00562    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.109     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.304     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 44800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.20014377 |
| stats_o/std                    | 0.10789046 |
| test/episodes                  | 290        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.352      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00873   |
| test/info_shaping_reward_mean  | -0.103     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -7.4934254 |
| test/Q_plus_P                  | -7.4934254 |
| test/reward_per_eps            | -25.9      |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0212     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.115     |
| train/info_shaping_reward_mean | -0.192     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.20010178 |
| stats_o/std                    | 0.10826193 |
| test/episodes                  | 300        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.347      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0206    |
| test/info_shaping_reward_mean  | -0.11      |
| test/info_shaping_reward_min   | -0.214     |
| test/Q                         | -7.7287445 |
| test/Q_plus_P                  | -7.7287445 |
| test/reward_per_eps            | -26.1      |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0319     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0959    |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 30         |
| stats_o/mean                   | 0.20035735 |
| stats_o/std                    | 0.10898138 |
| test/episodes                  | 310        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.273      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00632   |
| test/info_shaping_reward_mean  | -0.108     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -8.029058  |
| test/Q_plus_P                  | -8.029058  |
| test/reward_per_eps            | -29.1      |
| test/steps                     | 12400      |
| train/episodes                 | 1240       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00813    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.105     |
| train/info_shaping_reward_mean | -0.204     |
| train/info_shaping_reward_min  | -0.43      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 49600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 31         |
| stats_o/mean                   | 0.2003609  |
| stats_o/std                    | 0.10915413 |
| test/episodes                  | 320        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.333      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0123    |
| test/info_shaping_reward_mean  | -0.115     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -8.188759  |
| test/Q_plus_P                  | -8.188759  |
| test/reward_per_eps            | -26.7      |
| test/steps                     | 12800      |
| train/episodes                 | 1280       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0281     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.116     |
| train/info_shaping_reward_mean | -0.213     |
| train/info_shaping_reward_min  | -0.448     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 51200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 32         |
| stats_o/mean                   | 0.2005157  |
| stats_o/std                    | 0.10991154 |
| test/episodes                  | 330        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.6        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00621   |
| test/info_shaping_reward_mean  | -0.079     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -4.874485  |
| test/Q_plus_P                  | -4.874485  |
| test/reward_per_eps            | -16        |
| test/steps                     | 13200      |
| train/episodes                 | 1320       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0212     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.086     |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 52800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 33         |
| stats_o/mean                   | 0.20059103 |
| stats_o/std                    | 0.11052975 |
| test/episodes                  | 340        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.405      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.0984    |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -7.0253572 |
| test/Q_plus_P                  | -7.0253572 |
| test/reward_per_eps            | -23.8      |
| test/steps                     | 13600      |
| train/episodes                 | 1360       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0325     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.112     |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.39      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 54400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 34         |
| stats_o/mean                   | 0.20085695 |
| stats_o/std                    | 0.11025983 |
| test/episodes                  | 350        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.347      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00405   |
| test/info_shaping_reward_mean  | -0.1       |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -8.166202  |
| test/Q_plus_P                  | -8.166202  |
| test/reward_per_eps            | -26.1      |
| test/steps                     | 14000      |
| train/episodes                 | 1400       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0125     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0966    |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.369     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 56000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.20071003 |
| stats_o/std                    | 0.11087368 |
| test/episodes                  | 360        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.305      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00526   |
| test/info_shaping_reward_mean  | -0.126     |
| test/info_shaping_reward_min   | -0.601     |
| test/Q                         | -9.847932  |
| test/Q_plus_P                  | -9.847932  |
| test/reward_per_eps            | -27.8      |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0119     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0983    |
| train/info_shaping_reward_mean | -0.203     |
| train/info_shaping_reward_min  | -0.401     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 36         |
| stats_o/mean                   | 0.2006612  |
| stats_o/std                    | 0.11097867 |
| test/episodes                  | 370        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.398      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00279   |
| test/info_shaping_reward_mean  | -0.105     |
| test/info_shaping_reward_min   | -0.219     |
| test/Q                         | -8.794827  |
| test/Q_plus_P                  | -8.794827  |
| test/reward_per_eps            | -24.1      |
| test/steps                     | 14800      |
| train/episodes                 | 1480       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0481     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0761    |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.371     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.1      |
| train/steps                    | 59200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 37         |
| stats_o/mean                   | 0.20093481 |
| stats_o/std                    | 0.11183626 |
| test/episodes                  | 380        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.295      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00389   |
| test/info_shaping_reward_mean  | -0.113     |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -9.444505  |
| test/Q_plus_P                  | -9.444505  |
| test/reward_per_eps            | -28.2      |
| test/steps                     | 15200      |
| train/episodes                 | 1520       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0256     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.101     |
| train/info_shaping_reward_mean | -0.207     |
| train/info_shaping_reward_min  | -0.445     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 60800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 38         |
| stats_o/mean                   | 0.20088126 |
| stats_o/std                    | 0.11275335 |
| test/episodes                  | 390        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.472      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00917   |
| test/info_shaping_reward_mean  | -0.0998    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -7.9197736 |
| test/Q_plus_P                  | -7.9197736 |
| test/reward_per_eps            | -21.1      |
| test/steps                     | 15600      |
| train/episodes                 | 1560       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0575     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0628    |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.43      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.7      |
| train/steps                    | 62400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 39          |
| stats_o/mean                   | 0.20065336  |
| stats_o/std                    | 0.113270216 |
| test/episodes                  | 400         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.468       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00347    |
| test/info_shaping_reward_mean  | -0.0917     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -7.4353275  |
| test/Q_plus_P                  | -7.4353275  |
| test/reward_per_eps            | -21.3       |
| test/steps                     | 16000       |
| train/episodes                 | 1600        |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0606      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0706     |
| train/info_shaping_reward_mean | -0.166      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.6       |
| train/steps                    | 64000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.20054421  |
| stats_o/std                    | 0.113655634 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.318       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00506    |
| test/info_shaping_reward_mean  | -0.127      |
| test/info_shaping_reward_min   | -0.208      |
| test/Q                         | -10.124351  |
| test/Q_plus_P                  | -10.124351  |
| test/reward_per_eps            | -27.3       |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0256      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0975     |
| train/info_shaping_reward_mean | -0.217      |
| train/info_shaping_reward_min  | -0.428      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39         |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 41         |
| stats_o/mean                   | 0.20055793 |
| stats_o/std                    | 0.11381698 |
| test/episodes                  | 420        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00675   |
| test/info_shaping_reward_mean  | -0.138     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -13.259631 |
| test/Q_plus_P                  | -13.259631 |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 16800      |
| train/episodes                 | 1680       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0991    |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.317     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 67200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 42         |
| stats_o/mean                   | 0.20052963 |
| stats_o/std                    | 0.11412228 |
| test/episodes                  | 430        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.495      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00831   |
| test/info_shaping_reward_mean  | -0.0834    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -7.2931185 |
| test/Q_plus_P                  | -7.2931185 |
| test/reward_per_eps            | -20.2      |
| test/steps                     | 17200      |
| train/episodes                 | 1720       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.03       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.106     |
| train/info_shaping_reward_mean | -0.21      |
| train/info_shaping_reward_min  | -0.428     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 68800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 43          |
| stats_o/mean                   | 0.20038986  |
| stats_o/std                    | 0.114543706 |
| test/episodes                  | 440         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.487       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00345    |
| test/info_shaping_reward_mean  | -0.0869     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -8.342237   |
| test/Q_plus_P                  | -8.342237   |
| test/reward_per_eps            | -20.5       |
| test/steps                     | 17600       |
| train/episodes                 | 1760        |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0563      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0695     |
| train/info_shaping_reward_mean | -0.185      |
| train/info_shaping_reward_min  | -0.384      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.8       |
| train/steps                    | 70400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 44         |
| stats_o/mean                   | 0.20069356 |
| stats_o/std                    | 0.11526697 |
| test/episodes                  | 450        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.435      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00319   |
| test/info_shaping_reward_mean  | -0.107     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -7.5402923 |
| test/Q_plus_P                  | -7.5402923 |
| test/reward_per_eps            | -22.6      |
| test/steps                     | 18000      |
| train/episodes                 | 1800       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0138     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.109     |
| train/info_shaping_reward_mean | -0.207     |
| train/info_shaping_reward_min  | -0.49      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 72000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 45         |
| stats_o/mean                   | 0.200878   |
| stats_o/std                    | 0.1156776  |
| test/episodes                  | 460        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.463      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00197   |
| test/info_shaping_reward_mean  | -0.0965    |
| test/info_shaping_reward_min   | -0.216     |
| test/Q                         | -8.2347355 |
| test/Q_plus_P                  | -8.2347355 |
| test/reward_per_eps            | -21.5      |
| test/steps                     | 18400      |
| train/episodes                 | 1840       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.02       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.096     |
| train/info_shaping_reward_mean | -0.21      |
| train/info_shaping_reward_min  | -0.517     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 73600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 46         |
| stats_o/mean                   | 0.20094319 |
| stats_o/std                    | 0.1156266  |
| test/episodes                  | 470        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.32       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00523   |
| test/info_shaping_reward_mean  | -0.106     |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -10.152316 |
| test/Q_plus_P                  | -10.152316 |
| test/reward_per_eps            | -27.2      |
| test/steps                     | 18800      |
| train/episodes                 | 1880       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.01       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.118     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.293     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 75200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 47         |
| stats_o/mean                   | 0.20080976 |
| stats_o/std                    | 0.11610706 |
| test/episodes                  | 480        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.525      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00274   |
| test/info_shaping_reward_mean  | -0.0787    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -6.8588853 |
| test/Q_plus_P                  | -6.8588853 |
| test/reward_per_eps            | -19        |
| test/steps                     | 19200      |
| train/episodes                 | 1920       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.045      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0785    |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.2      |
| train/steps                    | 76800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 48         |
| stats_o/mean                   | 0.2007845  |
| stats_o/std                    | 0.11631617 |
| test/episodes                  | 490        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.398      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00279   |
| test/info_shaping_reward_mean  | -0.0999    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -9.264207  |
| test/Q_plus_P                  | -9.264207  |
| test/reward_per_eps            | -24.1      |
| test/steps                     | 19600      |
| train/episodes                 | 1960       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0281     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.101     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.3       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 78400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 49         |
| stats_o/mean                   | 0.20085771 |
| stats_o/std                    | 0.1164205  |
| test/episodes                  | 500        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.477      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00234   |
| test/info_shaping_reward_mean  | -0.0899    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -7.3862596 |
| test/Q_plus_P                  | -7.3862596 |
| test/reward_per_eps            | -20.9      |
| test/steps                     | 20000      |
| train/episodes                 | 2000       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0625     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0598    |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.398     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 80000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.20080723 |
| stats_o/std                    | 0.116578   |
| test/episodes                  | 510        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.492      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0035    |
| test/info_shaping_reward_mean  | -0.0831    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -7.7311707 |
| test/Q_plus_P                  | -7.7311707 |
| test/reward_per_eps            | -20.3      |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0194     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0714    |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.301     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.20074013 |
| stats_o/std                    | 0.11707844 |
| test/episodes                  | 520        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.552      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00102   |
| test/info_shaping_reward_mean  | -0.0739    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -6.602367  |
| test/Q_plus_P                  | -6.602367  |
| test/reward_per_eps            | -17.9      |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.04       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0907    |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 52          |
| stats_o/mean                   | 0.20076162  |
| stats_o/std                    | 0.116788864 |
| test/episodes                  | 530         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.603       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00207    |
| test/info_shaping_reward_mean  | -0.0695     |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -5.3141747  |
| test/Q_plus_P                  | -5.3141747  |
| test/reward_per_eps            | -15.9       |
| test/steps                     | 21200       |
| train/episodes                 | 2120        |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.025       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0815     |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39         |
| train/steps                    | 84800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 53         |
| stats_o/mean                   | 0.20067857 |
| stats_o/std                    | 0.1168284  |
| test/episodes                  | 540        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.42       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00291   |
| test/info_shaping_reward_mean  | -0.0958    |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -9.29064   |
| test/Q_plus_P                  | -9.29064   |
| test/reward_per_eps            | -23.2      |
| test/steps                     | 21600      |
| train/episodes                 | 2160       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0475     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0667    |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.415     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.1      |
| train/steps                    | 86400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 54         |
| stats_o/mean                   | 0.20059592 |
| stats_o/std                    | 0.1169605  |
| test/episodes                  | 550        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.328      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000505  |
| test/info_shaping_reward_mean  | -0.106     |
| test/info_shaping_reward_min   | -0.211     |
| test/Q                         | -11.915191 |
| test/Q_plus_P                  | -11.915191 |
| test/reward_per_eps            | -26.9      |
| test/steps                     | 22000      |
| train/episodes                 | 2200       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.05       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0591    |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.326     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38        |
| train/steps                    | 88000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 55         |
| stats_o/mean                   | 0.20067881 |
| stats_o/std                    | 0.11702615 |
| test/episodes                  | 560        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.345      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00129   |
| test/info_shaping_reward_mean  | -0.0966    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -10.373538 |
| test/Q_plus_P                  | -10.373538 |
| test/reward_per_eps            | -26.2      |
| test/steps                     | 22400      |
| train/episodes                 | 2240       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0406     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0641    |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.288     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 89600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 56         |
| stats_o/mean                   | 0.20076433 |
| stats_o/std                    | 0.11732874 |
| test/episodes                  | 570        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.512      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00129   |
| test/info_shaping_reward_mean  | -0.0785    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -6.844908  |
| test/Q_plus_P                  | -6.844908  |
| test/reward_per_eps            | -19.5      |
| test/steps                     | 22800      |
| train/episodes                 | 2280       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0125     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.103     |
| train/info_shaping_reward_mean | -0.204     |
| train/info_shaping_reward_min  | -0.409     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 91200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 57          |
| stats_o/mean                   | 0.20078334  |
| stats_o/std                    | 0.117703535 |
| test/episodes                  | 580         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.412       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00127    |
| test/info_shaping_reward_mean  | -0.102      |
| test/info_shaping_reward_min   | -0.324      |
| test/Q                         | -9.779447   |
| test/Q_plus_P                  | -9.779447   |
| test/reward_per_eps            | -23.5       |
| test/steps                     | 23200       |
| train/episodes                 | 2320        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0119      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.106      |
| train/info_shaping_reward_mean | -0.199      |
| train/info_shaping_reward_min  | -0.372      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 92800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 58         |
| stats_o/mean                   | 0.2007135  |
| stats_o/std                    | 0.11792688 |
| test/episodes                  | 590        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.365      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00145   |
| test/info_shaping_reward_mean  | -0.103     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -10.024622 |
| test/Q_plus_P                  | -10.024622 |
| test/reward_per_eps            | -25.4      |
| test/steps                     | 23600      |
| train/episodes                 | 2360       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0119     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0826    |
| train/info_shaping_reward_mean | -0.207     |
| train/info_shaping_reward_min  | -0.594     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 94400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 59         |
| stats_o/mean                   | 0.20093888 |
| stats_o/std                    | 0.11831818 |
| test/episodes                  | 600        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.495      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000875  |
| test/info_shaping_reward_mean  | -0.0829    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -7.5063105 |
| test/Q_plus_P                  | -7.5063105 |
| test/reward_per_eps            | -20.2      |
| test/steps                     | 24000      |
| train/episodes                 | 2400       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0194     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0993    |
| train/info_shaping_reward_mean | -0.204     |
| train/info_shaping_reward_min  | -0.44      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 96000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 60         |
| stats_o/mean                   | 0.20104828 |
| stats_o/std                    | 0.11885435 |
| test/episodes                  | 610        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.443      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00256   |
| test/info_shaping_reward_mean  | -0.0939    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -8.729733  |
| test/Q_plus_P                  | -8.729733  |
| test/reward_per_eps            | -22.3      |
| test/steps                     | 24400      |
| train/episodes                 | 2440       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0144     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.101     |
| train/info_shaping_reward_mean | -0.213     |
| train/info_shaping_reward_min  | -0.564     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 97600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 61         |
| stats_o/mean                   | 0.20106195 |
| stats_o/std                    | 0.11923726 |
| test/episodes                  | 620        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.45       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00261   |
| test/info_shaping_reward_mean  | -0.091     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -8.183101  |
| test/Q_plus_P                  | -8.183101  |
| test/reward_per_eps            | -22        |
| test/steps                     | 24800      |
| train/episodes                 | 2480       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0381     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0861    |
| train/info_shaping_reward_mean | -0.196     |
| train/info_shaping_reward_min  | -0.422     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 99200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 62         |
| stats_o/mean                   | 0.20098902 |
| stats_o/std                    | 0.11944816 |
| test/episodes                  | 630        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.417      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00437   |
| test/info_shaping_reward_mean  | -0.116     |
| test/info_shaping_reward_min   | -0.677     |
| test/Q                         | -8.974836  |
| test/Q_plus_P                  | -8.974836  |
| test/reward_per_eps            | -23.3      |
| test/steps                     | 25200      |
| train/episodes                 | 2520       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0263     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0763    |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 100800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 63         |
| stats_o/mean                   | 0.20105666 |
| stats_o/std                    | 0.11958728 |
| test/episodes                  | 640        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.27       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.117     |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -11.519147 |
| test/Q_plus_P                  | -11.519147 |
| test/reward_per_eps            | -29.2      |
| test/steps                     | 25600      |
| train/episodes                 | 2560       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00188    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.106     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.365     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 102400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 64          |
| stats_o/mean                   | 0.20109086  |
| stats_o/std                    | 0.119530305 |
| test/episodes                  | 650         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.33        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00132    |
| test/info_shaping_reward_mean  | -0.111      |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -10.794177  |
| test/Q_plus_P                  | -10.794177  |
| test/reward_per_eps            | -26.8       |
| test/steps                     | 26000       |
| train/episodes                 | 2600        |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.01        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0962     |
| train/info_shaping_reward_mean | -0.189      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 104000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 65         |
| stats_o/mean                   | 0.2011677  |
| stats_o/std                    | 0.11932213 |
| test/episodes                  | 660        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.37       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00204   |
| test/info_shaping_reward_mean  | -0.106     |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -10.39826  |
| test/Q_plus_P                  | -10.39826  |
| test/reward_per_eps            | -25.2      |
| test/steps                     | 26400      |
| train/episodes                 | 2640       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00375    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 105600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 66          |
| stats_o/mean                   | 0.20123483  |
| stats_o/std                    | 0.119113006 |
| test/episodes                  | 670         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.325       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.101      |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -9.979159   |
| test/Q_plus_P                  | -9.979159   |
| test/reward_per_eps            | -27         |
| test/steps                     | 26800       |
| train/episodes                 | 2680        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0194      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.105      |
| train/info_shaping_reward_mean | -0.18       |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 107200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 67          |
| stats_o/mean                   | 0.20126607  |
| stats_o/std                    | 0.118703455 |
| test/episodes                  | 680         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.287       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00153    |
| test/info_shaping_reward_mean  | -0.124      |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -11.874568  |
| test/Q_plus_P                  | -11.874568  |
| test/reward_per_eps            | -28.5       |
| test/steps                     | 27200       |
| train/episodes                 | 2720        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0269      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.114      |
| train/info_shaping_reward_mean | -0.166      |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.9       |
| train/steps                    | 108800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 68         |
| stats_o/mean                   | 0.20125602 |
| stats_o/std                    | 0.11838347 |
| test/episodes                  | 690        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.393      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00184   |
| test/info_shaping_reward_mean  | -0.0993    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -9.597261  |
| test/Q_plus_P                  | -9.597261  |
| test/reward_per_eps            | -24.3      |
| test/steps                     | 27600      |
| train/episodes                 | 2760       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0212     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0835    |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 110400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 69         |
| stats_o/mean                   | 0.2013308  |
| stats_o/std                    | 0.11829092 |
| test/episodes                  | 700        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.48       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00158   |
| test/info_shaping_reward_mean  | -0.0878    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -7.7806044 |
| test/Q_plus_P                  | -7.7806044 |
| test/reward_per_eps            | -20.8      |
| test/steps                     | 28000      |
| train/episodes                 | 2800       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0256     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0896    |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 112000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.20128167 |
| stats_o/std                    | 0.11831751 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.49       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00096   |
| test/info_shaping_reward_mean  | -0.0871    |
| test/info_shaping_reward_min   | -0.203     |
| test/Q                         | -7.4062223 |
| test/Q_plus_P                  | -7.4062223 |
| test/reward_per_eps            | -20.4      |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.025      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0612    |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 71         |
| stats_o/mean                   | 0.20121834 |
| stats_o/std                    | 0.1181122  |
| test/episodes                  | 720        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.45       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00154   |
| test/info_shaping_reward_mean  | -0.0968    |
| test/info_shaping_reward_min   | -0.226     |
| test/Q                         | -9.440546  |
| test/Q_plus_P                  | -9.440546  |
| test/reward_per_eps            | -22        |
| test/steps                     | 28800      |
| train/episodes                 | 2880       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0275     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0719    |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 115200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 72         |
| stats_o/mean                   | 0.20131947 |
| stats_o/std                    | 0.11802346 |
| test/episodes                  | 730        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.555      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0782    |
| test/info_shaping_reward_min   | -0.202     |
| test/Q                         | -6.813714  |
| test/Q_plus_P                  | -6.813714  |
| test/reward_per_eps            | -17.8      |
| test/steps                     | 29200      |
| train/episodes                 | 2920       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.035      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0573    |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 116800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.20135112 |
| stats_o/std                    | 0.1177749  |
| test/episodes                  | 740        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.55       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00236   |
| test/info_shaping_reward_mean  | -0.0767    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -7.0757656 |
| test/Q_plus_P                  | -7.0757656 |
| test/reward_per_eps            | -18        |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0287     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0722    |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 74         |
| stats_o/mean                   | 0.20134111 |
| stats_o/std                    | 0.1176519  |
| test/episodes                  | 750        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.33       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000961  |
| test/info_shaping_reward_mean  | -0.119     |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -11.508217 |
| test/Q_plus_P                  | -11.508217 |
| test/reward_per_eps            | -26.8      |
| test/steps                     | 30000      |
| train/episodes                 | 3000       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.04       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0802    |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.301     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 120000     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 75        |
| stats_o/mean                   | 0.2013402 |
| stats_o/std                    | 0.1173703 |
| test/episodes                  | 760       |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.422     |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.00429  |
| test/info_shaping_reward_mean  | -0.113    |
| test/info_shaping_reward_min   | -0.399    |
| test/Q                         | -9.293362 |
| test/Q_plus_P                  | -9.293362 |
| test/reward_per_eps            | -23.1     |
| test/steps                     | 30400     |
| train/episodes                 | 3040      |
| train/info_is_success_max      | 0.4       |
| train/info_is_success_mean     | 0.0363    |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.0733   |
| train/info_shaping_reward_mean | -0.165    |
| train/info_shaping_reward_min  | -0.231    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -38.5     |
| train/steps                    | 121600    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 76         |
| stats_o/mean                   | 0.20140518 |
| stats_o/std                    | 0.11710201 |
| test/episodes                  | 770        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.53       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00181   |
| test/info_shaping_reward_mean  | -0.0838    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -8.324447  |
| test/Q_plus_P                  | -8.324447  |
| test/reward_per_eps            | -18.8      |
| test/steps                     | 30800      |
| train/episodes                 | 3080       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0456     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0724    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.2      |
| train/steps                    | 123200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 77         |
| stats_o/mean                   | 0.20132598 |
| stats_o/std                    | 0.1171918  |
| test/episodes                  | 780        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.27       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00412   |
| test/info_shaping_reward_mean  | -0.12      |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -11.673509 |
| test/Q_plus_P                  | -11.673509 |
| test/reward_per_eps            | -29.2      |
| test/steps                     | 31200      |
| train/episodes                 | 3120       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0594     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0617    |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.6      |
| train/steps                    | 124800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 78         |
| stats_o/mean                   | 0.20126894 |
| stats_o/std                    | 0.11686543 |
| test/episodes                  | 790        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.522      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00381   |
| test/info_shaping_reward_mean  | -0.0796    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -7.5777125 |
| test/Q_plus_P                  | -7.5777125 |
| test/reward_per_eps            | -19.1      |
| test/steps                     | 31600      |
| train/episodes                 | 3160       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0156     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.073     |
| train/info_shaping_reward_mean | -0.162     |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 126400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 79         |
| stats_o/mean                   | 0.20137878 |
| stats_o/std                    | 0.11674502 |
| test/episodes                  | 800        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.455      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00329   |
| test/info_shaping_reward_mean  | -0.108     |
| test/info_shaping_reward_min   | -0.516     |
| test/Q                         | -8.30151   |
| test/Q_plus_P                  | -8.30151   |
| test/reward_per_eps            | -21.8      |
| test/steps                     | 32000      |
| train/episodes                 | 3200       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0375     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0616    |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.291     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 128000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.20135076  |
| stats_o/std                    | 0.116641104 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.54        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.0798     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -6.916824   |
| test/Q_plus_P                  | -6.916824   |
| test/reward_per_eps            | -18.4       |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0494      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0461     |
| train/info_shaping_reward_mean | -0.163      |
| train/info_shaping_reward_min  | -0.28       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38         |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 81         |
| stats_o/mean                   | 0.20144928 |
| stats_o/std                    | 0.11647643 |
| test/episodes                  | 820        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.47       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000718  |
| test/info_shaping_reward_mean  | -0.0866    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -7.931069  |
| test/Q_plus_P                  | -7.931069  |
| test/reward_per_eps            | -21.2      |
| test/steps                     | 32800      |
| train/episodes                 | 3280       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0338     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0626    |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 131200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 82         |
| stats_o/mean                   | 0.201505   |
| stats_o/std                    | 0.11624274 |
| test/episodes                  | 830        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.49       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00245   |
| test/info_shaping_reward_mean  | -0.086     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -7.2480903 |
| test/Q_plus_P                  | -7.2480903 |
| test/reward_per_eps            | -20.4      |
| test/steps                     | 33200      |
| train/episodes                 | 3320       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0563     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0607    |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.301     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 132800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 83         |
| stats_o/mean                   | 0.20155388 |
| stats_o/std                    | 0.11605654 |
| test/episodes                  | 840        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.542      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00487   |
| test/info_shaping_reward_mean  | -0.0767    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -5.849616  |
| test/Q_plus_P                  | -5.849616  |
| test/reward_per_eps            | -18.3      |
| test/steps                     | 33600      |
| train/episodes                 | 3360       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0713     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0446    |
| train/info_shaping_reward_mean | -0.152     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.1      |
| train/steps                    | 134400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 84          |
| stats_o/mean                   | 0.20163448  |
| stats_o/std                    | 0.116194546 |
| test/episodes                  | 850         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.515       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.0767     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -7.3706164  |
| test/Q_plus_P                  | -7.3706164  |
| test/reward_per_eps            | -19.4       |
| test/steps                     | 34000       |
| train/episodes                 | 3400        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0487      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0552     |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38         |
| train/steps                    | 136000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.20172969  |
| stats_o/std                    | 0.116002284 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.5         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00186    |
| test/info_shaping_reward_mean  | -0.0884     |
| test/info_shaping_reward_min   | -0.194      |
| test/Q                         | -8.304461   |
| test/Q_plus_P                  | -8.304461   |
| test/reward_per_eps            | -20         |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.0794      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0339     |
| train/info_shaping_reward_mean | -0.151      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.8       |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 86         |
| stats_o/mean                   | 0.20172882 |
| stats_o/std                    | 0.11607479 |
| test/episodes                  | 870        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.54       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000693  |
| test/info_shaping_reward_mean  | -0.0761    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -6.272215  |
| test/Q_plus_P                  | -6.272215  |
| test/reward_per_eps            | -18.4      |
| test/steps                     | 34800      |
| train/episodes                 | 3480       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0306     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0474    |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.307     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 139200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 87          |
| stats_o/mean                   | 0.2017344   |
| stats_o/std                    | 0.115847304 |
| test/episodes                  | 880         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.485       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00182    |
| test/info_shaping_reward_mean  | -0.0814     |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -7.3619018  |
| test/Q_plus_P                  | -7.3619018  |
| test/reward_per_eps            | -20.6       |
| test/steps                     | 35200       |
| train/episodes                 | 3520        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.0494      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0319     |
| train/info_shaping_reward_mean | -0.157      |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38         |
| train/steps                    | 140800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 88         |
| stats_o/mean                   | 0.20174576 |
| stats_o/std                    | 0.11573689 |
| test/episodes                  | 890        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.525      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0012    |
| test/info_shaping_reward_mean  | -0.0825    |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -7.707725  |
| test/Q_plus_P                  | -7.707725  |
| test/reward_per_eps            | -19        |
| test/steps                     | 35600      |
| train/episodes                 | 3560       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0713     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0421    |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.1      |
| train/steps                    | 142400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 89         |
| stats_o/mean                   | 0.20180915 |
| stats_o/std                    | 0.11567506 |
| test/episodes                  | 900        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.5        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00114   |
| test/info_shaping_reward_mean  | -0.0814    |
| test/info_shaping_reward_min   | -0.211     |
| test/Q                         | -6.0544157 |
| test/Q_plus_P                  | -6.0544157 |
| test/reward_per_eps            | -20        |
| test/steps                     | 36000      |
| train/episodes                 | 3600       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0369     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0469    |
| train/info_shaping_reward_mean | -0.167     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 144000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 90         |
| stats_o/mean                   | 0.20186825 |
| stats_o/std                    | 0.1158407  |
| test/episodes                  | 910        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.547      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00135   |
| test/info_shaping_reward_mean  | -0.0789    |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -6.571781  |
| test/Q_plus_P                  | -6.571781  |
| test/reward_per_eps            | -18.1      |
| test/steps                     | 36400      |
| train/episodes                 | 3640       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.0881     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0267    |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.402     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.5      |
| train/steps                    | 145600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 91         |
| stats_o/mean                   | 0.20186776 |
| stats_o/std                    | 0.11600601 |
| test/episodes                  | 920        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.527      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0006    |
| test/info_shaping_reward_mean  | -0.0851    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -6.7339735 |
| test/Q_plus_P                  | -6.7339735 |
| test/reward_per_eps            | -18.9      |
| test/steps                     | 36800      |
| train/episodes                 | 3680       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0338     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.053     |
| train/info_shaping_reward_mean | -0.196     |
| train/info_shaping_reward_min  | -0.483     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 147200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 92         |
| stats_o/mean                   | 0.2019464  |
| stats_o/std                    | 0.11626567 |
| test/episodes                  | 930        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.565      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00353   |
| test/info_shaping_reward_mean  | -0.0764    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -5.3941407 |
| test/Q_plus_P                  | -5.3941407 |
| test/reward_per_eps            | -17.4      |
| test/steps                     | 37200      |
| train/episodes                 | 3720       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0925     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0443    |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.426     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.3      |
| train/steps                    | 148800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 93         |
| stats_o/mean                   | 0.2019472  |
| stats_o/std                    | 0.11637695 |
| test/episodes                  | 940        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.525      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.0798    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -7.14047   |
| test/Q_plus_P                  | -7.14047   |
| test/reward_per_eps            | -19        |
| test/steps                     | 37600      |
| train/episodes                 | 3760       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0694     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0355    |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.398     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.2      |
| train/steps                    | 150400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 94          |
| stats_o/mean                   | 0.20206071  |
| stats_o/std                    | 0.116557576 |
| test/episodes                  | 950         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.48        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00112    |
| test/info_shaping_reward_mean  | -0.0924     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -8.249733   |
| test/Q_plus_P                  | -8.249733   |
| test/reward_per_eps            | -20.8       |
| test/steps                     | 38000       |
| train/episodes                 | 3800        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.0437      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0388     |
| train/info_shaping_reward_mean | -0.185      |
| train/info_shaping_reward_min  | -0.375      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.2       |
| train/steps                    | 152000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 95         |
| stats_o/mean                   | 0.20212275 |
| stats_o/std                    | 0.1164873  |
| test/episodes                  | 960        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.525      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00103   |
| test/info_shaping_reward_mean  | -0.0838    |
| test/info_shaping_reward_min   | -0.217     |
| test/Q                         | -6.099006  |
| test/Q_plus_P                  | -6.099006  |
| test/reward_per_eps            | -19        |
| test/steps                     | 38400      |
| train/episodes                 | 3840       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0869     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0316    |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.5      |
| train/steps                    | 153600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 96         |
| stats_o/mean                   | 0.20215708 |
| stats_o/std                    | 0.11644168 |
| test/episodes                  | 970        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.505      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000923  |
| test/info_shaping_reward_mean  | -0.0831    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -6.8801546 |
| test/Q_plus_P                  | -6.8801546 |
| test/reward_per_eps            | -19.8      |
| test/steps                     | 38800      |
| train/episodes                 | 3880       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.06       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0438    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.305     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.6      |
| train/steps                    | 155200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 97          |
| stats_o/mean                   | 0.20216548  |
| stats_o/std                    | 0.116354555 |
| test/episodes                  | 980         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.398       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00145    |
| test/info_shaping_reward_mean  | -0.0957     |
| test/info_shaping_reward_min   | -0.199      |
| test/Q                         | -7.9028873  |
| test/Q_plus_P                  | -7.9028873  |
| test/reward_per_eps            | -24.1       |
| test/steps                     | 39200       |
| train/episodes                 | 3920        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.0669      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0333     |
| train/info_shaping_reward_mean | -0.149      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.3       |
| train/steps                    | 156800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 98          |
| stats_o/mean                   | 0.2022445   |
| stats_o/std                    | 0.116152294 |
| test/episodes                  | 990         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.427       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.0954     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -7.6803174  |
| test/Q_plus_P                  | -7.6803174  |
| test/reward_per_eps            | -22.9       |
| test/steps                     | 39600       |
| train/episodes                 | 3960        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.0544      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0346     |
| train/info_shaping_reward_mean | -0.146      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.8       |
| train/steps                    | 158400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.20226711 |
| stats_o/std                    | 0.11608821 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.525      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00196   |
| test/info_shaping_reward_mean  | -0.0803    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -6.0865474 |
| test/Q_plus_P                  | -6.0865474 |
| test/reward_per_eps            | -19        |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.0831     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0306    |
| train/info_shaping_reward_mean | -0.159     |
| train/info_shaping_reward_min  | -0.291     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.7      |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 100        |
| stats_o/mean                   | 0.20228535 |
| stats_o/std                    | 0.11603188 |
| test/episodes                  | 1010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.415      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00144   |
| test/info_shaping_reward_mean  | -0.0975    |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -8.965168  |
| test/Q_plus_P                  | -8.965168  |
| test/reward_per_eps            | -23.4      |
| test/steps                     | 40400      |
| train/episodes                 | 4040       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0531     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0405    |
| train/info_shaping_reward_mean | -0.157     |
| train/info_shaping_reward_min  | -0.276     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.9      |
| train/steps                    | 161600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 101        |
| stats_o/mean                   | 0.202315   |
| stats_o/std                    | 0.11587936 |
| test/episodes                  | 1020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.495      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00388   |
| test/info_shaping_reward_mean  | -0.0877    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -6.7573924 |
| test/Q_plus_P                  | -6.7573924 |
| test/reward_per_eps            | -20.2      |
| test/steps                     | 40800      |
| train/episodes                 | 4080       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.125      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.024     |
| train/info_shaping_reward_mean | -0.152     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35        |
| train/steps                    | 163200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 102         |
| stats_o/mean                   | 0.20229818  |
| stats_o/std                    | 0.115665846 |
| test/episodes                  | 1030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.53        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00184    |
| test/info_shaping_reward_mean  | -0.0824     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -6.5369473  |
| test/Q_plus_P                  | -6.5369473  |
| test/reward_per_eps            | -18.8       |
| test/steps                     | 41200       |
| train/episodes                 | 4120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.0894      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0218     |
| train/info_shaping_reward_mean | -0.15       |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.4       |
| train/steps                    | 164800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 103        |
| stats_o/mean                   | 0.20238687 |
| stats_o/std                    | 0.11559271 |
| test/episodes                  | 1040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.59       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0718    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -5.8965273 |
| test/Q_plus_P                  | -5.8965273 |
| test/reward_per_eps            | -16.4      |
| test/steps                     | 41600      |
| train/episodes                 | 4160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.17       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0151    |
| train/info_shaping_reward_mean | -0.139     |
| train/info_shaping_reward_min  | -0.287     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.2      |
| train/steps                    | 166400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 104         |
| stats_o/mean                   | 0.20238234  |
| stats_o/std                    | 0.115409784 |
| test/episodes                  | 1050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.58        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00163    |
| test/info_shaping_reward_mean  | -0.0766     |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -6.085471   |
| test/Q_plus_P                  | -6.085471   |
| test/reward_per_eps            | -16.8       |
| test/steps                     | 42000       |
| train/episodes                 | 4200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.179       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0143     |
| train/info_shaping_reward_mean | -0.145      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.9       |
| train/steps                    | 168000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 105         |
| stats_o/mean                   | 0.2024597   |
| stats_o/std                    | 0.115207806 |
| test/episodes                  | 1060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.487       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00167    |
| test/info_shaping_reward_mean  | -0.0919     |
| test/info_shaping_reward_min   | -0.499      |
| test/Q                         | -7.3616905  |
| test/Q_plus_P                  | -7.3616905  |
| test/reward_per_eps            | -20.5       |
| test/steps                     | 42400       |
| train/episodes                 | 4240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.186       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.013      |
| train/info_shaping_reward_mean | -0.143      |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.5       |
| train/steps                    | 169600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 106        |
| stats_o/mean                   | 0.20246923 |
| stats_o/std                    | 0.11500821 |
| test/episodes                  | 1070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.618      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00278   |
| test/info_shaping_reward_mean  | -0.0682    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -5.3615184 |
| test/Q_plus_P                  | -5.3615184 |
| test/reward_per_eps            | -15.3      |
| test/steps                     | 42800      |
| train/episodes                 | 4280       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.159      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0225    |
| train/info_shaping_reward_mean | -0.131     |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.6      |
| train/steps                    | 171200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 107        |
| stats_o/mean                   | 0.20247419 |
| stats_o/std                    | 0.11495457 |
| test/episodes                  | 1080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.595      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00212   |
| test/info_shaping_reward_mean  | -0.0725    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -5.372101  |
| test/Q_plus_P                  | -5.372101  |
| test/reward_per_eps            | -16.2      |
| test/steps                     | 43200      |
| train/episodes                 | 4320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.121      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.021     |
| train/info_shaping_reward_mean | -0.152     |
| train/info_shaping_reward_min  | -0.275     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.2      |
| train/steps                    | 172800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 108        |
| stats_o/mean                   | 0.20244822 |
| stats_o/std                    | 0.11475019 |
| test/episodes                  | 1090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.562      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00217   |
| test/info_shaping_reward_mean  | -0.0783    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -6.787014  |
| test/Q_plus_P                  | -6.787014  |
| test/reward_per_eps            | -17.5      |
| test/steps                     | 43600      |
| train/episodes                 | 4360       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.144      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0219    |
| train/info_shaping_reward_mean | -0.144     |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.2      |
| train/steps                    | 174400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 109        |
| stats_o/mean                   | 0.20247011 |
| stats_o/std                    | 0.11442913 |
| test/episodes                  | 1100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.495      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0048    |
| test/info_shaping_reward_mean  | -0.0876    |
| test/info_shaping_reward_min   | -0.213     |
| test/Q                         | -6.4691076 |
| test/Q_plus_P                  | -6.4691076 |
| test/reward_per_eps            | -20.2      |
| test/steps                     | 44000      |
| train/episodes                 | 4400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.183      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0191    |
| train/info_shaping_reward_mean | -0.119     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.7      |
| train/steps                    | 176000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 110        |
| stats_o/mean                   | 0.20246062 |
| stats_o/std                    | 0.11420224 |
| test/episodes                  | 1110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.605      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00128   |
| test/info_shaping_reward_mean  | -0.0698    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -5.1226416 |
| test/Q_plus_P                  | -5.1226416 |
| test/reward_per_eps            | -15.8      |
| test/steps                     | 44400      |
| train/episodes                 | 4440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.159      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0181    |
| train/info_shaping_reward_mean | -0.123     |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.6      |
| train/steps                    | 177600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 111        |
| stats_o/mean                   | 0.20247547 |
| stats_o/std                    | 0.1141418  |
| test/episodes                  | 1120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.565      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00115   |
| test/info_shaping_reward_mean  | -0.0782    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -6.1498766 |
| test/Q_plus_P                  | -6.1498766 |
| test/reward_per_eps            | -17.4      |
| test/steps                     | 44800      |
| train/episodes                 | 4480       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.107      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.022     |
| train/info_shaping_reward_mean | -0.146     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.7      |
| train/steps                    | 179200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 112         |
| stats_o/mean                   | 0.20239344  |
| stats_o/std                    | 0.114025764 |
| test/episodes                  | 1130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.583       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00216    |
| test/info_shaping_reward_mean  | -0.0755     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -5.4471507  |
| test/Q_plus_P                  | -5.4471507  |
| test/reward_per_eps            | -16.7       |
| test/steps                     | 45200       |
| train/episodes                 | 4520        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.149       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0203     |
| train/info_shaping_reward_mean | -0.141      |
| train/info_shaping_reward_min  | -0.299      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34         |
| train/steps                    | 180800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 113        |
| stats_o/mean                   | 0.20239222 |
| stats_o/std                    | 0.1137529  |
| test/episodes                  | 1140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.627      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0014    |
| test/info_shaping_reward_mean  | -0.0658    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -4.410315  |
| test/Q_plus_P                  | -4.410315  |
| test/reward_per_eps            | -14.9      |
| test/steps                     | 45600      |
| train/episodes                 | 4560       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.143      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0234    |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.3      |
| train/steps                    | 182400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 114         |
| stats_o/mean                   | 0.20240211  |
| stats_o/std                    | 0.113473386 |
| test/episodes                  | 1150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.65        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000773   |
| test/info_shaping_reward_mean  | -0.0619     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -4.662438   |
| test/Q_plus_P                  | -4.662438   |
| test/reward_per_eps            | -14         |
| test/steps                     | 46000       |
| train/episodes                 | 4600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.189       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0125     |
| train/info_shaping_reward_mean | -0.12       |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.5       |
| train/steps                    | 184000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 115        |
| stats_o/mean                   | 0.2024299  |
| stats_o/std                    | 0.11319004 |
| test/episodes                  | 1160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.598      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00152   |
| test/info_shaping_reward_mean  | -0.0725    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -5.437925  |
| test/Q_plus_P                  | -5.437925  |
| test/reward_per_eps            | -16.1      |
| test/steps                     | 46400      |
| train/episodes                 | 4640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.207      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0125    |
| train/info_shaping_reward_mean | -0.121     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.7      |
| train/steps                    | 185600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 116        |
| stats_o/mean                   | 0.20241147 |
| stats_o/std                    | 0.11304726 |
| test/episodes                  | 1170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.515      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00331   |
| test/info_shaping_reward_mean  | -0.088     |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -6.513653  |
| test/Q_plus_P                  | -6.513653  |
| test/reward_per_eps            | -19.4      |
| test/steps                     | 46800      |
| train/episodes                 | 4680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.185      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0166    |
| train/info_shaping_reward_mean | -0.131     |
| train/info_shaping_reward_min  | -0.289     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.6      |
| train/steps                    | 187200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 117        |
| stats_o/mean                   | 0.20242076 |
| stats_o/std                    | 0.11285391 |
| test/episodes                  | 1180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.527      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00114   |
| test/info_shaping_reward_mean  | -0.0803    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -5.963379  |
| test/Q_plus_P                  | -5.963379  |
| test/reward_per_eps            | -18.9      |
| test/steps                     | 47200      |
| train/episodes                 | 4720       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.145      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0235    |
| train/info_shaping_reward_mean | -0.136     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.2      |
| train/steps                    | 188800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.20241967  |
| stats_o/std                    | 0.112892196 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.593       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0714     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -4.381456   |
| test/Q_plus_P                  | -4.381456   |
| test/reward_per_eps            | -16.3       |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.176       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0116     |
| train/info_shaping_reward_mean | -0.139      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33         |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 119        |
| stats_o/mean                   | 0.20242785 |
| stats_o/std                    | 0.11275095 |
| test/episodes                  | 1200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.412      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00383   |
| test/info_shaping_reward_mean  | -0.0915    |
| test/info_shaping_reward_min   | -0.227     |
| test/Q                         | -8.270108  |
| test/Q_plus_P                  | -8.270108  |
| test/reward_per_eps            | -23.5      |
| test/steps                     | 48000      |
| train/episodes                 | 4800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.168      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0142    |
| train/info_shaping_reward_mean | -0.144     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.3      |
| train/steps                    | 192000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 120         |
| stats_o/mean                   | 0.20243135  |
| stats_o/std                    | 0.112540826 |
| test/episodes                  | 1210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.515       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00371    |
| test/info_shaping_reward_mean  | -0.079      |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -5.9009748  |
| test/Q_plus_P                  | -5.9009748  |
| test/reward_per_eps            | -19.4       |
| test/steps                     | 48400       |
| train/episodes                 | 4840        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.168       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.014      |
| train/info_shaping_reward_mean | -0.131      |
| train/info_shaping_reward_min  | -0.29       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.3       |
| train/steps                    | 193600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.20238283 |
| stats_o/std                    | 0.11237372 |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.573      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00593   |
| test/info_shaping_reward_mean  | -0.0771    |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -5.0773225 |
| test/Q_plus_P                  | -5.0773225 |
| test/reward_per_eps            | -17.1      |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.154      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.014     |
| train/info_shaping_reward_mean | -0.132     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.9      |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 122        |
| stats_o/mean                   | 0.2023572  |
| stats_o/std                    | 0.11226446 |
| test/episodes                  | 1230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.627      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00201   |
| test/info_shaping_reward_mean  | -0.0691    |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -3.9818509 |
| test/Q_plus_P                  | -3.9818509 |
| test/reward_per_eps            | -14.9      |
| test/steps                     | 49200      |
| train/episodes                 | 4920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.183      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0139    |
| train/info_shaping_reward_mean | -0.141     |
| train/info_shaping_reward_min  | -0.294     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.7      |
| train/steps                    | 196800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 123         |
| stats_o/mean                   | 0.20235519  |
| stats_o/std                    | 0.112219855 |
| test/episodes                  | 1240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.575       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00163    |
| test/info_shaping_reward_mean  | -0.0728     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -4.784882   |
| test/Q_plus_P                  | -4.784882   |
| test/reward_per_eps            | -17         |
| test/steps                     | 49600       |
| train/episodes                 | 4960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.222       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0165     |
| train/info_shaping_reward_mean | -0.13       |
| train/info_shaping_reward_min  | -0.293      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.1       |
| train/steps                    | 198400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.20242397  |
| stats_o/std                    | 0.112038195 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.603       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0721     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -4.566787   |
| test/Q_plus_P                  | -4.566787   |
| test/reward_per_eps            | -15.9       |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.205       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0143     |
| train/info_shaping_reward_mean | -0.141      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.8       |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 125        |
| stats_o/mean                   | 0.20244314 |
| stats_o/std                    | 0.11178988 |
| test/episodes                  | 1260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000389  |
| test/info_shaping_reward_mean  | -0.068     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -4.2786803 |
| test/Q_plus_P                  | -4.2786803 |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 50400      |
| train/episodes                 | 5040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.216      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0139    |
| train/info_shaping_reward_mean | -0.124     |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.4      |
| train/steps                    | 201600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 126        |
| stats_o/mean                   | 0.20245901 |
| stats_o/std                    | 0.11158494 |
| test/episodes                  | 1270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.615      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.0676    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -4.0503893 |
| test/Q_plus_P                  | -4.0503893 |
| test/reward_per_eps            | -15.4      |
| test/steps                     | 50800      |
| train/episodes                 | 5080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.194      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0114    |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.2      |
| train/steps                    | 203200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 127        |
| stats_o/mean                   | 0.20250212 |
| stats_o/std                    | 0.11142102 |
| test/episodes                  | 1280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.565      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00252   |
| test/info_shaping_reward_mean  | -0.0764    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -5.438287  |
| test/Q_plus_P                  | -5.438287  |
| test/reward_per_eps            | -17.4      |
| test/steps                     | 51200      |
| train/episodes                 | 5120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.184      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0195    |
| train/info_shaping_reward_mean | -0.127     |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.6      |
| train/steps                    | 204800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.20246965 |
| stats_o/std                    | 0.1114     |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.642      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00233   |
| test/info_shaping_reward_mean  | -0.0662    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -3.3633184 |
| test/Q_plus_P                  | -3.3633184 |
| test/reward_per_eps            | -14.3      |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.16       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0138    |
| train/info_shaping_reward_mean | -0.136     |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.6      |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 129        |
| stats_o/mean                   | 0.20251079 |
| stats_o/std                    | 0.11121832 |
| test/episodes                  | 1300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.603      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00395   |
| test/info_shaping_reward_mean  | -0.0714    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -3.7113492 |
| test/Q_plus_P                  | -3.7113492 |
| test/reward_per_eps            | -15.9      |
| test/steps                     | 52000      |
| train/episodes                 | 5200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.202      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0138    |
| train/info_shaping_reward_mean | -0.12      |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.9      |
| train/steps                    | 208000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 130         |
| stats_o/mean                   | 0.202541    |
| stats_o/std                    | 0.110973485 |
| test/episodes                  | 1310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.64        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00153    |
| test/info_shaping_reward_mean  | -0.0641     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -3.694684   |
| test/Q_plus_P                  | -3.694684   |
| test/reward_per_eps            | -14.4       |
| test/steps                     | 52400       |
| train/episodes                 | 5240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.233       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0132     |
| train/info_shaping_reward_mean | -0.121      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.7       |
| train/steps                    | 209600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 131        |
| stats_o/mean                   | 0.20255996 |
| stats_o/std                    | 0.11071673 |
| test/episodes                  | 1320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.63       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00203   |
| test/info_shaping_reward_mean  | -0.0668    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -4.127018  |
| test/Q_plus_P                  | -4.127018  |
| test/reward_per_eps            | -14.8      |
| test/steps                     | 52800      |
| train/episodes                 | 5280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.196      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0158    |
| train/info_shaping_reward_mean | -0.118     |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.2      |
| train/steps                    | 211200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 132        |
| stats_o/mean                   | 0.20253016 |
| stats_o/std                    | 0.11052536 |
| test/episodes                  | 1330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.585      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.073     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -3.9991844 |
| test/Q_plus_P                  | -3.9991844 |
| test/reward_per_eps            | -16.6      |
| test/steps                     | 53200      |
| train/episodes                 | 5320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.241      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00964   |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.4      |
| train/steps                    | 212800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 133         |
| stats_o/mean                   | 0.20257868  |
| stats_o/std                    | 0.110385515 |
| test/episodes                  | 1340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.657       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000739   |
| test/info_shaping_reward_mean  | -0.0591     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -2.8018613  |
| test/Q_plus_P                  | -2.8018613  |
| test/reward_per_eps            | -13.7       |
| test/steps                     | 53600       |
| train/episodes                 | 5360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.207       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0125     |
| train/info_shaping_reward_mean | -0.127      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.7       |
| train/steps                    | 214400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 134        |
| stats_o/mean                   | 0.20258747 |
| stats_o/std                    | 0.11011846 |
| test/episodes                  | 1350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000604  |
| test/info_shaping_reward_mean  | -0.0669    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.6347227 |
| test/Q_plus_P                  | -3.6347227 |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 54000      |
| train/episodes                 | 5400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.226      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0118    |
| train/info_shaping_reward_mean | -0.112     |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31        |
| train/steps                    | 216000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 135        |
| stats_o/mean                   | 0.202546   |
| stats_o/std                    | 0.10994132 |
| test/episodes                  | 1360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000837  |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.5321493 |
| test/Q_plus_P                  | -2.5321493 |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 54400      |
| train/episodes                 | 5440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.217      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0136    |
| train/info_shaping_reward_mean | -0.119     |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.3      |
| train/steps                    | 217600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 136        |
| stats_o/mean                   | 0.20254764 |
| stats_o/std                    | 0.10976317 |
| test/episodes                  | 1370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.642      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0015    |
| test/info_shaping_reward_mean  | -0.0652    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -3.5109916 |
| test/Q_plus_P                  | -3.5109916 |
| test/reward_per_eps            | -14.3      |
| test/steps                     | 54800      |
| train/episodes                 | 5480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.219      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0088    |
| train/info_shaping_reward_mean | -0.123     |
| train/info_shaping_reward_min  | -0.283     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.2      |
| train/steps                    | 219200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 137         |
| stats_o/mean                   | 0.20257746  |
| stats_o/std                    | 0.109587304 |
| test/episodes                  | 1380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.585       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000754   |
| test/info_shaping_reward_mean  | -0.0792     |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -5.5551605  |
| test/Q_plus_P                  | -5.5551605  |
| test/reward_per_eps            | -16.6       |
| test/steps                     | 55200       |
| train/episodes                 | 5520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.264       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00931    |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.4       |
| train/steps                    | 220800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.20260179 |
| stats_o/std                    | 0.10950687 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0645    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.9240317 |
| test/Q_plus_P                  | -3.9240317 |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.214      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0114    |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.4      |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.20259763 |
| stats_o/std                    | 0.10930996 |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00138   |
| test/info_shaping_reward_mean  | -0.0646    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -3.3060007 |
| test/Q_plus_P                  | -3.3060007 |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.245      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00968   |
| train/info_shaping_reward_mean | -0.117     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.2      |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 140        |
| stats_o/mean                   | 0.20256157 |
| stats_o/std                    | 0.10916716 |
| test/episodes                  | 1410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.632      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000886  |
| test/info_shaping_reward_mean  | -0.0654    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -3.527275  |
| test/Q_plus_P                  | -3.527275  |
| test/reward_per_eps            | -14.7      |
| test/steps                     | 56400      |
| train/episodes                 | 5640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.202      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0112    |
| train/info_shaping_reward_mean | -0.119     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.9      |
| train/steps                    | 225600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.20254825 |
| stats_o/std                    | 0.10887063 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00146   |
| test/info_shaping_reward_mean  | -0.0588    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -2.87676   |
| test/Q_plus_P                  | -2.87676   |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.233      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0103    |
| train/info_shaping_reward_mean | -0.106     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.7      |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 142         |
| stats_o/mean                   | 0.20252407  |
| stats_o/std                    | 0.108659446 |
| test/episodes                  | 1430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.605       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00308    |
| test/info_shaping_reward_mean  | -0.0699     |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -4.95409    |
| test/Q_plus_P                  | -4.95409    |
| test/reward_per_eps            | -15.8       |
| test/steps                     | 57200       |
| train/episodes                 | 5720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.276       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0103     |
| train/info_shaping_reward_mean | -0.112      |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.9       |
| train/steps                    | 228800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 143        |
| stats_o/mean                   | 0.20254661 |
| stats_o/std                    | 0.10841931 |
| test/episodes                  | 1440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.637      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000846  |
| test/info_shaping_reward_mean  | -0.0641    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -3.4880145 |
| test/Q_plus_P                  | -3.4880145 |
| test/reward_per_eps            | -14.5      |
| test/steps                     | 57600      |
| train/episodes                 | 5760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.257      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0102    |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.7      |
| train/steps                    | 230400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.20259784  |
| stats_o/std                    | 0.108470745 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.672       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.0613     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -2.848974   |
| test/Q_plus_P                  | -2.848974   |
| test/reward_per_eps            | -13.1       |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.239       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0117     |
| train/info_shaping_reward_mean | -0.152      |
| train/info_shaping_reward_min  | -0.418      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.4       |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 145        |
| stats_o/mean                   | 0.20261778 |
| stats_o/std                    | 0.10838781 |
| test/episodes                  | 1460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.593      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00105   |
| test/info_shaping_reward_mean  | -0.104     |
| test/info_shaping_reward_min   | -0.608     |
| test/Q                         | -5.206096  |
| test/Q_plus_P                  | -5.206096  |
| test/reward_per_eps            | -16.3      |
| test/steps                     | 58400      |
| train/episodes                 | 5840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.214      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0118    |
| train/info_shaping_reward_mean | -0.136     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.4      |
| train/steps                    | 233600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 146        |
| stats_o/mean                   | 0.20263065 |
| stats_o/std                    | 0.10821196 |
| test/episodes                  | 1470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00419   |
| test/info_shaping_reward_mean  | -0.0643    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.79137   |
| test/Q_plus_P                  | -2.79137   |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 58800      |
| train/episodes                 | 5880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.234      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00953   |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.288     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.6      |
| train/steps                    | 235200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 147        |
| stats_o/mean                   | 0.20267834 |
| stats_o/std                    | 0.10804895 |
| test/episodes                  | 1480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00163   |
| test/info_shaping_reward_mean  | -0.0656    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -3.4029849 |
| test/Q_plus_P                  | -3.4029849 |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 59200      |
| train/episodes                 | 5920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.243      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0105    |
| train/info_shaping_reward_mean | -0.121     |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.3      |
| train/steps                    | 236800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 148        |
| stats_o/mean                   | 0.20272385 |
| stats_o/std                    | 0.10784404 |
| test/episodes                  | 1490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.51       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00105   |
| test/info_shaping_reward_mean  | -0.0863    |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -4.9403586 |
| test/Q_plus_P                  | -4.9403586 |
| test/reward_per_eps            | -19.6      |
| test/steps                     | 59600      |
| train/episodes                 | 5960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.223      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0138    |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.1      |
| train/steps                    | 238400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 149        |
| stats_o/mean                   | 0.20273216 |
| stats_o/std                    | 0.10769581 |
| test/episodes                  | 1500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00253   |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.785957  |
| test/Q_plus_P                  | -2.785957  |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 60000      |
| train/episodes                 | 6000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.244      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0134    |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.2      |
| train/steps                    | 240000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 150         |
| stats_o/mean                   | 0.20272394  |
| stats_o/std                    | 0.107515275 |
| test/episodes                  | 1510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.675       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.0569     |
| test/info_shaping_reward_min   | -0.207      |
| test/Q                         | -2.8737433  |
| test/Q_plus_P                  | -2.8737433  |
| test/reward_per_eps            | -13         |
| test/steps                     | 60400       |
| train/episodes                 | 6040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.307       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0132     |
| train/info_shaping_reward_mean | -0.112      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.7       |
| train/steps                    | 241600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 151        |
| stats_o/mean                   | 0.20271014 |
| stats_o/std                    | 0.10735689 |
| test/episodes                  | 1520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.0568    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -2.6591692 |
| test/Q_plus_P                  | -2.6591692 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 60800      |
| train/episodes                 | 6080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.251      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0125    |
| train/info_shaping_reward_mean | -0.12      |
| train/info_shaping_reward_min  | -0.285     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.9      |
| train/steps                    | 243200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 152        |
| stats_o/mean                   | 0.20270899 |
| stats_o/std                    | 0.10715012 |
| test/episodes                  | 1530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.618      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000919  |
| test/info_shaping_reward_mean  | -0.0666    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -3.9176922 |
| test/Q_plus_P                  | -3.9176922 |
| test/reward_per_eps            | -15.3      |
| test/steps                     | 61200      |
| train/episodes                 | 6120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.293      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.011     |
| train/info_shaping_reward_mean | -0.12      |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.3      |
| train/steps                    | 244800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 153         |
| stats_o/mean                   | 0.20270489  |
| stats_o/std                    | 0.106973074 |
| test/episodes                  | 1540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.0538     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -2.4149194  |
| test/Q_plus_P                  | -2.4149194  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 61600       |
| train/episodes                 | 6160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.267       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00989    |
| train/info_shaping_reward_mean | -0.11       |
| train/info_shaping_reward_min  | -0.201      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.3       |
| train/steps                    | 246400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 154         |
| stats_o/mean                   | 0.20274782  |
| stats_o/std                    | 0.106864884 |
| test/episodes                  | 1550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.647       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00299    |
| test/info_shaping_reward_mean  | -0.0594     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -2.7361917  |
| test/Q_plus_P                  | -2.7361917  |
| test/reward_per_eps            | -14.1       |
| test/steps                     | 62000       |
| train/episodes                 | 6200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.283       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.013      |
| train/info_shaping_reward_mean | -0.119      |
| train/info_shaping_reward_min  | -0.284      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.7       |
| train/steps                    | 248000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.20272753 |
| stats_o/std                    | 0.10663926 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000854  |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.307489  |
| test/Q_plus_P                  | -2.307489  |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.294      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00835   |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.2      |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 156         |
| stats_o/mean                   | 0.20272548  |
| stats_o/std                    | 0.106449805 |
| test/episodes                  | 1570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0015     |
| test/info_shaping_reward_mean  | -0.0559     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -2.3848836  |
| test/Q_plus_P                  | -2.3848836  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 62800       |
| train/episodes                 | 6280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.312       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00732    |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.226      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.5       |
| train/steps                    | 251200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 157        |
| stats_o/mean                   | 0.20273244 |
| stats_o/std                    | 0.10624709 |
| test/episodes                  | 1580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.625      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00455   |
| test/info_shaping_reward_mean  | -0.0698    |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -3.8748834 |
| test/Q_plus_P                  | -3.8748834 |
| test/reward_per_eps            | -15        |
| test/steps                     | 63200      |
| train/episodes                 | 6320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.246      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0149    |
| train/info_shaping_reward_mean | -0.114     |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.1      |
| train/steps                    | 252800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 158        |
| stats_o/mean                   | 0.20272993 |
| stats_o/std                    | 0.10606508 |
| test/episodes                  | 1590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.65       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0021    |
| test/info_shaping_reward_mean  | -0.0627    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -2.9915056 |
| test/Q_plus_P                  | -2.9915056 |
| test/reward_per_eps            | -14        |
| test/steps                     | 63600      |
| train/episodes                 | 6360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.285      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00859   |
| train/info_shaping_reward_mean | -0.113     |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.6      |
| train/steps                    | 254400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.20278229  |
| stats_o/std                    | 0.105890825 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.502       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00443    |
| test/info_shaping_reward_mean  | -0.0879     |
| test/info_shaping_reward_min   | -0.191      |
| test/Q                         | -5.2138414  |
| test/Q_plus_P                  | -5.2138414  |
| test/reward_per_eps            | -19.9       |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.251       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0114     |
| train/info_shaping_reward_mean | -0.112      |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.9       |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 160         |
| stats_o/mean                   | 0.20278366  |
| stats_o/std                    | 0.105911635 |
| test/episodes                  | 1610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.58        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00134    |
| test/info_shaping_reward_mean  | -0.0746     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -4.720585   |
| test/Q_plus_P                  | -4.720585   |
| test/reward_per_eps            | -16.8       |
| test/steps                     | 64400       |
| train/episodes                 | 6440        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.155       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0297     |
| train/info_shaping_reward_mean | -0.158      |
| train/info_shaping_reward_min  | -0.384      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.8       |
| train/steps                    | 257600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.20279706  |
| stats_o/std                    | 0.105716966 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.615       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0036     |
| test/info_shaping_reward_mean  | -0.0708     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -3.180713   |
| test/Q_plus_P                  | -3.180713   |
| test/reward_per_eps            | -15.4       |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.268       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00936    |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.3       |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 162        |
| stats_o/mean                   | 0.20278706 |
| stats_o/std                    | 0.10570532 |
| test/episodes                  | 1630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.655      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00246   |
| test/info_shaping_reward_mean  | -0.0653    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -3.0274441 |
| test/Q_plus_P                  | -3.0274441 |
| test/reward_per_eps            | -13.8      |
| test/steps                     | 65200      |
| train/episodes                 | 6520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.284      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0116    |
| train/info_shaping_reward_mean | -0.137     |
| train/info_shaping_reward_min  | -0.354     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.6      |
| train/steps                    | 260800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 163        |
| stats_o/mean                   | 0.20277862 |
| stats_o/std                    | 0.10560288 |
| test/episodes                  | 1640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.67       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00433   |
| test/info_shaping_reward_mean  | -0.0632    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.3833926 |
| test/Q_plus_P                  | -2.3833926 |
| test/reward_per_eps            | -13.2      |
| test/steps                     | 65600      |
| train/episodes                 | 6560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.266      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00963   |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.4      |
| train/steps                    | 262400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 164        |
| stats_o/mean                   | 0.20277491 |
| stats_o/std                    | 0.10543588 |
| test/episodes                  | 1650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.59       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00294   |
| test/info_shaping_reward_mean  | -0.0728    |
| test/info_shaping_reward_min   | -0.227     |
| test/Q                         | -3.848796  |
| test/Q_plus_P                  | -3.848796  |
| test/reward_per_eps            | -16.4      |
| test/steps                     | 66000      |
| train/episodes                 | 6600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.257      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0109    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.7      |
| train/steps                    | 264000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 165         |
| stats_o/mean                   | 0.20279393  |
| stats_o/std                    | 0.105298474 |
| test/episodes                  | 1660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.675       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00289    |
| test/info_shaping_reward_mean  | -0.0614     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -2.397843   |
| test/Q_plus_P                  | -2.397843   |
| test/reward_per_eps            | -13         |
| test/steps                     | 66400       |
| train/episodes                 | 6640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.246       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00942    |
| train/info_shaping_reward_mean | -0.121      |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.2       |
| train/steps                    | 265600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 166        |
| stats_o/mean                   | 0.20280904 |
| stats_o/std                    | 0.10506238 |
| test/episodes                  | 1670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.615      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0028    |
| test/info_shaping_reward_mean  | -0.0703    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -3.3456976 |
| test/Q_plus_P                  | -3.3456976 |
| test/reward_per_eps            | -15.4      |
| test/steps                     | 66800      |
| train/episodes                 | 6680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.289      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00821   |
| train/info_shaping_reward_mean | -0.106     |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.4      |
| train/steps                    | 267200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.2028111  |
| stats_o/std                    | 0.10500965 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00157   |
| test/info_shaping_reward_mean  | -0.0645    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -2.6864254 |
| test/Q_plus_P                  | -2.6864254 |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.308      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00779   |
| train/info_shaping_reward_mean | -0.115     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.7      |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 168        |
| stats_o/mean                   | 0.20285156 |
| stats_o/std                    | 0.10488114 |
| test/episodes                  | 1690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.625      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00114   |
| test/info_shaping_reward_mean  | -0.107     |
| test/info_shaping_reward_min   | -0.651     |
| test/Q                         | -5.3363914 |
| test/Q_plus_P                  | -5.3363914 |
| test/reward_per_eps            | -15        |
| test/steps                     | 67600      |
| train/episodes                 | 6760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.28       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.009     |
| train/info_shaping_reward_mean | -0.114     |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.8      |
| train/steps                    | 270400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.20285745 |
| stats_o/std                    | 0.10477661 |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.657      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00265   |
| test/info_shaping_reward_mean  | -0.0623    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -2.9110758 |
| test/Q_plus_P                  | -2.9110758 |
| test/reward_per_eps            | -13.7      |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.279      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00836   |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.282     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.8      |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 170        |
| stats_o/mean                   | 0.20288393 |
| stats_o/std                    | 0.10459433 |
| test/episodes                  | 1710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.0654    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.8922384 |
| test/Q_plus_P                  | -2.8922384 |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 68400      |
| train/episodes                 | 6840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.284      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00896   |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.6      |
| train/steps                    | 273600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 171         |
| stats_o/mean                   | 0.20289408  |
| stats_o/std                    | 0.104410924 |
| test/episodes                  | 1720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.645       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00338    |
| test/info_shaping_reward_mean  | -0.0649     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -2.9626608  |
| test/Q_plus_P                  | -2.9626608  |
| test/reward_per_eps            | -14.2       |
| test/steps                     | 68800       |
| train/episodes                 | 6880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.325       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00926    |
| train/info_shaping_reward_mean | -0.101      |
| train/info_shaping_reward_min  | -0.205      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27         |
| train/steps                    | 275200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 172        |
| stats_o/mean                   | 0.20288858 |
| stats_o/std                    | 0.10427717 |
| test/episodes                  | 1730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00214   |
| test/info_shaping_reward_mean  | -0.0615    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -2.6141944 |
| test/Q_plus_P                  | -2.6141944 |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 69200      |
| train/episodes                 | 6920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.274      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0105    |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29        |
| train/steps                    | 276800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 173        |
| stats_o/mean                   | 0.20288606 |
| stats_o/std                    | 0.10412968 |
| test/episodes                  | 1740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.627      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00321   |
| test/info_shaping_reward_mean  | -0.065     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -3.2173061 |
| test/Q_plus_P                  | -3.2173061 |
| test/reward_per_eps            | -14.9      |
| test/steps                     | 69600      |
| train/episodes                 | 6960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.28       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00905   |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.8      |
| train/steps                    | 278400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 174        |
| stats_o/mean                   | 0.20291974 |
| stats_o/std                    | 0.10409911 |
| test/episodes                  | 1750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000809  |
| test/info_shaping_reward_mean  | -0.0595    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -2.3346348 |
| test/Q_plus_P                  | -2.3346348 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 70000      |
| train/episodes                 | 7000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.291      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00804   |
| train/info_shaping_reward_mean | -0.12      |
| train/info_shaping_reward_min  | -0.275     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.4      |
| train/steps                    | 280000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 175        |
| stats_o/mean                   | 0.20293325 |
| stats_o/std                    | 0.10398735 |
| test/episodes                  | 1760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00431   |
| test/info_shaping_reward_mean  | -0.064     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.3648357 |
| test/Q_plus_P                  | -2.3648357 |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 70400      |
| train/episodes                 | 7040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.262      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0119    |
| train/info_shaping_reward_mean | -0.115     |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.5      |
| train/steps                    | 281600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 176        |
| stats_o/mean                   | 0.20297576 |
| stats_o/std                    | 0.10384998 |
| test/episodes                  | 1770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00271   |
| test/info_shaping_reward_mean  | -0.0585    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.6668656 |
| test/Q_plus_P                  | -2.6668656 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 70800      |
| train/episodes                 | 7080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.298      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0101    |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.1      |
| train/steps                    | 283200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 177        |
| stats_o/mean                   | 0.20299378 |
| stats_o/std                    | 0.10364846 |
| test/episodes                  | 1780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00208   |
| test/info_shaping_reward_mean  | -0.0638    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -3.010809  |
| test/Q_plus_P                  | -3.010809  |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 71200      |
| train/episodes                 | 7120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.378      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00829   |
| train/info_shaping_reward_mean | -0.0957    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.9      |
| train/steps                    | 284800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.20303898 |
| stats_o/std                    | 0.10360094 |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000988  |
| test/info_shaping_reward_mean  | -0.0627    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -2.5752952 |
| test/Q_plus_P                  | -2.5752952 |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.273      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0108    |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.1      |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 179        |
| stats_o/mean                   | 0.20303765 |
| stats_o/std                    | 0.10344284 |
| test/episodes                  | 1800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00274   |
| test/info_shaping_reward_mean  | -0.0585    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -2.4022453 |
| test/Q_plus_P                  | -2.4022453 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 72000      |
| train/episodes                 | 7200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.344      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00893   |
| train/info_shaping_reward_mean | -0.1       |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.2      |
| train/steps                    | 288000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 180        |
| stats_o/mean                   | 0.2030373  |
| stats_o/std                    | 0.10334151 |
| test/episodes                  | 1810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00268   |
| test/info_shaping_reward_mean  | -0.0613    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.552537  |
| test/Q_plus_P                  | -2.552537  |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 72400      |
| train/episodes                 | 7240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.326      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00935   |
| train/info_shaping_reward_mean | -0.121     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27        |
| train/steps                    | 289600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 181        |
| stats_o/mean                   | 0.20304532 |
| stats_o/std                    | 0.10320353 |
| test/episodes                  | 1820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00328   |
| test/info_shaping_reward_mean  | -0.0592    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.4590676 |
| test/Q_plus_P                  | -2.4590676 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 72800      |
| train/episodes                 | 7280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.361      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00967   |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.6      |
| train/steps                    | 291200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 182        |
| stats_o/mean                   | 0.20304954 |
| stats_o/std                    | 0.10303085 |
| test/episodes                  | 1830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.655      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00291   |
| test/info_shaping_reward_mean  | -0.0625    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -2.6856284 |
| test/Q_plus_P                  | -2.6856284 |
| test/reward_per_eps            | -13.8      |
| test/steps                     | 73200      |
| train/episodes                 | 7320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.365      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00693   |
| train/info_shaping_reward_mean | -0.0967    |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.4      |
| train/steps                    | 292800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 183        |
| stats_o/mean                   | 0.20304604 |
| stats_o/std                    | 0.10284089 |
| test/episodes                  | 1840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00228   |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.144629  |
| test/Q_plus_P                  | -2.144629  |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 73600      |
| train/episodes                 | 7360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.356      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00764   |
| train/info_shaping_reward_mean | -0.0943    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.8      |
| train/steps                    | 294400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 184         |
| stats_o/mean                   | 0.20304811  |
| stats_o/std                    | 0.102638766 |
| test/episodes                  | 1850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00419    |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -2.1486557  |
| test/Q_plus_P                  | -2.1486557  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 74000       |
| train/episodes                 | 7400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.334       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00792    |
| train/info_shaping_reward_mean | -0.0999     |
| train/info_shaping_reward_min  | -0.208      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.6       |
| train/steps                    | 296000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 185        |
| stats_o/mean                   | 0.20306048 |
| stats_o/std                    | 0.10246864 |
| test/episodes                  | 1860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00173   |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.2724938 |
| test/Q_plus_P                  | -2.2724938 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 74400      |
| train/episodes                 | 7440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.356      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00562   |
| train/info_shaping_reward_mean | -0.0955    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.8      |
| train/steps                    | 297600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 186        |
| stats_o/mean                   | 0.20308213 |
| stats_o/std                    | 0.10227724 |
| test/episodes                  | 1870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00402   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -2.2152617 |
| test/Q_plus_P                  | -2.2152617 |
| test/reward_per_eps            | -11        |
| test/steps                     | 74800      |
| train/episodes                 | 7480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.404      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00692   |
| train/info_shaping_reward_mean | -0.0862    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.8      |
| train/steps                    | 299200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.20311147 |
| stats_o/std                    | 0.10217912 |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00129   |
| test/info_shaping_reward_mean  | -0.0557    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.022358  |
| test/Q_plus_P                  | -2.022358  |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.384      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00628   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.6      |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 188        |
| stats_o/mean                   | 0.20312344 |
| stats_o/std                    | 0.10203    |
| test/episodes                  | 1890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00451   |
| test/info_shaping_reward_mean  | -0.0588    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.0777423 |
| test/Q_plus_P                  | -2.0777423 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 75600      |
| train/episodes                 | 7560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.397      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0073    |
| train/info_shaping_reward_mean | -0.1       |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.1      |
| train/steps                    | 302400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.20312393  |
| stats_o/std                    | 0.101951316 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0602     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -2.541604   |
| test/Q_plus_P                  | -2.541604   |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.309       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00902    |
| train/info_shaping_reward_mean | -0.108      |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.6       |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 190         |
| stats_o/mean                   | 0.20312661  |
| stats_o/std                    | 0.101799145 |
| test/episodes                  | 1910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.637       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.0622     |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -2.999286   |
| test/Q_plus_P                  | -2.999286   |
| test/reward_per_eps            | -14.5       |
| test/steps                     | 76400       |
| train/episodes                 | 7640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.359       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00699    |
| train/info_shaping_reward_mean | -0.0982     |
| train/info_shaping_reward_min  | -0.202      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.6       |
| train/steps                    | 305600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 191        |
| stats_o/mean                   | 0.20311384 |
| stats_o/std                    | 0.10169956 |
| test/episodes                  | 1920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000805  |
| test/info_shaping_reward_mean  | -0.0581    |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -2.0701756 |
| test/Q_plus_P                  | -2.0701756 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 76800      |
| train/episodes                 | 7680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.344      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00608   |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.2      |
| train/steps                    | 307200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.20311771  |
| stats_o/std                    | 0.101602554 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00287    |
| test/info_shaping_reward_mean  | -0.0552     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -2.0130777  |
| test/Q_plus_P                  | -2.0130777  |
| test/reward_per_eps            | -11         |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.405       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00615    |
| train/info_shaping_reward_mean | -0.102      |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.8       |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 193        |
| stats_o/mean                   | 0.20310636 |
| stats_o/std                    | 0.10146332 |
| test/episodes                  | 1940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00201   |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.7351772 |
| test/Q_plus_P                  | -1.7351772 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 77600      |
| train/episodes                 | 7760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.406      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00681   |
| train/info_shaping_reward_mean | -0.0946    |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.8      |
| train/steps                    | 310400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 194        |
| stats_o/mean                   | 0.20313808 |
| stats_o/std                    | 0.10132179 |
| test/episodes                  | 1950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00263   |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5064465 |
| test/Q_plus_P                  | -1.5064465 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 78000      |
| train/episodes                 | 7800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.387      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00605   |
| train/info_shaping_reward_mean | -0.0948    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.5      |
| train/steps                    | 312000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 195        |
| stats_o/mean                   | 0.20315789 |
| stats_o/std                    | 0.10124482 |
| test/episodes                  | 1960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00278   |
| test/info_shaping_reward_mean  | -0.0578    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -2.2481515 |
| test/Q_plus_P                  | -2.2481515 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 78400      |
| train/episodes                 | 7840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.341      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00582   |
| train/info_shaping_reward_mean | -0.114     |
| train/info_shaping_reward_min  | -0.307     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.4      |
| train/steps                    | 313600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 196        |
| stats_o/mean                   | 0.20314862 |
| stats_o/std                    | 0.10109339 |
| test/episodes                  | 1970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00184   |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -2.0206892 |
| test/Q_plus_P                  | -2.0206892 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 78800      |
| train/episodes                 | 7880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.44       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00838   |
| train/info_shaping_reward_mean | -0.0888    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.4      |
| train/steps                    | 315200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 197        |
| stats_o/mean                   | 0.20315973 |
| stats_o/std                    | 0.10106683 |
| test/episodes                  | 1980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00209   |
| test/info_shaping_reward_mean  | -0.0563    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.9383094 |
| test/Q_plus_P                  | -1.9383094 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 79200      |
| train/episodes                 | 7920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.402      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00842   |
| train/info_shaping_reward_mean | -0.0971    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.9      |
| train/steps                    | 316800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.20317997  |
| stats_o/std                    | 0.101017945 |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.652       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.0649     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -3.9771512  |
| test/Q_plus_P                  | -3.9771512  |
| test/reward_per_eps            | -13.9       |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.309       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00833    |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.6       |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.20317812 |
| stats_o/std                    | 0.10087308 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.627      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00197   |
| test/info_shaping_reward_mean  | -0.0665    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.6205795 |
| test/Q_plus_P                  | -2.6205795 |
| test/reward_per_eps            | -14.9      |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.382      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00891   |
| train/info_shaping_reward_mean | -0.097     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.7      |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 200        |
| stats_o/mean                   | 0.2031917  |
| stats_o/std                    | 0.10075454 |
| test/episodes                  | 2010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00285   |
| test/info_shaping_reward_mean  | -0.0626    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -2.3892078 |
| test/Q_plus_P                  | -2.3892078 |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 80400      |
| train/episodes                 | 8040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.333      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00806   |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.7      |
| train/steps                    | 321600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 201        |
| stats_o/mean                   | 0.20320764 |
| stats_o/std                    | 0.10071523 |
| test/episodes                  | 2020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000879  |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.208     |
| test/Q                         | -1.9254206 |
| test/Q_plus_P                  | -1.9254206 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 80800      |
| train/episodes                 | 8080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.368      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.106     |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.3      |
| train/steps                    | 323200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 202        |
| stats_o/mean                   | 0.20323351 |
| stats_o/std                    | 0.10061757 |
| test/episodes                  | 2030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000945  |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.7810271 |
| test/Q_plus_P                  | -1.7810271 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 81200      |
| train/episodes                 | 8120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.389      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00654   |
| train/info_shaping_reward_mean | -0.09      |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.4      |
| train/steps                    | 324800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 203        |
| stats_o/mean                   | 0.2032169  |
| stats_o/std                    | 0.10060144 |
| test/episodes                  | 2040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00346   |
| test/info_shaping_reward_mean  | -0.0609    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.2107368 |
| test/Q_plus_P                  | -2.2107368 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 81600      |
| train/episodes                 | 8160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.323      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00864   |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.367     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.1      |
| train/steps                    | 326400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 204         |
| stats_o/mean                   | 0.20322506  |
| stats_o/std                    | 0.100523844 |
| test/episodes                  | 2050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00218    |
| test/info_shaping_reward_mean  | -0.0554     |
| test/info_shaping_reward_min   | -0.197      |
| test/Q                         | -2.0521688  |
| test/Q_plus_P                  | -2.0521688  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 82000       |
| train/episodes                 | 8200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.374       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00767    |
| train/info_shaping_reward_mean | -0.109      |
| train/info_shaping_reward_min  | -0.276      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.1       |
| train/steps                    | 328000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 205         |
| stats_o/mean                   | 0.20325066  |
| stats_o/std                    | 0.100427024 |
| test/episodes                  | 2060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0028     |
| test/info_shaping_reward_mean  | -0.0595     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.9363037  |
| test/Q_plus_P                  | -1.9363037  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 82400       |
| train/episodes                 | 8240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.438       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00486    |
| train/info_shaping_reward_mean | -0.0932     |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.5       |
| train/steps                    | 329600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 206        |
| stats_o/mean                   | 0.20326792 |
| stats_o/std                    | 0.10039913 |
| test/episodes                  | 2070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00199   |
| test/info_shaping_reward_mean  | -0.0608    |
| test/info_shaping_reward_min   | -0.211     |
| test/Q                         | -1.97871   |
| test/Q_plus_P                  | -1.97871   |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 82800      |
| train/episodes                 | 8280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.376      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00739   |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.9      |
| train/steps                    | 331200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.20331222 |
| stats_o/std                    | 0.10031077 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.625      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00252   |
| test/info_shaping_reward_mean  | -0.0691    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -4.0528793 |
| test/Q_plus_P                  | -4.0528793 |
| test/reward_per_eps            | -15        |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.368      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.011     |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.3      |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 208         |
| stats_o/mean                   | 0.20331839  |
| stats_o/std                    | 0.100251496 |
| test/episodes                  | 2090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.688       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00231    |
| test/info_shaping_reward_mean  | -0.0599     |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -2.03446    |
| test/Q_plus_P                  | -2.03446    |
| test/reward_per_eps            | -12.5       |
| test/steps                     | 83600       |
| train/episodes                 | 8360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.316       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00887    |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.278      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.4       |
| train/steps                    | 334400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 209        |
| stats_o/mean                   | 0.20333837 |
| stats_o/std                    | 0.10020508 |
| test/episodes                  | 2100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00128   |
| test/info_shaping_reward_mean  | -0.0607    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -2.3106992 |
| test/Q_plus_P                  | -2.3106992 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 84000      |
| train/episodes                 | 8400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.35       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0055    |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26        |
| train/steps                    | 336000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 210        |
| stats_o/mean                   | 0.20333882 |
| stats_o/std                    | 0.10011652 |
| test/episodes                  | 2110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00302   |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.0107963 |
| test/Q_plus_P                  | -2.0107963 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 84400      |
| train/episodes                 | 8440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.46       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00775   |
| train/info_shaping_reward_mean | -0.0928    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 337600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 211        |
| stats_o/mean                   | 0.20333521 |
| stats_o/std                    | 0.10000786 |
| test/episodes                  | 2120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00153   |
| test/info_shaping_reward_mean  | -0.0585    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.1015973 |
| test/Q_plus_P                  | -2.1015973 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 84800      |
| train/episodes                 | 8480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.376      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00524   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.9      |
| train/steps                    | 339200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 212        |
| stats_o/mean                   | 0.2033231  |
| stats_o/std                    | 0.09991742 |
| test/episodes                  | 2130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00238   |
| test/info_shaping_reward_mean  | -0.0547    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.0350482 |
| test/Q_plus_P                  | -2.0350482 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 85200      |
| train/episodes                 | 8520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.388      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00693   |
| train/info_shaping_reward_mean | -0.106     |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.5      |
| train/steps                    | 340800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 213        |
| stats_o/mean                   | 0.20335253 |
| stats_o/std                    | 0.09993485 |
| test/episodes                  | 2140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00121   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.7804724 |
| test/Q_plus_P                  | -1.7804724 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 85600      |
| train/episodes                 | 8560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.415      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0056    |
| train/info_shaping_reward_mean | -0.111     |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.4      |
| train/steps                    | 342400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 214        |
| stats_o/mean                   | 0.20336166 |
| stats_o/std                    | 0.09982926 |
| test/episodes                  | 2150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00252   |
| test/info_shaping_reward_mean  | -0.0599    |
| test/info_shaping_reward_min   | -0.209     |
| test/Q                         | -1.8314182 |
| test/Q_plus_P                  | -1.8314182 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 86000      |
| train/episodes                 | 8600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.437      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0074    |
| train/info_shaping_reward_mean | -0.0904    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.5      |
| train/steps                    | 344000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 215        |
| stats_o/mean                   | 0.20338917 |
| stats_o/std                    | 0.0997028  |
| test/episodes                  | 2160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.642      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00138   |
| test/info_shaping_reward_mean  | -0.0658    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -3.2100232 |
| test/Q_plus_P                  | -3.2100232 |
| test/reward_per_eps            | -14.3      |
| test/steps                     | 86400      |
| train/episodes                 | 8640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.44       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00649   |
| train/info_shaping_reward_mean | -0.0893    |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.4      |
| train/steps                    | 345600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 216         |
| stats_o/mean                   | 0.20338562  |
| stats_o/std                    | 0.099610284 |
| test/episodes                  | 2170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.002      |
| test/info_shaping_reward_mean  | -0.0614     |
| test/info_shaping_reward_min   | -0.205      |
| test/Q                         | -2.2968414  |
| test/Q_plus_P                  | -2.2968414  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 86800       |
| train/episodes                 | 8680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.464       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.0868     |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.4       |
| train/steps                    | 347200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 217        |
| stats_o/mean                   | 0.20336695 |
| stats_o/std                    | 0.09953938 |
| test/episodes                  | 2180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00233   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.7227932 |
| test/Q_plus_P                  | -1.7227932 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 87200      |
| train/episodes                 | 8720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.409      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00825   |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.6      |
| train/steps                    | 348800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 218        |
| stats_o/mean                   | 0.20336494 |
| stats_o/std                    | 0.09953887 |
| test/episodes                  | 2190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00247   |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.9396151 |
| test/Q_plus_P                  | -1.9396151 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 87600      |
| train/episodes                 | 8760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.372      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00717   |
| train/info_shaping_reward_mean | -0.113     |
| train/info_shaping_reward_min  | -0.274     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.1      |
| train/steps                    | 350400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.2033489   |
| stats_o/std                    | 0.099463746 |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00337    |
| test/info_shaping_reward_mean  | -0.0558     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.8572001  |
| test/Q_plus_P                  | -1.8572001  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.415       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00515    |
| train/info_shaping_reward_mean | -0.0993     |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.4       |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 220        |
| stats_o/mean                   | 0.20335874 |
| stats_o/std                    | 0.0993482  |
| test/episodes                  | 2210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00276   |
| test/info_shaping_reward_mean  | -0.0616    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.254003  |
| test/Q_plus_P                  | -2.254003  |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 88400      |
| train/episodes                 | 8840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.388      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0074    |
| train/info_shaping_reward_mean | -0.09      |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.5      |
| train/steps                    | 353600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 221        |
| stats_o/mean                   | 0.20337173 |
| stats_o/std                    | 0.09923117 |
| test/episodes                  | 2220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000144  |
| test/info_shaping_reward_mean  | -0.0549    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5411215 |
| test/Q_plus_P                  | -1.5411215 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 88800      |
| train/episodes                 | 8880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.378      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00789   |
| train/info_shaping_reward_mean | -0.0989    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.9      |
| train/steps                    | 355200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.20337874 |
| stats_o/std                    | 0.09915647 |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00186   |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -1.8924344 |
| test/Q_plus_P                  | -1.8924344 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.414      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00715   |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.4      |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 223        |
| stats_o/mean                   | 0.2033856  |
| stats_o/std                    | 0.09908914 |
| test/episodes                  | 2240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5898068 |
| test/Q_plus_P                  | -1.5898068 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 89600      |
| train/episodes                 | 8960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.407      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00629   |
| train/info_shaping_reward_mean | -0.0969    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.7      |
| train/steps                    | 358400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.20338935 |
| stats_o/std                    | 0.09894251 |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4852072 |
| test/Q_plus_P                  | -1.4852072 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.48       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0054    |
| train/info_shaping_reward_mean | -0.082     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 225        |
| stats_o/mean                   | 0.20339218 |
| stats_o/std                    | 0.09882971 |
| test/episodes                  | 2260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00226   |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.606236  |
| test/Q_plus_P                  | -1.606236  |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 90400      |
| train/episodes                 | 9040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.476      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00468   |
| train/info_shaping_reward_mean | -0.0894    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21        |
| train/steps                    | 361600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 226        |
| stats_o/mean                   | 0.20338626 |
| stats_o/std                    | 0.09868355 |
| test/episodes                  | 2270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00565   |
| test/info_shaping_reward_mean  | -0.0597    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -1.5441763 |
| test/Q_plus_P                  | -1.5441763 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 90800      |
| train/episodes                 | 9080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.42       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00622   |
| train/info_shaping_reward_mean | -0.0929    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.2      |
| train/steps                    | 363200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 227         |
| stats_o/mean                   | 0.20340829  |
| stats_o/std                    | 0.098578066 |
| test/episodes                  | 2280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00212    |
| test/info_shaping_reward_mean  | -0.055      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -2.0134983  |
| test/Q_plus_P                  | -2.0134983  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 91200       |
| train/episodes                 | 9120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.411       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00502    |
| train/info_shaping_reward_mean | -0.0985     |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.6       |
| train/steps                    | 364800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 228        |
| stats_o/mean                   | 0.20342107 |
| stats_o/std                    | 0.09846215 |
| test/episodes                  | 2290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00119   |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.7286307 |
| test/Q_plus_P                  | -1.7286307 |
| test/reward_per_eps            | -11        |
| test/steps                     | 91600      |
| train/episodes                 | 9160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.376      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00564   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25        |
| train/steps                    | 366400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 229        |
| stats_o/mean                   | 0.20341267 |
| stats_o/std                    | 0.09834013 |
| test/episodes                  | 2300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00035   |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.8525703 |
| test/Q_plus_P                  | -1.8525703 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 92000      |
| train/episodes                 | 9200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.433      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00539   |
| train/info_shaping_reward_mean | -0.088     |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.7      |
| train/steps                    | 368000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 230        |
| stats_o/mean                   | 0.20342311 |
| stats_o/std                    | 0.0982265  |
| test/episodes                  | 2310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -1.8742241 |
| test/Q_plus_P                  | -1.8742241 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 92400      |
| train/episodes                 | 9240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.472      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00551   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 369600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 231         |
| stats_o/mean                   | 0.2034225   |
| stats_o/std                    | 0.098091915 |
| test/episodes                  | 2320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00322    |
| test/info_shaping_reward_mean  | -0.0551     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.8788843  |
| test/Q_plus_P                  | -1.8788843  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 92800       |
| train/episodes                 | 9280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.47        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00553    |
| train/info_shaping_reward_mean | -0.087      |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.2       |
| train/steps                    | 371200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 232        |
| stats_o/mean                   | 0.2034225  |
| stats_o/std                    | 0.09795355 |
| test/episodes                  | 2330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000542  |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5154757 |
| test/Q_plus_P                  | -1.5154757 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 93200      |
| train/episodes                 | 9320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.467      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00562   |
| train/info_shaping_reward_mean | -0.0839    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.3      |
| train/steps                    | 372800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 233        |
| stats_o/mean                   | 0.20342608 |
| stats_o/std                    | 0.09780242 |
| test/episodes                  | 2340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00313   |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -1.7657129 |
| test/Q_plus_P                  | -1.7657129 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 93600      |
| train/episodes                 | 9360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.455      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00442   |
| train/info_shaping_reward_mean | -0.0851    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 374400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.20341964 |
| stats_o/std                    | 0.09769374 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000931  |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.5402075 |
| test/Q_plus_P                  | -1.5402075 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.479      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0062    |
| train/info_shaping_reward_mean | -0.0864    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 235        |
| stats_o/mean                   | 0.20341948 |
| stats_o/std                    | 0.0976052  |
| test/episodes                  | 2360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5309678 |
| test/Q_plus_P                  | -1.5309678 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 94400      |
| train/episodes                 | 9440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.487      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00543   |
| train/info_shaping_reward_mean | -0.0955    |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.5      |
| train/steps                    | 377600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 236        |
| stats_o/mean                   | 0.20341395 |
| stats_o/std                    | 0.09745322 |
| test/episodes                  | 2370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00203   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.321185  |
| test/Q_plus_P                  | -1.321185  |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 94800      |
| train/episodes                 | 9480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.5        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00512   |
| train/info_shaping_reward_mean | -0.078     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20        |
| train/steps                    | 379200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 237        |
| stats_o/mean                   | 0.20343088 |
| stats_o/std                    | 0.09737034 |
| test/episodes                  | 2380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.6570537 |
| test/Q_plus_P                  | -1.6570537 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 95200      |
| train/episodes                 | 9520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00477   |
| train/info_shaping_reward_mean | -0.0843    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 380800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 238        |
| stats_o/mean                   | 0.20340118 |
| stats_o/std                    | 0.09728089 |
| test/episodes                  | 2390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00218   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.3692943 |
| test/Q_plus_P                  | -1.3692943 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 95600      |
| train/episodes                 | 9560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.487      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.0889    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.5      |
| train/steps                    | 382400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 239         |
| stats_o/mean                   | 0.20341167  |
| stats_o/std                    | 0.097217895 |
| test/episodes                  | 2400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00248    |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.507999   |
| test/Q_plus_P                  | -1.507999   |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 96000       |
| train/episodes                 | 9600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.438       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0065     |
| train/info_shaping_reward_mean | -0.092      |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.5       |
| train/steps                    | 384000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 240         |
| stats_o/mean                   | 0.20343888  |
| stats_o/std                    | 0.097237565 |
| test/episodes                  | 2410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00192    |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.5466416  |
| test/Q_plus_P                  | -1.5466416  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 96400       |
| train/episodes                 | 9640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.458       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00468    |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.7       |
| train/steps                    | 385600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 241        |
| stats_o/mean                   | 0.20342867 |
| stats_o/std                    | 0.09717724 |
| test/episodes                  | 2420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000993  |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.571908  |
| test/Q_plus_P                  | -1.571908  |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 96800      |
| train/episodes                 | 9680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.41       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0084    |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.6      |
| train/steps                    | 387200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 242         |
| stats_o/mean                   | 0.20345427  |
| stats_o/std                    | 0.097120024 |
| test/episodes                  | 2430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00186    |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.6541231  |
| test/Q_plus_P                  | -1.6541231  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 97200       |
| train/episodes                 | 9720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.438       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00568    |
| train/info_shaping_reward_mean | -0.0936     |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.5       |
| train/steps                    | 388800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 243        |
| stats_o/mean                   | 0.20343997 |
| stats_o/std                    | 0.09702543 |
| test/episodes                  | 2440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00404   |
| test/info_shaping_reward_mean  | -0.0611    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.0528183 |
| test/Q_plus_P                  | -3.0528183 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 97600      |
| train/episodes                 | 9760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.442      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00505   |
| train/info_shaping_reward_mean | -0.0989    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.3      |
| train/steps                    | 390400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.2034461  |
| stats_o/std                    | 0.09692988 |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000682  |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3318769 |
| test/Q_plus_P                  | -1.3318769 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.436      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00587   |
| train/info_shaping_reward_mean | -0.0894    |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.6      |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 245         |
| stats_o/mean                   | 0.2034556   |
| stats_o/std                    | 0.096834175 |
| test/episodes                  | 2460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00322    |
| test/info_shaping_reward_mean  | -0.0551     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.5206245  |
| test/Q_plus_P                  | -1.5206245  |
| test/reward_per_eps            | -11         |
| test/steps                     | 98400       |
| train/episodes                 | 9840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.48        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00474    |
| train/info_shaping_reward_mean | -0.0892     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.8       |
| train/steps                    | 393600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 246        |
| stats_o/mean                   | 0.2034577  |
| stats_o/std                    | 0.09674181 |
| test/episodes                  | 2470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00137   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4284985 |
| test/Q_plus_P                  | -1.4284985 |
| test/reward_per_eps            | -10        |
| test/steps                     | 98800      |
| train/episodes                 | 9880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.425      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00684   |
| train/info_shaping_reward_mean | -0.0954    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23        |
| train/steps                    | 395200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 247        |
| stats_o/mean                   | 0.20346513 |
| stats_o/std                    | 0.09666674 |
| test/episodes                  | 2480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00346   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4505816 |
| test/Q_plus_P                  | -1.4505816 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 99200      |
| train/episodes                 | 9920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.424      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0937    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23        |
| train/steps                    | 396800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.20347813 |
| stats_o/std                    | 0.09656351 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -1.6316298 |
| test/Q_plus_P                  | -1.6316298 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.488      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00499   |
| train/info_shaping_reward_mean | -0.0829    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.5      |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 249        |
| stats_o/mean                   | 0.20350769 |
| stats_o/std                    | 0.09654876 |
| test/episodes                  | 2500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0022    |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -2.007494  |
| test/Q_plus_P                  | -2.007494  |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 100000     |
| train/episodes                 | 10000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.423      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00495   |
| train/info_shaping_reward_mean | -0.106     |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.1      |
| train/steps                    | 400000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 250        |
| stats_o/mean                   | 0.20352611 |
| stats_o/std                    | 0.09644182 |
| test/episodes                  | 2510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00107   |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5693626 |
| test/Q_plus_P                  | -1.5693626 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 100400     |
| train/episodes                 | 10040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.447      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00551   |
| train/info_shaping_reward_mean | -0.0865    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.1      |
| train/steps                    | 401600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 251        |
| stats_o/mean                   | 0.20353793 |
| stats_o/std                    | 0.09633888 |
| test/episodes                  | 2520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00242   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6507286 |
| test/Q_plus_P                  | -1.6507286 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 100800     |
| train/episodes                 | 10080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.509      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.005     |
| train/info_shaping_reward_mean | -0.0891    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 403200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 252         |
| stats_o/mean                   | 0.20353809  |
| stats_o/std                    | 0.096217096 |
| test/episodes                  | 2530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00255    |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -1.3714598  |
| test/Q_plus_P                  | -1.3714598  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 101200      |
| train/episodes                 | 10120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.475       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00641    |
| train/info_shaping_reward_mean | -0.0888     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21         |
| train/steps                    | 404800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 253        |
| stats_o/mean                   | 0.20357165 |
| stats_o/std                    | 0.09616349 |
| test/episodes                  | 2540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00282   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4409189 |
| test/Q_plus_P                  | -1.4409189 |
| test/reward_per_eps            | -10        |
| test/steps                     | 101600     |
| train/episodes                 | 10160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.461      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00562   |
| train/info_shaping_reward_mean | -0.0903    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 406400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 254        |
| stats_o/mean                   | 0.20355347 |
| stats_o/std                    | 0.09606869 |
| test/episodes                  | 2550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000911  |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.5656285 |
| test/Q_plus_P                  | -1.5656285 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 102000     |
| train/episodes                 | 10200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.514      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00368   |
| train/info_shaping_reward_mean | -0.0824    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 408000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 255        |
| stats_o/mean                   | 0.20354961 |
| stats_o/std                    | 0.0960888  |
| test/episodes                  | 2560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00236   |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.4644809 |
| test/Q_plus_P                  | -1.4644809 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 102400     |
| train/episodes                 | 10240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.409      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00657   |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.287     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.6      |
| train/steps                    | 409600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 256        |
| stats_o/mean                   | 0.20354044 |
| stats_o/std                    | 0.09598834 |
| test/episodes                  | 2570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4564929 |
| test/Q_plus_P                  | -1.4564929 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 102800     |
| train/episodes                 | 10280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.459      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00426   |
| train/info_shaping_reward_mean | -0.0852    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 411200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 257         |
| stats_o/mean                   | 0.20351307  |
| stats_o/std                    | 0.095912114 |
| test/episodes                  | 2580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00143    |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -1.2467545  |
| test/Q_plus_P                  | -1.2467545  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 103200      |
| train/episodes                 | 10320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.489       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00699    |
| train/info_shaping_reward_mean | -0.0823     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.4       |
| train/steps                    | 412800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 258         |
| stats_o/mean                   | 0.20353705  |
| stats_o/std                    | 0.095844135 |
| test/episodes                  | 2590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000407   |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.5136266  |
| test/Q_plus_P                  | -1.5136266  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 103600      |
| train/episodes                 | 10360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.433       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00479    |
| train/info_shaping_reward_mean | -0.0924     |
| train/info_shaping_reward_min  | -0.209      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.7       |
| train/steps                    | 414400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 259        |
| stats_o/mean                   | 0.20353474 |
| stats_o/std                    | 0.09579707 |
| test/episodes                  | 2600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0015    |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3966991 |
| test/Q_plus_P                  | -1.3966991 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 104000     |
| train/episodes                 | 10400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.463      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0074    |
| train/info_shaping_reward_mean | -0.0908    |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.5      |
| train/steps                    | 416000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 260         |
| stats_o/mean                   | 0.20353685  |
| stats_o/std                    | 0.095697984 |
| test/episodes                  | 2610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.6511999  |
| test/Q_plus_P                  | -1.6511999  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 104400      |
| train/episodes                 | 10440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.458       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00465    |
| train/info_shaping_reward_mean | -0.0882     |
| train/info_shaping_reward_min  | -0.199      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.7       |
| train/steps                    | 417600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 261        |
| stats_o/mean                   | 0.20353706 |
| stats_o/std                    | 0.09560228 |
| test/episodes                  | 2620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000558  |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5090706 |
| test/Q_plus_P                  | -1.5090706 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 104800     |
| train/episodes                 | 10480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.522      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00538   |
| train/info_shaping_reward_mean | -0.0793    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 419200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 262        |
| stats_o/mean                   | 0.20353684 |
| stats_o/std                    | 0.09549699 |
| test/episodes                  | 2630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00178   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.217     |
| test/Q                         | -1.6747851 |
| test/Q_plus_P                  | -1.6747851 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 105200     |
| train/episodes                 | 10520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.494      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00437   |
| train/info_shaping_reward_mean | -0.0799    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 420800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.20354068  |
| stats_o/std                    | 0.095378704 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -1.4615643  |
| test/Q_plus_P                  | -1.4615643  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.536       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00457    |
| train/info_shaping_reward_mean | -0.0748     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 264        |
| stats_o/mean                   | 0.20353618 |
| stats_o/std                    | 0.09528446 |
| test/episodes                  | 2650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00286   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2859594 |
| test/Q_plus_P                  | -1.2859594 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 106000     |
| train/episodes                 | 10600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00475   |
| train/info_shaping_reward_mean | -0.0795    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 424000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 265        |
| stats_o/mean                   | 0.20354448 |
| stats_o/std                    | 0.09516297 |
| test/episodes                  | 2660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00197   |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5677714 |
| test/Q_plus_P                  | -1.5677714 |
| test/reward_per_eps            | -10        |
| test/steps                     | 106400     |
| train/episodes                 | 10640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.421      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0071    |
| train/info_shaping_reward_mean | -0.0886    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.1      |
| train/steps                    | 425600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 266        |
| stats_o/mean                   | 0.20356375 |
| stats_o/std                    | 0.09503854 |
| test/episodes                  | 2670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6735678 |
| test/Q_plus_P                  | -1.6735678 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 106800     |
| train/episodes                 | 10680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00542   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 427200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 267         |
| stats_o/mean                   | 0.20356934  |
| stats_o/std                    | 0.094991535 |
| test/episodes                  | 2680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0025     |
| test/info_shaping_reward_mean  | -0.0554     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.881438   |
| test/Q_plus_P                  | -1.881438   |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 107200      |
| train/episodes                 | 10720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.44        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00611    |
| train/info_shaping_reward_mean | -0.1        |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.4       |
| train/steps                    | 428800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 268        |
| stats_o/mean                   | 0.20357454 |
| stats_o/std                    | 0.09492774 |
| test/episodes                  | 2690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00166   |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6024226 |
| test/Q_plus_P                  | -1.6024226 |
| test/reward_per_eps            | -11        |
| test/steps                     | 107600     |
| train/episodes                 | 10760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.476      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00452   |
| train/info_shaping_reward_mean | -0.0878    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21        |
| train/steps                    | 430400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 269        |
| stats_o/mean                   | 0.2035901  |
| stats_o/std                    | 0.09482125 |
| test/episodes                  | 2700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00403   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.7917366 |
| test/Q_plus_P                  | -1.7917366 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 108000     |
| train/episodes                 | 10800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.454      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00516   |
| train/info_shaping_reward_mean | -0.0843    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.9      |
| train/steps                    | 432000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 270        |
| stats_o/mean                   | 0.20358269 |
| stats_o/std                    | 0.09471712 |
| test/episodes                  | 2710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00449   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4310036 |
| test/Q_plus_P                  | -1.4310036 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 108400     |
| train/episodes                 | 10840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.501      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00605   |
| train/info_shaping_reward_mean | -0.0793    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20        |
| train/steps                    | 433600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 271        |
| stats_o/mean                   | 0.20360376 |
| stats_o/std                    | 0.09465037 |
| test/episodes                  | 2720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00261   |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -1.6626407 |
| test/Q_plus_P                  | -1.6626407 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 108800     |
| train/episodes                 | 10880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.447      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00547   |
| train/info_shaping_reward_mean | -0.0906    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.1      |
| train/steps                    | 435200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 272        |
| stats_o/mean                   | 0.20362405 |
| stats_o/std                    | 0.09452766 |
| test/episodes                  | 2730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00143   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4167818 |
| test/Q_plus_P                  | -1.4167818 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 109200     |
| train/episodes                 | 10920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.494      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00638   |
| train/info_shaping_reward_mean | -0.0823    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 436800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 273        |
| stats_o/mean                   | 0.2036316  |
| stats_o/std                    | 0.09440978 |
| test/episodes                  | 2740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5238428 |
| test/Q_plus_P                  | -1.5238428 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 109600     |
| train/episodes                 | 10960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.503      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00523   |
| train/info_shaping_reward_mean | -0.0838    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 438400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 274         |
| stats_o/mean                   | 0.20363247  |
| stats_o/std                    | 0.094280384 |
| test/episodes                  | 2750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00154    |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.6198124  |
| test/Q_plus_P                  | -1.6198124  |
| test/reward_per_eps            | -10         |
| test/steps                     | 110000      |
| train/episodes                 | 11000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.519       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00397    |
| train/info_shaping_reward_mean | -0.0783     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.2       |
| train/steps                    | 440000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 275        |
| stats_o/mean                   | 0.20362005 |
| stats_o/std                    | 0.09418692 |
| test/episodes                  | 2760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000495  |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.6385769 |
| test/Q_plus_P                  | -1.6385769 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 110400     |
| train/episodes                 | 11040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00516   |
| train/info_shaping_reward_mean | -0.0762    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 441600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 276         |
| stats_o/mean                   | 0.20364612  |
| stats_o/std                    | 0.094138004 |
| test/episodes                  | 2770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00105    |
| test/info_shaping_reward_mean  | -0.0634     |
| test/info_shaping_reward_min   | -0.214      |
| test/Q                         | -2.3332176  |
| test/Q_plus_P                  | -2.3332176  |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 110800      |
| train/episodes                 | 11080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.5         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00378    |
| train/info_shaping_reward_mean | -0.091      |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20         |
| train/steps                    | 443200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 277        |
| stats_o/mean                   | 0.20364965 |
| stats_o/std                    | 0.09401184 |
| test/episodes                  | 2780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0028    |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.6700428 |
| test/Q_plus_P                  | -1.6700428 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 111200     |
| train/episodes                 | 11120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.0772    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 444800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 278        |
| stats_o/mean                   | 0.20365529 |
| stats_o/std                    | 0.09391    |
| test/episodes                  | 2790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00303   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.759216  |
| test/Q_plus_P                  | -1.759216  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 111600     |
| train/episodes                 | 11160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.468      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00562   |
| train/info_shaping_reward_mean | -0.0877    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.3      |
| train/steps                    | 446400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 279         |
| stats_o/mean                   | 0.20365672  |
| stats_o/std                    | 0.093822844 |
| test/episodes                  | 2800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00199    |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.4460921  |
| test/Q_plus_P                  | -1.4460921  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 112000      |
| train/episodes                 | 11200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.474       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0067     |
| train/info_shaping_reward_mean | -0.0839     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.1       |
| train/steps                    | 448000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 280        |
| stats_o/mean                   | 0.20366365 |
| stats_o/std                    | 0.09378056 |
| test/episodes                  | 2810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00199   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.321138  |
| test/Q_plus_P                  | -1.321138  |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 112400     |
| train/episodes                 | 11240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.523      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00409   |
| train/info_shaping_reward_mean | -0.0828    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 449600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.20366164 |
| stats_o/std                    | 0.09365975 |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00205   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.305373  |
| test/Q_plus_P                  | -1.305373  |
| test/reward_per_eps            | -10        |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.55       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00443   |
| train/info_shaping_reward_mean | -0.075     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.20365822 |
| stats_o/std                    | 0.09356527 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00105   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.335596  |
| test/Q_plus_P                  | -1.335596  |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.526      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00484   |
| train/info_shaping_reward_mean | -0.0777    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.2036536  |
| stats_o/std                    | 0.093514   |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00261   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5199599 |
| test/Q_plus_P                  | -1.5199599 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.456      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 284        |
| stats_o/mean                   | 0.20367175 |
| stats_o/std                    | 0.09354476 |
| test/episodes                  | 2850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00199   |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.4302515 |
| test/Q_plus_P                  | -1.4302515 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 114000     |
| train/episodes                 | 11400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.459      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00531   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 456000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 285        |
| stats_o/mean                   | 0.20368834 |
| stats_o/std                    | 0.09349448 |
| test/episodes                  | 2860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00367   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4730275 |
| test/Q_plus_P                  | -1.4730275 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 114400     |
| train/episodes                 | 11440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00615   |
| train/info_shaping_reward_mean | -0.0908    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 457600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 286         |
| stats_o/mean                   | 0.20368597  |
| stats_o/std                    | 0.093417734 |
| test/episodes                  | 2870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00269    |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5704482  |
| test/Q_plus_P                  | -1.5704482  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 114800      |
| train/episodes                 | 11480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.529       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00437    |
| train/info_shaping_reward_mean | -0.0848     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 459200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.20368946 |
| stats_o/std                    | 0.09330244 |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.665      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00321   |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.302653  |
| test/Q_plus_P                  | -2.302653  |
| test/reward_per_eps            | -13.4      |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00575   |
| train/info_shaping_reward_mean | -0.0777    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 288        |
| stats_o/mean                   | 0.20371027 |
| stats_o/std                    | 0.0933261  |
| test/episodes                  | 2890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00121   |
| test/info_shaping_reward_mean  | -0.05      |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3655679 |
| test/Q_plus_P                  | -1.3655679 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 115600     |
| train/episodes                 | 11560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.492      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00605   |
| train/info_shaping_reward_mean | -0.0964    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.3      |
| train/steps                    | 462400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 289        |
| stats_o/mean                   | 0.20370822 |
| stats_o/std                    | 0.09329693 |
| test/episodes                  | 2900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00101   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.3805957 |
| test/Q_plus_P                  | -1.3805957 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 116000     |
| train/episodes                 | 11600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00506   |
| train/info_shaping_reward_mean | -0.0964    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 464000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 290        |
| stats_o/mean                   | 0.20371974 |
| stats_o/std                    | 0.0932486  |
| test/episodes                  | 2910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000827  |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.320954  |
| test/Q_plus_P                  | -1.320954  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 116400     |
| train/episodes                 | 11640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.468      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00522   |
| train/info_shaping_reward_mean | -0.0853    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.3      |
| train/steps                    | 465600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.20372781  |
| stats_o/std                    | 0.093184546 |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00281    |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.5566174  |
| test/Q_plus_P                  | -1.5566174  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.501       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00427    |
| train/info_shaping_reward_mean | -0.0855     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20         |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 292        |
| stats_o/mean                   | 0.20374133 |
| stats_o/std                    | 0.09307506 |
| test/episodes                  | 2930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00253   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.238834  |
| test/Q_plus_P                  | -1.238834  |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 117200     |
| train/episodes                 | 11720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.498      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00522   |
| train/info_shaping_reward_mean | -0.0784    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 468800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 293        |
| stats_o/mean                   | 0.20373352 |
| stats_o/std                    | 0.0930239  |
| test/episodes                  | 2940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0027    |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2651244 |
| test/Q_plus_P                  | -1.2651244 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 117600     |
| train/episodes                 | 11760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.471      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0054    |
| train/info_shaping_reward_mean | -0.0982    |
| train/info_shaping_reward_min  | -0.265     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 470400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 294        |
| stats_o/mean                   | 0.20374164 |
| stats_o/std                    | 0.09293083 |
| test/episodes                  | 2950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0013    |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2054595 |
| test/Q_plus_P                  | -1.2054595 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 118000     |
| train/episodes                 | 11800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.49       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00522   |
| train/info_shaping_reward_mean | -0.0861    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 472000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 295        |
| stats_o/mean                   | 0.20376793 |
| stats_o/std                    | 0.09289012 |
| test/episodes                  | 2960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00221   |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6685083 |
| test/Q_plus_P                  | -1.6685083 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 118400     |
| train/episodes                 | 11840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.453      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00604   |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.9      |
| train/steps                    | 473600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 296        |
| stats_o/mean                   | 0.20377792 |
| stats_o/std                    | 0.09281979 |
| test/episodes                  | 2970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00212   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2468312 |
| test/Q_plus_P                  | -1.2468312 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 118800     |
| train/episodes                 | 11880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.513      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00364   |
| train/info_shaping_reward_mean | -0.0859    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.5      |
| train/steps                    | 475200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 297        |
| stats_o/mean                   | 0.20377888 |
| stats_o/std                    | 0.09273104 |
| test/episodes                  | 2980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00288   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -1.4969057 |
| test/Q_plus_P                  | -1.4969057 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 119200     |
| train/episodes                 | 11920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00429   |
| train/info_shaping_reward_mean | -0.0853    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 476800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 298        |
| stats_o/mean                   | 0.20377229 |
| stats_o/std                    | 0.09266862 |
| test/episodes                  | 2990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00247   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4203157 |
| test/Q_plus_P                  | -1.4203157 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 119600     |
| train/episodes                 | 11960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.511      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00453   |
| train/info_shaping_reward_mean | -0.0797    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 478400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 299        |
| stats_o/mean                   | 0.20377764 |
| stats_o/std                    | 0.0925783  |
| test/episodes                  | 3000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00147   |
| test/info_shaping_reward_mean  | -0.0458    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2279011 |
| test/Q_plus_P                  | -1.2279011 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 120000     |
| train/episodes                 | 12000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.0837    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 480000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 300        |
| stats_o/mean                   | 0.20378181 |
| stats_o/std                    | 0.09254081 |
| test/episodes                  | 3010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00101   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.3544072 |
| test/Q_plus_P                  | -1.3544072 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 120400     |
| train/episodes                 | 12040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.487      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00427   |
| train/info_shaping_reward_mean | -0.0902    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.5      |
| train/steps                    | 481600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.20378065 |
| stats_o/std                    | 0.09248319 |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00295   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2401673 |
| test/Q_plus_P                  | -1.2401673 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.489      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00491   |
| train/info_shaping_reward_mean | -0.0927    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 302        |
| stats_o/mean                   | 0.20378794 |
| stats_o/std                    | 0.09240457 |
| test/episodes                  | 3030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000815  |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -1.3070263 |
| test/Q_plus_P                  | -1.3070263 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 121200     |
| train/episodes                 | 12120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 484800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 303         |
| stats_o/mean                   | 0.20378561  |
| stats_o/std                    | 0.092303164 |
| test/episodes                  | 3040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00217    |
| test/info_shaping_reward_mean  | -0.0518     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.4241655  |
| test/Q_plus_P                  | -1.4241655  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 121600      |
| train/episodes                 | 12160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.555       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00494    |
| train/info_shaping_reward_mean | -0.0779     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 486400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 304        |
| stats_o/mean                   | 0.20378672 |
| stats_o/std                    | 0.09223407 |
| test/episodes                  | 3050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.4098849 |
| test/Q_plus_P                  | -1.4098849 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 122000     |
| train/episodes                 | 12200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0048    |
| train/info_shaping_reward_mean | -0.0841    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 488000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.20379698 |
| stats_o/std                    | 0.09223751 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000511  |
| test/info_shaping_reward_mean  | -0.05      |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.422522  |
| test/Q_plus_P                  | -1.422522  |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.45       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00571   |
| train/info_shaping_reward_mean | -0.0976    |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22        |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 306        |
| stats_o/mean                   | 0.20378877 |
| stats_o/std                    | 0.09218487 |
| test/episodes                  | 3070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00276   |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -1.7258482 |
| test/Q_plus_P                  | -1.7258482 |
| test/reward_per_eps            | -11        |
| test/steps                     | 122800     |
| train/episodes                 | 12280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.469      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.006     |
| train/info_shaping_reward_mean | -0.0878    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 491200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.20378846 |
| stats_o/std                    | 0.09208543 |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00346   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6408383 |
| test/Q_plus_P                  | -1.6408383 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.534      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00443   |
| train/info_shaping_reward_mean | -0.0785    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 308        |
| stats_o/mean                   | 0.20377934 |
| stats_o/std                    | 0.0920024  |
| test/episodes                  | 3090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.6285869 |
| test/Q_plus_P                  | -1.6285869 |
| test/reward_per_eps            | -12        |
| test/steps                     | 123600     |
| train/episodes                 | 12360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.428      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00597   |
| train/info_shaping_reward_mean | -0.0845    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.9      |
| train/steps                    | 494400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 309        |
| stats_o/mean                   | 0.20378983 |
| stats_o/std                    | 0.09195709 |
| test/episodes                  | 3100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00364   |
| test/info_shaping_reward_mean  | -0.0456    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4013648 |
| test/Q_plus_P                  | -1.4013648 |
| test/reward_per_eps            | -9         |
| test/steps                     | 124000     |
| train/episodes                 | 12400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.422      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00552   |
| train/info_shaping_reward_mean | -0.0881    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.1      |
| train/steps                    | 496000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 310        |
| stats_o/mean                   | 0.20379822 |
| stats_o/std                    | 0.09188799 |
| test/episodes                  | 3110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00587   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6497798 |
| test/Q_plus_P                  | -1.6497798 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 124400     |
| train/episodes                 | 12440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.479      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00617   |
| train/info_shaping_reward_mean | -0.0872    |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 497600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.20380253 |
| stats_o/std                    | 0.09183149 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00198   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6335776 |
| test/Q_plus_P                  | -1.6335776 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.506      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00403   |
| train/info_shaping_reward_mean | -0.0852    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 312        |
| stats_o/mean                   | 0.20380434 |
| stats_o/std                    | 0.09175566 |
| test/episodes                  | 3130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.0468    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.4087905 |
| test/Q_plus_P                  | -1.4087905 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 125200     |
| train/episodes                 | 12520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.509      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0058    |
| train/info_shaping_reward_mean | -0.0827    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 500800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.2037917   |
| stats_o/std                    | 0.091686934 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00229    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.4305159  |
| test/Q_plus_P                  | -1.4305159  |
| test/reward_per_eps            | -10         |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.509       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00533    |
| train/info_shaping_reward_mean | -0.086      |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.6       |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 314        |
| stats_o/mean                   | 0.20379536 |
| stats_o/std                    | 0.09163141 |
| test/episodes                  | 3150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00189   |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3718599 |
| test/Q_plus_P                  | -1.3718599 |
| test/reward_per_eps            | -10        |
| test/steps                     | 126000     |
| train/episodes                 | 12600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.506      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00566   |
| train/info_shaping_reward_mean | -0.0813    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 504000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 315        |
| stats_o/mean                   | 0.20380259 |
| stats_o/std                    | 0.09157054 |
| test/episodes                  | 3160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00313   |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.539086  |
| test/Q_plus_P                  | -1.539086  |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 126400     |
| train/episodes                 | 12640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.489      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0045    |
| train/info_shaping_reward_mean | -0.0879    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 505600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.20379776  |
| stats_o/std                    | 0.091524534 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00158    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.491401   |
| test/Q_plus_P                  | -1.491401   |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.504       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00453    |
| train/info_shaping_reward_mean | -0.0856     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.9       |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 317        |
| stats_o/mean                   | 0.20380843 |
| stats_o/std                    | 0.09144035 |
| test/episodes                  | 3180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000568  |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4032763 |
| test/Q_plus_P                  | -1.4032763 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 127200     |
| train/episodes                 | 12720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.454      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00676   |
| train/info_shaping_reward_mean | -0.0866    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 508800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 318        |
| stats_o/mean                   | 0.20381506 |
| stats_o/std                    | 0.09143083 |
| test/episodes                  | 3190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000549  |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2369461 |
| test/Q_plus_P                  | -1.2369461 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 127600     |
| train/episodes                 | 12760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00371   |
| train/info_shaping_reward_mean | -0.0895    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 510400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 319        |
| stats_o/mean                   | 0.20382443 |
| stats_o/std                    | 0.09136612 |
| test/episodes                  | 3200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000838  |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4321213 |
| test/Q_plus_P                  | -1.4321213 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 128000     |
| train/episodes                 | 12800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.489      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00538   |
| train/info_shaping_reward_mean | -0.0849    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 512000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 320        |
| stats_o/mean                   | 0.20382051 |
| stats_o/std                    | 0.0912636  |
| test/episodes                  | 3210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000759  |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -1.3723415 |
| test/Q_plus_P                  | -1.3723415 |
| test/reward_per_eps            | -10        |
| test/steps                     | 128400     |
| train/episodes                 | 12840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00479   |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 513600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 321        |
| stats_o/mean                   | 0.20382534 |
| stats_o/std                    | 0.09120461 |
| test/episodes                  | 3220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0026    |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.3598342 |
| test/Q_plus_P                  | -1.3598342 |
| test/reward_per_eps            | -10        |
| test/steps                     | 128800     |
| train/episodes                 | 12880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0832    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 515200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 322        |
| stats_o/mean                   | 0.20382392 |
| stats_o/std                    | 0.09111494 |
| test/episodes                  | 3230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00223   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -1.640894  |
| test/Q_plus_P                  | -1.640894  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 129200     |
| train/episodes                 | 12920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.558      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00503   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 516800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 323         |
| stats_o/mean                   | 0.20382649  |
| stats_o/std                    | 0.091064215 |
| test/episodes                  | 3240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00191    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.5044965  |
| test/Q_plus_P                  | -1.5044965  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 129600      |
| train/episodes                 | 12960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.532       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00651    |
| train/info_shaping_reward_mean | -0.083      |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 518400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 324        |
| stats_o/mean                   | 0.20381798 |
| stats_o/std                    | 0.09102955 |
| test/episodes                  | 3250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0023    |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3932952 |
| test/Q_plus_P                  | -1.3932952 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 130000     |
| train/episodes                 | 13000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.493      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.0939    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.3      |
| train/steps                    | 520000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 325        |
| stats_o/mean                   | 0.20383193 |
| stats_o/std                    | 0.09096349 |
| test/episodes                  | 3260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00236   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4078698 |
| test/Q_plus_P                  | -1.4078698 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 130400     |
| train/episodes                 | 13040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.511      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0794    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 521600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.2038308   |
| stats_o/std                    | 0.090875514 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2451724  |
| test/Q_plus_P                  | -1.2451724  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.567       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00413    |
| train/info_shaping_reward_mean | -0.0815     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 327         |
| stats_o/mean                   | 0.20383775  |
| stats_o/std                    | 0.090796515 |
| test/episodes                  | 3280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.5073576  |
| test/Q_plus_P                  | -1.5073576  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 131200      |
| train/episodes                 | 13120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.558       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00485    |
| train/info_shaping_reward_mean | -0.0813     |
| train/info_shaping_reward_min  | -0.225      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 524800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 328        |
| stats_o/mean                   | 0.20384559 |
| stats_o/std                    | 0.09070682 |
| test/episodes                  | 3290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0014    |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.442975  |
| test/Q_plus_P                  | -1.442975  |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 131600     |
| train/episodes                 | 13160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.524      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00554   |
| train/info_shaping_reward_mean | -0.0776    |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 526400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 329        |
| stats_o/mean                   | 0.2038596  |
| stats_o/std                    | 0.09065871 |
| test/episodes                  | 3300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00139   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3718789 |
| test/Q_plus_P                  | -1.3718789 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 132000     |
| train/episodes                 | 13200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.498      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00541   |
| train/info_shaping_reward_mean | -0.0907    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 528000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.20386735  |
| stats_o/std                    | 0.090583265 |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00275    |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.70444    |
| test/Q_plus_P                  | -1.70444    |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.472       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00538    |
| train/info_shaping_reward_mean | -0.085      |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.1       |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 331        |
| stats_o/mean                   | 0.20386785 |
| stats_o/std                    | 0.09049316 |
| test/episodes                  | 3320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00287   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3460845 |
| test/Q_plus_P                  | -1.3460845 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 132800     |
| train/episodes                 | 13280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00556   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 531200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 332        |
| stats_o/mean                   | 0.20386915 |
| stats_o/std                    | 0.09040952 |
| test/episodes                  | 3330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00203   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.3146113 |
| test/Q_plus_P                  | -1.3146113 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 133200     |
| train/episodes                 | 13320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.528      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00554   |
| train/info_shaping_reward_mean | -0.0796    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 532800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 333        |
| stats_o/mean                   | 0.20388013 |
| stats_o/std                    | 0.09033322 |
| test/episodes                  | 3340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00241   |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.467082  |
| test/Q_plus_P                  | -1.467082  |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 133600     |
| train/episodes                 | 13360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.476      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00524   |
| train/info_shaping_reward_mean | -0.0873    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21        |
| train/steps                    | 534400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 334        |
| stats_o/mean                   | 0.20386548 |
| stats_o/std                    | 0.09040256 |
| test/episodes                  | 3350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00316   |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5585407 |
| test/Q_plus_P                  | -1.5585407 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 134000     |
| train/episodes                 | 13400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.509      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0998    |
| train/info_shaping_reward_min  | -0.304     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 536000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.20385632 |
| stats_o/std                    | 0.09033442 |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0046    |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.4493508 |
| test/Q_plus_P                  | -1.4493508 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.51       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00483   |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.20385699  |
| stats_o/std                    | 0.090309925 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.4664344  |
| test/Q_plus_P                  | -1.4664344  |
| test/reward_per_eps            | -10         |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.502       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00417    |
| train/info_shaping_reward_mean | -0.0897     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.9       |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 337        |
| stats_o/mean                   | 0.20385492 |
| stats_o/std                    | 0.09026859 |
| test/episodes                  | 3380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00264   |
| test/info_shaping_reward_mean  | -0.046     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3242449 |
| test/Q_plus_P                  | -1.3242449 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 135200     |
| train/episodes                 | 13520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.511      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00564   |
| train/info_shaping_reward_mean | -0.089     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 540800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.20385608 |
| stats_o/std                    | 0.09018023 |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000205  |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3607994 |
| test/Q_plus_P                  | -1.3607994 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.492      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0049    |
| train/info_shaping_reward_mean | -0.0806    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.3      |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 339        |
| stats_o/mean                   | 0.20385785 |
| stats_o/std                    | 0.0901701  |
| test/episodes                  | 3400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3946592 |
| test/Q_plus_P                  | -1.3946592 |
| test/reward_per_eps            | -10        |
| test/steps                     | 136000     |
| train/episodes                 | 13600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.503      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.0885    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 544000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 340         |
| stats_o/mean                   | 0.20387512  |
| stats_o/std                    | 0.090087526 |
| test/episodes                  | 3410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000622   |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.2721115  |
| test/Q_plus_P                  | -1.2721115  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 136400      |
| train/episodes                 | 13640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.487       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00572    |
| train/info_shaping_reward_mean | -0.08       |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.5       |
| train/steps                    | 545600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 341        |
| stats_o/mean                   | 0.203882   |
| stats_o/std                    | 0.08999872 |
| test/episodes                  | 3420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00187   |
| test/info_shaping_reward_mean  | -0.0468    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2359295 |
| test/Q_plus_P                  | -1.2359295 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 136800     |
| train/episodes                 | 13680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.528      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00466   |
| train/info_shaping_reward_mean | -0.0798    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 547200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 342        |
| stats_o/mean                   | 0.20389108 |
| stats_o/std                    | 0.08995353 |
| test/episodes                  | 3430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00246   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3617432 |
| test/Q_plus_P                  | -1.3617432 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 137200     |
| train/episodes                 | 13720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.495      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00423   |
| train/info_shaping_reward_mean | -0.0802    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 548800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 343        |
| stats_o/mean                   | 0.20390594 |
| stats_o/std                    | 0.08990986 |
| test/episodes                  | 3440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00334   |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2948076 |
| test/Q_plus_P                  | -1.2948076 |
| test/reward_per_eps            | -10        |
| test/steps                     | 137600     |
| train/episodes                 | 13760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.465      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00548   |
| train/info_shaping_reward_mean | -0.0827    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 550400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 344        |
| stats_o/mean                   | 0.20390731 |
| stats_o/std                    | 0.08986071 |
| test/episodes                  | 3450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000546  |
| test/info_shaping_reward_mean  | -0.0457    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.5275519 |
| test/Q_plus_P                  | -1.5275519 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 138000     |
| train/episodes                 | 13800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00562   |
| train/info_shaping_reward_mean | -0.0836    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 552000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 345        |
| stats_o/mean                   | 0.20390502 |
| stats_o/std                    | 0.0898521  |
| test/episodes                  | 3460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00184   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3772612 |
| test/Q_plus_P                  | -1.3772612 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 138400     |
| train/episodes                 | 13840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.475      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00536   |
| train/info_shaping_reward_mean | -0.0965    |
| train/info_shaping_reward_min  | -0.293     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21        |
| train/steps                    | 553600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 346        |
| stats_o/mean                   | 0.20389998 |
| stats_o/std                    | 0.08980946 |
| test/episodes                  | 3470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00293   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -1.520927  |
| test/Q_plus_P                  | -1.520927  |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 138800     |
| train/episodes                 | 13880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.522      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00721   |
| train/info_shaping_reward_mean | -0.0818    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 555200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 347         |
| stats_o/mean                   | 0.20390764  |
| stats_o/std                    | 0.089764744 |
| test/episodes                  | 3480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00289    |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.1466887  |
| test/Q_plus_P                  | -1.1466887  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 139200      |
| train/episodes                 | 13920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.521       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00607    |
| train/info_shaping_reward_mean | -0.0826     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 556800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.20390138  |
| stats_o/std                    | 0.089677334 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00134    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.4030898  |
| test/Q_plus_P                  | -1.4030898  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.53        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00442    |
| train/info_shaping_reward_mean | -0.0762     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.8       |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 349        |
| stats_o/mean                   | 0.20388392 |
| stats_o/std                    | 0.08964142 |
| test/episodes                  | 3500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000633  |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4145916 |
| test/Q_plus_P                  | -1.4145916 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 140000     |
| train/episodes                 | 14000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.462      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00588   |
| train/info_shaping_reward_mean | -0.09      |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.5      |
| train/steps                    | 560000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 350         |
| stats_o/mean                   | 0.2038722   |
| stats_o/std                    | 0.089566186 |
| test/episodes                  | 3510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.662       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00325    |
| test/info_shaping_reward_mean  | -0.0592     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.5391803  |
| test/Q_plus_P                  | -2.5391803  |
| test/reward_per_eps            | -13.5       |
| test/steps                     | 140400      |
| train/episodes                 | 14040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.48        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0051     |
| train/info_shaping_reward_mean | -0.0828     |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.8       |
| train/steps                    | 561600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 351        |
| stats_o/mean                   | 0.20387849 |
| stats_o/std                    | 0.08949325 |
| test/episodes                  | 3520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000347  |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5005139 |
| test/Q_plus_P                  | -1.5005139 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 140800     |
| train/episodes                 | 14080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0784    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 563200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 352        |
| stats_o/mean                   | 0.20388924 |
| stats_o/std                    | 0.08948986 |
| test/episodes                  | 3530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00285   |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2950336 |
| test/Q_plus_P                  | -1.2950336 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 141200     |
| train/episodes                 | 14120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.446      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00408   |
| train/info_shaping_reward_mean | -0.0932    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.2      |
| train/steps                    | 564800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 353         |
| stats_o/mean                   | 0.20388383  |
| stats_o/std                    | 0.089457996 |
| test/episodes                  | 3540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.4407932  |
| test/Q_plus_P                  | -1.4407932  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 141600      |
| train/episodes                 | 14160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.55        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00459    |
| train/info_shaping_reward_mean | -0.0921     |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18         |
| train/steps                    | 566400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 354        |
| stats_o/mean                   | 0.20389524 |
| stats_o/std                    | 0.08940423 |
| test/episodes                  | 3550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00491   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4108021 |
| test/Q_plus_P                  | -1.4108021 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 142000     |
| train/episodes                 | 14200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.509      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00547   |
| train/info_shaping_reward_mean | -0.0809    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 568000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 355        |
| stats_o/mean                   | 0.20389707 |
| stats_o/std                    | 0.08936018 |
| test/episodes                  | 3560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00443   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.2514629 |
| test/Q_plus_P                  | -1.2514629 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 142400     |
| train/episodes                 | 14240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.005     |
| train/info_shaping_reward_mean | -0.0789    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 569600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 356        |
| stats_o/mean                   | 0.20388576 |
| stats_o/std                    | 0.08933112 |
| test/episodes                  | 3570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2405403 |
| test/Q_plus_P                  | -1.2405403 |
| test/reward_per_eps            | -9         |
| test/steps                     | 142800     |
| train/episodes                 | 14280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.5        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00513   |
| train/info_shaping_reward_mean | -0.087     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20        |
| train/steps                    | 571200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 357        |
| stats_o/mean                   | 0.20387213 |
| stats_o/std                    | 0.08926174 |
| test/episodes                  | 3580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00131   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6578434 |
| test/Q_plus_P                  | -1.6578434 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 143200     |
| train/episodes                 | 14320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00357   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 572800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 358        |
| stats_o/mean                   | 0.20387238 |
| stats_o/std                    | 0.08922592 |
| test/episodes                  | 3590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00222   |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4569527 |
| test/Q_plus_P                  | -1.4569527 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 143600     |
| train/episodes                 | 14360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.53       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00506   |
| train/info_shaping_reward_mean | -0.0775    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 574400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 359        |
| stats_o/mean                   | 0.20386983 |
| stats_o/std                    | 0.08916297 |
| test/episodes                  | 3600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00153   |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.5721043 |
| test/Q_plus_P                  | -1.5721043 |
| test/reward_per_eps            | -11        |
| test/steps                     | 144000     |
| train/episodes                 | 14400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00571   |
| train/info_shaping_reward_mean | -0.0745    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 576000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 360        |
| stats_o/mean                   | 0.2038732  |
| stats_o/std                    | 0.0891134  |
| test/episodes                  | 3610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00319   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.3848169 |
| test/Q_plus_P                  | -1.3848169 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 144400     |
| train/episodes                 | 14440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.516      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00537   |
| train/info_shaping_reward_mean | -0.0854    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 577600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 361        |
| stats_o/mean                   | 0.20387055 |
| stats_o/std                    | 0.08906181 |
| test/episodes                  | 3620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00312   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2440093 |
| test/Q_plus_P                  | -1.2440093 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 144800     |
| train/episodes                 | 14480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.511      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00385   |
| train/info_shaping_reward_mean | -0.0839    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 579200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 362         |
| stats_o/mean                   | 0.2038701   |
| stats_o/std                    | 0.088973135 |
| test/episodes                  | 3630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00294    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3973584  |
| test/Q_plus_P                  | -1.3973584  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 145200      |
| train/episodes                 | 14520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.54        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.0743     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 580800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 363        |
| stats_o/mean                   | 0.2038712  |
| stats_o/std                    | 0.0888823  |
| test/episodes                  | 3640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00188   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5954123 |
| test/Q_plus_P                  | -1.5954123 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 145600     |
| train/episodes                 | 14560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00487   |
| train/info_shaping_reward_mean | -0.0696    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 582400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 364        |
| stats_o/mean                   | 0.2038703  |
| stats_o/std                    | 0.08880983 |
| test/episodes                  | 3650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3699455 |
| test/Q_plus_P                  | -1.3699455 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 146000     |
| train/episodes                 | 14600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 584000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 365        |
| stats_o/mean                   | 0.20386654 |
| stats_o/std                    | 0.08876808 |
| test/episodes                  | 3660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00135   |
| test/info_shaping_reward_mean  | -0.05      |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6559871 |
| test/Q_plus_P                  | -1.6559871 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 146400     |
| train/episodes                 | 14640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00468   |
| train/info_shaping_reward_mean | -0.0784    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 585600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 366        |
| stats_o/mean                   | 0.20387702 |
| stats_o/std                    | 0.08870996 |
| test/episodes                  | 3670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00185   |
| test/info_shaping_reward_mean  | -0.0448    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1452216 |
| test/Q_plus_P                  | -1.1452216 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 146800     |
| train/episodes                 | 14680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.534      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00562   |
| train/info_shaping_reward_mean | -0.0774    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 587200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.20387916  |
| stats_o/std                    | 0.088642634 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00312    |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.4594284  |
| test/Q_plus_P                  | -1.4594284  |
| test/reward_per_eps            | -9          |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.53        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00497    |
| train/info_shaping_reward_mean | -0.0781     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.8       |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 368         |
| stats_o/mean                   | 0.20387772  |
| stats_o/std                    | 0.088571146 |
| test/episodes                  | 3690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.3195592  |
| test/Q_plus_P                  | -1.3195592  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 147600      |
| train/episodes                 | 14760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.522       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00493    |
| train/info_shaping_reward_mean | -0.0745     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 590400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 369        |
| stats_o/mean                   | 0.20389187 |
| stats_o/std                    | 0.08850812 |
| test/episodes                  | 3700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0024    |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3852707 |
| test/Q_plus_P                  | -1.3852707 |
| test/reward_per_eps            | -10        |
| test/steps                     | 148000     |
| train/episodes                 | 14800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0044    |
| train/info_shaping_reward_mean | -0.0766    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 592000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 370         |
| stats_o/mean                   | 0.20390199  |
| stats_o/std                    | 0.088443734 |
| test/episodes                  | 3710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000489   |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.3971738  |
| test/Q_plus_P                  | -1.3971738  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 148400      |
| train/episodes                 | 14840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0771     |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 593600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 371        |
| stats_o/mean                   | 0.2039     |
| stats_o/std                    | 0.0883911  |
| test/episodes                  | 3720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00198   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -1.3302368 |
| test/Q_plus_P                  | -1.3302368 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 148800     |
| train/episodes                 | 14880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.541      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00553   |
| train/info_shaping_reward_mean | -0.0853    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 595200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 372        |
| stats_o/mean                   | 0.20390888 |
| stats_o/std                    | 0.08835575 |
| test/episodes                  | 3730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00221   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3844893 |
| test/Q_plus_P                  | -1.3844893 |
| test/reward_per_eps            | -10        |
| test/steps                     | 149200     |
| train/episodes                 | 14920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0817    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 596800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 373        |
| stats_o/mean                   | 0.2039071  |
| stats_o/std                    | 0.08829029 |
| test/episodes                  | 3740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00122   |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5893458 |
| test/Q_plus_P                  | -1.5893458 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 149600     |
| train/episodes                 | 14960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00425   |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 598400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.20391431 |
| stats_o/std                    | 0.08821537 |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.3530997 |
| test/Q_plus_P                  | -1.3530997 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.555      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00444   |
| train/info_shaping_reward_mean | -0.0745    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.20391573  |
| stats_o/std                    | 0.088219985 |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000834   |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.1654929  |
| test/Q_plus_P                  | -1.1654929  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.48        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00488    |
| train/info_shaping_reward_mean | -0.0904     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.8       |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.20391478 |
| stats_o/std                    | 0.08814186 |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00165   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3959739 |
| test/Q_plus_P                  | -1.3959739 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.57       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00472   |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 377        |
| stats_o/mean                   | 0.20392376 |
| stats_o/std                    | 0.08810308 |
| test/episodes                  | 3780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00351   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.4144664 |
| test/Q_plus_P                  | -1.4144664 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 151200     |
| train/episodes                 | 15120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.514      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00549   |
| train/info_shaping_reward_mean | -0.0831    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 604800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 378        |
| stats_o/mean                   | 0.20392546 |
| stats_o/std                    | 0.08806677 |
| test/episodes                  | 3790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00155   |
| test/info_shaping_reward_mean  | -0.0463    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3233377 |
| test/Q_plus_P                  | -1.3233377 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 151600     |
| train/episodes                 | 15160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0045    |
| train/info_shaping_reward_mean | -0.0913    |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 606400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 379         |
| stats_o/mean                   | 0.20393738  |
| stats_o/std                    | 0.088031605 |
| test/episodes                  | 3800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00245    |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.2633778  |
| test/Q_plus_P                  | -1.2633778  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 152000      |
| train/episodes                 | 15200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.538       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00495    |
| train/info_shaping_reward_mean | -0.0811     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.5       |
| train/steps                    | 608000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 380        |
| stats_o/mean                   | 0.20393854 |
| stats_o/std                    | 0.08795817 |
| test/episodes                  | 3810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00289   |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3434577 |
| test/Q_plus_P                  | -1.3434577 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 152400     |
| train/episodes                 | 15240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00512   |
| train/info_shaping_reward_mean | -0.0738    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 609600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 381        |
| stats_o/mean                   | 0.20394641 |
| stats_o/std                    | 0.08789669 |
| test/episodes                  | 3820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0034    |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.142234  |
| test/Q_plus_P                  | -1.142234  |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 152800     |
| train/episodes                 | 15280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00457   |
| train/info_shaping_reward_mean | -0.0836    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 611200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.2039542  |
| stats_o/std                    | 0.0878701  |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00211   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1805732 |
| test/Q_plus_P                  | -1.1805732 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0055    |
| train/info_shaping_reward_mean | -0.0847    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 383        |
| stats_o/mean                   | 0.20396046 |
| stats_o/std                    | 0.08783583 |
| test/episodes                  | 3840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.637      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00155   |
| test/info_shaping_reward_mean  | -0.0693    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -2.1274328 |
| test/Q_plus_P                  | -2.1274328 |
| test/reward_per_eps            | -14.5      |
| test/steps                     | 153600     |
| train/episodes                 | 15360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.46       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00597   |
| train/info_shaping_reward_mean | -0.0843    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.6      |
| train/steps                    | 614400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 384        |
| stats_o/mean                   | 0.20397541 |
| stats_o/std                    | 0.08783685 |
| test/episodes                  | 3850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00236   |
| test/info_shaping_reward_mean  | -0.0567    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.6576507 |
| test/Q_plus_P                  | -1.6576507 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 154000     |
| train/episodes                 | 15400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.364      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00778   |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.4      |
| train/steps                    | 616000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 385        |
| stats_o/mean                   | 0.2039664  |
| stats_o/std                    | 0.08778906 |
| test/episodes                  | 3860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00254   |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6414942 |
| test/Q_plus_P                  | -1.6414942 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 154400     |
| train/episodes                 | 15440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.458      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00527   |
| train/info_shaping_reward_mean | -0.0877    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.7      |
| train/steps                    | 617600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 386        |
| stats_o/mean                   | 0.2039651  |
| stats_o/std                    | 0.08774521 |
| test/episodes                  | 3870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0011    |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.6445987 |
| test/Q_plus_P                  | -1.6445987 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 154800     |
| train/episodes                 | 15480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.522      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00573   |
| train/info_shaping_reward_mean | -0.0828    |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 619200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 387        |
| stats_o/mean                   | 0.20396519 |
| stats_o/std                    | 0.08774655 |
| test/episodes                  | 3880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00322   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6046953 |
| test/Q_plus_P                  | -1.6046953 |
| test/reward_per_eps            | -11        |
| test/steps                     | 155200     |
| train/episodes                 | 15520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.502      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00441   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 620800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 388         |
| stats_o/mean                   | 0.20396206  |
| stats_o/std                    | 0.087699674 |
| test/episodes                  | 3890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00208    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -1.363752   |
| test/Q_plus_P                  | -1.363752   |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 155600      |
| train/episodes                 | 15560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.493       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00527    |
| train/info_shaping_reward_mean | -0.086      |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.3       |
| train/steps                    | 622400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 389        |
| stats_o/mean                   | 0.20396435 |
| stats_o/std                    | 0.08766025 |
| test/episodes                  | 3900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00185   |
| test/info_shaping_reward_mean  | -0.0507    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2690121 |
| test/Q_plus_P                  | -1.2690121 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 156000     |
| train/episodes                 | 15600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.494      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0842    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 624000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 390        |
| stats_o/mean                   | 0.20395394 |
| stats_o/std                    | 0.08763205 |
| test/episodes                  | 3910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00222   |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2542988 |
| test/Q_plus_P                  | -1.2542988 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 156400     |
| train/episodes                 | 15640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.47       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00549   |
| train/info_shaping_reward_mean | -0.0925    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 625600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.20394899  |
| stats_o/std                    | 0.087603055 |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.667       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00299    |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -2.3907087  |
| test/Q_plus_P                  | -2.3907087  |
| test/reward_per_eps            | -13.3       |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.563       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00475    |
| train/info_shaping_reward_mean | -0.0784     |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 392        |
| stats_o/mean                   | 0.2039536  |
| stats_o/std                    | 0.08756146 |
| test/episodes                  | 3930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00154   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4949819 |
| test/Q_plus_P                  | -1.4949819 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 157200     |
| train/episodes                 | 15720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.525      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00499   |
| train/info_shaping_reward_mean | -0.0818    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 628800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 393         |
| stats_o/mean                   | 0.20397368  |
| stats_o/std                    | 0.087628394 |
| test/episodes                  | 3940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00505    |
| test/info_shaping_reward_mean  | -0.0553     |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -1.7430398  |
| test/Q_plus_P                  | -1.7430398  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 157600      |
| train/episodes                 | 15760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.494       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00503    |
| train/info_shaping_reward_mean | -0.104      |
| train/info_shaping_reward_min  | -0.292      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.2       |
| train/steps                    | 630400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 394        |
| stats_o/mean                   | 0.20398444 |
| stats_o/std                    | 0.08758478 |
| test/episodes                  | 3950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00547   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4943569 |
| test/Q_plus_P                  | -1.4943569 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 158000     |
| train/episodes                 | 15800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.476      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0052    |
| train/info_shaping_reward_mean | -0.0905    |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 632000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 395        |
| stats_o/mean                   | 0.2039941  |
| stats_o/std                    | 0.08755523 |
| test/episodes                  | 3960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0024    |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.478746  |
| test/Q_plus_P                  | -1.478746  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 158400     |
| train/episodes                 | 15840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.499      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00544   |
| train/info_shaping_reward_mean | -0.0926    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 633600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 396         |
| stats_o/mean                   | 0.20399627  |
| stats_o/std                    | 0.087475926 |
| test/episodes                  | 3970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00292    |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.4365075  |
| test/Q_plus_P                  | -1.4365075  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 158800      |
| train/episodes                 | 15880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.513       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00593    |
| train/info_shaping_reward_mean | -0.0777     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.5       |
| train/steps                    | 635200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 397        |
| stats_o/mean                   | 0.20398322 |
| stats_o/std                    | 0.08743839 |
| test/episodes                  | 3980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00206   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3403366 |
| test/Q_plus_P                  | -1.3403366 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 159200     |
| train/episodes                 | 15920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.528      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00583   |
| train/info_shaping_reward_mean | -0.0857    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 636800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 398        |
| stats_o/mean                   | 0.20397949 |
| stats_o/std                    | 0.08742561 |
| test/episodes                  | 3990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00217   |
| test/info_shaping_reward_mean  | -0.0472    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3273065 |
| test/Q_plus_P                  | -1.3273065 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 159600     |
| train/episodes                 | 15960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.491      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00356   |
| train/info_shaping_reward_mean | -0.0873    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 638400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 399        |
| stats_o/mean                   | 0.20397781 |
| stats_o/std                    | 0.0873897  |
| test/episodes                  | 4000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00149   |
| test/info_shaping_reward_mean  | -0.0468    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.539608  |
| test/Q_plus_P                  | -1.539608  |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 160000     |
| train/episodes                 | 16000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0056    |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 640000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 400         |
| stats_o/mean                   | 0.20397948  |
| stats_o/std                    | 0.087342024 |
| test/episodes                  | 4010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000979   |
| test/info_shaping_reward_mean  | -0.0472     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2536097  |
| test/Q_plus_P                  | -1.2536097  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 160400      |
| train/episodes                 | 16040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.483       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00557    |
| train/info_shaping_reward_mean | -0.0822     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.7       |
| train/steps                    | 641600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 401        |
| stats_o/mean                   | 0.20397764 |
| stats_o/std                    | 0.08727672 |
| test/episodes                  | 4020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00177   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3380548 |
| test/Q_plus_P                  | -1.3380548 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 160800     |
| train/episodes                 | 16080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.543      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00428   |
| train/info_shaping_reward_mean | -0.0759    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 643200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.20398745 |
| stats_o/std                    | 0.08722002 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00233   |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.4865619 |
| test/Q_plus_P                  | -1.4865619 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.553      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00519   |
| train/info_shaping_reward_mean | -0.081     |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 403        |
| stats_o/mean                   | 0.20399897 |
| stats_o/std                    | 0.08716743 |
| test/episodes                  | 4040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00144   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -1.274949  |
| test/Q_plus_P                  | -1.274949  |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 161600     |
| train/episodes                 | 16160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.522      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00543   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 646400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 404        |
| stats_o/mean                   | 0.20400652 |
| stats_o/std                    | 0.08711675 |
| test/episodes                  | 4050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3074536 |
| test/Q_plus_P                  | -1.3074536 |
| test/reward_per_eps            | -10        |
| test/steps                     | 162000     |
| train/episodes                 | 16200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.516      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00535   |
| train/info_shaping_reward_mean | -0.082     |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 648000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.20399861  |
| stats_o/std                    | 0.087083675 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00219    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.5233539  |
| test/Q_plus_P                  | -1.5233539  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.5         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00574    |
| train/info_shaping_reward_mean | -0.092      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20         |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 406         |
| stats_o/mean                   | 0.20399712  |
| stats_o/std                    | 0.087044716 |
| test/episodes                  | 4070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.004      |
| test/info_shaping_reward_mean  | -0.0488     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2623836  |
| test/Q_plus_P                  | -1.2623836  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 162800      |
| train/episodes                 | 16280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.528       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00458    |
| train/info_shaping_reward_mean | -0.0749     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 651200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 407        |
| stats_o/mean                   | 0.2040003  |
| stats_o/std                    | 0.08700051 |
| test/episodes                  | 4080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00263   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.430455  |
| test/Q_plus_P                  | -1.430455  |
| test/reward_per_eps            | -10        |
| test/steps                     | 163200     |
| train/episodes                 | 16320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.475      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0061    |
| train/info_shaping_reward_mean | -0.0849    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21        |
| train/steps                    | 652800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 408         |
| stats_o/mean                   | 0.20401272  |
| stats_o/std                    | 0.086960055 |
| test/episodes                  | 4090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00282    |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.4494902  |
| test/Q_plus_P                  | -1.4494902  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 163600      |
| train/episodes                 | 16360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.527       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00721    |
| train/info_shaping_reward_mean | -0.0839     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 654400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 409        |
| stats_o/mean                   | 0.2040198  |
| stats_o/std                    | 0.08693326 |
| test/episodes                  | 4100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00341   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4163686 |
| test/Q_plus_P                  | -1.4163686 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 164000     |
| train/episodes                 | 16400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.515      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00714   |
| train/info_shaping_reward_mean | -0.0885    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 656000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 410        |
| stats_o/mean                   | 0.20401976 |
| stats_o/std                    | 0.086931   |
| test/episodes                  | 4110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00169   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3269546 |
| test/Q_plus_P                  | -1.3269546 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 164400     |
| train/episodes                 | 16440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00483   |
| train/info_shaping_reward_mean | -0.0813    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 657600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 411        |
| stats_o/mean                   | 0.2040184  |
| stats_o/std                    | 0.08687846 |
| test/episodes                  | 4120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00402   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2359824 |
| test/Q_plus_P                  | -1.2359824 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 164800     |
| train/episodes                 | 16480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00494   |
| train/info_shaping_reward_mean | -0.0761    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 659200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 412        |
| stats_o/mean                   | 0.20401289 |
| stats_o/std                    | 0.08682844 |
| test/episodes                  | 4130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00277   |
| test/info_shaping_reward_mean  | -0.0864    |
| test/info_shaping_reward_min   | -0.552     |
| test/Q                         | -4.183313  |
| test/Q_plus_P                  | -4.183313  |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 165200     |
| train/episodes                 | 16520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00525   |
| train/info_shaping_reward_mean | -0.0774    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 660800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 413        |
| stats_o/mean                   | 0.20401925 |
| stats_o/std                    | 0.08675842 |
| test/episodes                  | 4140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00156   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3158504 |
| test/Q_plus_P                  | -1.3158504 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 165600     |
| train/episodes                 | 16560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.535      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00483   |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 662400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 414        |
| stats_o/mean                   | 0.20402676 |
| stats_o/std                    | 0.08675599 |
| test/episodes                  | 4150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00341   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0850022 |
| test/Q_plus_P                  | -1.0850022 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 166000     |
| train/episodes                 | 16600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.504      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00543   |
| train/info_shaping_reward_mean | -0.0948    |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 664000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 415        |
| stats_o/mean                   | 0.2040299  |
| stats_o/std                    | 0.08670454 |
| test/episodes                  | 4160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00292   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.236621  |
| test/Q_plus_P                  | -1.236621  |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 166400     |
| train/episodes                 | 16640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.524      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0054    |
| train/info_shaping_reward_mean | -0.0816    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 665600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 416        |
| stats_o/mean                   | 0.20403343 |
| stats_o/std                    | 0.08664975 |
| test/episodes                  | 4170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00342   |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.18584   |
| test/Q_plus_P                  | -1.18584   |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 166800     |
| train/episodes                 | 16680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00549   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 667200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 417        |
| stats_o/mean                   | 0.204044   |
| stats_o/std                    | 0.08660176 |
| test/episodes                  | 4180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000548  |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0960125 |
| test/Q_plus_P                  | -1.0960125 |
| test/reward_per_eps            | -9         |
| test/steps                     | 167200     |
| train/episodes                 | 16720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00511   |
| train/info_shaping_reward_mean | -0.0749    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 668800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 418        |
| stats_o/mean                   | 0.20403787 |
| stats_o/std                    | 0.08655126 |
| test/episodes                  | 4190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00162   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.047807  |
| test/Q_plus_P                  | -1.047807  |
| test/reward_per_eps            | -9         |
| test/steps                     | 167600     |
| train/episodes                 | 16760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0044    |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 670400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 419        |
| stats_o/mean                   | 0.2040398  |
| stats_o/std                    | 0.08649175 |
| test/episodes                  | 4200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00116   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1794059 |
| test/Q_plus_P                  | -1.1794059 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 168000     |
| train/episodes                 | 16800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0046    |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 672000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 420        |
| stats_o/mean                   | 0.20403728 |
| stats_o/std                    | 0.08643525 |
| test/episodes                  | 4210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1040367 |
| test/Q_plus_P                  | -1.1040367 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 168400     |
| train/episodes                 | 16840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00486   |
| train/info_shaping_reward_mean | -0.0741    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 673600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 421         |
| stats_o/mean                   | 0.20403615  |
| stats_o/std                    | 0.086362354 |
| test/episodes                  | 4220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000793   |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2882712  |
| test/Q_plus_P                  | -1.2882712  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 168800      |
| train/episodes                 | 16880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00446    |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 675200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 422        |
| stats_o/mean                   | 0.2040449  |
| stats_o/std                    | 0.0863359  |
| test/episodes                  | 4230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.67       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00269   |
| test/info_shaping_reward_mean  | -0.0634    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -3.4452085 |
| test/Q_plus_P                  | -3.4452085 |
| test/reward_per_eps            | -13.2      |
| test/steps                     | 169200     |
| train/episodes                 | 16920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.456      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00541   |
| train/info_shaping_reward_mean | -0.086     |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 676800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 423        |
| stats_o/mean                   | 0.20404321 |
| stats_o/std                    | 0.08633453 |
| test/episodes                  | 4240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000492  |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3280607 |
| test/Q_plus_P                  | -1.3280607 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 169600     |
| train/episodes                 | 16960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.497      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00542   |
| train/info_shaping_reward_mean | -0.0919    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 678400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 424        |
| stats_o/mean                   | 0.20404954 |
| stats_o/std                    | 0.08630225 |
| test/episodes                  | 4250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00226   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1849535 |
| test/Q_plus_P                  | -1.1849535 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 170000     |
| train/episodes                 | 17000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.501      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00496   |
| train/info_shaping_reward_mean | -0.0869    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20        |
| train/steps                    | 680000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 425        |
| stats_o/mean                   | 0.20403883 |
| stats_o/std                    | 0.0862677  |
| test/episodes                  | 4260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00226   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.4008806 |
| test/Q_plus_P                  | -1.4008806 |
| test/reward_per_eps            | -10        |
| test/steps                     | 170400     |
| train/episodes                 | 17040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.553      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00584   |
| train/info_shaping_reward_mean | -0.0746    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 681600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.204032    |
| stats_o/std                    | 0.086210296 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00242    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0603087  |
| test/Q_plus_P                  | -1.0603087  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.549       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00654    |
| train/info_shaping_reward_mean | -0.076      |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 427        |
| stats_o/mean                   | 0.20403105 |
| stats_o/std                    | 0.08614731 |
| test/episodes                  | 4280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00275   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -1.2628843 |
| test/Q_plus_P                  | -1.2628843 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 171200     |
| train/episodes                 | 17120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00631   |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 684800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.2040352   |
| stats_o/std                    | 0.086091384 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0475     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.3182083  |
| test/Q_plus_P                  | -1.3182083  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.575       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00558    |
| train/info_shaping_reward_mean | -0.0717     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 429        |
| stats_o/mean                   | 0.20404114 |
| stats_o/std                    | 0.08605644 |
| test/episodes                  | 4300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00358   |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3759129 |
| test/Q_plus_P                  | -1.3759129 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 172000     |
| train/episodes                 | 17200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00554   |
| train/info_shaping_reward_mean | -0.0793    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 688000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 430        |
| stats_o/mean                   | 0.20404695 |
| stats_o/std                    | 0.08598798 |
| test/episodes                  | 4310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00126   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2967786 |
| test/Q_plus_P                  | -1.2967786 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 172400     |
| train/episodes                 | 17240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0045    |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 689600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 431        |
| stats_o/mean                   | 0.20404883 |
| stats_o/std                    | 0.08593664 |
| test/episodes                  | 4320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000817  |
| test/info_shaping_reward_mean  | -0.046     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0385079 |
| test/Q_plus_P                  | -1.0385079 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 172800     |
| train/episodes                 | 17280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00417   |
| train/info_shaping_reward_mean | -0.0734    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 691200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 432        |
| stats_o/mean                   | 0.20404463 |
| stats_o/std                    | 0.08591121 |
| test/episodes                  | 4330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00143   |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.3394526 |
| test/Q_plus_P                  | -1.3394526 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 173200     |
| train/episodes                 | 17320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.504      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00515   |
| train/info_shaping_reward_mean | -0.0887    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 692800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 433         |
| stats_o/mean                   | 0.20404576  |
| stats_o/std                    | 0.085848905 |
| test/episodes                  | 4340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.1609883  |
| test/Q_plus_P                  | -1.1609883  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 173600      |
| train/episodes                 | 17360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.576       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0715     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 694400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 434         |
| stats_o/mean                   | 0.20403574  |
| stats_o/std                    | 0.085827105 |
| test/episodes                  | 4350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00262    |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1886483  |
| test/Q_plus_P                  | -1.1886483  |
| test/reward_per_eps            | -9          |
| test/steps                     | 174000      |
| train/episodes                 | 17400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.571       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00509    |
| train/info_shaping_reward_mean | -0.0743     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 696000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 435        |
| stats_o/mean                   | 0.20404153 |
| stats_o/std                    | 0.08578753 |
| test/episodes                  | 4360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00094   |
| test/info_shaping_reward_mean  | -0.0467    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2283179 |
| test/Q_plus_P                  | -1.2283179 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 174400     |
| train/episodes                 | 17440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.56       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00542   |
| train/info_shaping_reward_mean | -0.0812    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 697600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 436        |
| stats_o/mean                   | 0.20404844 |
| stats_o/std                    | 0.0857193  |
| test/episodes                  | 4370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000837  |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1014138 |
| test/Q_plus_P                  | -1.1014138 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 174800     |
| train/episodes                 | 17480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00466   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 699200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 437         |
| stats_o/mean                   | 0.20405644  |
| stats_o/std                    | 0.085681364 |
| test/episodes                  | 4380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00191    |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.190804   |
| test/Q_plus_P                  | -1.190804   |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 175200      |
| train/episodes                 | 17520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.543       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00486    |
| train/info_shaping_reward_mean | -0.0757     |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.3       |
| train/steps                    | 700800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 438        |
| stats_o/mean                   | 0.20405196 |
| stats_o/std                    | 0.0856454  |
| test/episodes                  | 4390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00178   |
| test/info_shaping_reward_mean  | -0.0458    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2242806 |
| test/Q_plus_P                  | -1.2242806 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 175600     |
| train/episodes                 | 17560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.534      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00427   |
| train/info_shaping_reward_mean | -0.0788    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 702400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 439        |
| stats_o/mean                   | 0.20404746 |
| stats_o/std                    | 0.08560753 |
| test/episodes                  | 4400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0018    |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.1278454 |
| test/Q_plus_P                  | -1.1278454 |
| test/reward_per_eps            | -9         |
| test/steps                     | 176000     |
| train/episodes                 | 17600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00492   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 704000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.2040404   |
| stats_o/std                    | 0.085586205 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00211    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.2311717  |
| test/Q_plus_P                  | -1.2311717  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.506       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00576    |
| train/info_shaping_reward_mean | -0.0828     |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.8       |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 441        |
| stats_o/mean                   | 0.20403591 |
| stats_o/std                    | 0.08553124 |
| test/episodes                  | 4420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00248   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0897343 |
| test/Q_plus_P                  | -1.0897343 |
| test/reward_per_eps            | -9         |
| test/steps                     | 176800     |
| train/episodes                 | 17680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00441   |
| train/info_shaping_reward_mean | -0.0708    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 707200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 442        |
| stats_o/mean                   | 0.20404074 |
| stats_o/std                    | 0.08548941 |
| test/episodes                  | 4430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00217   |
| test/info_shaping_reward_mean  | -0.0463    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0708487 |
| test/Q_plus_P                  | -1.0708487 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 177200     |
| train/episodes                 | 17720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00297   |
| train/info_shaping_reward_mean | -0.0824    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 708800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.20403576  |
| stats_o/std                    | 0.085451536 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00165    |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.357156   |
| test/Q_plus_P                  | -1.357156   |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00443    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 444        |
| stats_o/mean                   | 0.20403278 |
| stats_o/std                    | 0.08541803 |
| test/episodes                  | 4450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.417      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00612   |
| test/info_shaping_reward_mean  | -0.0916    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -3.1676753 |
| test/Q_plus_P                  | -3.1676753 |
| test/reward_per_eps            | -23.3      |
| test/steps                     | 178000     |
| train/episodes                 | 17800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.487      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00544   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.5      |
| train/steps                    | 712000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.20402664  |
| stats_o/std                    | 0.085371315 |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000249   |
| test/info_shaping_reward_mean  | -0.0496     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2323638  |
| test/Q_plus_P                  | -1.2323638  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.528       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.005      |
| train/info_shaping_reward_mean | -0.0828     |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 446        |
| stats_o/mean                   | 0.20402688 |
| stats_o/std                    | 0.0853164  |
| test/episodes                  | 4470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00237   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.345201  |
| test/Q_plus_P                  | -1.345201  |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 178800     |
| train/episodes                 | 17880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00401   |
| train/info_shaping_reward_mean | -0.0781    |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 715200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 447        |
| stats_o/mean                   | 0.20401841 |
| stats_o/std                    | 0.08527031 |
| test/episodes                  | 4480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00131   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.2440265 |
| test/Q_plus_P                  | -1.2440265 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 179200     |
| train/episodes                 | 17920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00551   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 716800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 448        |
| stats_o/mean                   | 0.20402628 |
| stats_o/std                    | 0.08523063 |
| test/episodes                  | 4490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00118   |
| test/info_shaping_reward_mean  | -0.0458    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.1747773 |
| test/Q_plus_P                  | -1.1747773 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 179600     |
| train/episodes                 | 17960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.527      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00532   |
| train/info_shaping_reward_mean | -0.0799    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 718400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 449        |
| stats_o/mean                   | 0.2040354  |
| stats_o/std                    | 0.08521142 |
| test/episodes                  | 4500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00458   |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2780962 |
| test/Q_plus_P                  | -1.2780962 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 180000     |
| train/episodes                 | 18000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.478      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00454   |
| train/info_shaping_reward_mean | -0.0883    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 720000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.20403269 |
| stats_o/std                    | 0.08515971 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00257   |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.3958168 |
| test/Q_plus_P                  | -1.3958168 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00402   |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 451        |
| stats_o/mean                   | 0.2040332  |
| stats_o/std                    | 0.08514136 |
| test/episodes                  | 4520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.3326716 |
| test/Q_plus_P                  | -1.3326716 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 180800     |
| train/episodes                 | 18080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.55       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0045    |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 723200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 452        |
| stats_o/mean                   | 0.20402957 |
| stats_o/std                    | 0.08510027 |
| test/episodes                  | 4530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0014    |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1732403 |
| test/Q_plus_P                  | -1.1732403 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 181200     |
| train/episodes                 | 18120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00464   |
| train/info_shaping_reward_mean | -0.0756    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 724800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 453        |
| stats_o/mean                   | 0.20403142 |
| stats_o/std                    | 0.08505741 |
| test/episodes                  | 4540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0034    |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3098334 |
| test/Q_plus_P                  | -1.3098334 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 181600     |
| train/episodes                 | 18160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.524      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00637   |
| train/info_shaping_reward_mean | -0.083     |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 726400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 454        |
| stats_o/mean                   | 0.20402078 |
| stats_o/std                    | 0.08501805 |
| test/episodes                  | 4550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00156   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.285964  |
| test/Q_plus_P                  | -1.285964  |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 182000     |
| train/episodes                 | 18200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0778    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 728000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 455        |
| stats_o/mean                   | 0.20403194 |
| stats_o/std                    | 0.08503789 |
| test/episodes                  | 4560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000646  |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2782884 |
| test/Q_plus_P                  | -1.2782884 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 182400     |
| train/episodes                 | 18240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0052    |
| train/info_shaping_reward_mean | -0.096     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 729600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 456        |
| stats_o/mean                   | 0.20403223 |
| stats_o/std                    | 0.08498949 |
| test/episodes                  | 4570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00322   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2566577 |
| test/Q_plus_P                  | -1.2566577 |
| test/reward_per_eps            | -10        |
| test/steps                     | 182800     |
| train/episodes                 | 18280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.005     |
| train/info_shaping_reward_mean | -0.0844    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 731200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 457        |
| stats_o/mean                   | 0.20401837 |
| stats_o/std                    | 0.08496632 |
| test/episodes                  | 4580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00342   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -1.1855148 |
| test/Q_plus_P                  | -1.1855148 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 183200     |
| train/episodes                 | 18320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00509   |
| train/info_shaping_reward_mean | -0.0789    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 732800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 458        |
| stats_o/mean                   | 0.20401606 |
| stats_o/std                    | 0.08491909 |
| test/episodes                  | 4590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.046     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1026745 |
| test/Q_plus_P                  | -1.1026745 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 183600     |
| train/episodes                 | 18360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.489      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.0857    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 734400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 459        |
| stats_o/mean                   | 0.20401715 |
| stats_o/std                    | 0.08488024 |
| test/episodes                  | 4600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00225   |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1262192 |
| test/Q_plus_P                  | -1.1262192 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 184000     |
| train/episodes                 | 18400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00508   |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 736000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 460        |
| stats_o/mean                   | 0.20401187 |
| stats_o/std                    | 0.08484973 |
| test/episodes                  | 4610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -1.1456563 |
| test/Q_plus_P                  | -1.1456563 |
| test/reward_per_eps            | -9         |
| test/steps                     | 184400     |
| train/episodes                 | 18440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.501      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00652   |
| train/info_shaping_reward_mean | -0.0854    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 737600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 461        |
| stats_o/mean                   | 0.20401478 |
| stats_o/std                    | 0.08480331 |
| test/episodes                  | 4620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00143   |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0006596 |
| test/Q_plus_P                  | -1.0006596 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 184800     |
| train/episodes                 | 18480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00489   |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 739200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 462        |
| stats_o/mean                   | 0.20401269 |
| stats_o/std                    | 0.08481279 |
| test/episodes                  | 4630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00187   |
| test/info_shaping_reward_mean  | -0.0454    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1097745 |
| test/Q_plus_P                  | -1.1097745 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 185200     |
| train/episodes                 | 18520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.528      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00463   |
| train/info_shaping_reward_mean | -0.0827    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 740800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 463         |
| stats_o/mean                   | 0.20401177  |
| stats_o/std                    | 0.084761515 |
| test/episodes                  | 4640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00258    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.1523948  |
| test/Q_plus_P                  | -1.1523948  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 185600      |
| train/episodes                 | 18560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.557       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00521    |
| train/info_shaping_reward_mean | -0.0717     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 742400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 464         |
| stats_o/mean                   | 0.20402098  |
| stats_o/std                    | 0.084718116 |
| test/episodes                  | 4650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00171    |
| test/info_shaping_reward_mean  | -0.0472     |
| test/info_shaping_reward_min   | -0.194      |
| test/Q                         | -1.1721945  |
| test/Q_plus_P                  | -1.1721945  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 186000      |
| train/episodes                 | 18600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00512    |
| train/info_shaping_reward_mean | -0.0686     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 744000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 465        |
| stats_o/mean                   | 0.20400442 |
| stats_o/std                    | 0.0846872  |
| test/episodes                  | 4660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00178   |
| test/info_shaping_reward_mean  | -0.0429    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0489787 |
| test/Q_plus_P                  | -1.0489787 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 186400     |
| train/episodes                 | 18640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.527      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0787    |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 745600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.20400225  |
| stats_o/std                    | 0.084642194 |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00316    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.143662   |
| test/Q_plus_P                  | -1.143662   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.565       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00384    |
| train/info_shaping_reward_mean | -0.0723     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 467         |
| stats_o/mean                   | 0.20399317  |
| stats_o/std                    | 0.084600426 |
| test/episodes                  | 4680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00305    |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.204      |
| test/Q                         | -1.4354813  |
| test/Q_plus_P                  | -1.4354813  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 187200      |
| train/episodes                 | 18720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.517       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00508    |
| train/info_shaping_reward_mean | -0.0746     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.3       |
| train/steps                    | 748800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 468        |
| stats_o/mean                   | 0.2039839  |
| stats_o/std                    | 0.08455971 |
| test/episodes                  | 4690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00138   |
| test/info_shaping_reward_mean  | -0.056     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.64554   |
| test/Q_plus_P                  | -1.64554   |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 187600     |
| train/episodes                 | 18760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.54       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.005     |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 750400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 469        |
| stats_o/mean                   | 0.20398223 |
| stats_o/std                    | 0.08451768 |
| test/episodes                  | 4700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00183   |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1483638 |
| test/Q_plus_P                  | -1.1483638 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 188000     |
| train/episodes                 | 18800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.538      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00266   |
| train/info_shaping_reward_mean | -0.0813    |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 752000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 470         |
| stats_o/mean                   | 0.20399389  |
| stats_o/std                    | 0.084486835 |
| test/episodes                  | 4710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000574   |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.1190795  |
| test/Q_plus_P                  | -1.1190795  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 188400      |
| train/episodes                 | 18840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.575       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00383    |
| train/info_shaping_reward_mean | -0.08       |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 753600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.20400223 |
| stats_o/std                    | 0.08445272 |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2540871 |
| test/Q_plus_P                  | -1.2540871 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00509   |
| train/info_shaping_reward_mean | -0.0768    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.20400533 |
| stats_o/std                    | 0.08445122 |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0023    |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3585194 |
| test/Q_plus_P                  | -1.3585194 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.482      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00465   |
| train/info_shaping_reward_mean | -0.0858    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.7      |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 473        |
| stats_o/mean                   | 0.2040059  |
| stats_o/std                    | 0.08441616 |
| test/episodes                  | 4740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00121   |
| test/info_shaping_reward_mean  | -0.0446    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1971031 |
| test/Q_plus_P                  | -1.1971031 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 189600     |
| train/episodes                 | 18960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0049    |
| train/info_shaping_reward_mean | -0.0727    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 758400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 474        |
| stats_o/mean                   | 0.20400885 |
| stats_o/std                    | 0.0844265  |
| test/episodes                  | 4750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00219   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1829633 |
| test/Q_plus_P                  | -1.1829633 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 190000     |
| train/episodes                 | 19000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.507      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00475   |
| train/info_shaping_reward_mean | -0.0892    |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.7      |
| train/steps                    | 760000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 475         |
| stats_o/mean                   | 0.20401266  |
| stats_o/std                    | 0.084372565 |
| test/episodes                  | 4760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.0573144  |
| test/Q_plus_P                  | -1.0573144  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 190400      |
| train/episodes                 | 19040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.549       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00515    |
| train/info_shaping_reward_mean | -0.0748     |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 761600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 476        |
| stats_o/mean                   | 0.20401448 |
| stats_o/std                    | 0.08432563 |
| test/episodes                  | 4770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00413   |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.2903731 |
| test/Q_plus_P                  | -1.2903731 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 190800     |
| train/episodes                 | 19080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0047    |
| train/info_shaping_reward_mean | -0.0759    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 763200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 477        |
| stats_o/mean                   | 0.2040282  |
| stats_o/std                    | 0.08429065 |
| test/episodes                  | 4780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.415336  |
| test/Q_plus_P                  | -1.415336  |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 191200     |
| train/episodes                 | 19120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00537   |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 764800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 478        |
| stats_o/mean                   | 0.20403285 |
| stats_o/std                    | 0.08425181 |
| test/episodes                  | 4790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0026    |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3480132 |
| test/Q_plus_P                  | -1.3480132 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 191600     |
| train/episodes                 | 19160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.526      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 766400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.20402049  |
| stats_o/std                    | 0.084244415 |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2093052  |
| test/Q_plus_P                  | -1.2093052  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00419    |
| train/info_shaping_reward_mean | -0.0861     |
| train/info_shaping_reward_min  | -0.276      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 480        |
| stats_o/mean                   | 0.20402627 |
| stats_o/std                    | 0.08420046 |
| test/episodes                  | 4810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00049   |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.0938054 |
| test/Q_plus_P                  | -1.0938054 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 192400     |
| train/episodes                 | 19240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.572      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 769600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 481        |
| stats_o/mean                   | 0.20401764 |
| stats_o/std                    | 0.08419421 |
| test/episodes                  | 4820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000553  |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2016082 |
| test/Q_plus_P                  | -1.2016082 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 192800     |
| train/episodes                 | 19280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.535      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00497   |
| train/info_shaping_reward_mean | -0.0747    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 771200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 482        |
| stats_o/mean                   | 0.20401533 |
| stats_o/std                    | 0.08414293 |
| test/episodes                  | 4830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00256   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3517315 |
| test/Q_plus_P                  | -1.3517315 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 193200     |
| train/episodes                 | 19320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.524      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00457   |
| train/info_shaping_reward_mean | -0.0772    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 772800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 483        |
| stats_o/mean                   | 0.2040094  |
| stats_o/std                    | 0.08411587 |
| test/episodes                  | 4840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00142   |
| test/info_shaping_reward_mean  | -0.046     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.0827421 |
| test/Q_plus_P                  | -1.0827421 |
| test/reward_per_eps            | -9         |
| test/steps                     | 193600     |
| train/episodes                 | 19360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0048    |
| train/info_shaping_reward_mean | -0.0838    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 774400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.20400386  |
| stats_o/std                    | 0.084070176 |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.677       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.003      |
| test/info_shaping_reward_mean  | -0.0874     |
| test/info_shaping_reward_min   | -0.497      |
| test/Q                         | -4.1129217  |
| test/Q_plus_P                  | -4.1129217  |
| test/reward_per_eps            | -12.9       |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.559       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00439    |
| train/info_shaping_reward_mean | -0.0728     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 485        |
| stats_o/mean                   | 0.20399262 |
| stats_o/std                    | 0.08402385 |
| test/episodes                  | 4860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00462   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1379063 |
| test/Q_plus_P                  | -1.1379063 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 194400     |
| train/episodes                 | 19440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00567   |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 777600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 486        |
| stats_o/mean                   | 0.20400155 |
| stats_o/std                    | 0.08397879 |
| test/episodes                  | 4870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000712  |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.213     |
| test/Q                         | -1.2217288 |
| test/Q_plus_P                  | -1.2217288 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 194800     |
| train/episodes                 | 19480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00528   |
| train/info_shaping_reward_mean | -0.072     |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 779200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 487        |
| stats_o/mean                   | 0.20400429 |
| stats_o/std                    | 0.08393775 |
| test/episodes                  | 4880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00115   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.2842116 |
| test/Q_plus_P                  | -1.2842116 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 195200     |
| train/episodes                 | 19520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.565      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00487   |
| train/info_shaping_reward_mean | -0.0733    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 780800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 488        |
| stats_o/mean                   | 0.20400251 |
| stats_o/std                    | 0.08389336 |
| test/episodes                  | 4890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00222   |
| test/info_shaping_reward_mean  | -0.047     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1130083 |
| test/Q_plus_P                  | -1.1130083 |
| test/reward_per_eps            | -9         |
| test/steps                     | 195600     |
| train/episodes                 | 19560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00528   |
| train/info_shaping_reward_mean | -0.0757    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 782400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 489         |
| stats_o/mean                   | 0.20399636  |
| stats_o/std                    | 0.083851516 |
| test/episodes                  | 4900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -1.3056297  |
| test/Q_plus_P                  | -1.3056297  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 196000      |
| train/episodes                 | 19600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00425    |
| train/info_shaping_reward_mean | -0.0725     |
| train/info_shaping_reward_min  | -0.194      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 784000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 490        |
| stats_o/mean                   | 0.20399807 |
| stats_o/std                    | 0.08379367 |
| test/episodes                  | 4910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00152   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.0461745 |
| test/Q_plus_P                  | -1.0461745 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 196400     |
| train/episodes                 | 19640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0046    |
| train/info_shaping_reward_mean | -0.0732    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 785600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 491        |
| stats_o/mean                   | 0.20399256 |
| stats_o/std                    | 0.08377511 |
| test/episodes                  | 4920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00135   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3015362 |
| test/Q_plus_P                  | -1.3015362 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 196800     |
| train/episodes                 | 19680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00591   |
| train/info_shaping_reward_mean | -0.0785    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 787200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 492        |
| stats_o/mean                   | 0.20399384 |
| stats_o/std                    | 0.083735   |
| test/episodes                  | 4930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00177   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1565441 |
| test/Q_plus_P                  | -1.1565441 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 197200     |
| train/episodes                 | 19720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00442   |
| train/info_shaping_reward_mean | -0.0762    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 788800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.20399085 |
| stats_o/std                    | 0.08372348 |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00156   |
| test/info_shaping_reward_mean  | -0.0679    |
| test/info_shaping_reward_min   | -0.211     |
| test/Q                         | -2.4738653 |
| test/Q_plus_P                  | -2.4738653 |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0057    |
| train/info_shaping_reward_mean | -0.0801    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.20398241  |
| stats_o/std                    | 0.083693676 |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00219    |
| test/info_shaping_reward_mean  | -0.0472     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.0887033  |
| test/Q_plus_P                  | -1.0887033  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.533       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00533    |
| train/info_shaping_reward_mean | -0.0731     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 495        |
| stats_o/mean                   | 0.20398374 |
| stats_o/std                    | 0.08363227 |
| test/episodes                  | 4960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00277   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.4003744 |
| test/Q_plus_P                  | -1.4003744 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 198400     |
| train/episodes                 | 19840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0039    |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 793600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 496        |
| stats_o/mean                   | 0.20398922 |
| stats_o/std                    | 0.08359781 |
| test/episodes                  | 4970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00656   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1854228 |
| test/Q_plus_P                  | -1.1854228 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 198800     |
| train/episodes                 | 19880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00492   |
| train/info_shaping_reward_mean | -0.0802    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 795200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 497        |
| stats_o/mean                   | 0.20399255 |
| stats_o/std                    | 0.08356325 |
| test/episodes                  | 4980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000846  |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0686922 |
| test/Q_plus_P                  | -1.0686922 |
| test/reward_per_eps            | -9         |
| test/steps                     | 199200     |
| train/episodes                 | 19920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00484   |
| train/info_shaping_reward_mean | -0.0753    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 796800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 498        |
| stats_o/mean                   | 0.2039995  |
| stats_o/std                    | 0.08350814 |
| test/episodes                  | 4990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00252   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.1616272 |
| test/Q_plus_P                  | -1.1616272 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 199600     |
| train/episodes                 | 19960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00453   |
| train/info_shaping_reward_mean | -0.0708    |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 798400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.20400055  |
| stats_o/std                    | 0.083485276 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00179    |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.207      |
| test/Q                         | -1.2131484  |
| test/Q_plus_P                  | -1.2131484  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.571       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00521    |
| train/info_shaping_reward_mean | -0.0728     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 500        |
| stats_o/mean                   | 0.20400308 |
| stats_o/std                    | 0.08344472 |
| test/episodes                  | 5010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00286   |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.2472646 |
| test/Q_plus_P                  | -1.2472646 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 200400     |
| train/episodes                 | 20040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.543      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00516   |
| train/info_shaping_reward_mean | -0.0752    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 801600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 501        |
| stats_o/mean                   | 0.20401041 |
| stats_o/std                    | 0.083409   |
| test/episodes                  | 5020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00185   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0250987 |
| test/Q_plus_P                  | -1.0250987 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 200800     |
| train/episodes                 | 20080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.54       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00537   |
| train/info_shaping_reward_mean | -0.0834    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 803200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 502        |
| stats_o/mean                   | 0.20400906 |
| stats_o/std                    | 0.0833872  |
| test/episodes                  | 5030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00735   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.351532  |
| test/Q_plus_P                  | -1.351532  |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 201200     |
| train/episodes                 | 20120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0056    |
| train/info_shaping_reward_mean | -0.0784    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 804800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 503        |
| stats_o/mean                   | 0.20400688 |
| stats_o/std                    | 0.08336384 |
| test/episodes                  | 5040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00908   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -1.0183427 |
| test/Q_plus_P                  | -1.0183427 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 201600     |
| train/episodes                 | 20160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00559   |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 806400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 504        |
| stats_o/mean                   | 0.20401132 |
| stats_o/std                    | 0.08333609 |
| test/episodes                  | 5050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00337   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2094802 |
| test/Q_plus_P                  | -1.2094802 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 202000     |
| train/episodes                 | 20200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.575      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00366   |
| train/info_shaping_reward_mean | -0.0771    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 808000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 505        |
| stats_o/mean                   | 0.20401174 |
| stats_o/std                    | 0.08330185 |
| test/episodes                  | 5060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0989    |
| test/info_shaping_reward_min   | -0.678     |
| test/Q                         | -4.3554206 |
| test/Q_plus_P                  | -4.3554206 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 202400     |
| train/episodes                 | 20240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00553   |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 809600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 506        |
| stats_o/mean                   | 0.20401008 |
| stats_o/std                    | 0.0832967  |
| test/episodes                  | 5070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0017    |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1632438 |
| test/Q_plus_P                  | -1.1632438 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 202800     |
| train/episodes                 | 20280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.518      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00535   |
| train/info_shaping_reward_mean | -0.0884    |
| train/info_shaping_reward_min  | -0.277     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 811200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 507         |
| stats_o/mean                   | 0.20401111  |
| stats_o/std                    | 0.083292186 |
| test/episodes                  | 5080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00384    |
| test/info_shaping_reward_mean  | -0.0561     |
| test/info_shaping_reward_min   | -0.204      |
| test/Q                         | -1.5393602  |
| test/Q_plus_P                  | -1.5393602  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 203200      |
| train/episodes                 | 20320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.502       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00501    |
| train/info_shaping_reward_mean | -0.0941     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.9       |
| train/steps                    | 812800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 508        |
| stats_o/mean                   | 0.2040109  |
| stats_o/std                    | 0.08325794 |
| test/episodes                  | 5090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00365   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3996967 |
| test/Q_plus_P                  | -1.3996967 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 203600     |
| train/episodes                 | 20360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0049    |
| train/info_shaping_reward_mean | -0.0802    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 814400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 509        |
| stats_o/mean                   | 0.20399584 |
| stats_o/std                    | 0.08324777 |
| test/episodes                  | 5100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -1.3094585 |
| test/Q_plus_P                  | -1.3094585 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 204000     |
| train/episodes                 | 20400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.514      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00544   |
| train/info_shaping_reward_mean | -0.0855    |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 816000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 510         |
| stats_o/mean                   | 0.20399925  |
| stats_o/std                    | 0.083218984 |
| test/episodes                  | 5110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00356    |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.201      |
| test/Q                         | -1.3027157  |
| test/Q_plus_P                  | -1.3027157  |
| test/reward_per_eps            | -10         |
| test/steps                     | 204400      |
| train/episodes                 | 20440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.501       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00511    |
| train/info_shaping_reward_mean | -0.0819     |
| train/info_shaping_reward_min  | -0.209      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.9       |
| train/steps                    | 817600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 511        |
| stats_o/mean                   | 0.20399658 |
| stats_o/std                    | 0.08319371 |
| test/episodes                  | 5120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00105   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6192443 |
| test/Q_plus_P                  | -1.6192443 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 204800     |
| train/episodes                 | 20480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0052    |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 819200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 512        |
| stats_o/mean                   | 0.2039998  |
| stats_o/std                    | 0.08317794 |
| test/episodes                  | 5130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00333   |
| test/info_shaping_reward_mean  | -0.0563    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.5863609 |
| test/Q_plus_P                  | -1.5863609 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 205200     |
| train/episodes                 | 20520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00513   |
| train/info_shaping_reward_mean | -0.0835    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 820800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 513        |
| stats_o/mean                   | 0.20401196 |
| stats_o/std                    | 0.08319142 |
| test/episodes                  | 5140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00434   |
| test/info_shaping_reward_mean  | -0.0865    |
| test/info_shaping_reward_min   | -0.523     |
| test/Q                         | -3.5916212 |
| test/Q_plus_P                  | -3.5916212 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 205600     |
| train/episodes                 | 20560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.525      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00597   |
| train/info_shaping_reward_mean | -0.0881    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 822400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 514        |
| stats_o/mean                   | 0.2040146  |
| stats_o/std                    | 0.0832296  |
| test/episodes                  | 5150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00193   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3472209 |
| test/Q_plus_P                  | -1.3472209 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 206000     |
| train/episodes                 | 20600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.534      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00436   |
| train/info_shaping_reward_mean | -0.0999    |
| train/info_shaping_reward_min  | -0.31      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 824000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 515        |
| stats_o/mean                   | 0.20400923 |
| stats_o/std                    | 0.0832278  |
| test/episodes                  | 5160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00246   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.3917215 |
| test/Q_plus_P                  | -1.3917215 |
| test/reward_per_eps            | -10        |
| test/steps                     | 206400     |
| train/episodes                 | 20640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.513      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00512   |
| train/info_shaping_reward_mean | -0.082     |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.5      |
| train/steps                    | 825600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 516        |
| stats_o/mean                   | 0.20400906 |
| stats_o/std                    | 0.08318928 |
| test/episodes                  | 5170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00139   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.2268296 |
| test/Q_plus_P                  | -1.2268296 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 206800     |
| train/episodes                 | 20680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.577      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00468   |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 827200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 517        |
| stats_o/mean                   | 0.20400853 |
| stats_o/std                    | 0.0831497  |
| test/episodes                  | 5180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -1.0196254 |
| test/Q_plus_P                  | -1.0196254 |
| test/reward_per_eps            | -9         |
| test/steps                     | 207200     |
| train/episodes                 | 20720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00447   |
| train/info_shaping_reward_mean | -0.0746    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 828800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 518        |
| stats_o/mean                   | 0.20399462 |
| stats_o/std                    | 0.08314071 |
| test/episodes                  | 5190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00296   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1864536 |
| test/Q_plus_P                  | -1.1864536 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 207600     |
| train/episodes                 | 20760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.528      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00515   |
| train/info_shaping_reward_mean | -0.0769    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 830400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 519        |
| stats_o/mean                   | 0.20399    |
| stats_o/std                    | 0.08310663 |
| test/episodes                  | 5200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00395   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2395438 |
| test/Q_plus_P                  | -1.2395438 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 208000     |
| train/episodes                 | 20800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.548      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0041    |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 832000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.20398721  |
| stats_o/std                    | 0.083087005 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00294    |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2344167  |
| test/Q_plus_P                  | -1.2344167  |
| test/reward_per_eps            | -9          |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00406    |
| train/info_shaping_reward_mean | -0.0773     |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 521        |
| stats_o/mean                   | 0.20397672 |
| stats_o/std                    | 0.0830669  |
| test/episodes                  | 5220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0022    |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3252331 |
| test/Q_plus_P                  | -1.3252331 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 208800     |
| train/episodes                 | 20880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00456   |
| train/info_shaping_reward_mean | -0.0815    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 835200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 522        |
| stats_o/mean                   | 0.20397541 |
| stats_o/std                    | 0.08304429 |
| test/episodes                  | 5230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00218   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2665104 |
| test/Q_plus_P                  | -1.2665104 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 209200     |
| train/episodes                 | 20920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.553      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00581   |
| train/info_shaping_reward_mean | -0.0812    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 836800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 523        |
| stats_o/mean                   | 0.20398031 |
| stats_o/std                    | 0.08303079 |
| test/episodes                  | 5240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00162   |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0799377 |
| test/Q_plus_P                  | -1.0799377 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 209600     |
| train/episodes                 | 20960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.582      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0041    |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 838400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 524        |
| stats_o/mean                   | 0.20397882 |
| stats_o/std                    | 0.08304051 |
| test/episodes                  | 5250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00183   |
| test/info_shaping_reward_mean  | -0.05      |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.3236972 |
| test/Q_plus_P                  | -1.3236972 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 210000     |
| train/episodes                 | 21000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.517      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00539   |
| train/info_shaping_reward_mean | -0.0919    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 840000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 525        |
| stats_o/mean                   | 0.20398282 |
| stats_o/std                    | 0.08301514 |
| test/episodes                  | 5260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00229   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1484374 |
| test/Q_plus_P                  | -1.1484374 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 210400     |
| train/episodes                 | 21040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00473   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 841600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 526        |
| stats_o/mean                   | 0.20397294 |
| stats_o/std                    | 0.08300658 |
| test/episodes                  | 5270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00244   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2257922 |
| test/Q_plus_P                  | -1.2257922 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 210800     |
| train/episodes                 | 21080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00494   |
| train/info_shaping_reward_mean | -0.0804    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 843200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 527         |
| stats_o/mean                   | 0.20397964  |
| stats_o/std                    | 0.083009556 |
| test/episodes                  | 5280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000857   |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.2260147  |
| test/Q_plus_P                  | -1.2260147  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 211200      |
| train/episodes                 | 21120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.532       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00502    |
| train/info_shaping_reward_mean | -0.0921     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 844800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 528        |
| stats_o/mean                   | 0.20398107 |
| stats_o/std                    | 0.08298081 |
| test/episodes                  | 5290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00261   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3525678 |
| test/Q_plus_P                  | -1.3525678 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 211600     |
| train/episodes                 | 21160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00388   |
| train/info_shaping_reward_mean | -0.0782    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 846400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 529        |
| stats_o/mean                   | 0.20397657 |
| stats_o/std                    | 0.08294382 |
| test/episodes                  | 5300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00051   |
| test/info_shaping_reward_mean  | -0.0458    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1169178 |
| test/Q_plus_P                  | -1.1169178 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 212000     |
| train/episodes                 | 21200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00333   |
| train/info_shaping_reward_mean | -0.0755    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 848000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 530        |
| stats_o/mean                   | 0.20397902 |
| stats_o/std                    | 0.0829304  |
| test/episodes                  | 5310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.3250159 |
| test/Q_plus_P                  | -1.3250159 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 212400     |
| train/episodes                 | 21240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.534      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00394   |
| train/info_shaping_reward_mean | -0.0849    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 849600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 531        |
| stats_o/mean                   | 0.20397255 |
| stats_o/std                    | 0.0828912  |
| test/episodes                  | 5320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000818  |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3187453 |
| test/Q_plus_P                  | -1.3187453 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 212800     |
| train/episodes                 | 21280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00378   |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 851200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 532        |
| stats_o/mean                   | 0.20397906 |
| stats_o/std                    | 0.08287294 |
| test/episodes                  | 5330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00194   |
| test/info_shaping_reward_mean  | -0.0454    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2318501 |
| test/Q_plus_P                  | -1.2318501 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 213200     |
| train/episodes                 | 21320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.507      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00375   |
| train/info_shaping_reward_mean | -0.0926    |
| train/info_shaping_reward_min  | -0.274     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.7      |
| train/steps                    | 852800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 533        |
| stats_o/mean                   | 0.20397606 |
| stats_o/std                    | 0.08283951 |
| test/episodes                  | 5340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00559   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2636049 |
| test/Q_plus_P                  | -1.2636049 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 213600     |
| train/episodes                 | 21360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00588   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 854400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 534        |
| stats_o/mean                   | 0.20397784 |
| stats_o/std                    | 0.08279364 |
| test/episodes                  | 5350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0016    |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3272709 |
| test/Q_plus_P                  | -1.3272709 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 214000     |
| train/episodes                 | 21400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00478   |
| train/info_shaping_reward_mean | -0.0715    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 856000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 535        |
| stats_o/mean                   | 0.20397882 |
| stats_o/std                    | 0.08276198 |
| test/episodes                  | 5360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00197   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2491043 |
| test/Q_plus_P                  | -1.2491043 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 214400     |
| train/episodes                 | 21440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00444   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 857600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 536        |
| stats_o/mean                   | 0.20398486 |
| stats_o/std                    | 0.08272427 |
| test/episodes                  | 5370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00209   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1949587 |
| test/Q_plus_P                  | -1.1949587 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 214800     |
| train/episodes                 | 21480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00554   |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 859200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 537         |
| stats_o/mean                   | 0.20399003  |
| stats_o/std                    | 0.082685806 |
| test/episodes                  | 5380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.2488958  |
| test/Q_plus_P                  | -1.2488958  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 215200      |
| train/episodes                 | 21520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.561       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00516    |
| train/info_shaping_reward_mean | -0.0722     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 860800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 538        |
| stats_o/mean                   | 0.2039842  |
| stats_o/std                    | 0.08266153 |
| test/episodes                  | 5390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00316   |
| test/info_shaping_reward_mean  | -0.047     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.239613  |
| test/Q_plus_P                  | -1.239613  |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 215600     |
| train/episodes                 | 21560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.548      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0045    |
| train/info_shaping_reward_mean | -0.0774    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 862400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 539        |
| stats_o/mean                   | 0.2039901  |
| stats_o/std                    | 0.08262212 |
| test/episodes                  | 5400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00184   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.3301457 |
| test/Q_plus_P                  | -1.3301457 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 216000     |
| train/episodes                 | 21600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0041    |
| train/info_shaping_reward_mean | -0.0764    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 864000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 540        |
| stats_o/mean                   | 0.20399524 |
| stats_o/std                    | 0.0826273  |
| test/episodes                  | 5410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00235   |
| test/info_shaping_reward_mean  | -0.047     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.274478  |
| test/Q_plus_P                  | -1.274478  |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 216400     |
| train/episodes                 | 21640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.533      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00457   |
| train/info_shaping_reward_mean | -0.0941    |
| train/info_shaping_reward_min  | -0.304     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 865600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 541        |
| stats_o/mean                   | 0.20399947 |
| stats_o/std                    | 0.08257804 |
| test/episodes                  | 5420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00382   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1972611 |
| test/Q_plus_P                  | -1.1972611 |
| test/reward_per_eps            | -9         |
| test/steps                     | 216800     |
| train/episodes                 | 21680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 867200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 542        |
| stats_o/mean                   | 0.20399691 |
| stats_o/std                    | 0.08255257 |
| test/episodes                  | 5430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.046     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1844271 |
| test/Q_plus_P                  | -1.1844271 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 217200     |
| train/episodes                 | 21720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 868800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 543        |
| stats_o/mean                   | 0.2039975  |
| stats_o/std                    | 0.08253374 |
| test/episodes                  | 5440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00163   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2122921 |
| test/Q_plus_P                  | -1.2122921 |
| test/reward_per_eps            | -9         |
| test/steps                     | 217600     |
| train/episodes                 | 21760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00517   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 870400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 544        |
| stats_o/mean                   | 0.20400496 |
| stats_o/std                    | 0.08249048 |
| test/episodes                  | 5450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0035    |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -1.2853225 |
| test/Q_plus_P                  | -1.2853225 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 218000     |
| train/episodes                 | 21800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00579   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 872000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 545        |
| stats_o/mean                   | 0.20400074 |
| stats_o/std                    | 0.08247344 |
| test/episodes                  | 5460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000673  |
| test/info_shaping_reward_mean  | -0.0458    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.213773  |
| test/Q_plus_P                  | -1.213773  |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 218400     |
| train/episodes                 | 21840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00429   |
| train/info_shaping_reward_mean | -0.0806    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 873600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.20400581  |
| stats_o/std                    | 0.082442686 |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00194    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2678652  |
| test/Q_plus_P                  | -1.2678652  |
| test/reward_per_eps            | -9          |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.517       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00387    |
| train/info_shaping_reward_mean | -0.0768     |
| train/info_shaping_reward_min  | -0.198      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.3       |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 547        |
| stats_o/mean                   | 0.20402044 |
| stats_o/std                    | 0.08241537 |
| test/episodes                  | 5480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00172   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.125435  |
| test/Q_plus_P                  | -1.125435  |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 219200     |
| train/episodes                 | 21920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.495      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00528   |
| train/info_shaping_reward_mean | -0.0835    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 876800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 548         |
| stats_o/mean                   | 0.2040237   |
| stats_o/std                    | 0.082385324 |
| test/episodes                  | 5490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.675       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000454   |
| test/info_shaping_reward_mean  | -0.0607     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -3.267019   |
| test/Q_plus_P                  | -3.267019   |
| test/reward_per_eps            | -13         |
| test/steps                     | 219600      |
| train/episodes                 | 21960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.535       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00444    |
| train/info_shaping_reward_mean | -0.0779     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 878400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 549        |
| stats_o/mean                   | 0.20402235 |
| stats_o/std                    | 0.08235322 |
| test/episodes                  | 5500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00224   |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.236     |
| test/Q                         | -1.3988668 |
| test/Q_plus_P                  | -1.3988668 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 220000     |
| train/episodes                 | 22000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.543      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00396   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 880000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 550        |
| stats_o/mean                   | 0.20402583 |
| stats_o/std                    | 0.08232334 |
| test/episodes                  | 5510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00157   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2950044 |
| test/Q_plus_P                  | -1.2950044 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 220400     |
| train/episodes                 | 22040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00613   |
| train/info_shaping_reward_mean | -0.0796    |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 881600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 551        |
| stats_o/mean                   | 0.20402403 |
| stats_o/std                    | 0.08229635 |
| test/episodes                  | 5520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000992  |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2860816 |
| test/Q_plus_P                  | -1.2860816 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 220800     |
| train/episodes                 | 22080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00439   |
| train/info_shaping_reward_mean | -0.076     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 883200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 552        |
| stats_o/mean                   | 0.20401673 |
| stats_o/std                    | 0.08226135 |
| test/episodes                  | 5530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1688058 |
| test/Q_plus_P                  | -1.1688058 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 221200     |
| train/episodes                 | 22120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00489   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 884800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 553        |
| stats_o/mean                   | 0.20402208 |
| stats_o/std                    | 0.08223328 |
| test/episodes                  | 5540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0028    |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2915907 |
| test/Q_plus_P                  | -1.2915907 |
| test/reward_per_eps            | -10        |
| test/steps                     | 221600     |
| train/episodes                 | 22160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.527      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00489   |
| train/info_shaping_reward_mean | -0.0812    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 886400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 554        |
| stats_o/mean                   | 0.20401725 |
| stats_o/std                    | 0.08220949 |
| test/episodes                  | 5550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0747606 |
| test/Q_plus_P                  | -1.0747606 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 222000     |
| train/episodes                 | 22200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.521      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00585   |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 888000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 555        |
| stats_o/mean                   | 0.20401238 |
| stats_o/std                    | 0.08220846 |
| test/episodes                  | 5560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00301   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.2819242 |
| test/Q_plus_P                  | -1.2819242 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 222400     |
| train/episodes                 | 22240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.543      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00378   |
| train/info_shaping_reward_mean | -0.0826    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 889600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 556        |
| stats_o/mean                   | 0.20400758 |
| stats_o/std                    | 0.08219006 |
| test/episodes                  | 5570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00209   |
| test/info_shaping_reward_mean  | -0.0439    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1685209 |
| test/Q_plus_P                  | -1.1685209 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 222800     |
| train/episodes                 | 22280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.533      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0049    |
| train/info_shaping_reward_mean | -0.0803    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 891200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 557        |
| stats_o/mean                   | 0.20400299 |
| stats_o/std                    | 0.08215523 |
| test/episodes                  | 5580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0029    |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.4913015 |
| test/Q_plus_P                  | -1.4913015 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 223200     |
| train/episodes                 | 22320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.482      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0058    |
| train/info_shaping_reward_mean | -0.0868    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.7      |
| train/steps                    | 892800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 558        |
| stats_o/mean                   | 0.20400478 |
| stats_o/std                    | 0.08212041 |
| test/episodes                  | 5590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00113   |
| test/info_shaping_reward_mean  | -0.0441    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.099415  |
| test/Q_plus_P                  | -1.099415  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 223600     |
| train/episodes                 | 22360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00379   |
| train/info_shaping_reward_mean | -0.0738    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 894400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 559        |
| stats_o/mean                   | 0.20401177 |
| stats_o/std                    | 0.082093   |
| test/episodes                  | 5600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00268   |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0980436 |
| test/Q_plus_P                  | -1.0980436 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 224000     |
| train/episodes                 | 22400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00435   |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 896000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 560        |
| stats_o/mean                   | 0.20400572 |
| stats_o/std                    | 0.0820718  |
| test/episodes                  | 5610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00169   |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.23      |
| test/Q                         | -1.2390729 |
| test/Q_plus_P                  | -1.2390729 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 224400     |
| train/episodes                 | 22440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00419   |
| train/info_shaping_reward_mean | -0.0798    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 897600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 561        |
| stats_o/mean                   | 0.20400359 |
| stats_o/std                    | 0.08205383 |
| test/episodes                  | 5620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00115   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1542668 |
| test/Q_plus_P                  | -1.1542668 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 224800     |
| train/episodes                 | 22480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00656   |
| train/info_shaping_reward_mean | -0.0752    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 899200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 562        |
| stats_o/mean                   | 0.20400919 |
| stats_o/std                    | 0.08209115 |
| test/episodes                  | 5630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000779  |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2752299 |
| test/Q_plus_P                  | -1.2752299 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 225200     |
| train/episodes                 | 22520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.484      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00573   |
| train/info_shaping_reward_mean | -0.112     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 900800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 563        |
| stats_o/mean                   | 0.20401734 |
| stats_o/std                    | 0.08211252 |
| test/episodes                  | 5640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.03454   |
| test/Q_plus_P                  | -1.03454   |
| test/reward_per_eps            | -9         |
| test/steps                     | 225600     |
| train/episodes                 | 22560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00445   |
| train/info_shaping_reward_mean | -0.0956    |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 902400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 564        |
| stats_o/mean                   | 0.20401569 |
| stats_o/std                    | 0.08206753 |
| test/episodes                  | 5650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00503   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.309792  |
| test/Q_plus_P                  | -1.309792  |
| test/reward_per_eps            | -10        |
| test/steps                     | 226000     |
| train/episodes                 | 22600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.528      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00332   |
| train/info_shaping_reward_mean | -0.0769    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 904000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 565        |
| stats_o/mean                   | 0.20402113 |
| stats_o/std                    | 0.08204088 |
| test/episodes                  | 5660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00383   |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2249848 |
| test/Q_plus_P                  | -1.2249848 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 226400     |
| train/episodes                 | 22640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.544      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00513   |
| train/info_shaping_reward_mean | -0.0828    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 905600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 566        |
| stats_o/mean                   | 0.20402028 |
| stats_o/std                    | 0.08202301 |
| test/episodes                  | 5670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3317637 |
| test/Q_plus_P                  | -1.3317637 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 226800     |
| train/episodes                 | 22680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.548      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00368   |
| train/info_shaping_reward_mean | -0.0829    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 907200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 567        |
| stats_o/mean                   | 0.20402399 |
| stats_o/std                    | 0.08198409 |
| test/episodes                  | 5680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00348   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -1.1028125 |
| test/Q_plus_P                  | -1.1028125 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 227200     |
| train/episodes                 | 22720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00506   |
| train/info_shaping_reward_mean | -0.0726    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 908800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 568        |
| stats_o/mean                   | 0.20402099 |
| stats_o/std                    | 0.08197212 |
| test/episodes                  | 5690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00165   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2571661 |
| test/Q_plus_P                  | -1.2571661 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 227600     |
| train/episodes                 | 22760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.541      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0049    |
| train/info_shaping_reward_mean | -0.0942    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 910400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 569        |
| stats_o/mean                   | 0.2040188  |
| stats_o/std                    | 0.08192843 |
| test/episodes                  | 5700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00149   |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.2587607 |
| test/Q_plus_P                  | -1.2587607 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 228000     |
| train/episodes                 | 22800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.526      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00558   |
| train/info_shaping_reward_mean | -0.078     |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 912000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 570        |
| stats_o/mean                   | 0.20402297 |
| stats_o/std                    | 0.08191619 |
| test/episodes                  | 5710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00297   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0775446 |
| test/Q_plus_P                  | -1.0775446 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 228400     |
| train/episodes                 | 22840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00448   |
| train/info_shaping_reward_mean | -0.0838    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 913600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 571        |
| stats_o/mean                   | 0.20403114 |
| stats_o/std                    | 0.08189667 |
| test/episodes                  | 5720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00221   |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.09667   |
| test/Q_plus_P                  | -1.09667   |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 228800     |
| train/episodes                 | 22880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.553      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00505   |
| train/info_shaping_reward_mean | -0.0843    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 915200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 572        |
| stats_o/mean                   | 0.20403434 |
| stats_o/std                    | 0.0818757  |
| test/episodes                  | 5730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00177   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1707196 |
| test/Q_plus_P                  | -1.1707196 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 229200     |
| train/episodes                 | 22920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00449   |
| train/info_shaping_reward_mean | -0.0778    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 916800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 573        |
| stats_o/mean                   | 0.20402989 |
| stats_o/std                    | 0.08186252 |
| test/episodes                  | 5740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1855751 |
| test/Q_plus_P                  | -1.1855751 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 229600     |
| train/episodes                 | 22960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00553   |
| train/info_shaping_reward_mean | -0.0918    |
| train/info_shaping_reward_min  | -0.265     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 918400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 574        |
| stats_o/mean                   | 0.20402211 |
| stats_o/std                    | 0.08182341 |
| test/episodes                  | 5750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0019    |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.194889  |
| test/Q_plus_P                  | -1.194889  |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 230000     |
| train/episodes                 | 23000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0051    |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 920000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 575        |
| stats_o/mean                   | 0.2040254  |
| stats_o/std                    | 0.08178186 |
| test/episodes                  | 5760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00228   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1991851 |
| test/Q_plus_P                  | -1.1991851 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 230400     |
| train/episodes                 | 23040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0037    |
| train/info_shaping_reward_mean | -0.0727    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 921600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.20402901  |
| stats_o/std                    | 0.08175404  |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000611   |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.98033315 |
| test/Q_plus_P                  | -0.98033315 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00364    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 577         |
| stats_o/mean                   | 0.20403256  |
| stats_o/std                    | 0.081727654 |
| test/episodes                  | 5780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000844   |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.2001781  |
| test/Q_plus_P                  | -1.2001781  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 231200      |
| train/episodes                 | 23120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0051     |
| train/info_shaping_reward_mean | -0.0722     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 924800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.20402178 |
| stats_o/std                    | 0.08171934 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000878  |
| test/info_shaping_reward_mean  | -0.0428    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0256227 |
| test/Q_plus_P                  | -1.0256227 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.491      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00514   |
| train/info_shaping_reward_mean | -0.0825    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 579        |
| stats_o/mean                   | 0.20402215 |
| stats_o/std                    | 0.08170143 |
| test/episodes                  | 5800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3465153 |
| test/Q_plus_P                  | -1.3465153 |
| test/reward_per_eps            | -10        |
| test/steps                     | 232000     |
| train/episodes                 | 23200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0829    |
| train/info_shaping_reward_min  | -0.267     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 928000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 580        |
| stats_o/mean                   | 0.20402478 |
| stats_o/std                    | 0.08169343 |
| test/episodes                  | 5810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00036   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1033169 |
| test/Q_plus_P                  | -1.1033169 |
| test/reward_per_eps            | -9         |
| test/steps                     | 232400     |
| train/episodes                 | 23240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00536   |
| train/info_shaping_reward_mean | -0.0876    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 929600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 581         |
| stats_o/mean                   | 0.2040237   |
| stats_o/std                    | 0.081687205 |
| test/episodes                  | 5820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00187    |
| test/info_shaping_reward_mean  | -0.0472     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1949108  |
| test/Q_plus_P                  | -1.1949108  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 232800      |
| train/episodes                 | 23280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.508       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00446    |
| train/info_shaping_reward_mean | -0.0845     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.7       |
| train/steps                    | 931200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 582        |
| stats_o/mean                   | 0.2040243  |
| stats_o/std                    | 0.08167087 |
| test/episodes                  | 5830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00249   |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1471598 |
| test/Q_plus_P                  | -1.1471598 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 233200     |
| train/episodes                 | 23320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0808    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 932800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 583         |
| stats_o/mean                   | 0.20402431  |
| stats_o/std                    | 0.081622384 |
| test/episodes                  | 5840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.1104548  |
| test/Q_plus_P                  | -1.1104548  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 233600      |
| train/episodes                 | 23360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00407    |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 934400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 584        |
| stats_o/mean                   | 0.20402011 |
| stats_o/std                    | 0.08159198 |
| test/episodes                  | 5850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00212   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.2847226 |
| test/Q_plus_P                  | -1.2847226 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 234000     |
| train/episodes                 | 23400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00495   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 936000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 585        |
| stats_o/mean                   | 0.20401642 |
| stats_o/std                    | 0.08155516 |
| test/episodes                  | 5860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00146   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1524248 |
| test/Q_plus_P                  | -1.1524248 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 234400     |
| train/episodes                 | 23440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00611   |
| train/info_shaping_reward_mean | -0.0764    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 937600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 586        |
| stats_o/mean                   | 0.20402668 |
| stats_o/std                    | 0.08156875 |
| test/episodes                  | 5870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00237   |
| test/info_shaping_reward_mean  | -0.0848    |
| test/info_shaping_reward_min   | -0.545     |
| test/Q                         | -3.637569  |
| test/Q_plus_P                  | -3.637569  |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 234800     |
| train/episodes                 | 23480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00547   |
| train/info_shaping_reward_mean | -0.0848    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 939200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 587        |
| stats_o/mean                   | 0.20402221 |
| stats_o/std                    | 0.08155078 |
| test/episodes                  | 5880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00264   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.130344  |
| test/Q_plus_P                  | -1.130344  |
| test/reward_per_eps            | -9         |
| test/steps                     | 235200     |
| train/episodes                 | 23520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00498   |
| train/info_shaping_reward_mean | -0.0825    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 940800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 588         |
| stats_o/mean                   | 0.20402011  |
| stats_o/std                    | 0.08152478  |
| test/episodes                  | 5890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00221    |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.99940246 |
| test/Q_plus_P                  | -0.99940246 |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 235600      |
| train/episodes                 | 23560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00563    |
| train/info_shaping_reward_mean | -0.0708     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 942400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 589        |
| stats_o/mean                   | 0.20402338 |
| stats_o/std                    | 0.08149287 |
| test/episodes                  | 5900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000997  |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.1194648 |
| test/Q_plus_P                  | -1.1194648 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 236000     |
| train/episodes                 | 23600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00558   |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 944000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.20402978  |
| stats_o/std                    | 0.081465855 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00218    |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.3825216  |
| test/Q_plus_P                  | -1.3825216  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00464    |
| train/info_shaping_reward_mean | -0.0712     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 591        |
| stats_o/mean                   | 0.2040381  |
| stats_o/std                    | 0.08145152 |
| test/episodes                  | 5920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00268   |
| test/info_shaping_reward_mean  | -0.0562    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.8008537 |
| test/Q_plus_P                  | -1.8008537 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 236800     |
| train/episodes                 | 23680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.441      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00753   |
| train/info_shaping_reward_mean | -0.093     |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.4      |
| train/steps                    | 947200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 592        |
| stats_o/mean                   | 0.20403536 |
| stats_o/std                    | 0.08142954 |
| test/episodes                  | 5930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000462  |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.5018754 |
| test/Q_plus_P                  | -1.5018754 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 237200     |
| train/episodes                 | 23720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.45       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00651   |
| train/info_shaping_reward_mean | -0.0865    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22        |
| train/steps                    | 948800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 593        |
| stats_o/mean                   | 0.2040359  |
| stats_o/std                    | 0.081391   |
| test/episodes                  | 5940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00156   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.4811496 |
| test/Q_plus_P                  | -1.4811496 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 237600     |
| train/episodes                 | 23760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00468   |
| train/info_shaping_reward_mean | -0.0756    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 950400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 594        |
| stats_o/mean                   | 0.20404166 |
| stats_o/std                    | 0.08135833 |
| test/episodes                  | 5950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00409   |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3727136 |
| test/Q_plus_P                  | -1.3727136 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 238000     |
| train/episodes                 | 23800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00544   |
| train/info_shaping_reward_mean | -0.0753    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 952000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 595         |
| stats_o/mean                   | 0.20404245  |
| stats_o/std                    | 0.081320085 |
| test/episodes                  | 5960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00188    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.149052   |
| test/Q_plus_P                  | -1.149052   |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 238400      |
| train/episodes                 | 23840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.55        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00473    |
| train/info_shaping_reward_mean | -0.0758     |
| train/info_shaping_reward_min  | -0.197      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18         |
| train/steps                    | 953600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 596        |
| stats_o/mean                   | 0.20404434 |
| stats_o/std                    | 0.08131275 |
| test/episodes                  | 5970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00296   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.3403769 |
| test/Q_plus_P                  | -1.3403769 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 238800     |
| train/episodes                 | 23880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.512      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00346   |
| train/info_shaping_reward_mean | -0.0844    |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.5      |
| train/steps                    | 955200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 597        |
| stats_o/mean                   | 0.20404285 |
| stats_o/std                    | 0.08127818 |
| test/episodes                  | 5980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.208     |
| test/Q                         | -1.2631311 |
| test/Q_plus_P                  | -1.2631311 |
| test/reward_per_eps            | -10        |
| test/steps                     | 239200     |
| train/episodes                 | 23920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.565      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00509   |
| train/info_shaping_reward_mean | -0.0732    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 956800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 598        |
| stats_o/mean                   | 0.20404041 |
| stats_o/std                    | 0.08124297 |
| test/episodes                  | 5990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00408   |
| test/info_shaping_reward_mean  | -0.0748    |
| test/info_shaping_reward_min   | -0.56      |
| test/Q                         | -3.0058944 |
| test/Q_plus_P                  | -3.0058944 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 239600     |
| train/episodes                 | 23960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00401   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 958400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 599         |
| stats_o/mean                   | 0.20403326  |
| stats_o/std                    | 0.081253625 |
| test/episodes                  | 6000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.176621   |
| test/Q_plus_P                  | -1.176621   |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 240000      |
| train/episodes                 | 24000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.479       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00558    |
| train/info_shaping_reward_mean | -0.0927     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.9       |
| train/steps                    | 960000      |
------------------------------------------------
Saving latest policy.
