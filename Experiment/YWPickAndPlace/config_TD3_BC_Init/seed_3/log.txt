Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPickAndPlace/config_TD3_BC_Init/seed_3
epoch: 19 policy initialization loss: 0.006507625803351402
epoch: 39 policy initialization loss: 0.005798411555588245
epoch: 59 policy initialization loss: 0.0017906595021486282
epoch: 79 policy initialization loss: 0.0005433607148006558
epoch: 99 policy initialization loss: 0.0006787416059523821
epoch: 119 policy initialization loss: 0.001463399501517415
epoch: 139 policy initialization loss: 0.0006920094019733369
epoch: 159 policy initialization loss: 0.0004809211823157966
epoch: 179 policy initialization loss: 0.0003347973106428981
epoch: 199 policy initialization loss: 0.00032086315331980586
epoch: 219 policy initialization loss: 0.0011363837402313948
epoch: 239 policy initialization loss: 0.0005101801361888647
epoch: 259 policy initialization loss: 0.00023992382921278477
epoch: 279 policy initialization loss: 0.00020661475718952715
epoch: 299 policy initialization loss: 0.00017561906133778393
epoch: 319 policy initialization loss: 0.000167689606314525
epoch: 339 policy initialization loss: 0.00012536977010313421
epoch: 359 policy initialization loss: 9.02934989426285e-05
epoch: 379 policy initialization loss: 0.00012191726273158565
epoch: 399 policy initialization loss: 0.000366313848644495
epoch: 419 policy initialization loss: 0.00020183641754556447
epoch: 439 policy initialization loss: 0.0006712405593134463
epoch: 459 policy initialization loss: 8.464990241918713e-05
epoch: 479 policy initialization loss: 0.0004976240452378988
epoch: 499 policy initialization loss: 0.00027000796399079263
epoch: 519 policy initialization loss: 7.763841131236404e-05
epoch: 539 policy initialization loss: 0.00034516159212216735
epoch: 559 policy initialization loss: 0.00018134046695195138
epoch: 579 policy initialization loss: 0.00012654607417061925
epoch: 599 policy initialization loss: 6.283956463448703e-05
epoch: 619 policy initialization loss: 0.00016235341900028288
epoch: 639 policy initialization loss: 6.240520451683551e-05
epoch: 659 policy initialization loss: 7.180028478614986e-05
epoch: 679 policy initialization loss: 7.6935080869589e-05
epoch: 699 policy initialization loss: 0.0002625606721267104
epoch: 719 policy initialization loss: 0.00010062652290798724
epoch: 739 policy initialization loss: 0.00137199810706079
epoch: 759 policy initialization loss: 3.8910264265723526e-05
epoch: 779 policy initialization loss: 8.238444570451975e-05
epoch: 799 policy initialization loss: 9.740024688653648e-05
epoch: 819 policy initialization loss: 0.00015057469136081636
epoch: 839 policy initialization loss: 9.291901369579136e-05
epoch: 859 policy initialization loss: 7.267636829055846e-05
epoch: 879 policy initialization loss: 6.921753083588555e-05
epoch: 899 policy initialization loss: 0.00011587488552322611
epoch: 919 policy initialization loss: 3.756288788281381e-05
epoch: 939 policy initialization loss: 4.854890357819386e-05
epoch: 959 policy initialization loss: 5.409048389992677e-05
epoch: 979 policy initialization loss: 4.475260357139632e-05
epoch: 999 policy initialization loss: 0.0002792924642562866
epoch: 1019 policy initialization loss: 3.407405165489763e-05
epoch: 1039 policy initialization loss: 7.940853538457304e-05
epoch: 1059 policy initialization loss: 6.698706420138478e-05
epoch: 1079 policy initialization loss: 3.8386024243663996e-05
epoch: 1099 policy initialization loss: 0.0007777486462146044
epoch: 1119 policy initialization loss: 5.39381435373798e-05
epoch: 1139 policy initialization loss: 1.788374902389478e-05
epoch: 1159 policy initialization loss: 0.0004371278337202966
epoch: 1179 policy initialization loss: 0.0002594618417788297
epoch: 1199 policy initialization loss: 2.936901364591904e-05
epoch: 1219 policy initialization loss: 0.00010316328553017229
epoch: 1239 policy initialization loss: 4.834277569898404e-05
epoch: 1259 policy initialization loss: 1.8978746084030718e-05
epoch: 1279 policy initialization loss: 2.75000165856909e-05
epoch: 1299 policy initialization loss: 9.142015187535435e-05
epoch: 1319 policy initialization loss: 2.0571867935359478e-05
epoch: 1339 policy initialization loss: 0.0004240251728333533
epoch: 1359 policy initialization loss: 0.0005727602983824909
epoch: 1379 policy initialization loss: 1.2444972526282072e-05
epoch: 1399 policy initialization loss: 2.3217582565848716e-05
epoch: 1419 policy initialization loss: 1.3601034879684448e-05
epoch: 1439 policy initialization loss: 5.196173515287228e-05
epoch: 1459 policy initialization loss: 4.386052751215175e-05
epoch: 1479 policy initialization loss: 1.616147164895665e-05
epoch: 1499 policy initialization loss: 1.81382019945886e-05
epoch: 1519 policy initialization loss: 2.0082341507077217e-05
epoch: 1539 policy initialization loss: 0.00011009442096110433
epoch: 1559 policy initialization loss: 2.161587144655641e-05
epoch: 1579 policy initialization loss: 0.0001769277296261862
epoch: 1599 policy initialization loss: 8.467776751786005e-06
epoch: 1619 policy initialization loss: 0.00013288401532918215
epoch: 1639 policy initialization loss: 3.383823059266433e-05
epoch: 1659 policy initialization loss: 2.3335014702752233e-05
epoch: 1679 policy initialization loss: 2.605440022307448e-05
epoch: 1699 policy initialization loss: 2.59044627455296e-05
epoch: 1719 policy initialization loss: 0.0003584588412195444
epoch: 1739 policy initialization loss: 1.9132876332150772e-05
epoch: 1759 policy initialization loss: 7.462028861482395e-06
epoch: 1779 policy initialization loss: 1.001848158921348e-05
epoch: 1799 policy initialization loss: 0.0001167370137409307
epoch: 1819 policy initialization loss: 1.0083234883495606e-05
epoch: 1839 policy initialization loss: 2.0501258404692635e-05
epoch: 1859 policy initialization loss: 0.00015696890477556735
epoch: 1879 policy initialization loss: 5.02305083500687e-05
epoch: 1899 policy initialization loss: 0.00020259569282643497
epoch: 1919 policy initialization loss: 8.696527947904542e-05
epoch: 1939 policy initialization loss: 4.891881690127775e-05
epoch: 1959 policy initialization loss: 1.0133742762263864e-05
epoch: 1979 policy initialization loss: 0.0004826346703339368
epoch: 1999 policy initialization loss: 0.0005124580347910523
Saving initial policy.
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.20199838  |
| stats_o/std                    | 0.058671374 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.4424527  |
| test/Q_plus_P                  | -1.4424527  |
| test/reward_per_eps            | -40         |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.192      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.20330282  |
| stats_o/std                    | 0.056723174 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.7526028  |
| test/Q_plus_P                  | -1.7526028  |
| test/reward_per_eps            | -40         |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 2           |
| stats_o/mean                   | 0.20359856  |
| stats_o/std                    | 0.055233695 |
| test/episodes                  | 30          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.1247075  |
| test/Q_plus_P                  | -2.1247075  |
| test/reward_per_eps            | -40         |
| test/steps                     | 1200        |
| train/episodes                 | 120         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 4800        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 3           |
| stats_o/mean                   | 0.20353292  |
| stats_o/std                    | 0.052896567 |
| test/episodes                  | 40          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.446493   |
| test/Q_plus_P                  | -2.446493   |
| test/reward_per_eps            | -40         |
| test/steps                     | 1600        |
| train/episodes                 | 160         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 6400        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 4           |
| stats_o/mean                   | 0.2034304   |
| stats_o/std                    | 0.051108696 |
| test/episodes                  | 50          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.8417518  |
| test/Q_plus_P                  | -2.8417518  |
| test/reward_per_eps            | -40         |
| test/steps                     | 2000        |
| train/episodes                 | 200         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 8000        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 5           |
| stats_o/mean                   | 0.20389262  |
| stats_o/std                    | 0.050445467 |
| test/episodes                  | 60          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -3.2156298  |
| test/Q_plus_P                  | -3.2156298  |
| test/reward_per_eps            | -40         |
| test/steps                     | 2400        |
| train/episodes                 | 240         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 9600        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 6          |
| stats_o/mean                   | 0.20378326 |
| stats_o/std                    | 0.04935789 |
| test/episodes                  | 70         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.6528425 |
| test/Q_plus_P                  | -3.6528425 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2800       |
| train/episodes                 | 280        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 11200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 7           |
| stats_o/mean                   | 0.20372555  |
| stats_o/std                    | 0.048538327 |
| test/episodes                  | 80          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -4.0032444  |
| test/Q_plus_P                  | -4.0032444  |
| test/reward_per_eps            | -40         |
| test/steps                     | 3200        |
| train/episodes                 | 320         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 12800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 8          |
| stats_o/mean                   | 0.20382586 |
| stats_o/std                    | 0.04782239 |
| test/episodes                  | 90         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -4.4441695 |
| test/Q_plus_P                  | -4.4441695 |
| test/reward_per_eps            | -40        |
| test/steps                     | 3600       |
| train/episodes                 | 360        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 14400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 9          |
| stats_o/mean                   | 0.20391539 |
| stats_o/std                    | 0.0472535  |
| test/episodes                  | 100        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -4.828773  |
| test/Q_plus_P                  | -4.828773  |
| test/reward_per_eps            | -40        |
| test/steps                     | 4000       |
| train/episodes                 | 400        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 16000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 10         |
| stats_o/mean                   | 0.20359191 |
| stats_o/std                    | 0.04675397 |
| test/episodes                  | 110        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -5.1817875 |
| test/Q_plus_P                  | -5.1817875 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4400       |
| train/episodes                 | 440        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 17600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 11          |
| stats_o/mean                   | 0.20383626  |
| stats_o/std                    | 0.046392497 |
| test/episodes                  | 120         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -5.529462   |
| test/Q_plus_P                  | -5.529462   |
| test/reward_per_eps            | -40         |
| test/steps                     | 4800        |
| train/episodes                 | 480         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 19200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.20393042  |
| stats_o/std                    | 0.046133775 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -5.9400806  |
| test/Q_plus_P                  | -5.9400806  |
| test/reward_per_eps            | -40         |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 13          |
| stats_o/mean                   | 0.20393059  |
| stats_o/std                    | 0.045935676 |
| test/episodes                  | 140         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -6.261924   |
| test/Q_plus_P                  | -6.261924   |
| test/reward_per_eps            | -40         |
| test/steps                     | 5600        |
| train/episodes                 | 560         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 22400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 14          |
| stats_o/mean                   | 0.20407471  |
| stats_o/std                    | 0.045629725 |
| test/episodes                  | 150         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -6.6202965  |
| test/Q_plus_P                  | -6.6202965  |
| test/reward_per_eps            | -40         |
| test/steps                     | 6000        |
| train/episodes                 | 600         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 24000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 15         |
| stats_o/mean                   | 0.20401041 |
| stats_o/std                    | 0.04544609 |
| test/episodes                  | 160        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -7.071378  |
| test/Q_plus_P                  | -7.071378  |
| test/reward_per_eps            | -40        |
| test/steps                     | 6400       |
| train/episodes                 | 640        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 25600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 16         |
| stats_o/mean                   | 0.20406233 |
| stats_o/std                    | 0.04524706 |
| test/episodes                  | 170        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -7.451028  |
| test/Q_plus_P                  | -7.451028  |
| test/reward_per_eps            | -40        |
| test/steps                     | 6800       |
| train/episodes                 | 680        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 27200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 17         |
| stats_o/mean                   | 0.20389864 |
| stats_o/std                    | 0.04509284 |
| test/episodes                  | 180        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -7.811643  |
| test/Q_plus_P                  | -7.811643  |
| test/reward_per_eps            | -40        |
| test/steps                     | 7200       |
| train/episodes                 | 720        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 28800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 18          |
| stats_o/mean                   | 0.20401247  |
| stats_o/std                    | 0.044966135 |
| test/episodes                  | 190         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -8.100509   |
| test/Q_plus_P                  | -8.100509   |
| test/reward_per_eps            | -40         |
| test/steps                     | 7600        |
| train/episodes                 | 760         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 30400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 19          |
| stats_o/mean                   | 0.20395572  |
| stats_o/std                    | 0.044877548 |
| test/episodes                  | 200         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -8.491001   |
| test/Q_plus_P                  | -8.491001   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8000        |
| train/episodes                 | 800         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 32000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 20          |
| stats_o/mean                   | 0.20390934  |
| stats_o/std                    | 0.044820517 |
| test/episodes                  | 210         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -8.81526    |
| test/Q_plus_P                  | -8.81526    |
| test/reward_per_eps            | -40         |
| test/steps                     | 8400        |
| train/episodes                 | 840         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 33600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 21          |
| stats_o/mean                   | 0.20407112  |
| stats_o/std                    | 0.044758666 |
| test/episodes                  | 220         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -9.238524   |
| test/Q_plus_P                  | -9.238524   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8800        |
| train/episodes                 | 880         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 35200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 22          |
| stats_o/mean                   | 0.20384987  |
| stats_o/std                    | 0.044631205 |
| test/episodes                  | 230         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -9.5111885  |
| test/Q_plus_P                  | -9.5111885  |
| test/reward_per_eps            | -40         |
| test/steps                     | 9200        |
| train/episodes                 | 920         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 36800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 23          |
| stats_o/mean                   | 0.20375715  |
| stats_o/std                    | 0.044592094 |
| test/episodes                  | 240         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -9.862535   |
| test/Q_plus_P                  | -9.862535   |
| test/reward_per_eps            | -40         |
| test/steps                     | 9600        |
| train/episodes                 | 960         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 38400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.20373063  |
| stats_o/std                    | 0.044519804 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -10.247415  |
| test/Q_plus_P                  | -10.247415  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 25          |
| stats_o/mean                   | 0.20362368  |
| stats_o/std                    | 0.044444896 |
| test/episodes                  | 260         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -10.488608  |
| test/Q_plus_P                  | -10.488608  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10400       |
| train/episodes                 | 1040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 41600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 26         |
| stats_o/mean                   | 0.20360653 |
| stats_o/std                    | 0.04440517 |
| test/episodes                  | 270        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -10.916533 |
| test/Q_plus_P                  | -10.916533 |
| test/reward_per_eps            | -40        |
| test/steps                     | 10800      |
| train/episodes                 | 1080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 43200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 27         |
| stats_o/mean                   | 0.20353356 |
| stats_o/std                    | 0.04433419 |
| test/episodes                  | 280        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -11.191178 |
| test/Q_plus_P                  | -11.191178 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11200      |
| train/episodes                 | 1120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 44800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.20353357 |
| stats_o/std                    | 0.04425294 |
| test/episodes                  | 290        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -11.591304 |
| test/Q_plus_P                  | -11.591304 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 29          |
| stats_o/mean                   | 0.20363563  |
| stats_o/std                    | 0.044201106 |
| test/episodes                  | 300         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -11.836023  |
| test/Q_plus_P                  | -11.836023  |
| test/reward_per_eps            | -40         |
| test/steps                     | 12000       |
| train/episodes                 | 1200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 48000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 30          |
| stats_o/mean                   | 0.2036701   |
| stats_o/std                    | 0.044202164 |
| test/episodes                  | 310         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -12.231582  |
| test/Q_plus_P                  | -12.231582  |
| test/reward_per_eps            | -40         |
| test/steps                     | 12400       |
| train/episodes                 | 1240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 49600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 31         |
| stats_o/mean                   | 0.20365463 |
| stats_o/std                    | 0.04414213 |
| test/episodes                  | 320        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -12.576261 |
| test/Q_plus_P                  | -12.576261 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12800      |
| train/episodes                 | 1280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 51200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 32         |
| stats_o/mean                   | 0.20364502 |
| stats_o/std                    | 0.04412118 |
| test/episodes                  | 330        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -12.843432 |
| test/Q_plus_P                  | -12.843432 |
| test/reward_per_eps            | -40        |
| test/steps                     | 13200      |
| train/episodes                 | 1320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 52800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 33          |
| stats_o/mean                   | 0.2037827   |
| stats_o/std                    | 0.044115562 |
| test/episodes                  | 340         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -13.180168  |
| test/Q_plus_P                  | -13.180168  |
| test/reward_per_eps            | -40         |
| test/steps                     | 13600       |
| train/episodes                 | 1360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 54400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 34          |
| stats_o/mean                   | 0.20381589  |
| stats_o/std                    | 0.044077296 |
| test/episodes                  | 350         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -13.555295  |
| test/Q_plus_P                  | -13.555295  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14000       |
| train/episodes                 | 1400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 56000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 35          |
| stats_o/mean                   | 0.20391142  |
| stats_o/std                    | 0.044037275 |
| test/episodes                  | 360         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -13.656483  |
| test/Q_plus_P                  | -13.656483  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14400       |
| train/episodes                 | 1440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 57600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 36          |
| stats_o/mean                   | 0.2038936   |
| stats_o/std                    | 0.044168647 |
| test/episodes                  | 370         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -14.181526  |
| test/Q_plus_P                  | -14.181526  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14800       |
| train/episodes                 | 1480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 59200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.2038493   |
| stats_o/std                    | 0.046311397 |
| test/episodes                  | 380         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.198      |
| test/info_shaping_reward_min   | -0.516      |
| test/Q                         | -14.221233  |
| test/Q_plus_P                  | -14.221233  |
| test/reward_per_eps            | -40         |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.196      |
| train/info_shaping_reward_min  | -0.428      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 38         |
| stats_o/mean                   | 0.20389114 |
| stats_o/std                    | 0.04834066 |
| test/episodes                  | 390        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.188     |
| test/info_shaping_reward_min   | -0.448     |
| test/Q                         | -13.978452 |
| test/Q_plus_P                  | -13.978452 |
| test/reward_per_eps            | -40        |
| test/steps                     | 15600      |
| train/episodes                 | 1560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 62400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 39         |
| stats_o/mean                   | 0.20382158 |
| stats_o/std                    | 0.05391209 |
| test/episodes                  | 400        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.181     |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -14.569567 |
| test/Q_plus_P                  | -14.569567 |
| test/reward_per_eps            | -40        |
| test/steps                     | 16000      |
| train/episodes                 | 1600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.212     |
| train/info_shaping_reward_min  | -0.458     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 64000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.20375448  |
| stats_o/std                    | 0.056345038 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.183      |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -15.000724  |
| test/Q_plus_P                  | -15.000724  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.214      |
| train/info_shaping_reward_min  | -0.428      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.20378357  |
| stats_o/std                    | 0.060025588 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.185      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -14.705842  |
| test/Q_plus_P                  | -14.705842  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.189      |
| train/info_shaping_reward_min  | -0.292      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 42         |
| stats_o/mean                   | 0.20409991 |
| stats_o/std                    | 0.06414559 |
| test/episodes                  | 430        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.679     |
| test/Q                         | -14.939822 |
| test/Q_plus_P                  | -14.939822 |
| test/reward_per_eps            | -40        |
| test/steps                     | 17200      |
| train/episodes                 | 1720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.214     |
| train/info_shaping_reward_min  | -0.479     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 68800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 43          |
| stats_o/mean                   | 0.20390052  |
| stats_o/std                    | 0.066662915 |
| test/episodes                  | 440         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.585      |
| test/Q                         | -15.384802  |
| test/Q_plus_P                  | -15.384802  |
| test/reward_per_eps            | -40         |
| test/steps                     | 17600       |
| train/episodes                 | 1760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.519      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 70400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 44         |
| stats_o/mean                   | 0.20383883 |
| stats_o/std                    | 0.06857308 |
| test/episodes                  | 450        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.204     |
| test/info_shaping_reward_min   | -0.403     |
| test/Q                         | -15.966833 |
| test/Q_plus_P                  | -15.966833 |
| test/reward_per_eps            | -40        |
| test/steps                     | 18000      |
| train/episodes                 | 1800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.625     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 72000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 45         |
| stats_o/mean                   | 0.20391476 |
| stats_o/std                    | 0.06981784 |
| test/episodes                  | 460        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.179     |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -15.257587 |
| test/Q_plus_P                  | -15.257587 |
| test/reward_per_eps            | -40        |
| test/steps                     | 18400      |
| train/episodes                 | 1840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.202     |
| train/info_shaping_reward_min  | -0.456     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 73600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 46          |
| stats_o/mean                   | 0.203839    |
| stats_o/std                    | 0.070811905 |
| test/episodes                  | 470         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -15.480264  |
| test/Q_plus_P                  | -15.480264  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18800       |
| train/episodes                 | 1880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.507      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 75200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 47          |
| stats_o/mean                   | 0.20386395  |
| stats_o/std                    | 0.070827834 |
| test/episodes                  | 480         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.112      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.198      |
| test/Q                         | -15.6389475 |
| test/Q_plus_P                  | -15.6389475 |
| test/reward_per_eps            | -40         |
| test/steps                     | 19200       |
| train/episodes                 | 1920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.288      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 76800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 48          |
| stats_o/mean                   | 0.20387377  |
| stats_o/std                    | 0.071053036 |
| test/episodes                  | 490         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.175      |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -15.962751  |
| test/Q_plus_P                  | -15.962751  |
| test/reward_per_eps            | -40         |
| test/steps                     | 19600       |
| train/episodes                 | 1960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.194      |
| train/info_shaping_reward_min  | -0.374      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 78400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 49         |
| stats_o/mean                   | 0.20389722 |
| stats_o/std                    | 0.07160095 |
| test/episodes                  | 500        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -16.574846 |
| test/Q_plus_P                  | -16.574846 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20000      |
| train/episodes                 | 2000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.267     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 80000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.20389986 |
| stats_o/std                    | 0.07226452 |
| test/episodes                  | 510        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -16.691107 |
| test/Q_plus_P                  | -16.691107 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.326     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.20394237 |
| stats_o/std                    | 0.07244551 |
| test/episodes                  | 520        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.149     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -16.351923 |
| test/Q_plus_P                  | -16.351923 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.2039495  |
| stats_o/std                    | 0.07224497 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.132     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.23      |
| test/Q                         | -16.647707 |
| test/Q_plus_P                  | -16.647707 |
| test/reward_per_eps            | -40        |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.20398502  |
| stats_o/std                    | 0.072345674 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -16.889734  |
| test/Q_plus_P                  | -16.889734  |
| test/reward_per_eps            | -40         |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.198      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 54          |
| stats_o/mean                   | 0.2039611   |
| stats_o/std                    | 0.072255194 |
| test/episodes                  | 550         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0906     |
| test/info_shaping_reward_mean  | -0.17       |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -16.797836  |
| test/Q_plus_P                  | -16.797836  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22000       |
| train/episodes                 | 2200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 88000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 55         |
| stats_o/mean                   | 0.20391649 |
| stats_o/std                    | 0.07206469 |
| test/episodes                  | 560        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -17.493332 |
| test/Q_plus_P                  | -17.493332 |
| test/reward_per_eps            | -40        |
| test/steps                     | 22400      |
| train/episodes                 | 2240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 89600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 56          |
| stats_o/mean                   | 0.20392896  |
| stats_o/std                    | 0.071908034 |
| test/episodes                  | 570         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -17.43503   |
| test/Q_plus_P                  | -17.43503   |
| test/reward_per_eps            | -40         |
| test/steps                     | 22800       |
| train/episodes                 | 2280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 91200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 57         |
| stats_o/mean                   | 0.2039325  |
| stats_o/std                    | 0.07180814 |
| test/episodes                  | 580        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -17.425026 |
| test/Q_plus_P                  | -17.425026 |
| test/reward_per_eps            | -40        |
| test/steps                     | 23200      |
| train/episodes                 | 2320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 92800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 58         |
| stats_o/mean                   | 0.20394364 |
| stats_o/std                    | 0.07177226 |
| test/episodes                  | 590        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0125     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0421    |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -17.624895 |
| test/Q_plus_P                  | -17.624895 |
| test/reward_per_eps            | -39.5      |
| test/steps                     | 23600      |
| train/episodes                 | 2360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 94400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 59         |
| stats_o/mean                   | 0.20398904 |
| stats_o/std                    | 0.07169257 |
| test/episodes                  | 600        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -17.977364 |
| test/Q_plus_P                  | -17.977364 |
| test/reward_per_eps            | -40        |
| test/steps                     | 24000      |
| train/episodes                 | 2400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 96000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 60         |
| stats_o/mean                   | 0.2039426  |
| stats_o/std                    | 0.07160757 |
| test/episodes                  | 610        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -17.789715 |
| test/Q_plus_P                  | -17.789715 |
| test/reward_per_eps            | -40        |
| test/steps                     | 24400      |
| train/episodes                 | 2440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 97600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 61         |
| stats_o/mean                   | 0.20392151 |
| stats_o/std                    | 0.07160052 |
| test/episodes                  | 620        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -18.111021 |
| test/Q_plus_P                  | -18.111021 |
| test/reward_per_eps            | -40        |
| test/steps                     | 24800      |
| train/episodes                 | 2480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 99200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 62          |
| stats_o/mean                   | 0.2039197   |
| stats_o/std                    | 0.071564764 |
| test/episodes                  | 630         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -18.279966  |
| test/Q_plus_P                  | -18.279966  |
| test/reward_per_eps            | -40         |
| test/steps                     | 25200       |
| train/episodes                 | 2520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 100800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 63          |
| stats_o/mean                   | 0.20389381  |
| stats_o/std                    | 0.071618564 |
| test/episodes                  | 640         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -18.41482   |
| test/Q_plus_P                  | -18.41482   |
| test/reward_per_eps            | -40         |
| test/steps                     | 25600       |
| train/episodes                 | 2560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.225      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 102400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.2038621  |
| stats_o/std                    | 0.07161417 |
| test/episodes                  | 650        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -18.723621 |
| test/Q_plus_P                  | -18.723621 |
| test/reward_per_eps            | -40        |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 65         |
| stats_o/mean                   | 0.20394637 |
| stats_o/std                    | 0.07161406 |
| test/episodes                  | 660        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -18.732517 |
| test/Q_plus_P                  | -18.732517 |
| test/reward_per_eps            | -40        |
| test/steps                     | 26400      |
| train/episodes                 | 2640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 105600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 66         |
| stats_o/mean                   | 0.20401551 |
| stats_o/std                    | 0.07152945 |
| test/episodes                  | 670        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -18.85039  |
| test/Q_plus_P                  | -18.85039  |
| test/reward_per_eps            | -40        |
| test/steps                     | 26800      |
| train/episodes                 | 2680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 107200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 67         |
| stats_o/mean                   | 0.20398575 |
| stats_o/std                    | 0.07150649 |
| test/episodes                  | 680        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -19.079271 |
| test/Q_plus_P                  | -19.079271 |
| test/reward_per_eps            | -40        |
| test/steps                     | 27200      |
| train/episodes                 | 2720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 108800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 68         |
| stats_o/mean                   | 0.20400326 |
| stats_o/std                    | 0.0715834  |
| test/episodes                  | 690        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -19.158836 |
| test/Q_plus_P                  | -19.158836 |
| test/reward_per_eps            | -40        |
| test/steps                     | 27600      |
| train/episodes                 | 2760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 110400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 69          |
| stats_o/mean                   | 0.20397437  |
| stats_o/std                    | 0.071585186 |
| test/episodes                  | 700         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -19.50248   |
| test/Q_plus_P                  | -19.50248   |
| test/reward_per_eps            | -40         |
| test/steps                     | 28000       |
| train/episodes                 | 2800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 112000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.20396738 |
| stats_o/std                    | 0.07185959 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -19.812136 |
| test/Q_plus_P                  | -19.812136 |
| test/reward_per_eps            | -40        |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 71         |
| stats_o/mean                   | 0.20398386 |
| stats_o/std                    | 0.07183959 |
| test/episodes                  | 720        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.022583 |
| test/Q_plus_P                  | -20.022583 |
| test/reward_per_eps            | -40        |
| test/steps                     | 28800      |
| train/episodes                 | 2880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 115200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 72         |
| stats_o/mean                   | 0.20396355 |
| stats_o/std                    | 0.07193783 |
| test/episodes                  | 730        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.061085 |
| test/Q_plus_P                  | -20.061085 |
| test/reward_per_eps            | -40        |
| test/steps                     | 29200      |
| train/episodes                 | 2920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 116800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.20399971 |
| stats_o/std                    | 0.07189741 |
| test/episodes                  | 740        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.41814  |
| test/Q_plus_P                  | -20.41814  |
| test/reward_per_eps            | -40        |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 74         |
| stats_o/mean                   | 0.20401135 |
| stats_o/std                    | 0.07189851 |
| test/episodes                  | 750        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.191     |
| test/info_shaping_reward_min   | -0.84      |
| test/Q                         | -20.452135 |
| test/Q_plus_P                  | -20.452135 |
| test/reward_per_eps            | -40        |
| test/steps                     | 30000      |
| train/episodes                 | 3000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 120000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 75         |
| stats_o/mean                   | 0.20399068 |
| stats_o/std                    | 0.07212239 |
| test/episodes                  | 760        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.203     |
| test/info_shaping_reward_min   | -0.787     |
| test/Q                         | -20.618841 |
| test/Q_plus_P                  | -20.618841 |
| test/reward_per_eps            | -40        |
| test/steps                     | 30400      |
| train/episodes                 | 3040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 121600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 76          |
| stats_o/mean                   | 0.2040148   |
| stats_o/std                    | 0.072741024 |
| test/episodes                  | 770         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.188      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -20.721077  |
| test/Q_plus_P                  | -20.721077  |
| test/reward_per_eps            | -40         |
| test/steps                     | 30800       |
| train/episodes                 | 3080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.187      |
| train/info_shaping_reward_min  | -0.313      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 123200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 77         |
| stats_o/mean                   | 0.20399167 |
| stats_o/std                    | 0.07303924 |
| test/episodes                  | 780        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.178     |
| test/info_shaping_reward_min   | -0.236     |
| test/Q                         | -21.048512 |
| test/Q_plus_P                  | -21.048512 |
| test/reward_per_eps            | -40        |
| test/steps                     | 31200      |
| train/episodes                 | 3120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.199     |
| train/info_shaping_reward_min  | -0.469     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 124800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 78          |
| stats_o/mean                   | 0.20396489  |
| stats_o/std                    | 0.073365204 |
| test/episodes                  | 790         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -21.363503  |
| test/Q_plus_P                  | -21.363503  |
| test/reward_per_eps            | -40         |
| test/steps                     | 31600       |
| train/episodes                 | 3160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.197      |
| train/info_shaping_reward_min  | -0.437      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 126400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 79         |
| stats_o/mean                   | 0.20391108 |
| stats_o/std                    | 0.07341898 |
| test/episodes                  | 800        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.178     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -21.691723 |
| test/Q_plus_P                  | -21.691723 |
| test/reward_per_eps            | -40        |
| test/steps                     | 32000      |
| train/episodes                 | 3200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 128000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 80         |
| stats_o/mean                   | 0.20391446 |
| stats_o/std                    | 0.07334694 |
| test/episodes                  | 810        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.18      |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -21.430166 |
| test/Q_plus_P                  | -21.430166 |
| test/reward_per_eps            | -40        |
| test/steps                     | 32400      |
| train/episodes                 | 3240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 129600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 81         |
| stats_o/mean                   | 0.20386973 |
| stats_o/std                    | 0.07356336 |
| test/episodes                  | 820        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.191     |
| test/info_shaping_reward_min   | -0.616     |
| test/Q                         | -21.77984  |
| test/Q_plus_P                  | -21.77984  |
| test/reward_per_eps            | -40        |
| test/steps                     | 32800      |
| train/episodes                 | 3280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 131200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.20391785  |
| stats_o/std                    | 0.073473275 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -21.94103   |
| test/Q_plus_P                  | -21.94103   |
| test/reward_per_eps            | -40         |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 83         |
| stats_o/mean                   | 0.20389006 |
| stats_o/std                    | 0.0734482  |
| test/episodes                  | 840        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.14      |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -22.158594 |
| test/Q_plus_P                  | -22.158594 |
| test/reward_per_eps            | -40        |
| test/steps                     | 33600      |
| train/episodes                 | 3360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 134400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 84         |
| stats_o/mean                   | 0.20392059 |
| stats_o/std                    | 0.07376467 |
| test/episodes                  | 850        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.192     |
| test/info_shaping_reward_min   | -0.339     |
| test/Q                         | -22.673994 |
| test/Q_plus_P                  | -22.673994 |
| test/reward_per_eps            | -40        |
| test/steps                     | 34000      |
| train/episodes                 | 3400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.192     |
| train/info_shaping_reward_min  | -0.3       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 136000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 85         |
| stats_o/mean                   | 0.20392534 |
| stats_o/std                    | 0.07389574 |
| test/episodes                  | 860        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -22.756748 |
| test/Q_plus_P                  | -22.756748 |
| test/reward_per_eps            | -40        |
| test/steps                     | 34400      |
| train/episodes                 | 3440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 137600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 86         |
| stats_o/mean                   | 0.20391357 |
| stats_o/std                    | 0.07407591 |
| test/episodes                  | 870        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -23.172373 |
| test/Q_plus_P                  | -23.172373 |
| test/reward_per_eps            | -40        |
| test/steps                     | 34800      |
| train/episodes                 | 3480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 139200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 87         |
| stats_o/mean                   | 0.20393808 |
| stats_o/std                    | 0.0741575  |
| test/episodes                  | 880        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -23.038477 |
| test/Q_plus_P                  | -23.038477 |
| test/reward_per_eps            | -40        |
| test/steps                     | 35200      |
| train/episodes                 | 3520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 140800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 88          |
| stats_o/mean                   | 0.20388097  |
| stats_o/std                    | 0.074152686 |
| test/episodes                  | 890         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.179      |
| test/info_shaping_reward_min   | -0.393      |
| test/Q                         | -23.445765  |
| test/Q_plus_P                  | -23.445765  |
| test/reward_per_eps            | -40         |
| test/steps                     | 35600       |
| train/episodes                 | 3560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.203      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 142400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 89         |
| stats_o/mean                   | 0.2038663  |
| stats_o/std                    | 0.07427122 |
| test/episodes                  | 900        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.112     |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -23.480925 |
| test/Q_plus_P                  | -23.480925 |
| test/reward_per_eps            | -40        |
| test/steps                     | 36000      |
| train/episodes                 | 3600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 144000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 90         |
| stats_o/mean                   | 0.20388608 |
| stats_o/std                    | 0.07423677 |
| test/episodes                  | 910        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -23.76405  |
| test/Q_plus_P                  | -23.76405  |
| test/reward_per_eps            | -40        |
| test/steps                     | 36400      |
| train/episodes                 | 3640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 145600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 91         |
| stats_o/mean                   | 0.20387638 |
| stats_o/std                    | 0.0741785  |
| test/episodes                  | 920        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.129     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -23.997593 |
| test/Q_plus_P                  | -23.997593 |
| test/reward_per_eps            | -40        |
| test/steps                     | 36800      |
| train/episodes                 | 3680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 147200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 92         |
| stats_o/mean                   | 0.20382975 |
| stats_o/std                    | 0.07428608 |
| test/episodes                  | 930        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -23.956457 |
| test/Q_plus_P                  | -23.956457 |
| test/reward_per_eps            | -40        |
| test/steps                     | 37200      |
| train/episodes                 | 3720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 148800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 93         |
| stats_o/mean                   | 0.20387569 |
| stats_o/std                    | 0.0742407  |
| test/episodes                  | 940        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.177     |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -24.431519 |
| test/Q_plus_P                  | -24.431519 |
| test/reward_per_eps            | -40        |
| test/steps                     | 37600      |
| train/episodes                 | 3760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 150400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 94         |
| stats_o/mean                   | 0.20387588 |
| stats_o/std                    | 0.07408702 |
| test/episodes                  | 950        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -24.418135 |
| test/Q_plus_P                  | -24.418135 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38000      |
| train/episodes                 | 3800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 152000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 95         |
| stats_o/mean                   | 0.20388632 |
| stats_o/std                    | 0.07392751 |
| test/episodes                  | 960        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -24.60568  |
| test/Q_plus_P                  | -24.60568  |
| test/reward_per_eps            | -40        |
| test/steps                     | 38400      |
| train/episodes                 | 3840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 153600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 96         |
| stats_o/mean                   | 0.20391136 |
| stats_o/std                    | 0.07377668 |
| test/episodes                  | 970        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -24.831327 |
| test/Q_plus_P                  | -24.831327 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38800      |
| train/episodes                 | 3880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 155200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 97          |
| stats_o/mean                   | 0.2038987   |
| stats_o/std                    | 0.073598474 |
| test/episodes                  | 980         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.172      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -24.97609   |
| test/Q_plus_P                  | -24.97609   |
| test/reward_per_eps            | -40         |
| test/steps                     | 39200       |
| train/episodes                 | 3920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 156800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 98          |
| stats_o/mean                   | 0.20386861  |
| stats_o/std                    | 0.073818475 |
| test/episodes                  | 990         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.191      |
| test/Q                         | -25.289316  |
| test/Q_plus_P                  | -25.289316  |
| test/reward_per_eps            | -40         |
| test/steps                     | 39600       |
| train/episodes                 | 3960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.181      |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 158400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.20387831 |
| stats_o/std                    | 0.07371287 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.209     |
| test/Q                         | -25.30313  |
| test/Q_plus_P                  | -25.30313  |
| test/reward_per_eps            | -40        |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 100        |
| stats_o/mean                   | 0.20393819 |
| stats_o/std                    | 0.0739862  |
| test/episodes                  | 1010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.145     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -25.515942 |
| test/Q_plus_P                  | -25.515942 |
| test/reward_per_eps            | -40        |
| test/steps                     | 40400      |
| train/episodes                 | 4040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.293     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 161600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 101        |
| stats_o/mean                   | 0.20390348 |
| stats_o/std                    | 0.07402123 |
| test/episodes                  | 1020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.1       |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -25.752325 |
| test/Q_plus_P                  | -25.752325 |
| test/reward_per_eps            | -40        |
| test/steps                     | 40800      |
| train/episodes                 | 4080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 163200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 102        |
| stats_o/mean                   | 0.20386198 |
| stats_o/std                    | 0.07387733 |
| test/episodes                  | 1030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -1         |
| test/Q                         | -25.916006 |
| test/Q_plus_P                  | -25.916006 |
| test/reward_per_eps            | -40        |
| test/steps                     | 41200      |
| train/episodes                 | 4120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.274     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 164800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 103         |
| stats_o/mean                   | 0.20384203  |
| stats_o/std                    | 0.073724106 |
| test/episodes                  | 1040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.172      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -25.986326  |
| test/Q_plus_P                  | -25.986326  |
| test/reward_per_eps            | -40         |
| test/steps                     | 41600       |
| train/episodes                 | 4160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.21       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 166400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 104         |
| stats_o/mean                   | 0.20385714  |
| stats_o/std                    | 0.073660836 |
| test/episodes                  | 1050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.179      |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -26.206612  |
| test/Q_plus_P                  | -26.206612  |
| test/reward_per_eps            | -40         |
| test/steps                     | 42000       |
| train/episodes                 | 4200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 168000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 105        |
| stats_o/mean                   | 0.20382056 |
| stats_o/std                    | 0.07348552 |
| test/episodes                  | 1060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -26.14674  |
| test/Q_plus_P                  | -26.14674  |
| test/reward_per_eps            | -40        |
| test/steps                     | 42400      |
| train/episodes                 | 4240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 169600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 106         |
| stats_o/mean                   | 0.20380823  |
| stats_o/std                    | 0.073376305 |
| test/episodes                  | 1070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -26.418037  |
| test/Q_plus_P                  | -26.418037  |
| test/reward_per_eps            | -40         |
| test/steps                     | 42800       |
| train/episodes                 | 4280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 171200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 107        |
| stats_o/mean                   | 0.20378296 |
| stats_o/std                    | 0.07333818 |
| test/episodes                  | 1080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -26.495562 |
| test/Q_plus_P                  | -26.495562 |
| test/reward_per_eps            | -40        |
| test/steps                     | 43200      |
| train/episodes                 | 4320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 172800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 108        |
| stats_o/mean                   | 0.203763   |
| stats_o/std                    | 0.0731747  |
| test/episodes                  | 1090       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0885    |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -27.046867 |
| test/Q_plus_P                  | -27.046867 |
| test/reward_per_eps            | -40        |
| test/steps                     | 43600      |
| train/episodes                 | 4360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 174400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 109         |
| stats_o/mean                   | 0.20377989  |
| stats_o/std                    | 0.072995536 |
| test/episodes                  | 1100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.175      |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -26.8979    |
| test/Q_plus_P                  | -26.8979    |
| test/reward_per_eps            | -40         |
| test/steps                     | 44000       |
| train/episodes                 | 4400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 176000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 110        |
| stats_o/mean                   | 0.20378919 |
| stats_o/std                    | 0.0728221  |
| test/episodes                  | 1110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -27.294844 |
| test/Q_plus_P                  | -27.294844 |
| test/reward_per_eps            | -40        |
| test/steps                     | 44400      |
| train/episodes                 | 4440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 177600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 111        |
| stats_o/mean                   | 0.20378447 |
| stats_o/std                    | 0.07270195 |
| test/episodes                  | 1120       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.133     |
| test/info_shaping_reward_mean  | -0.185     |
| test/info_shaping_reward_min   | -0.601     |
| test/Q                         | -27.196018 |
| test/Q_plus_P                  | -27.196018 |
| test/reward_per_eps            | -40        |
| test/steps                     | 44800      |
| train/episodes                 | 4480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 179200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 112         |
| stats_o/mean                   | 0.2037801   |
| stats_o/std                    | 0.072605364 |
| test/episodes                  | 1130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -27.49823   |
| test/Q_plus_P                  | -27.49823   |
| test/reward_per_eps            | -40         |
| test/steps                     | 45200       |
| train/episodes                 | 4520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.198      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 180800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 113        |
| stats_o/mean                   | 0.20377441 |
| stats_o/std                    | 0.07254896 |
| test/episodes                  | 1140       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -27.605713 |
| test/Q_plus_P                  | -27.605713 |
| test/reward_per_eps            | -40        |
| test/steps                     | 45600      |
| train/episodes                 | 4560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 182400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 114        |
| stats_o/mean                   | 0.20374583 |
| stats_o/std                    | 0.07244683 |
| test/episodes                  | 1150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -27.812122 |
| test/Q_plus_P                  | -27.812122 |
| test/reward_per_eps            | -40        |
| test/steps                     | 46000      |
| train/episodes                 | 4600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 184000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 115         |
| stats_o/mean                   | 0.20374194  |
| stats_o/std                    | 0.072370656 |
| test/episodes                  | 1160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -27.802979  |
| test/Q_plus_P                  | -27.802979  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46400       |
| train/episodes                 | 4640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 185600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 116        |
| stats_o/mean                   | 0.20375217 |
| stats_o/std                    | 0.07235786 |
| test/episodes                  | 1170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -27.845667 |
| test/Q_plus_P                  | -27.845667 |
| test/reward_per_eps            | -40        |
| test/steps                     | 46800      |
| train/episodes                 | 4680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 187200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 117         |
| stats_o/mean                   | 0.20374177  |
| stats_o/std                    | 0.072280474 |
| test/episodes                  | 1180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.139      |
| test/info_shaping_reward_mean  | -0.175      |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -28.266386  |
| test/Q_plus_P                  | -28.266386  |
| test/reward_per_eps            | -40         |
| test/steps                     | 47200       |
| train/episodes                 | 4720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 188800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.20370676  |
| stats_o/std                    | 0.072153896 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -28.24621   |
| test/Q_plus_P                  | -28.24621   |
| test/reward_per_eps            | -40         |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 119        |
| stats_o/mean                   | 0.2036962  |
| stats_o/std                    | 0.0721022  |
| test/episodes                  | 1200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.149     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -28.290205 |
| test/Q_plus_P                  | -28.290205 |
| test/reward_per_eps            | -40        |
| test/steps                     | 48000      |
| train/episodes                 | 4800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 192000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 120        |
| stats_o/mean                   | 0.20367682 |
| stats_o/std                    | 0.07203647 |
| test/episodes                  | 1210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -28.501158 |
| test/Q_plus_P                  | -28.501158 |
| test/reward_per_eps            | -40        |
| test/steps                     | 48400      |
| train/episodes                 | 4840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 193600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 121         |
| stats_o/mean                   | 0.2036742   |
| stats_o/std                    | 0.071892984 |
| test/episodes                  | 1220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -28.704697  |
| test/Q_plus_P                  | -28.704697  |
| test/reward_per_eps            | -40         |
| test/steps                     | 48800       |
| train/episodes                 | 4880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 195200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 122        |
| stats_o/mean                   | 0.20366621 |
| stats_o/std                    | 0.0717626  |
| test/episodes                  | 1230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -28.82911  |
| test/Q_plus_P                  | -28.82911  |
| test/reward_per_eps            | -40        |
| test/steps                     | 49200      |
| train/episodes                 | 4920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 196800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 123        |
| stats_o/mean                   | 0.20367508 |
| stats_o/std                    | 0.07167245 |
| test/episodes                  | 1240       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.179     |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -29.024487 |
| test/Q_plus_P                  | -29.024487 |
| test/reward_per_eps            | -40        |
| test/steps                     | 49600      |
| train/episodes                 | 4960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 198400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 124        |
| stats_o/mean                   | 0.20371859 |
| stats_o/std                    | 0.07153695 |
| test/episodes                  | 1250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -28.985222 |
| test/Q_plus_P                  | -28.985222 |
| test/reward_per_eps            | -40        |
| test/steps                     | 50000      |
| train/episodes                 | 5000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 200000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 125        |
| stats_o/mean                   | 0.2037275  |
| stats_o/std                    | 0.07139419 |
| test/episodes                  | 1260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -29.25035  |
| test/Q_plus_P                  | -29.25035  |
| test/reward_per_eps            | -40        |
| test/steps                     | 50400      |
| train/episodes                 | 5040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 201600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 126        |
| stats_o/mean                   | 0.20374613 |
| stats_o/std                    | 0.07124726 |
| test/episodes                  | 1270       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.134     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -29.528326 |
| test/Q_plus_P                  | -29.528326 |
| test/reward_per_eps            | -40        |
| test/steps                     | 50800      |
| train/episodes                 | 5080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 203200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 127        |
| stats_o/mean                   | 0.20377688 |
| stats_o/std                    | 0.0712143  |
| test/episodes                  | 1280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -29.40055  |
| test/Q_plus_P                  | -29.40055  |
| test/reward_per_eps            | -40        |
| test/steps                     | 51200      |
| train/episodes                 | 5120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 204800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.20379986 |
| stats_o/std                    | 0.07130323 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -29.626328 |
| test/Q_plus_P                  | -29.626328 |
| test/reward_per_eps            | -40        |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 129        |
| stats_o/mean                   | 0.20380464 |
| stats_o/std                    | 0.07130357 |
| test/episodes                  | 1300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -29.847912 |
| test/Q_plus_P                  | -29.847912 |
| test/reward_per_eps            | -40        |
| test/steps                     | 52000      |
| train/episodes                 | 5200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 208000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 130         |
| stats_o/mean                   | 0.20379741  |
| stats_o/std                    | 0.071150616 |
| test/episodes                  | 1310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.193      |
| test/Q                         | -29.987202  |
| test/Q_plus_P                  | -29.987202  |
| test/reward_per_eps            | -40         |
| test/steps                     | 52400       |
| train/episodes                 | 5240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 209600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 131         |
| stats_o/mean                   | 0.20380387  |
| stats_o/std                    | 0.071108945 |
| test/episodes                  | 1320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.199      |
| test/Q                         | -29.964266  |
| test/Q_plus_P                  | -29.964266  |
| test/reward_per_eps            | -40         |
| test/steps                     | 52800       |
| train/episodes                 | 5280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 211200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 132        |
| stats_o/mean                   | 0.20377691 |
| stats_o/std                    | 0.07115292 |
| test/episodes                  | 1330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.132     |
| test/info_shaping_reward_mean  | -0.189     |
| test/info_shaping_reward_min   | -0.676     |
| test/Q                         | -30.17321  |
| test/Q_plus_P                  | -30.17321  |
| test/reward_per_eps            | -40        |
| test/steps                     | 53200      |
| train/episodes                 | 5320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 212800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 133        |
| stats_o/mean                   | 0.20375472 |
| stats_o/std                    | 0.07108435 |
| test/episodes                  | 1340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -29.98644  |
| test/Q_plus_P                  | -29.98644  |
| test/reward_per_eps            | -40        |
| test/steps                     | 53600      |
| train/episodes                 | 5360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 214400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 134        |
| stats_o/mean                   | 0.20373227 |
| stats_o/std                    | 0.07121689 |
| test/episodes                  | 1350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -30.256548 |
| test/Q_plus_P                  | -30.256548 |
| test/reward_per_eps            | -40        |
| test/steps                     | 54000      |
| train/episodes                 | 5400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 216000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 135        |
| stats_o/mean                   | 0.20373751 |
| stats_o/std                    | 0.07123456 |
| test/episodes                  | 1360       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -29.222027 |
| test/Q_plus_P                  | -29.222027 |
| test/reward_per_eps            | -40        |
| test/steps                     | 54400      |
| train/episodes                 | 5440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.295     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 217600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 136        |
| stats_o/mean                   | 0.20370927 |
| stats_o/std                    | 0.07139208 |
| test/episodes                  | 1370       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -30.160288 |
| test/Q_plus_P                  | -30.160288 |
| test/reward_per_eps            | -40        |
| test/steps                     | 54800      |
| train/episodes                 | 5480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 219200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 137        |
| stats_o/mean                   | 0.20372507 |
| stats_o/std                    | 0.07140215 |
| test/episodes                  | 1380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.226     |
| test/Q                         | -30.06715  |
| test/Q_plus_P                  | -30.06715  |
| test/reward_per_eps            | -40        |
| test/steps                     | 55200      |
| train/episodes                 | 5520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 220800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.20372798 |
| stats_o/std                    | 0.07133449 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -30.031988 |
| test/Q_plus_P                  | -30.031988 |
| test/reward_per_eps            | -40        |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.2037077  |
| stats_o/std                    | 0.07125156 |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.217     |
| test/Q                         | -30.27898  |
| test/Q_plus_P                  | -30.27898  |
| test/reward_per_eps            | -40        |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 140        |
| stats_o/mean                   | 0.2037369  |
| stats_o/std                    | 0.07128306 |
| test/episodes                  | 1410       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -30.544458 |
| test/Q_plus_P                  | -30.544458 |
| test/reward_per_eps            | -40        |
| test/steps                     | 56400      |
| train/episodes                 | 5640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 225600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 141         |
| stats_o/mean                   | 0.20372242  |
| stats_o/std                    | 0.071235515 |
| test/episodes                  | 1420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -30.584688  |
| test/Q_plus_P                  | -30.584688  |
| test/reward_per_eps            | -40         |
| test/steps                     | 56800       |
| train/episodes                 | 5680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 227200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 142        |
| stats_o/mean                   | 0.20372856 |
| stats_o/std                    | 0.0711414  |
| test/episodes                  | 1430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -30.79024  |
| test/Q_plus_P                  | -30.79024  |
| test/reward_per_eps            | -40        |
| test/steps                     | 57200      |
| train/episodes                 | 5720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 228800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 143        |
| stats_o/mean                   | 0.2037156  |
| stats_o/std                    | 0.07106887 |
| test/episodes                  | 1440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -30.564001 |
| test/Q_plus_P                  | -30.564001 |
| test/reward_per_eps            | -40        |
| test/steps                     | 57600      |
| train/episodes                 | 5760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 230400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 144        |
| stats_o/mean                   | 0.20370077 |
| stats_o/std                    | 0.0709928  |
| test/episodes                  | 1450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -30.481663 |
| test/Q_plus_P                  | -30.481663 |
| test/reward_per_eps            | -40        |
| test/steps                     | 58000      |
| train/episodes                 | 5800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 232000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 145        |
| stats_o/mean                   | 0.20372027 |
| stats_o/std                    | 0.07089815 |
| test/episodes                  | 1460       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -31.087029 |
| test/Q_plus_P                  | -31.087029 |
| test/reward_per_eps            | -40        |
| test/steps                     | 58400      |
| train/episodes                 | 5840       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00313    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 233600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 146        |
| stats_o/mean                   | 0.20370932 |
| stats_o/std                    | 0.07088444 |
| test/episodes                  | 1470       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.131     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -31.276533 |
| test/Q_plus_P                  | -31.276533 |
| test/reward_per_eps            | -40        |
| test/steps                     | 58800      |
| train/episodes                 | 5880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 235200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 147        |
| stats_o/mean                   | 0.20369741 |
| stats_o/std                    | 0.07081286 |
| test/episodes                  | 1480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -31.005596 |
| test/Q_plus_P                  | -31.005596 |
| test/reward_per_eps            | -40        |
| test/steps                     | 59200      |
| train/episodes                 | 5920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 236800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 148         |
| stats_o/mean                   | 0.20366764  |
| stats_o/std                    | 0.070719406 |
| test/episodes                  | 1490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -30.954222  |
| test/Q_plus_P                  | -30.954222  |
| test/reward_per_eps            | -40         |
| test/steps                     | 59600       |
| train/episodes                 | 5960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 238400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 149        |
| stats_o/mean                   | 0.20367818 |
| stats_o/std                    | 0.07062261 |
| test/episodes                  | 1500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.138     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -31.689995 |
| test/Q_plus_P                  | -31.689995 |
| test/reward_per_eps            | -40        |
| test/steps                     | 60000      |
| train/episodes                 | 6000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 240000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 150         |
| stats_o/mean                   | 0.20368066  |
| stats_o/std                    | 0.070542544 |
| test/episodes                  | 1510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -31.875416  |
| test/Q_plus_P                  | -31.875416  |
| test/reward_per_eps            | -40         |
| test/steps                     | 60400       |
| train/episodes                 | 6040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 241600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 151         |
| stats_o/mean                   | 0.20369929  |
| stats_o/std                    | 0.070442826 |
| test/episodes                  | 1520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -31.936491  |
| test/Q_plus_P                  | -31.936491  |
| test/reward_per_eps            | -40         |
| test/steps                     | 60800       |
| train/episodes                 | 6080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 243200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 152         |
| stats_o/mean                   | 0.2037041   |
| stats_o/std                    | 0.070351355 |
| test/episodes                  | 1530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -31.702374  |
| test/Q_plus_P                  | -31.702374  |
| test/reward_per_eps            | -40         |
| test/steps                     | 61200       |
| train/episodes                 | 6120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 244800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 153        |
| stats_o/mean                   | 0.20370376 |
| stats_o/std                    | 0.07025663 |
| test/episodes                  | 1540       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -31.914213 |
| test/Q_plus_P                  | -31.914213 |
| test/reward_per_eps            | -40        |
| test/steps                     | 61600      |
| train/episodes                 | 6160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 246400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 154         |
| stats_o/mean                   | 0.20370318  |
| stats_o/std                    | 0.070211425 |
| test/episodes                  | 1550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -32.164604  |
| test/Q_plus_P                  | -32.164604  |
| test/reward_per_eps            | -40         |
| test/steps                     | 62000       |
| train/episodes                 | 6200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 248000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 155         |
| stats_o/mean                   | 0.20372938  |
| stats_o/std                    | 0.070107184 |
| test/episodes                  | 1560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -31.967016  |
| test/Q_plus_P                  | -31.967016  |
| test/reward_per_eps            | -40         |
| test/steps                     | 62400       |
| train/episodes                 | 6240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 249600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 156         |
| stats_o/mean                   | 0.20370805  |
| stats_o/std                    | 0.070021674 |
| test/episodes                  | 1570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -31.747124  |
| test/Q_plus_P                  | -31.747124  |
| test/reward_per_eps            | -40         |
| test/steps                     | 62800       |
| train/episodes                 | 6280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 251200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 157         |
| stats_o/mean                   | 0.20370546  |
| stats_o/std                    | 0.069922194 |
| test/episodes                  | 1580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -32.63513   |
| test/Q_plus_P                  | -32.63513   |
| test/reward_per_eps            | -40         |
| test/steps                     | 63200       |
| train/episodes                 | 6320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 252800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 158        |
| stats_o/mean                   | 0.20372222 |
| stats_o/std                    | 0.06982101 |
| test/episodes                  | 1590       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -32.255295 |
| test/Q_plus_P                  | -32.255295 |
| test/reward_per_eps            | -40        |
| test/steps                     | 63600      |
| train/episodes                 | 6360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 254400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 159        |
| stats_o/mean                   | 0.20374241 |
| stats_o/std                    | 0.06971268 |
| test/episodes                  | 1600       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -32.714382 |
| test/Q_plus_P                  | -32.714382 |
| test/reward_per_eps            | -40        |
| test/steps                     | 64000      |
| train/episodes                 | 6400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 256000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 160        |
| stats_o/mean                   | 0.20376545 |
| stats_o/std                    | 0.06960004 |
| test/episodes                  | 1610       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -32.486916 |
| test/Q_plus_P                  | -32.486916 |
| test/reward_per_eps            | -40        |
| test/steps                     | 64400      |
| train/episodes                 | 6440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 257600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.20376144  |
| stats_o/std                    | 0.069508046 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -32.48462   |
| test/Q_plus_P                  | -32.48462   |
| test/reward_per_eps            | -40         |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 162        |
| stats_o/mean                   | 0.20377621 |
| stats_o/std                    | 0.0694781  |
| test/episodes                  | 1630       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -32.660168 |
| test/Q_plus_P                  | -32.660168 |
| test/reward_per_eps            | -40        |
| test/steps                     | 65200      |
| train/episodes                 | 6520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 260800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 163         |
| stats_o/mean                   | 0.20377849  |
| stats_o/std                    | 0.069384724 |
| test/episodes                  | 1640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -32.56119   |
| test/Q_plus_P                  | -32.56119   |
| test/reward_per_eps            | -40         |
| test/steps                     | 65600       |
| train/episodes                 | 6560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 262400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 164        |
| stats_o/mean                   | 0.20378771 |
| stats_o/std                    | 0.06928463 |
| test/episodes                  | 1650       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -32.686394 |
| test/Q_plus_P                  | -32.686394 |
| test/reward_per_eps            | -40        |
| test/steps                     | 66000      |
| train/episodes                 | 6600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 264000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 165        |
| stats_o/mean                   | 0.20379055 |
| stats_o/std                    | 0.06917987 |
| test/episodes                  | 1660       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -32.84847  |
| test/Q_plus_P                  | -32.84847  |
| test/reward_per_eps            | -40        |
| test/steps                     | 66400      |
| train/episodes                 | 6640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 265600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 166        |
| stats_o/mean                   | 0.20381947 |
| stats_o/std                    | 0.06908498 |
| test/episodes                  | 1670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -32.948414 |
| test/Q_plus_P                  | -32.948414 |
| test/reward_per_eps            | -40        |
| test/steps                     | 66800      |
| train/episodes                 | 6680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 267200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.20382059 |
| stats_o/std                    | 0.06899316 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -33.33658  |
| test/Q_plus_P                  | -33.33658  |
| test/reward_per_eps            | -40        |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 168        |
| stats_o/mean                   | 0.20382902 |
| stats_o/std                    | 0.06891002 |
| test/episodes                  | 1690       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -33.145565 |
| test/Q_plus_P                  | -33.145565 |
| test/reward_per_eps            | -40        |
| test/steps                     | 67600      |
| train/episodes                 | 6760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 270400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.20382957 |
| stats_o/std                    | 0.0688205  |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -32.85241  |
| test/Q_plus_P                  | -32.85241  |
| test/reward_per_eps            | -40        |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 170        |
| stats_o/mean                   | 0.20383795 |
| stats_o/std                    | 0.06872097 |
| test/episodes                  | 1710       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -33.3226   |
| test/Q_plus_P                  | -33.3226   |
| test/reward_per_eps            | -40        |
| test/steps                     | 68400      |
| train/episodes                 | 6840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 273600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 171         |
| stats_o/mean                   | 0.20383598  |
| stats_o/std                    | 0.068637475 |
| test/episodes                  | 1720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -33.135033  |
| test/Q_plus_P                  | -33.135033  |
| test/reward_per_eps            | -40         |
| test/steps                     | 68800       |
| train/episodes                 | 6880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 275200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 172        |
| stats_o/mean                   | 0.20380326 |
| stats_o/std                    | 0.06861118 |
| test/episodes                  | 1730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -33.386898 |
| test/Q_plus_P                  | -33.386898 |
| test/reward_per_eps            | -40        |
| test/steps                     | 69200      |
| train/episodes                 | 6920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 276800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 173        |
| stats_o/mean                   | 0.2038182  |
| stats_o/std                    | 0.06852015 |
| test/episodes                  | 1740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -33.256107 |
| test/Q_plus_P                  | -33.256107 |
| test/reward_per_eps            | -40        |
| test/steps                     | 69600      |
| train/episodes                 | 6960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 278400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 174        |
| stats_o/mean                   | 0.2038451  |
| stats_o/std                    | 0.0684303  |
| test/episodes                  | 1750       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -33.556263 |
| test/Q_plus_P                  | -33.556263 |
| test/reward_per_eps            | -40        |
| test/steps                     | 70000      |
| train/episodes                 | 7000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 280000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 175        |
| stats_o/mean                   | 0.20384714 |
| stats_o/std                    | 0.06833169 |
| test/episodes                  | 1760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -33.479317 |
| test/Q_plus_P                  | -33.479317 |
| test/reward_per_eps            | -40        |
| test/steps                     | 70400      |
| train/episodes                 | 7040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 281600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 176        |
| stats_o/mean                   | 0.20384371 |
| stats_o/std                    | 0.06827033 |
| test/episodes                  | 1770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0932    |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -33.69769  |
| test/Q_plus_P                  | -33.69769  |
| test/reward_per_eps            | -40        |
| test/steps                     | 70800      |
| train/episodes                 | 7080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 283200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.20386899  |
| stats_o/std                    | 0.068171114 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -33.68656   |
| test/Q_plus_P                  | -33.68656   |
| test/reward_per_eps            | -40         |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.20387883 |
| stats_o/std                    | 0.06808814 |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -34.04006  |
| test/Q_plus_P                  | -34.04006  |
| test/reward_per_eps            | -40        |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 179         |
| stats_o/mean                   | 0.20390229  |
| stats_o/std                    | 0.068040796 |
| test/episodes                  | 1800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -33.926952  |
| test/Q_plus_P                  | -33.926952  |
| test/reward_per_eps            | -40         |
| test/steps                     | 72000       |
| train/episodes                 | 7200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 288000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 180         |
| stats_o/mean                   | 0.20391193  |
| stats_o/std                    | 0.067955025 |
| test/episodes                  | 1810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.106      |
| test/info_shaping_reward_mean  | -0.172      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -34.123074  |
| test/Q_plus_P                  | -34.123074  |
| test/reward_per_eps            | -40         |
| test/steps                     | 72400       |
| train/episodes                 | 7240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 289600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 181         |
| stats_o/mean                   | 0.20391388  |
| stats_o/std                    | 0.067866355 |
| test/episodes                  | 1820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -34.13604   |
| test/Q_plus_P                  | -34.13604   |
| test/reward_per_eps            | -40         |
| test/steps                     | 72800       |
| train/episodes                 | 7280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 291200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 182        |
| stats_o/mean                   | 0.20389828 |
| stats_o/std                    | 0.06778063 |
| test/episodes                  | 1830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -34.126514 |
| test/Q_plus_P                  | -34.126514 |
| test/reward_per_eps            | -40        |
| test/steps                     | 73200      |
| train/episodes                 | 7320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 292800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 183        |
| stats_o/mean                   | 0.20391503 |
| stats_o/std                    | 0.06769058 |
| test/episodes                  | 1840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -34.23395  |
| test/Q_plus_P                  | -34.23395  |
| test/reward_per_eps            | -40        |
| test/steps                     | 73600      |
| train/episodes                 | 7360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 294400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 184        |
| stats_o/mean                   | 0.20391293 |
| stats_o/std                    | 0.06760461 |
| test/episodes                  | 1850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -34.297188 |
| test/Q_plus_P                  | -34.297188 |
| test/reward_per_eps            | -40        |
| test/steps                     | 74000      |
| train/episodes                 | 7400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 296000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 185        |
| stats_o/mean                   | 0.20391124 |
| stats_o/std                    | 0.06758496 |
| test/episodes                  | 1860       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -34.462704 |
| test/Q_plus_P                  | -34.462704 |
| test/reward_per_eps            | -40        |
| test/steps                     | 74400      |
| train/episodes                 | 7440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 297600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 186         |
| stats_o/mean                   | 0.20391238  |
| stats_o/std                    | 0.067538865 |
| test/episodes                  | 1870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -34.430817  |
| test/Q_plus_P                  | -34.430817  |
| test/reward_per_eps            | -40         |
| test/steps                     | 74800       |
| train/episodes                 | 7480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 299200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.20390695 |
| stats_o/std                    | 0.06745767 |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -34.537487 |
| test/Q_plus_P                  | -34.537487 |
| test/reward_per_eps            | -40        |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 188         |
| stats_o/mean                   | 0.20388685  |
| stats_o/std                    | 0.067375496 |
| test/episodes                  | 1890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -34.4029    |
| test/Q_plus_P                  | -34.4029    |
| test/reward_per_eps            | -40         |
| test/steps                     | 75600       |
| train/episodes                 | 7560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 302400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 189        |
| stats_o/mean                   | 0.20388912 |
| stats_o/std                    | 0.06729202 |
| test/episodes                  | 1900       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -34.64157  |
| test/Q_plus_P                  | -34.64157  |
| test/reward_per_eps            | -40        |
| test/steps                     | 76000      |
| train/episodes                 | 7600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 304000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 190        |
| stats_o/mean                   | 0.203896   |
| stats_o/std                    | 0.06720758 |
| test/episodes                  | 1910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -34.649246 |
| test/Q_plus_P                  | -34.649246 |
| test/reward_per_eps            | -40        |
| test/steps                     | 76400      |
| train/episodes                 | 7640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 305600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 191         |
| stats_o/mean                   | 0.20389368  |
| stats_o/std                    | 0.067130305 |
| test/episodes                  | 1920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -34.69151   |
| test/Q_plus_P                  | -34.69151   |
| test/reward_per_eps            | -40         |
| test/steps                     | 76800       |
| train/episodes                 | 7680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 307200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 192        |
| stats_o/mean                   | 0.2038871  |
| stats_o/std                    | 0.06704517 |
| test/episodes                  | 1930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.149     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -34.826794 |
| test/Q_plus_P                  | -34.826794 |
| test/reward_per_eps            | -40        |
| test/steps                     | 77200      |
| train/episodes                 | 7720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 308800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 193        |
| stats_o/mean                   | 0.20387684 |
| stats_o/std                    | 0.0669598  |
| test/episodes                  | 1940       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -34.826096 |
| test/Q_plus_P                  | -34.826096 |
| test/reward_per_eps            | -40        |
| test/steps                     | 77600      |
| train/episodes                 | 7760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 310400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 194        |
| stats_o/mean                   | 0.20388043 |
| stats_o/std                    | 0.06688056 |
| test/episodes                  | 1950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -34.892853 |
| test/Q_plus_P                  | -34.892853 |
| test/reward_per_eps            | -40        |
| test/steps                     | 78000      |
| train/episodes                 | 7800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 312000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 195         |
| stats_o/mean                   | 0.20388074  |
| stats_o/std                    | 0.066803865 |
| test/episodes                  | 1960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.193      |
| test/Q                         | -35.125927  |
| test/Q_plus_P                  | -35.125927  |
| test/reward_per_eps            | -40         |
| test/steps                     | 78400       |
| train/episodes                 | 7840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 313600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 196         |
| stats_o/mean                   | 0.20387854  |
| stats_o/std                    | 0.066729546 |
| test/episodes                  | 1970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -35.059998  |
| test/Q_plus_P                  | -35.059998  |
| test/reward_per_eps            | -40         |
| test/steps                     | 78800       |
| train/episodes                 | 7880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 315200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 197         |
| stats_o/mean                   | 0.20387802  |
| stats_o/std                    | 0.066654705 |
| test/episodes                  | 1980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -35.2224    |
| test/Q_plus_P                  | -35.2224    |
| test/reward_per_eps            | -40         |
| test/steps                     | 79200       |
| train/episodes                 | 7920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 316800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 198        |
| stats_o/mean                   | 0.20387614 |
| stats_o/std                    | 0.06657824 |
| test/episodes                  | 1990       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -35.282524 |
| test/Q_plus_P                  | -35.282524 |
| test/reward_per_eps            | -40        |
| test/steps                     | 79600      |
| train/episodes                 | 7960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 318400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.20385933 |
| stats_o/std                    | 0.06650486 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -35.152576 |
| test/Q_plus_P                  | -35.152576 |
| test/reward_per_eps            | -40        |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 200        |
| stats_o/mean                   | 0.20384783 |
| stats_o/std                    | 0.0664863  |
| test/episodes                  | 2010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -35.209835 |
| test/Q_plus_P                  | -35.209835 |
| test/reward_per_eps            | -40        |
| test/steps                     | 80400      |
| train/episodes                 | 8040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 321600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 201        |
| stats_o/mean                   | 0.20383972 |
| stats_o/std                    | 0.0664084  |
| test/episodes                  | 2020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -35.346336 |
| test/Q_plus_P                  | -35.346336 |
| test/reward_per_eps            | -40        |
| test/steps                     | 80800      |
| train/episodes                 | 8080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 323200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 202        |
| stats_o/mean                   | 0.20384204 |
| stats_o/std                    | 0.06644213 |
| test/episodes                  | 2030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.145     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -35.524532 |
| test/Q_plus_P                  | -35.524532 |
| test/reward_per_eps            | -40        |
| test/steps                     | 81200      |
| train/episodes                 | 8120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 324800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 203        |
| stats_o/mean                   | 0.20384777 |
| stats_o/std                    | 0.06636041 |
| test/episodes                  | 2040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -35.510406 |
| test/Q_plus_P                  | -35.510406 |
| test/reward_per_eps            | -40        |
| test/steps                     | 81600      |
| train/episodes                 | 8160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 326400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 204        |
| stats_o/mean                   | 0.20383312 |
| stats_o/std                    | 0.06628631 |
| test/episodes                  | 2050       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -35.63743  |
| test/Q_plus_P                  | -35.63743  |
| test/reward_per_eps            | -40        |
| test/steps                     | 82000      |
| train/episodes                 | 8200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 328000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 205         |
| stats_o/mean                   | 0.20383081  |
| stats_o/std                    | 0.066211134 |
| test/episodes                  | 2060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -35.48594   |
| test/Q_plus_P                  | -35.48594   |
| test/reward_per_eps            | -40         |
| test/steps                     | 82400       |
| train/episodes                 | 8240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 329600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 206        |
| stats_o/mean                   | 0.20382702 |
| stats_o/std                    | 0.06613967 |
| test/episodes                  | 2070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -35.74386  |
| test/Q_plus_P                  | -35.74386  |
| test/reward_per_eps            | -40        |
| test/steps                     | 82800      |
| train/episodes                 | 8280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 331200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.20382187 |
| stats_o/std                    | 0.06606704 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -35.464947 |
| test/Q_plus_P                  | -35.464947 |
| test/reward_per_eps            | -40        |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 208        |
| stats_o/mean                   | 0.20381759 |
| stats_o/std                    | 0.06599874 |
| test/episodes                  | 2090       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -35.634434 |
| test/Q_plus_P                  | -35.634434 |
| test/reward_per_eps            | -40        |
| test/steps                     | 83600      |
| train/episodes                 | 8360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 334400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 209        |
| stats_o/mean                   | 0.20382117 |
| stats_o/std                    | 0.06592699 |
| test/episodes                  | 2100       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -35.68262  |
| test/Q_plus_P                  | -35.68262  |
| test/reward_per_eps            | -40        |
| test/steps                     | 84000      |
| train/episodes                 | 8400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 336000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 210        |
| stats_o/mean                   | 0.20383261 |
| stats_o/std                    | 0.06585934 |
| test/episodes                  | 2110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -35.781105 |
| test/Q_plus_P                  | -35.781105 |
| test/reward_per_eps            | -40        |
| test/steps                     | 84400      |
| train/episodes                 | 8440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 337600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 211        |
| stats_o/mean                   | 0.20382655 |
| stats_o/std                    | 0.06578923 |
| test/episodes                  | 2120       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -35.938694 |
| test/Q_plus_P                  | -35.938694 |
| test/reward_per_eps            | -40        |
| test/steps                     | 84800      |
| train/episodes                 | 8480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 339200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 212        |
| stats_o/mean                   | 0.20383245 |
| stats_o/std                    | 0.06571417 |
| test/episodes                  | 2130       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -35.927696 |
| test/Q_plus_P                  | -35.927696 |
| test/reward_per_eps            | -40        |
| test/steps                     | 85200      |
| train/episodes                 | 8520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 340800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 213         |
| stats_o/mean                   | 0.20384546  |
| stats_o/std                    | 0.065655805 |
| test/episodes                  | 2140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -36.120617  |
| test/Q_plus_P                  | -36.120617  |
| test/reward_per_eps            | -40         |
| test/steps                     | 85600       |
| train/episodes                 | 8560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 342400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 214        |
| stats_o/mean                   | 0.20384663 |
| stats_o/std                    | 0.06558322 |
| test/episodes                  | 2150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.03604  |
| test/Q_plus_P                  | -36.03604  |
| test/reward_per_eps            | -40        |
| test/steps                     | 86000      |
| train/episodes                 | 8600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 344000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 215         |
| stats_o/mean                   | 0.20384045  |
| stats_o/std                    | 0.065512136 |
| test/episodes                  | 2160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -35.978603  |
| test/Q_plus_P                  | -35.978603  |
| test/reward_per_eps            | -40         |
| test/steps                     | 86400       |
| train/episodes                 | 8640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 345600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 216        |
| stats_o/mean                   | 0.20386003 |
| stats_o/std                    | 0.06544487 |
| test/episodes                  | 2170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.11316  |
| test/Q_plus_P                  | -36.11316  |
| test/reward_per_eps            | -40        |
| test/steps                     | 86800      |
| train/episodes                 | 8680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 347200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 217        |
| stats_o/mean                   | 0.2038775  |
| stats_o/std                    | 0.06538137 |
| test/episodes                  | 2180       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -36.259453 |
| test/Q_plus_P                  | -36.259453 |
| test/reward_per_eps            | -40        |
| test/steps                     | 87200      |
| train/episodes                 | 8720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 348800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 218        |
| stats_o/mean                   | 0.20386761 |
| stats_o/std                    | 0.06531519 |
| test/episodes                  | 2190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.09994  |
| test/Q_plus_P                  | -36.09994  |
| test/reward_per_eps            | -40        |
| test/steps                     | 87600      |
| train/episodes                 | 8760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 350400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.20386948  |
| stats_o/std                    | 0.065247685 |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -36.207508  |
| test/Q_plus_P                  | -36.207508  |
| test/reward_per_eps            | -40         |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 220        |
| stats_o/mean                   | 0.20385359 |
| stats_o/std                    | 0.06517659 |
| test/episodes                  | 2210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -36.27675  |
| test/Q_plus_P                  | -36.27675  |
| test/reward_per_eps            | -40        |
| test/steps                     | 88400      |
| train/episodes                 | 8840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 353600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 221         |
| stats_o/mean                   | 0.20384848  |
| stats_o/std                    | 0.065111056 |
| test/episodes                  | 2220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -36.314877  |
| test/Q_plus_P                  | -36.314877  |
| test/reward_per_eps            | -40         |
| test/steps                     | 88800       |
| train/episodes                 | 8880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 355200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.20386526 |
| stats_o/std                    | 0.06504747 |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.36126  |
| test/Q_plus_P                  | -36.36126  |
| test/reward_per_eps            | -40        |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 223         |
| stats_o/mean                   | 0.20387277  |
| stats_o/std                    | 0.064982794 |
| test/episodes                  | 2240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -36.32144   |
| test/Q_plus_P                  | -36.32144   |
| test/reward_per_eps            | -40         |
| test/steps                     | 89600       |
| train/episodes                 | 8960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 358400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.20386662 |
| stats_o/std                    | 0.0649179  |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.35834  |
| test/Q_plus_P                  | -36.35834  |
| test/reward_per_eps            | -40        |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 225        |
| stats_o/mean                   | 0.20388071 |
| stats_o/std                    | 0.06484542 |
| test/episodes                  | 2260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.45566  |
| test/Q_plus_P                  | -36.45566  |
| test/reward_per_eps            | -40        |
| test/steps                     | 90400      |
| train/episodes                 | 9040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 361600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 226        |
| stats_o/mean                   | 0.20389132 |
| stats_o/std                    | 0.06478251 |
| test/episodes                  | 2270       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.559944 |
| test/Q_plus_P                  | -36.559944 |
| test/reward_per_eps            | -40        |
| test/steps                     | 90800      |
| train/episodes                 | 9080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 363200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 227        |
| stats_o/mean                   | 0.20389266 |
| stats_o/std                    | 0.06471556 |
| test/episodes                  | 2280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.466568 |
| test/Q_plus_P                  | -36.466568 |
| test/reward_per_eps            | -40        |
| test/steps                     | 91200      |
| train/episodes                 | 9120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 364800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 228        |
| stats_o/mean                   | 0.2038945  |
| stats_o/std                    | 0.06465068 |
| test/episodes                  | 2290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.641514 |
| test/Q_plus_P                  | -36.641514 |
| test/reward_per_eps            | -40        |
| test/steps                     | 91600      |
| train/episodes                 | 9160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 366400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 229        |
| stats_o/mean                   | 0.2038938  |
| stats_o/std                    | 0.06458409 |
| test/episodes                  | 2300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.681137 |
| test/Q_plus_P                  | -36.681137 |
| test/reward_per_eps            | -40        |
| test/steps                     | 92000      |
| train/episodes                 | 9200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 368000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 230         |
| stats_o/mean                   | 0.2038917   |
| stats_o/std                    | 0.064517744 |
| test/episodes                  | 2310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -36.613052  |
| test/Q_plus_P                  | -36.613052  |
| test/reward_per_eps            | -40         |
| test/steps                     | 92400       |
| train/episodes                 | 9240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 369600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 231        |
| stats_o/mean                   | 0.20390192 |
| stats_o/std                    | 0.06444734 |
| test/episodes                  | 2320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.85545  |
| test/Q_plus_P                  | -36.85545  |
| test/reward_per_eps            | -40        |
| test/steps                     | 92800      |
| train/episodes                 | 9280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 371200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 232        |
| stats_o/mean                   | 0.20391323 |
| stats_o/std                    | 0.06438499 |
| test/episodes                  | 2330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.77536  |
| test/Q_plus_P                  | -36.77536  |
| test/reward_per_eps            | -40        |
| test/steps                     | 93200      |
| train/episodes                 | 9320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 372800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 233        |
| stats_o/mean                   | 0.20391525 |
| stats_o/std                    | 0.0643194  |
| test/episodes                  | 2340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.73833  |
| test/Q_plus_P                  | -36.73833  |
| test/reward_per_eps            | -40        |
| test/steps                     | 93600      |
| train/episodes                 | 9360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 374400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.20391224 |
| stats_o/std                    | 0.06425691 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.889473 |
| test/Q_plus_P                  | -36.889473 |
| test/reward_per_eps            | -40        |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.203909    |
| stats_o/std                    | 0.064194895 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -36.92765   |
| test/Q_plus_P                  | -36.92765   |
| test/reward_per_eps            | -40         |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 236        |
| stats_o/mean                   | 0.20391858 |
| stats_o/std                    | 0.06412965 |
| test/episodes                  | 2370       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.984028 |
| test/Q_plus_P                  | -36.984028 |
| test/reward_per_eps            | -40        |
| test/steps                     | 94800      |
| train/episodes                 | 9480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 379200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 237        |
| stats_o/mean                   | 0.2039128  |
| stats_o/std                    | 0.06407564 |
| test/episodes                  | 2380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -36.95136  |
| test/Q_plus_P                  | -36.95136  |
| test/reward_per_eps            | -40        |
| test/steps                     | 95200      |
| train/episodes                 | 9520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 380800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 238        |
| stats_o/mean                   | 0.20391901 |
| stats_o/std                    | 0.06401855 |
| test/episodes                  | 2390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.029125 |
| test/Q_plus_P                  | -37.029125 |
| test/reward_per_eps            | -40        |
| test/steps                     | 95600      |
| train/episodes                 | 9560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 382400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 239         |
| stats_o/mean                   | 0.20391807  |
| stats_o/std                    | 0.063954145 |
| test/episodes                  | 2400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.018177  |
| test/Q_plus_P                  | -37.018177  |
| test/reward_per_eps            | -40         |
| test/steps                     | 96000       |
| train/episodes                 | 9600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 384000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 240        |
| stats_o/mean                   | 0.20392206 |
| stats_o/std                    | 0.06388973 |
| test/episodes                  | 2410       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.079697 |
| test/Q_plus_P                  | -37.079697 |
| test/reward_per_eps            | -40        |
| test/steps                     | 96400      |
| train/episodes                 | 9640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 385600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 241        |
| stats_o/mean                   | 0.20392355 |
| stats_o/std                    | 0.06383021 |
| test/episodes                  | 2420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.076847 |
| test/Q_plus_P                  | -37.076847 |
| test/reward_per_eps            | -40        |
| test/steps                     | 96800      |
| train/episodes                 | 9680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 387200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 242         |
| stats_o/mean                   | 0.20394059  |
| stats_o/std                    | 0.063767955 |
| test/episodes                  | 2430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.233017  |
| test/Q_plus_P                  | -37.233017  |
| test/reward_per_eps            | -40         |
| test/steps                     | 97200       |
| train/episodes                 | 9720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 388800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 243        |
| stats_o/mean                   | 0.203947   |
| stats_o/std                    | 0.06370847 |
| test/episodes                  | 2440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.195423 |
| test/Q_plus_P                  | -37.195423 |
| test/reward_per_eps            | -40        |
| test/steps                     | 97600      |
| train/episodes                 | 9760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 390400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.20395036 |
| stats_o/std                    | 0.0636445  |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.145607 |
| test/Q_plus_P                  | -37.145607 |
| test/reward_per_eps            | -40        |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 245        |
| stats_o/mean                   | 0.20394218 |
| stats_o/std                    | 0.06358001 |
| test/episodes                  | 2460       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.24658  |
| test/Q_plus_P                  | -37.24658  |
| test/reward_per_eps            | -40        |
| test/steps                     | 98400      |
| train/episodes                 | 9840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 393600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 246        |
| stats_o/mean                   | 0.20394176 |
| stats_o/std                    | 0.06352495 |
| test/episodes                  | 2470       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.303    |
| test/Q_plus_P                  | -37.303    |
| test/reward_per_eps            | -40        |
| test/steps                     | 98800      |
| train/episodes                 | 9880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 395200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 247         |
| stats_o/mean                   | 0.20393497  |
| stats_o/std                    | 0.063466385 |
| test/episodes                  | 2480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.27728   |
| test/Q_plus_P                  | -37.27728   |
| test/reward_per_eps            | -40         |
| test/steps                     | 99200       |
| train/episodes                 | 9920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 396800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 248         |
| stats_o/mean                   | 0.20392677  |
| stats_o/std                    | 0.063406006 |
| test/episodes                  | 2490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.338676  |
| test/Q_plus_P                  | -37.338676  |
| test/reward_per_eps            | -40         |
| test/steps                     | 99600       |
| train/episodes                 | 9960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 398400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 249        |
| stats_o/mean                   | 0.2039288  |
| stats_o/std                    | 0.06334409 |
| test/episodes                  | 2500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.377426 |
| test/Q_plus_P                  | -37.377426 |
| test/reward_per_eps            | -40        |
| test/steps                     | 100000     |
| train/episodes                 | 10000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 400000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 250        |
| stats_o/mean                   | 0.20393275 |
| stats_o/std                    | 0.06328386 |
| test/episodes                  | 2510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.43289  |
| test/Q_plus_P                  | -37.43289  |
| test/reward_per_eps            | -40        |
| test/steps                     | 100400     |
| train/episodes                 | 10040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 401600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 251        |
| stats_o/mean                   | 0.20392069 |
| stats_o/std                    | 0.06322596 |
| test/episodes                  | 2520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.458424 |
| test/Q_plus_P                  | -37.458424 |
| test/reward_per_eps            | -40        |
| test/steps                     | 100800     |
| train/episodes                 | 10080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 403200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 252         |
| stats_o/mean                   | 0.20392445  |
| stats_o/std                    | 0.063168444 |
| test/episodes                  | 2530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.498653  |
| test/Q_plus_P                  | -37.498653  |
| test/reward_per_eps            | -40         |
| test/steps                     | 101200      |
| train/episodes                 | 10120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 404800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 253        |
| stats_o/mean                   | 0.20392388 |
| stats_o/std                    | 0.06311575 |
| test/episodes                  | 2540       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.50942  |
| test/Q_plus_P                  | -37.50942  |
| test/reward_per_eps            | -40        |
| test/steps                     | 101600     |
| train/episodes                 | 10160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 406400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.20392483  |
| stats_o/std                    | 0.063060306 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.47676   |
| test/Q_plus_P                  | -37.47676   |
| test/reward_per_eps            | -40         |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 255         |
| stats_o/mean                   | 0.20393121  |
| stats_o/std                    | 0.063003294 |
| test/episodes                  | 2560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.550385  |
| test/Q_plus_P                  | -37.550385  |
| test/reward_per_eps            | -40         |
| test/steps                     | 102400      |
| train/episodes                 | 10240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 409600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 256        |
| stats_o/mean                   | 0.20393868 |
| stats_o/std                    | 0.0629435  |
| test/episodes                  | 2570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.621674 |
| test/Q_plus_P                  | -37.621674 |
| test/reward_per_eps            | -40        |
| test/steps                     | 102800     |
| train/episodes                 | 10280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 411200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 257        |
| stats_o/mean                   | 0.20393656 |
| stats_o/std                    | 0.06288202 |
| test/episodes                  | 2580       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.660797 |
| test/Q_plus_P                  | -37.660797 |
| test/reward_per_eps            | -40        |
| test/steps                     | 103200     |
| train/episodes                 | 10320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 412800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 258         |
| stats_o/mean                   | 0.20393685  |
| stats_o/std                    | 0.062825486 |
| test/episodes                  | 2590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.59903   |
| test/Q_plus_P                  | -37.59903   |
| test/reward_per_eps            | -40         |
| test/steps                     | 103600      |
| train/episodes                 | 10360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 414400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 259         |
| stats_o/mean                   | 0.20394075  |
| stats_o/std                    | 0.062781304 |
| test/episodes                  | 2600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.67385   |
| test/Q_plus_P                  | -37.67385   |
| test/reward_per_eps            | -40         |
| test/steps                     | 104000      |
| train/episodes                 | 10400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 416000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 260        |
| stats_o/mean                   | 0.20394142 |
| stats_o/std                    | 0.06272371 |
| test/episodes                  | 2610       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.675903 |
| test/Q_plus_P                  | -37.675903 |
| test/reward_per_eps            | -40        |
| test/steps                     | 104400     |
| train/episodes                 | 10440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 417600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 261        |
| stats_o/mean                   | 0.20394093 |
| stats_o/std                    | 0.06266913 |
| test/episodes                  | 2620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.776913 |
| test/Q_plus_P                  | -37.776913 |
| test/reward_per_eps            | -40        |
| test/steps                     | 104800     |
| train/episodes                 | 10480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 419200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.20393746  |
| stats_o/std                    | 0.062612325 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.79645   |
| test/Q_plus_P                  | -37.79645   |
| test/reward_per_eps            | -40         |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.20392904  |
| stats_o/std                    | 0.062557265 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.812298  |
| test/Q_plus_P                  | -37.812298  |
| test/reward_per_eps            | -40         |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 264         |
| stats_o/mean                   | 0.2039348   |
| stats_o/std                    | 0.062499892 |
| test/episodes                  | 2650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.749912  |
| test/Q_plus_P                  | -37.749912  |
| test/reward_per_eps            | -40         |
| test/steps                     | 106000      |
| train/episodes                 | 10600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 424000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 265         |
| stats_o/mean                   | 0.20392492  |
| stats_o/std                    | 0.062441424 |
| test/episodes                  | 2660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.72946   |
| test/Q_plus_P                  | -37.72946   |
| test/reward_per_eps            | -40         |
| test/steps                     | 106400      |
| train/episodes                 | 10640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 425600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.20391418  |
| stats_o/std                    | 0.062391877 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.767204  |
| test/Q_plus_P                  | -37.767204  |
| test/reward_per_eps            | -40         |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 267        |
| stats_o/mean                   | 0.20391409 |
| stats_o/std                    | 0.06235641 |
| test/episodes                  | 2680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.81443  |
| test/Q_plus_P                  | -37.81443  |
| test/reward_per_eps            | -40        |
| test/steps                     | 107200     |
| train/episodes                 | 10720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 428800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.20391974  |
| stats_o/std                    | 0.062302396 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.850708  |
| test/Q_plus_P                  | -37.850708  |
| test/reward_per_eps            | -40         |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 269        |
| stats_o/mean                   | 0.20392957 |
| stats_o/std                    | 0.06224786 |
| test/episodes                  | 2700       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.97852  |
| test/Q_plus_P                  | -37.97852  |
| test/reward_per_eps            | -40        |
| test/steps                     | 108000     |
| train/episodes                 | 10800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 432000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.20392697  |
| stats_o/std                    | 0.062192198 |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.878273  |
| test/Q_plus_P                  | -37.878273  |
| test/reward_per_eps            | -40         |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 271         |
| stats_o/mean                   | 0.20392393  |
| stats_o/std                    | 0.062134754 |
| test/episodes                  | 2720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.889095  |
| test/Q_plus_P                  | -37.889095  |
| test/reward_per_eps            | -40         |
| test/steps                     | 108800      |
| train/episodes                 | 10880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 435200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.20394205  |
| stats_o/std                    | 0.062080674 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -37.959633  |
| test/Q_plus_P                  | -37.959633  |
| test/reward_per_eps            | -40         |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 273        |
| stats_o/mean                   | 0.20393597 |
| stats_o/std                    | 0.06202379 |
| test/episodes                  | 2740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.00081  |
| test/Q_plus_P                  | -38.00081  |
| test/reward_per_eps            | -40        |
| test/steps                     | 109600     |
| train/episodes                 | 10960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 438400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 274        |
| stats_o/mean                   | 0.20393243 |
| stats_o/std                    | 0.06197149 |
| test/episodes                  | 2750       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.897263 |
| test/Q_plus_P                  | -37.897263 |
| test/reward_per_eps            | -40        |
| test/steps                     | 110000     |
| train/episodes                 | 11000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 440000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 275        |
| stats_o/mean                   | 0.2039509  |
| stats_o/std                    | 0.06191802 |
| test/episodes                  | 2760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.031906 |
| test/Q_plus_P                  | -38.031906 |
| test/reward_per_eps            | -40        |
| test/steps                     | 110400     |
| train/episodes                 | 11040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 441600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 276        |
| stats_o/mean                   | 0.20395292 |
| stats_o/std                    | 0.0618654  |
| test/episodes                  | 2770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.99496  |
| test/Q_plus_P                  | -37.99496  |
| test/reward_per_eps            | -40        |
| test/steps                     | 110800     |
| train/episodes                 | 11080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 443200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 277         |
| stats_o/mean                   | 0.20395195  |
| stats_o/std                    | 0.061813626 |
| test/episodes                  | 2780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.042915  |
| test/Q_plus_P                  | -38.042915  |
| test/reward_per_eps            | -40         |
| test/steps                     | 111200      |
| train/episodes                 | 11120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 444800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 278         |
| stats_o/mean                   | 0.20394954  |
| stats_o/std                    | 0.061762158 |
| test/episodes                  | 2790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.01558   |
| test/Q_plus_P                  | -38.01558   |
| test/reward_per_eps            | -40         |
| test/steps                     | 111600      |
| train/episodes                 | 11160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 446400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 279        |
| stats_o/mean                   | 0.20395342 |
| stats_o/std                    | 0.06170709 |
| test/episodes                  | 2800       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -37.996307 |
| test/Q_plus_P                  | -37.996307 |
| test/reward_per_eps            | -40        |
| test/steps                     | 112000     |
| train/episodes                 | 11200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 448000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 280        |
| stats_o/mean                   | 0.20394617 |
| stats_o/std                    | 0.06165877 |
| test/episodes                  | 2810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.10256  |
| test/Q_plus_P                  | -38.10256  |
| test/reward_per_eps            | -40        |
| test/steps                     | 112400     |
| train/episodes                 | 11240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 449600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 281         |
| stats_o/mean                   | 0.2039464   |
| stats_o/std                    | 0.061606243 |
| test/episodes                  | 2820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.148388  |
| test/Q_plus_P                  | -38.148388  |
| test/reward_per_eps            | -40         |
| test/steps                     | 112800      |
| train/episodes                 | 11280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 451200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 282         |
| stats_o/mean                   | 0.20392963  |
| stats_o/std                    | 0.061553475 |
| test/episodes                  | 2830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.15608   |
| test/Q_plus_P                  | -38.15608   |
| test/reward_per_eps            | -40         |
| test/steps                     | 113200      |
| train/episodes                 | 11320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 452800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.20392555 |
| stats_o/std                    | 0.06150125 |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.12327  |
| test/Q_plus_P                  | -38.12327  |
| test/reward_per_eps            | -40        |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 284        |
| stats_o/mean                   | 0.2039188  |
| stats_o/std                    | 0.06145097 |
| test/episodes                  | 2850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.156094 |
| test/Q_plus_P                  | -38.156094 |
| test/reward_per_eps            | -40        |
| test/steps                     | 114000     |
| train/episodes                 | 11400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 456000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 285        |
| stats_o/mean                   | 0.20391567 |
| stats_o/std                    | 0.06139628 |
| test/episodes                  | 2860       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.193737 |
| test/Q_plus_P                  | -38.193737 |
| test/reward_per_eps            | -40        |
| test/steps                     | 114400     |
| train/episodes                 | 11440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 457600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 286         |
| stats_o/mean                   | 0.20390409  |
| stats_o/std                    | 0.061343975 |
| test/episodes                  | 2870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.310688  |
| test/Q_plus_P                  | -38.310688  |
| test/reward_per_eps            | -40         |
| test/steps                     | 114800      |
| train/episodes                 | 11480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 459200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 287         |
| stats_o/mean                   | 0.2039091   |
| stats_o/std                    | 0.061295476 |
| test/episodes                  | 2880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.29407   |
| test/Q_plus_P                  | -38.29407   |
| test/reward_per_eps            | -40         |
| test/steps                     | 115200      |
| train/episodes                 | 11520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 460800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 288         |
| stats_o/mean                   | 0.20390995  |
| stats_o/std                    | 0.061249085 |
| test/episodes                  | 2890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.31006   |
| test/Q_plus_P                  | -38.31006   |
| test/reward_per_eps            | -40         |
| test/steps                     | 115600      |
| train/episodes                 | 11560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 462400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.2039068   |
| stats_o/std                    | 0.061199516 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.369946  |
| test/Q_plus_P                  | -38.369946  |
| test/reward_per_eps            | -40         |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 290        |
| stats_o/mean                   | 0.2039068  |
| stats_o/std                    | 0.06116102 |
| test/episodes                  | 2910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.314293 |
| test/Q_plus_P                  | -38.314293 |
| test/reward_per_eps            | -40        |
| test/steps                     | 116400     |
| train/episodes                 | 11640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 465600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.20390436  |
| stats_o/std                    | 0.061109863 |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.255474  |
| test/Q_plus_P                  | -38.255474  |
| test/reward_per_eps            | -40         |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 292         |
| stats_o/mean                   | 0.20390108  |
| stats_o/std                    | 0.061059933 |
| test/episodes                  | 2930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.384155  |
| test/Q_plus_P                  | -38.384155  |
| test/reward_per_eps            | -40         |
| test/steps                     | 117200      |
| train/episodes                 | 11720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 468800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.20389368  |
| stats_o/std                    | 0.061009634 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.327145  |
| test/Q_plus_P                  | -38.327145  |
| test/reward_per_eps            | -40         |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.2039153   |
| stats_o/std                    | 0.060960732 |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.415646  |
| test/Q_plus_P                  | -38.415646  |
| test/reward_per_eps            | -40         |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 295         |
| stats_o/mean                   | 0.20390604  |
| stats_o/std                    | 0.060909998 |
| test/episodes                  | 2960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.409534  |
| test/Q_plus_P                  | -38.409534  |
| test/reward_per_eps            | -40         |
| test/steps                     | 118400      |
| train/episodes                 | 11840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 473600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 296        |
| stats_o/mean                   | 0.20391123 |
| stats_o/std                    | 0.06086331 |
| test/episodes                  | 2970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.46895  |
| test/Q_plus_P                  | -38.46895  |
| test/reward_per_eps            | -40        |
| test/steps                     | 118800     |
| train/episodes                 | 11880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 475200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 297         |
| stats_o/mean                   | 0.20390955  |
| stats_o/std                    | 0.060811747 |
| test/episodes                  | 2980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.431816  |
| test/Q_plus_P                  | -38.431816  |
| test/reward_per_eps            | -40         |
| test/steps                     | 119200      |
| train/episodes                 | 11920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 476800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 298        |
| stats_o/mean                   | 0.20390414 |
| stats_o/std                    | 0.06075813 |
| test/episodes                  | 2990       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.48292  |
| test/Q_plus_P                  | -38.48292  |
| test/reward_per_eps            | -40        |
| test/steps                     | 119600     |
| train/episodes                 | 11960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 478400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 299         |
| stats_o/mean                   | 0.20390424  |
| stats_o/std                    | 0.060708568 |
| test/episodes                  | 3000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.395897  |
| test/Q_plus_P                  | -38.395897  |
| test/reward_per_eps            | -40         |
| test/steps                     | 120000      |
| train/episodes                 | 12000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 480000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.20391673  |
| stats_o/std                    | 0.060661662 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.533886  |
| test/Q_plus_P                  | -38.533886  |
| test/reward_per_eps            | -40         |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.20392208 |
| stats_o/std                    | 0.06061203 |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.510605 |
| test/Q_plus_P                  | -38.510605 |
| test/reward_per_eps            | -40        |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 302         |
| stats_o/mean                   | 0.20392512  |
| stats_o/std                    | 0.060560897 |
| test/episodes                  | 3030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.507977  |
| test/Q_plus_P                  | -38.507977  |
| test/reward_per_eps            | -40         |
| test/steps                     | 121200      |
| train/episodes                 | 12120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 484800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 303         |
| stats_o/mean                   | 0.20392331  |
| stats_o/std                    | 0.060512837 |
| test/episodes                  | 3040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.58453   |
| test/Q_plus_P                  | -38.58453   |
| test/reward_per_eps            | -40         |
| test/steps                     | 121600      |
| train/episodes                 | 12160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 486400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.20392615  |
| stats_o/std                    | 0.060463466 |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.554672  |
| test/Q_plus_P                  | -38.554672  |
| test/reward_per_eps            | -40         |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 305         |
| stats_o/mean                   | 0.20392054  |
| stats_o/std                    | 0.060415845 |
| test/episodes                  | 3060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.59466   |
| test/Q_plus_P                  | -38.59466   |
| test/reward_per_eps            | -40         |
| test/steps                     | 122400      |
| train/episodes                 | 12240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 489600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 306         |
| stats_o/mean                   | 0.20392893  |
| stats_o/std                    | 0.060371865 |
| test/episodes                  | 3070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.546936  |
| test/Q_plus_P                  | -38.546936  |
| test/reward_per_eps            | -40         |
| test/steps                     | 122800      |
| train/episodes                 | 12280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 491200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.20393884 |
| stats_o/std                    | 0.06032438 |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.631046 |
| test/Q_plus_P                  | -38.631046 |
| test/reward_per_eps            | -40        |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 308        |
| stats_o/mean                   | 0.20392114 |
| stats_o/std                    | 0.06027451 |
| test/episodes                  | 3090       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.69938  |
| test/Q_plus_P                  | -38.69938  |
| test/reward_per_eps            | -40        |
| test/steps                     | 123600     |
| train/episodes                 | 12360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 494400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 309         |
| stats_o/mean                   | 0.20392834  |
| stats_o/std                    | 0.060226973 |
| test/episodes                  | 3100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.68277   |
| test/Q_plus_P                  | -38.68277   |
| test/reward_per_eps            | -40         |
| test/steps                     | 124000      |
| train/episodes                 | 12400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 496000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.20393248  |
| stats_o/std                    | 0.060180362 |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.688095  |
| test/Q_plus_P                  | -38.688095  |
| test/reward_per_eps            | -40         |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.20393568 |
| stats_o/std                    | 0.06013501 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.649113 |
| test/Q_plus_P                  | -38.649113 |
| test/reward_per_eps            | -40        |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 312         |
| stats_o/mean                   | 0.20393915  |
| stats_o/std                    | 0.060089745 |
| test/episodes                  | 3130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.735058  |
| test/Q_plus_P                  | -38.735058  |
| test/reward_per_eps            | -40         |
| test/steps                     | 125200      |
| train/episodes                 | 12520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 500800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.20394644  |
| stats_o/std                    | 0.060043506 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.742775  |
| test/Q_plus_P                  | -38.742775  |
| test/reward_per_eps            | -40         |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 314         |
| stats_o/mean                   | 0.20395434  |
| stats_o/std                    | 0.059997752 |
| test/episodes                  | 3150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.738472  |
| test/Q_plus_P                  | -38.738472  |
| test/reward_per_eps            | -40         |
| test/steps                     | 126000      |
| train/episodes                 | 12600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 504000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 315         |
| stats_o/mean                   | 0.20394634  |
| stats_o/std                    | 0.059949026 |
| test/episodes                  | 3160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.785545  |
| test/Q_plus_P                  | -38.785545  |
| test/reward_per_eps            | -40         |
| test/steps                     | 126400      |
| train/episodes                 | 12640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 505600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 316        |
| stats_o/mean                   | 0.20395742 |
| stats_o/std                    | 0.05990444 |
| test/episodes                  | 3170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.774117 |
| test/Q_plus_P                  | -38.774117 |
| test/reward_per_eps            | -40        |
| test/steps                     | 126800     |
| train/episodes                 | 12680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 507200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 317        |
| stats_o/mean                   | 0.20395505 |
| stats_o/std                    | 0.05986069 |
| test/episodes                  | 3180       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.83691  |
| test/Q_plus_P                  | -38.83691  |
| test/reward_per_eps            | -40        |
| test/steps                     | 127200     |
| train/episodes                 | 12720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 508800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 318         |
| stats_o/mean                   | 0.20396607  |
| stats_o/std                    | 0.059814263 |
| test/episodes                  | 3190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.874622  |
| test/Q_plus_P                  | -38.874622  |
| test/reward_per_eps            | -40         |
| test/steps                     | 127600      |
| train/episodes                 | 12760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 510400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 319         |
| stats_o/mean                   | 0.20395857  |
| stats_o/std                    | 0.059767384 |
| test/episodes                  | 3200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.83883   |
| test/Q_plus_P                  | -38.83883   |
| test/reward_per_eps            | -40         |
| test/steps                     | 128000      |
| train/episodes                 | 12800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 512000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 320         |
| stats_o/mean                   | 0.20394625  |
| stats_o/std                    | 0.059721347 |
| test/episodes                  | 3210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.855576  |
| test/Q_plus_P                  | -38.855576  |
| test/reward_per_eps            | -40         |
| test/steps                     | 128400      |
| train/episodes                 | 12840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 513600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.20394297  |
| stats_o/std                    | 0.059677396 |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.900867  |
| test/Q_plus_P                  | -38.900867  |
| test/reward_per_eps            | -40         |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 322         |
| stats_o/mean                   | 0.203937    |
| stats_o/std                    | 0.059631232 |
| test/episodes                  | 3230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.839424  |
| test/Q_plus_P                  | -38.839424  |
| test/reward_per_eps            | -40         |
| test/steps                     | 129200      |
| train/episodes                 | 12920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 516800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 323        |
| stats_o/mean                   | 0.20393577 |
| stats_o/std                    | 0.05958802 |
| test/episodes                  | 3240       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.882217 |
| test/Q_plus_P                  | -38.882217 |
| test/reward_per_eps            | -40        |
| test/steps                     | 129600     |
| train/episodes                 | 12960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 518400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 324        |
| stats_o/mean                   | 0.20393524 |
| stats_o/std                    | 0.05954575 |
| test/episodes                  | 3250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.918606 |
| test/Q_plus_P                  | -38.918606 |
| test/reward_per_eps            | -40        |
| test/steps                     | 130000     |
| train/episodes                 | 13000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 520000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 325        |
| stats_o/mean                   | 0.2039453  |
| stats_o/std                    | 0.05950065 |
| test/episodes                  | 3260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.914684 |
| test/Q_plus_P                  | -38.914684 |
| test/reward_per_eps            | -40        |
| test/steps                     | 130400     |
| train/episodes                 | 13040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 521600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.20395066  |
| stats_o/std                    | 0.059454683 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.957397  |
| test/Q_plus_P                  | -38.957397  |
| test/reward_per_eps            | -40         |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 327         |
| stats_o/mean                   | 0.20395622  |
| stats_o/std                    | 0.059414826 |
| test/episodes                  | 3280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.843964  |
| test/Q_plus_P                  | -38.843964  |
| test/reward_per_eps            | -40         |
| test/steps                     | 131200      |
| train/episodes                 | 13120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 524800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 328         |
| stats_o/mean                   | 0.2039523   |
| stats_o/std                    | 0.059371557 |
| test/episodes                  | 3290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.94308   |
| test/Q_plus_P                  | -38.94308   |
| test/reward_per_eps            | -40         |
| test/steps                     | 131600      |
| train/episodes                 | 13160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 526400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 329         |
| stats_o/mean                   | 0.20394997  |
| stats_o/std                    | 0.059326395 |
| test/episodes                  | 3300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -38.95169   |
| test/Q_plus_P                  | -38.95169   |
| test/reward_per_eps            | -40         |
| test/steps                     | 132000      |
| train/episodes                 | 13200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 528000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.20396042  |
| stats_o/std                    | 0.059283312 |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.0089    |
| test/Q_plus_P                  | -39.0089    |
| test/reward_per_eps            | -40         |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 331        |
| stats_o/mean                   | 0.20396142 |
| stats_o/std                    | 0.05923963 |
| test/episodes                  | 3320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.924957 |
| test/Q_plus_P                  | -38.924957 |
| test/reward_per_eps            | -40        |
| test/steps                     | 132800     |
| train/episodes                 | 13280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 531200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 332        |
| stats_o/mean                   | 0.20396851 |
| stats_o/std                    | 0.05925446 |
| test/episodes                  | 3330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.01918  |
| test/Q_plus_P                  | -39.01918  |
| test/reward_per_eps            | -40        |
| test/steps                     | 133200     |
| train/episodes                 | 13320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 532800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 333        |
| stats_o/mean                   | 0.2039591  |
| stats_o/std                    | 0.05921217 |
| test/episodes                  | 3340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.960655 |
| test/Q_plus_P                  | -38.960655 |
| test/reward_per_eps            | -40        |
| test/steps                     | 133600     |
| train/episodes                 | 13360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 534400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 334         |
| stats_o/mean                   | 0.20395225  |
| stats_o/std                    | 0.059169706 |
| test/episodes                  | 3350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.00237   |
| test/Q_plus_P                  | -39.00237   |
| test/reward_per_eps            | -40         |
| test/steps                     | 134000      |
| train/episodes                 | 13400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 536000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 335         |
| stats_o/mean                   | 0.203948    |
| stats_o/std                    | 0.059128314 |
| test/episodes                  | 3360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.005894  |
| test/Q_plus_P                  | -39.005894  |
| test/reward_per_eps            | -40         |
| test/steps                     | 134400      |
| train/episodes                 | 13440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 537600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 336        |
| stats_o/mean                   | 0.20396152 |
| stats_o/std                    | 0.05908591 |
| test/episodes                  | 3370       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -38.999294 |
| test/Q_plus_P                  | -38.999294 |
| test/reward_per_eps            | -40        |
| test/steps                     | 134800     |
| train/episodes                 | 13480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 539200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.20396361  |
| stats_o/std                    | 0.059044342 |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.03871   |
| test/Q_plus_P                  | -39.03871   |
| test/reward_per_eps            | -40         |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 338         |
| stats_o/mean                   | 0.20396286  |
| stats_o/std                    | 0.059005782 |
| test/episodes                  | 3390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.049484  |
| test/Q_plus_P                  | -39.049484  |
| test/reward_per_eps            | -40         |
| test/steps                     | 135600      |
| train/episodes                 | 13560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 542400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.20397311  |
| stats_o/std                    | 0.058966685 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.131714  |
| test/Q_plus_P                  | -39.131714  |
| test/reward_per_eps            | -40         |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 340        |
| stats_o/mean                   | 0.20396285 |
| stats_o/std                    | 0.05892971 |
| test/episodes                  | 3410       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.054638 |
| test/Q_plus_P                  | -39.054638 |
| test/reward_per_eps            | -40        |
| test/steps                     | 136400     |
| train/episodes                 | 13640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 545600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 341         |
| stats_o/mean                   | 0.203969    |
| stats_o/std                    | 0.058887664 |
| test/episodes                  | 3420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.09309   |
| test/Q_plus_P                  | -39.09309   |
| test/reward_per_eps            | -40         |
| test/steps                     | 136800      |
| train/episodes                 | 13680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 547200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 342         |
| stats_o/mean                   | 0.20397078  |
| stats_o/std                    | 0.058847222 |
| test/episodes                  | 3430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.032993  |
| test/Q_plus_P                  | -39.032993  |
| test/reward_per_eps            | -40         |
| test/steps                     | 137200      |
| train/episodes                 | 13720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 548800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 343         |
| stats_o/mean                   | 0.20397316  |
| stats_o/std                    | 0.058803435 |
| test/episodes                  | 3440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.15043   |
| test/Q_plus_P                  | -39.15043   |
| test/reward_per_eps            | -40         |
| test/steps                     | 137600      |
| train/episodes                 | 13760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 550400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 344        |
| stats_o/mean                   | 0.20396584 |
| stats_o/std                    | 0.05876606 |
| test/episodes                  | 3450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.092754 |
| test/Q_plus_P                  | -39.092754 |
| test/reward_per_eps            | -40        |
| test/steps                     | 138000     |
| train/episodes                 | 13800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 552000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.20397216  |
| stats_o/std                    | 0.058725692 |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.11024   |
| test/Q_plus_P                  | -39.11024   |
| test/reward_per_eps            | -40         |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 346         |
| stats_o/mean                   | 0.20395254  |
| stats_o/std                    | 0.058686256 |
| test/episodes                  | 3470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.12816   |
| test/Q_plus_P                  | -39.12816   |
| test/reward_per_eps            | -40         |
| test/steps                     | 138800      |
| train/episodes                 | 13880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 555200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 347         |
| stats_o/mean                   | 0.2039425   |
| stats_o/std                    | 0.058644384 |
| test/episodes                  | 3480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.094387  |
| test/Q_plus_P                  | -39.094387  |
| test/reward_per_eps            | -40         |
| test/steps                     | 139200      |
| train/episodes                 | 13920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 556800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.20393987  |
| stats_o/std                    | 0.058603026 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.11828   |
| test/Q_plus_P                  | -39.11828   |
| test/reward_per_eps            | -40         |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 349        |
| stats_o/mean                   | 0.20393944 |
| stats_o/std                    | 0.05856111 |
| test/episodes                  | 3500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.21728  |
| test/Q_plus_P                  | -39.21728  |
| test/reward_per_eps            | -40        |
| test/steps                     | 140000     |
| train/episodes                 | 14000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 560000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 350         |
| stats_o/mean                   | 0.20396015  |
| stats_o/std                    | 0.058519233 |
| test/episodes                  | 3510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.1929    |
| test/Q_plus_P                  | -39.1929    |
| test/reward_per_eps            | -40         |
| test/steps                     | 140400      |
| train/episodes                 | 14040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 561600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 351        |
| stats_o/mean                   | 0.20396519 |
| stats_o/std                    | 0.05847734 |
| test/episodes                  | 3520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.138535 |
| test/Q_plus_P                  | -39.138535 |
| test/reward_per_eps            | -40        |
| test/steps                     | 140800     |
| train/episodes                 | 14080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 563200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 352         |
| stats_o/mean                   | 0.20396024  |
| stats_o/std                    | 0.058435593 |
| test/episodes                  | 3530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.19095   |
| test/Q_plus_P                  | -39.19095   |
| test/reward_per_eps            | -40         |
| test/steps                     | 141200      |
| train/episodes                 | 14120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 564800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 353        |
| stats_o/mean                   | 0.20395313 |
| stats_o/std                    | 0.05839477 |
| test/episodes                  | 3540       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.20943  |
| test/Q_plus_P                  | -39.20943  |
| test/reward_per_eps            | -40        |
| test/steps                     | 141600     |
| train/episodes                 | 14160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 566400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 354        |
| stats_o/mean                   | 0.20394681 |
| stats_o/std                    | 0.05835633 |
| test/episodes                  | 3550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.229324 |
| test/Q_plus_P                  | -39.229324 |
| test/reward_per_eps            | -40        |
| test/steps                     | 142000     |
| train/episodes                 | 14200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 568000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.203946    |
| stats_o/std                    | 0.058316387 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.21846   |
| test/Q_plus_P                  | -39.21846   |
| test/reward_per_eps            | -40         |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 356         |
| stats_o/mean                   | 0.20394537  |
| stats_o/std                    | 0.058275186 |
| test/episodes                  | 3570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.242233  |
| test/Q_plus_P                  | -39.242233  |
| test/reward_per_eps            | -40         |
| test/steps                     | 142800      |
| train/episodes                 | 14280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 571200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 357         |
| stats_o/mean                   | 0.20394126  |
| stats_o/std                    | 0.058235116 |
| test/episodes                  | 3580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.237415  |
| test/Q_plus_P                  | -39.237415  |
| test/reward_per_eps            | -40         |
| test/steps                     | 143200      |
| train/episodes                 | 14320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 572800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 358         |
| stats_o/mean                   | 0.20394912  |
| stats_o/std                    | 0.058198985 |
| test/episodes                  | 3590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.25586   |
| test/Q_plus_P                  | -39.25586   |
| test/reward_per_eps            | -40         |
| test/steps                     | 143600      |
| train/episodes                 | 14360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 574400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.2039493   |
| stats_o/std                    | 0.058162626 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.271034  |
| test/Q_plus_P                  | -39.271034  |
| test/reward_per_eps            | -40         |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 360         |
| stats_o/mean                   | 0.20394371  |
| stats_o/std                    | 0.058123708 |
| test/episodes                  | 3610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.280743  |
| test/Q_plus_P                  | -39.280743  |
| test/reward_per_eps            | -40         |
| test/steps                     | 144400      |
| train/episodes                 | 14440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 577600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 361        |
| stats_o/mean                   | 0.2039507  |
| stats_o/std                    | 0.05808804 |
| test/episodes                  | 3620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.281593 |
| test/Q_plus_P                  | -39.281593 |
| test/reward_per_eps            | -40        |
| test/steps                     | 144800     |
| train/episodes                 | 14480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 579200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 362         |
| stats_o/mean                   | 0.2039509   |
| stats_o/std                    | 0.058051173 |
| test/episodes                  | 3630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.28512   |
| test/Q_plus_P                  | -39.28512   |
| test/reward_per_eps            | -40         |
| test/steps                     | 145200      |
| train/episodes                 | 14520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 580800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.20396198  |
| stats_o/std                    | 0.058019336 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.23381   |
| test/Q_plus_P                  | -39.23381   |
| test/reward_per_eps            | -40         |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 364         |
| stats_o/mean                   | 0.20396191  |
| stats_o/std                    | 0.057983648 |
| test/episodes                  | 3650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.30093   |
| test/Q_plus_P                  | -39.30093   |
| test/reward_per_eps            | -40         |
| test/steps                     | 146000      |
| train/episodes                 | 14600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 584000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.20396793  |
| stats_o/std                    | 0.057943832 |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.300465  |
| test/Q_plus_P                  | -39.300465  |
| test/reward_per_eps            | -40         |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 366         |
| stats_o/mean                   | 0.20396411  |
| stats_o/std                    | 0.057906594 |
| test/episodes                  | 3670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.273006  |
| test/Q_plus_P                  | -39.273006  |
| test/reward_per_eps            | -40         |
| test/steps                     | 146800      |
| train/episodes                 | 14680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 587200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.20397581  |
| stats_o/std                    | 0.057878338 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.326656  |
| test/Q_plus_P                  | -39.326656  |
| test/reward_per_eps            | -40         |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 368        |
| stats_o/mean                   | 0.20397884 |
| stats_o/std                    | 0.05784317 |
| test/episodes                  | 3690       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.419006 |
| test/Q_plus_P                  | -39.419006 |
| test/reward_per_eps            | -40        |
| test/steps                     | 147600     |
| train/episodes                 | 14760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 590400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 369        |
| stats_o/mean                   | 0.20397207 |
| stats_o/std                    | 0.05780612 |
| test/episodes                  | 3700       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.332558 |
| test/Q_plus_P                  | -39.332558 |
| test/reward_per_eps            | -40        |
| test/steps                     | 148000     |
| train/episodes                 | 14800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 592000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 370        |
| stats_o/mean                   | 0.20397083 |
| stats_o/std                    | 0.05777254 |
| test/episodes                  | 3710       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.351704 |
| test/Q_plus_P                  | -39.351704 |
| test/reward_per_eps            | -40        |
| test/steps                     | 148400     |
| train/episodes                 | 14840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 593600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 371         |
| stats_o/mean                   | 0.2039714   |
| stats_o/std                    | 0.057736006 |
| test/episodes                  | 3720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.33888   |
| test/Q_plus_P                  | -39.33888   |
| test/reward_per_eps            | -40         |
| test/steps                     | 148800      |
| train/episodes                 | 14880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 595200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 372         |
| stats_o/mean                   | 0.203965    |
| stats_o/std                    | 0.057699643 |
| test/episodes                  | 3730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.3546    |
| test/Q_plus_P                  | -39.3546    |
| test/reward_per_eps            | -40         |
| test/steps                     | 149200      |
| train/episodes                 | 14920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 596800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 373         |
| stats_o/mean                   | 0.20396252  |
| stats_o/std                    | 0.057662174 |
| test/episodes                  | 3740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.45183   |
| test/Q_plus_P                  | -39.45183   |
| test/reward_per_eps            | -40         |
| test/steps                     | 149600      |
| train/episodes                 | 14960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 598400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 374         |
| stats_o/mean                   | 0.20396633  |
| stats_o/std                    | 0.057625894 |
| test/episodes                  | 3750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.438133  |
| test/Q_plus_P                  | -39.438133  |
| test/reward_per_eps            | -40         |
| test/steps                     | 150000      |
| train/episodes                 | 15000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 600000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.20396872  |
| stats_o/std                    | 0.057587713 |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.40437   |
| test/Q_plus_P                  | -39.40437   |
| test/reward_per_eps            | -40         |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.20396739 |
| stats_o/std                    | 0.05755239 |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.450912 |
| test/Q_plus_P                  | -39.450912 |
| test/reward_per_eps            | -40        |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.20396104  |
| stats_o/std                    | 0.057514954 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.447018  |
| test/Q_plus_P                  | -39.447018  |
| test/reward_per_eps            | -40         |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 378         |
| stats_o/mean                   | 0.20396207  |
| stats_o/std                    | 0.057478476 |
| test/episodes                  | 3790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.46178   |
| test/Q_plus_P                  | -39.46178   |
| test/reward_per_eps            | -40         |
| test/steps                     | 151600      |
| train/episodes                 | 15160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 606400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 379         |
| stats_o/mean                   | 0.20396532  |
| stats_o/std                    | 0.057442106 |
| test/episodes                  | 3800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.44259   |
| test/Q_plus_P                  | -39.44259   |
| test/reward_per_eps            | -40         |
| test/steps                     | 152000      |
| train/episodes                 | 15200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 608000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.20396227  |
| stats_o/std                    | 0.057407513 |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.485275  |
| test/Q_plus_P                  | -39.485275  |
| test/reward_per_eps            | -40         |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 381        |
| stats_o/mean                   | 0.20396805 |
| stats_o/std                    | 0.05737177 |
| test/episodes                  | 3820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.420315 |
| test/Q_plus_P                  | -39.420315 |
| test/reward_per_eps            | -40        |
| test/steps                     | 152800     |
| train/episodes                 | 15280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 611200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.20396505 |
| stats_o/std                    | 0.05733698 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.4807   |
| test/Q_plus_P                  | -39.4807   |
| test/reward_per_eps            | -40        |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 383        |
| stats_o/mean                   | 0.20396501 |
| stats_o/std                    | 0.05730165 |
| test/episodes                  | 3840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.463913 |
| test/Q_plus_P                  | -39.463913 |
| test/reward_per_eps            | -40        |
| test/steps                     | 153600     |
| train/episodes                 | 15360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 614400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 384        |
| stats_o/mean                   | 0.20395878 |
| stats_o/std                    | 0.05726521 |
| test/episodes                  | 3850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.472343 |
| test/Q_plus_P                  | -39.472343 |
| test/reward_per_eps            | -40        |
| test/steps                     | 154000     |
| train/episodes                 | 15400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 616000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 385         |
| stats_o/mean                   | 0.20396219  |
| stats_o/std                    | 0.057228412 |
| test/episodes                  | 3860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.46362   |
| test/Q_plus_P                  | -39.46362   |
| test/reward_per_eps            | -40         |
| test/steps                     | 154400      |
| train/episodes                 | 15440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 617600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 386        |
| stats_o/mean                   | 0.20396332 |
| stats_o/std                    | 0.05719481 |
| test/episodes                  | 3870       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.475155 |
| test/Q_plus_P                  | -39.475155 |
| test/reward_per_eps            | -40        |
| test/steps                     | 154800     |
| train/episodes                 | 15480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 619200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 387        |
| stats_o/mean                   | 0.20396252 |
| stats_o/std                    | 0.05716136 |
| test/episodes                  | 3880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.51591  |
| test/Q_plus_P                  | -39.51591  |
| test/reward_per_eps            | -40        |
| test/steps                     | 155200     |
| train/episodes                 | 15520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 620800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 388         |
| stats_o/mean                   | 0.20396104  |
| stats_o/std                    | 0.057126537 |
| test/episodes                  | 3890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.52524   |
| test/Q_plus_P                  | -39.52524   |
| test/reward_per_eps            | -40         |
| test/steps                     | 155600      |
| train/episodes                 | 15560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 622400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 389        |
| stats_o/mean                   | 0.20395897 |
| stats_o/std                    | 0.05709143 |
| test/episodes                  | 3900       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.47755  |
| test/Q_plus_P                  | -39.47755  |
| test/reward_per_eps            | -40        |
| test/steps                     | 156000     |
| train/episodes                 | 15600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 624000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 390        |
| stats_o/mean                   | 0.20395604 |
| stats_o/std                    | 0.05708677 |
| test/episodes                  | 3910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.514553 |
| test/Q_plus_P                  | -39.514553 |
| test/reward_per_eps            | -40        |
| test/steps                     | 156400     |
| train/episodes                 | 15640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 625600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.20395914  |
| stats_o/std                    | 0.057051778 |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.524517  |
| test/Q_plus_P                  | -39.524517  |
| test/reward_per_eps            | -40         |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 392         |
| stats_o/mean                   | 0.20395792  |
| stats_o/std                    | 0.057016242 |
| test/episodes                  | 3930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.55499   |
| test/Q_plus_P                  | -39.55499   |
| test/reward_per_eps            | -40         |
| test/steps                     | 157200      |
| train/episodes                 | 15720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 628800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 393        |
| stats_o/mean                   | 0.20395426 |
| stats_o/std                    | 0.05698242 |
| test/episodes                  | 3940       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.569363 |
| test/Q_plus_P                  | -39.569363 |
| test/reward_per_eps            | -40        |
| test/steps                     | 157600     |
| train/episodes                 | 15760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 630400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 394         |
| stats_o/mean                   | 0.2039493   |
| stats_o/std                    | 0.056946922 |
| test/episodes                  | 3950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.505726  |
| test/Q_plus_P                  | -39.505726  |
| test/reward_per_eps            | -40         |
| test/steps                     | 158000      |
| train/episodes                 | 15800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 632000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 395        |
| stats_o/mean                   | 0.20395362 |
| stats_o/std                    | 0.05691312 |
| test/episodes                  | 3960       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.557526 |
| test/Q_plus_P                  | -39.557526 |
| test/reward_per_eps            | -40        |
| test/steps                     | 158400     |
| train/episodes                 | 15840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 633600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 396        |
| stats_o/mean                   | 0.20395969 |
| stats_o/std                    | 0.05688116 |
| test/episodes                  | 3970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.51416  |
| test/Q_plus_P                  | -39.51416  |
| test/reward_per_eps            | -40        |
| test/steps                     | 158800     |
| train/episodes                 | 15880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 635200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.20395818  |
| stats_o/std                    | 0.056847107 |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.553066  |
| test/Q_plus_P                  | -39.553066  |
| test/reward_per_eps            | -40         |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 398         |
| stats_o/mean                   | 0.20395306  |
| stats_o/std                    | 0.056814138 |
| test/episodes                  | 3990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.51021   |
| test/Q_plus_P                  | -39.51021   |
| test/reward_per_eps            | -40         |
| test/steps                     | 159600      |
| train/episodes                 | 15960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 638400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 399        |
| stats_o/mean                   | 0.20395353 |
| stats_o/std                    | 0.05677901 |
| test/episodes                  | 4000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.542145 |
| test/Q_plus_P                  | -39.542145 |
| test/reward_per_eps            | -40        |
| test/steps                     | 160000     |
| train/episodes                 | 16000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 640000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 400        |
| stats_o/mean                   | 0.20395687 |
| stats_o/std                    | 0.05674698 |
| test/episodes                  | 4010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.529663 |
| test/Q_plus_P                  | -39.529663 |
| test/reward_per_eps            | -40        |
| test/steps                     | 160400     |
| train/episodes                 | 16040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 641600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 401         |
| stats_o/mean                   | 0.20396125  |
| stats_o/std                    | 0.056714114 |
| test/episodes                  | 4020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.55735   |
| test/Q_plus_P                  | -39.55735   |
| test/reward_per_eps            | -40         |
| test/steps                     | 160800      |
| train/episodes                 | 16080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 643200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.20395851 |
| stats_o/std                    | 0.05668119 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.521004 |
| test/Q_plus_P                  | -39.521004 |
| test/reward_per_eps            | -40        |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 403        |
| stats_o/mean                   | 0.20395783 |
| stats_o/std                    | 0.05664849 |
| test/episodes                  | 4040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.563717 |
| test/Q_plus_P                  | -39.563717 |
| test/reward_per_eps            | -40        |
| test/steps                     | 161600     |
| train/episodes                 | 16160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 646400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.20396231  |
| stats_o/std                    | 0.056615166 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.595547  |
| test/Q_plus_P                  | -39.595547  |
| test/reward_per_eps            | -40         |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.20397384  |
| stats_o/std                    | 0.056582004 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.567196  |
| test/Q_plus_P                  | -39.567196  |
| test/reward_per_eps            | -40         |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 406        |
| stats_o/mean                   | 0.20397314 |
| stats_o/std                    | 0.05654852 |
| test/episodes                  | 4070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.58019  |
| test/Q_plus_P                  | -39.58019  |
| test/reward_per_eps            | -40        |
| test/steps                     | 162800     |
| train/episodes                 | 16280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 651200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.20397371  |
| stats_o/std                    | 0.056515373 |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.57997   |
| test/Q_plus_P                  | -39.57997   |
| test/reward_per_eps            | -40         |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 408         |
| stats_o/mean                   | 0.20397425  |
| stats_o/std                    | 0.056482133 |
| test/episodes                  | 4090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.563656  |
| test/Q_plus_P                  | -39.563656  |
| test/reward_per_eps            | -40         |
| test/steps                     | 163600      |
| train/episodes                 | 16360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 654400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 409         |
| stats_o/mean                   | 0.203973    |
| stats_o/std                    | 0.056449614 |
| test/episodes                  | 4100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.5634    |
| test/Q_plus_P                  | -39.5634    |
| test/reward_per_eps            | -40         |
| test/steps                     | 164000      |
| train/episodes                 | 16400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 656000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 410        |
| stats_o/mean                   | 0.20396002 |
| stats_o/std                    | 0.05641613 |
| test/episodes                  | 4110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.583298 |
| test/Q_plus_P                  | -39.583298 |
| test/reward_per_eps            | -40        |
| test/steps                     | 164400     |
| train/episodes                 | 16440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 657600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 411         |
| stats_o/mean                   | 0.20395021  |
| stats_o/std                    | 0.056383986 |
| test/episodes                  | 4120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.56755   |
| test/Q_plus_P                  | -39.56755   |
| test/reward_per_eps            | -40         |
| test/steps                     | 164800      |
| train/episodes                 | 16480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 659200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 412         |
| stats_o/mean                   | 0.20395172  |
| stats_o/std                    | 0.056352865 |
| test/episodes                  | 4130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.637352  |
| test/Q_plus_P                  | -39.637352  |
| test/reward_per_eps            | -40         |
| test/steps                     | 165200      |
| train/episodes                 | 16520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 660800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.20394081  |
| stats_o/std                    | 0.056327917 |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.582035  |
| test/Q_plus_P                  | -39.582035  |
| test/reward_per_eps            | -40         |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 414        |
| stats_o/mean                   | 0.20394245 |
| stats_o/std                    | 0.05629592 |
| test/episodes                  | 4150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.589622 |
| test/Q_plus_P                  | -39.589622 |
| test/reward_per_eps            | -40        |
| test/steps                     | 166000     |
| train/episodes                 | 16600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 664000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 415        |
| stats_o/mean                   | 0.20394067 |
| stats_o/std                    | 0.05626477 |
| test/episodes                  | 4160       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.668312 |
| test/Q_plus_P                  | -39.668312 |
| test/reward_per_eps            | -40        |
| test/steps                     | 166400     |
| train/episodes                 | 16640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 665600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 416        |
| stats_o/mean                   | 0.20393792 |
| stats_o/std                    | 0.05623381 |
| test/episodes                  | 4170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.662106 |
| test/Q_plus_P                  | -39.662106 |
| test/reward_per_eps            | -40        |
| test/steps                     | 166800     |
| train/episodes                 | 16680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 667200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 417         |
| stats_o/mean                   | 0.20393652  |
| stats_o/std                    | 0.056201186 |
| test/episodes                  | 4180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.633335  |
| test/Q_plus_P                  | -39.633335  |
| test/reward_per_eps            | -40         |
| test/steps                     | 167200      |
| train/episodes                 | 16720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 668800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.20394368  |
| stats_o/std                    | 0.056166988 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.595226  |
| test/Q_plus_P                  | -39.595226  |
| test/reward_per_eps            | -40         |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 419         |
| stats_o/mean                   | 0.2039387   |
| stats_o/std                    | 0.056136098 |
| test/episodes                  | 4200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.665638  |
| test/Q_plus_P                  | -39.665638  |
| test/reward_per_eps            | -40         |
| test/steps                     | 168000      |
| train/episodes                 | 16800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 672000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 420         |
| stats_o/mean                   | 0.20393094  |
| stats_o/std                    | 0.056105312 |
| test/episodes                  | 4210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.618668  |
| test/Q_plus_P                  | -39.618668  |
| test/reward_per_eps            | -40         |
| test/steps                     | 168400      |
| train/episodes                 | 16840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 673600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 421        |
| stats_o/mean                   | 0.20393632 |
| stats_o/std                    | 0.05607149 |
| test/episodes                  | 4220       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.657833 |
| test/Q_plus_P                  | -39.657833 |
| test/reward_per_eps            | -40        |
| test/steps                     | 168800     |
| train/episodes                 | 16880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 675200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 422        |
| stats_o/mean                   | 0.2039488  |
| stats_o/std                    | 0.05603983 |
| test/episodes                  | 4230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.693947 |
| test/Q_plus_P                  | -39.693947 |
| test/reward_per_eps            | -40        |
| test/steps                     | 169200     |
| train/episodes                 | 16920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 676800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.20395006  |
| stats_o/std                    | 0.056010026 |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.644703  |
| test/Q_plus_P                  | -39.644703  |
| test/reward_per_eps            | -40         |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 424         |
| stats_o/mean                   | 0.20395754  |
| stats_o/std                    | 0.055979263 |
| test/episodes                  | 4250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.65165   |
| test/Q_plus_P                  | -39.65165   |
| test/reward_per_eps            | -40         |
| test/steps                     | 170000      |
| train/episodes                 | 17000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 680000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 425        |
| stats_o/mean                   | 0.20395415 |
| stats_o/std                    | 0.05594787 |
| test/episodes                  | 4260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.674747 |
| test/Q_plus_P                  | -39.674747 |
| test/reward_per_eps            | -40        |
| test/steps                     | 170400     |
| train/episodes                 | 17040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 681600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.2039542   |
| stats_o/std                    | 0.055916194 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.679165  |
| test/Q_plus_P                  | -39.679165  |
| test/reward_per_eps            | -40         |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 427         |
| stats_o/mean                   | 0.20395383  |
| stats_o/std                    | 0.055885497 |
| test/episodes                  | 4280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.67864   |
| test/Q_plus_P                  | -39.67864   |
| test/reward_per_eps            | -40         |
| test/steps                     | 171200      |
| train/episodes                 | 17120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 684800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.20395304  |
| stats_o/std                    | 0.055857502 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.61331   |
| test/Q_plus_P                  | -39.61331   |
| test/reward_per_eps            | -40         |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 429        |
| stats_o/mean                   | 0.2039544  |
| stats_o/std                    | 0.05582741 |
| test/episodes                  | 4300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.669262 |
| test/Q_plus_P                  | -39.669262 |
| test/reward_per_eps            | -40        |
| test/steps                     | 172000     |
| train/episodes                 | 17200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 688000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 430        |
| stats_o/mean                   | 0.20395403 |
| stats_o/std                    | 0.05579666 |
| test/episodes                  | 4310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.695778 |
| test/Q_plus_P                  | -39.695778 |
| test/reward_per_eps            | -40        |
| test/steps                     | 172400     |
| train/episodes                 | 17240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 689600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 431        |
| stats_o/mean                   | 0.20394824 |
| stats_o/std                    | 0.05577058 |
| test/episodes                  | 4320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.675304 |
| test/Q_plus_P                  | -39.675304 |
| test/reward_per_eps            | -40        |
| test/steps                     | 172800     |
| train/episodes                 | 17280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 691200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 432         |
| stats_o/mean                   | 0.20394258  |
| stats_o/std                    | 0.055751387 |
| test/episodes                  | 4330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.70804   |
| test/Q_plus_P                  | -39.70804   |
| test/reward_per_eps            | -40         |
| test/steps                     | 173200      |
| train/episodes                 | 17320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 692800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 433         |
| stats_o/mean                   | 0.2039385   |
| stats_o/std                    | 0.055721663 |
| test/episodes                  | 4340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.68407   |
| test/Q_plus_P                  | -39.68407   |
| test/reward_per_eps            | -40         |
| test/steps                     | 173600      |
| train/episodes                 | 17360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 694400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 434         |
| stats_o/mean                   | 0.20393452  |
| stats_o/std                    | 0.055692587 |
| test/episodes                  | 4350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.70346   |
| test/Q_plus_P                  | -39.70346   |
| test/reward_per_eps            | -40         |
| test/steps                     | 174000      |
| train/episodes                 | 17400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 696000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 435        |
| stats_o/mean                   | 0.2039446  |
| stats_o/std                    | 0.05566429 |
| test/episodes                  | 4360       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.698788 |
| test/Q_plus_P                  | -39.698788 |
| test/reward_per_eps            | -40        |
| test/steps                     | 174400     |
| train/episodes                 | 17440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 697600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 436         |
| stats_o/mean                   | 0.20394377  |
| stats_o/std                    | 0.055635013 |
| test/episodes                  | 4370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.72149   |
| test/Q_plus_P                  | -39.72149   |
| test/reward_per_eps            | -40         |
| test/steps                     | 174800      |
| train/episodes                 | 17480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 699200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 437         |
| stats_o/mean                   | 0.2039443   |
| stats_o/std                    | 0.055607043 |
| test/episodes                  | 4380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.73555   |
| test/Q_plus_P                  | -39.73555   |
| test/reward_per_eps            | -40         |
| test/steps                     | 175200      |
| train/episodes                 | 17520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 700800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 438         |
| stats_o/mean                   | 0.20393871  |
| stats_o/std                    | 0.055578902 |
| test/episodes                  | 4390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.72661   |
| test/Q_plus_P                  | -39.72661   |
| test/reward_per_eps            | -40         |
| test/steps                     | 175600      |
| train/episodes                 | 17560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 702400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 439         |
| stats_o/mean                   | 0.20393926  |
| stats_o/std                    | 0.055549428 |
| test/episodes                  | 4400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.69963   |
| test/Q_plus_P                  | -39.69963   |
| test/reward_per_eps            | -40         |
| test/steps                     | 176000      |
| train/episodes                 | 17600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 704000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.20394157  |
| stats_o/std                    | 0.055520896 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.6983    |
| test/Q_plus_P                  | -39.6983    |
| test/reward_per_eps            | -40         |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.20394318  |
| stats_o/std                    | 0.055491537 |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.71847   |
| test/Q_plus_P                  | -39.71847   |
| test/reward_per_eps            | -40         |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 442        |
| stats_o/mean                   | 0.20393974 |
| stats_o/std                    | 0.05546176 |
| test/episodes                  | 4430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.74028  |
| test/Q_plus_P                  | -39.74028  |
| test/reward_per_eps            | -40        |
| test/steps                     | 177200     |
| train/episodes                 | 17720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 708800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.20394196  |
| stats_o/std                    | 0.055433176 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.71574   |
| test/Q_plus_P                  | -39.71574   |
| test/reward_per_eps            | -40         |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 444         |
| stats_o/mean                   | 0.20394222  |
| stats_o/std                    | 0.055405367 |
| test/episodes                  | 4450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.73123   |
| test/Q_plus_P                  | -39.73123   |
| test/reward_per_eps            | -40         |
| test/steps                     | 178000      |
| train/episodes                 | 17800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 712000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.20393957  |
| stats_o/std                    | 0.055377387 |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.745377  |
| test/Q_plus_P                  | -39.745377  |
| test/reward_per_eps            | -40         |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 446         |
| stats_o/mean                   | 0.20394082  |
| stats_o/std                    | 0.055348706 |
| test/episodes                  | 4470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.737114  |
| test/Q_plus_P                  | -39.737114  |
| test/reward_per_eps            | -40         |
| test/steps                     | 178800      |
| train/episodes                 | 17880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 715200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 447        |
| stats_o/mean                   | 0.20393662 |
| stats_o/std                    | 0.0553199  |
| test/episodes                  | 4480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.736523 |
| test/Q_plus_P                  | -39.736523 |
| test/reward_per_eps            | -40        |
| test/steps                     | 179200     |
| train/episodes                 | 17920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 716800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 448         |
| stats_o/mean                   | 0.20393877  |
| stats_o/std                    | 0.055291697 |
| test/episodes                  | 4490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.741745  |
| test/Q_plus_P                  | -39.741745  |
| test/reward_per_eps            | -40         |
| test/steps                     | 179600      |
| train/episodes                 | 17960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 718400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 449         |
| stats_o/mean                   | 0.2039389   |
| stats_o/std                    | 0.055262145 |
| test/episodes                  | 4500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.739063  |
| test/Q_plus_P                  | -39.739063  |
| test/reward_per_eps            | -40         |
| test/steps                     | 180000      |
| train/episodes                 | 18000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 720000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.20393984 |
| stats_o/std                    | 0.05523327 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.758965 |
| test/Q_plus_P                  | -39.758965 |
| test/reward_per_eps            | -40        |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 451         |
| stats_o/mean                   | 0.20393603  |
| stats_o/std                    | 0.055208184 |
| test/episodes                  | 4520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.753822  |
| test/Q_plus_P                  | -39.753822  |
| test/reward_per_eps            | -40         |
| test/steps                     | 180800      |
| train/episodes                 | 18080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 723200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 452         |
| stats_o/mean                   | 0.20394078  |
| stats_o/std                    | 0.055187844 |
| test/episodes                  | 4530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.75668   |
| test/Q_plus_P                  | -39.75668   |
| test/reward_per_eps            | -40         |
| test/steps                     | 181200      |
| train/episodes                 | 18120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.205      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 724800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 453         |
| stats_o/mean                   | 0.20394531  |
| stats_o/std                    | 0.055161335 |
| test/episodes                  | 4540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.756107  |
| test/Q_plus_P                  | -39.756107  |
| test/reward_per_eps            | -40         |
| test/steps                     | 181600      |
| train/episodes                 | 18160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 726400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.20394911  |
| stats_o/std                    | 0.055132646 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.74285   |
| test/Q_plus_P                  | -39.74285   |
| test/reward_per_eps            | -40         |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.20395298  |
| stats_o/std                    | 0.055106238 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.764294  |
| test/Q_plus_P                  | -39.764294  |
| test/reward_per_eps            | -40         |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 456        |
| stats_o/mean                   | 0.20395252 |
| stats_o/std                    | 0.05507852 |
| test/episodes                  | 4570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.820812 |
| test/Q_plus_P                  | -39.820812 |
| test/reward_per_eps            | -40        |
| test/steps                     | 182800     |
| train/episodes                 | 18280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 731200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.2039588   |
| stats_o/std                    | 0.055053383 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.774612  |
| test/Q_plus_P                  | -39.774612  |
| test/reward_per_eps            | -40         |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.20395531  |
| stats_o/std                    | 0.055034548 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.76949   |
| test/Q_plus_P                  | -39.76949   |
| test/reward_per_eps            | -40         |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 459        |
| stats_o/mean                   | 0.20395434 |
| stats_o/std                    | 0.05500723 |
| test/episodes                  | 4600       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.805042 |
| test/Q_plus_P                  | -39.805042 |
| test/reward_per_eps            | -40        |
| test/steps                     | 184000     |
| train/episodes                 | 18400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 736000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.20395792  |
| stats_o/std                    | 0.054980416 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.7724    |
| test/Q_plus_P                  | -39.7724    |
| test/reward_per_eps            | -40         |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 461         |
| stats_o/mean                   | 0.20395166  |
| stats_o/std                    | 0.054952513 |
| test/episodes                  | 4620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.77412   |
| test/Q_plus_P                  | -39.77412   |
| test/reward_per_eps            | -40         |
| test/steps                     | 184800      |
| train/episodes                 | 18480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 739200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 462         |
| stats_o/mean                   | 0.20395102  |
| stats_o/std                    | 0.054931458 |
| test/episodes                  | 4630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.768475  |
| test/Q_plus_P                  | -39.768475  |
| test/reward_per_eps            | -40         |
| test/steps                     | 185200      |
| train/episodes                 | 18520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 740800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 463         |
| stats_o/mean                   | 0.20394295  |
| stats_o/std                    | 0.054904677 |
| test/episodes                  | 4640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.767056  |
| test/Q_plus_P                  | -39.767056  |
| test/reward_per_eps            | -40         |
| test/steps                     | 185600      |
| train/episodes                 | 18560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 742400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 464         |
| stats_o/mean                   | 0.20394443  |
| stats_o/std                    | 0.054877575 |
| test/episodes                  | 4650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.76205   |
| test/Q_plus_P                  | -39.76205   |
| test/reward_per_eps            | -40         |
| test/steps                     | 186000      |
| train/episodes                 | 18600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 744000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 465        |
| stats_o/mean                   | 0.20394795 |
| stats_o/std                    | 0.05485059 |
| test/episodes                  | 4660       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.782852 |
| test/Q_plus_P                  | -39.782852 |
| test/reward_per_eps            | -40        |
| test/steps                     | 186400     |
| train/episodes                 | 18640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 745600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 466        |
| stats_o/mean                   | 0.20395601 |
| stats_o/std                    | 0.05482677 |
| test/episodes                  | 4670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.78299  |
| test/Q_plus_P                  | -39.78299  |
| test/reward_per_eps            | -40        |
| test/steps                     | 186800     |
| train/episodes                 | 18680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 747200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 467         |
| stats_o/mean                   | 0.2039609   |
| stats_o/std                    | 0.054799695 |
| test/episodes                  | 4680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.77578   |
| test/Q_plus_P                  | -39.77578   |
| test/reward_per_eps            | -40         |
| test/steps                     | 187200      |
| train/episodes                 | 18720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 748800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 468        |
| stats_o/mean                   | 0.20396782 |
| stats_o/std                    | 0.05477388 |
| test/episodes                  | 4690       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.796097 |
| test/Q_plus_P                  | -39.796097 |
| test/reward_per_eps            | -40        |
| test/steps                     | 187600     |
| train/episodes                 | 18760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 750400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 469        |
| stats_o/mean                   | 0.20397036 |
| stats_o/std                    | 0.05474738 |
| test/episodes                  | 4700       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.80532  |
| test/Q_plus_P                  | -39.80532  |
| test/reward_per_eps            | -40        |
| test/steps                     | 188000     |
| train/episodes                 | 18800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 752000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.20396775 |
| stats_o/std                    | 0.05471965 |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.767426 |
| test/Q_plus_P                  | -39.767426 |
| test/reward_per_eps            | -40        |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.20396996 |
| stats_o/std                    | 0.05469453 |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.78203  |
| test/Q_plus_P                  | -39.78203  |
| test/reward_per_eps            | -40        |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.20396233 |
| stats_o/std                    | 0.05466861 |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.796413 |
| test/Q_plus_P                  | -39.796413 |
| test/reward_per_eps            | -40        |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 473         |
| stats_o/mean                   | 0.20396215  |
| stats_o/std                    | 0.054641668 |
| test/episodes                  | 4740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.84604   |
| test/Q_plus_P                  | -39.84604   |
| test/reward_per_eps            | -40         |
| test/steps                     | 189600      |
| train/episodes                 | 18960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 758400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 474         |
| stats_o/mean                   | 0.20396423  |
| stats_o/std                    | 0.054617886 |
| test/episodes                  | 4750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.756172  |
| test/Q_plus_P                  | -39.756172  |
| test/reward_per_eps            | -40         |
| test/steps                     | 190000      |
| train/episodes                 | 19000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 760000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 475        |
| stats_o/mean                   | 0.20397042 |
| stats_o/std                    | 0.05461162 |
| test/episodes                  | 4760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.80212  |
| test/Q_plus_P                  | -39.80212  |
| test/reward_per_eps            | -40        |
| test/steps                     | 190400     |
| train/episodes                 | 19040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 761600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.20397371  |
| stats_o/std                    | 0.054586135 |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.812183  |
| test/Q_plus_P                  | -39.812183  |
| test/reward_per_eps            | -40         |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.20397623  |
| stats_o/std                    | 0.054559764 |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.79893   |
| test/Q_plus_P                  | -39.79893   |
| test/reward_per_eps            | -40         |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 478         |
| stats_o/mean                   | 0.20396803  |
| stats_o/std                    | 0.054567486 |
| test/episodes                  | 4790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.85046   |
| test/Q_plus_P                  | -39.85046   |
| test/reward_per_eps            | -40         |
| test/steps                     | 191600      |
| train/episodes                 | 19160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 766400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.20396987  |
| stats_o/std                    | 0.054541524 |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.811047  |
| test/Q_plus_P                  | -39.811047  |
| test/reward_per_eps            | -40         |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 480        |
| stats_o/mean                   | 0.20397127 |
| stats_o/std                    | 0.05451597 |
| test/episodes                  | 4810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.785133 |
| test/Q_plus_P                  | -39.785133 |
| test/reward_per_eps            | -40        |
| test/steps                     | 192400     |
| train/episodes                 | 19240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 769600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 481        |
| stats_o/mean                   | 0.20396718 |
| stats_o/std                    | 0.05449227 |
| test/episodes                  | 4820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.831074 |
| test/Q_plus_P                  | -39.831074 |
| test/reward_per_eps            | -40        |
| test/steps                     | 192800     |
| train/episodes                 | 19280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 771200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.20396622  |
| stats_o/std                    | 0.054467775 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.82106   |
| test/Q_plus_P                  | -39.82106   |
| test/reward_per_eps            | -40         |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 483         |
| stats_o/mean                   | 0.20397024  |
| stats_o/std                    | 0.054443818 |
| test/episodes                  | 4840        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.810997  |
| test/Q_plus_P                  | -39.810997  |
| test/reward_per_eps            | -40         |
| test/steps                     | 193600      |
| train/episodes                 | 19360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 774400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.2039744   |
| stats_o/std                    | 0.054419145 |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.80417   |
| test/Q_plus_P                  | -39.80417   |
| test/reward_per_eps            | -40         |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.20397209  |
| stats_o/std                    | 0.054402214 |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.80638   |
| test/Q_plus_P                  | -39.80638   |
| test/reward_per_eps            | -40         |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 486        |
| stats_o/mean                   | 0.20396851 |
| stats_o/std                    | 0.05437885 |
| test/episodes                  | 4870       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.80926  |
| test/Q_plus_P                  | -39.80926  |
| test/reward_per_eps            | -40        |
| test/steps                     | 194800     |
| train/episodes                 | 19480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 779200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 487         |
| stats_o/mean                   | 0.2039662   |
| stats_o/std                    | 0.054354582 |
| test/episodes                  | 4880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.817825  |
| test/Q_plus_P                  | -39.817825  |
| test/reward_per_eps            | -40         |
| test/steps                     | 195200      |
| train/episodes                 | 19520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 780800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 488         |
| stats_o/mean                   | 0.2039693   |
| stats_o/std                    | 0.054330558 |
| test/episodes                  | 4890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.819725  |
| test/Q_plus_P                  | -39.819725  |
| test/reward_per_eps            | -40         |
| test/steps                     | 195600      |
| train/episodes                 | 19560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 782400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 489        |
| stats_o/mean                   | 0.20396973 |
| stats_o/std                    | 0.05430623 |
| test/episodes                  | 4900       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.82683  |
| test/Q_plus_P                  | -39.82683  |
| test/reward_per_eps            | -40        |
| test/steps                     | 196000     |
| train/episodes                 | 19600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 784000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 490         |
| stats_o/mean                   | 0.20396383  |
| stats_o/std                    | 0.054282643 |
| test/episodes                  | 4910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.813347  |
| test/Q_plus_P                  | -39.813347  |
| test/reward_per_eps            | -40         |
| test/steps                     | 196400      |
| train/episodes                 | 19640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 785600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.20396252  |
| stats_o/std                    | 0.054258175 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.8251    |
| test/Q_plus_P                  | -39.8251    |
| test/reward_per_eps            | -40         |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 492         |
| stats_o/mean                   | 0.20396464  |
| stats_o/std                    | 0.054233957 |
| test/episodes                  | 4930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.84162   |
| test/Q_plus_P                  | -39.84162   |
| test/reward_per_eps            | -40         |
| test/steps                     | 197200      |
| train/episodes                 | 19720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 788800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.20395851 |
| stats_o/std                    | 0.05421023 |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.82298  |
| test/Q_plus_P                  | -39.82298  |
| test/reward_per_eps            | -40        |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.20396475  |
| stats_o/std                    | 0.054187644 |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.824913  |
| test/Q_plus_P                  | -39.824913  |
| test/reward_per_eps            | -40         |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 495         |
| stats_o/mean                   | 0.20396298  |
| stats_o/std                    | 0.054164372 |
| test/episodes                  | 4960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.838352  |
| test/Q_plus_P                  | -39.838352  |
| test/reward_per_eps            | -40         |
| test/steps                     | 198400      |
| train/episodes                 | 19840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 793600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 496         |
| stats_o/mean                   | 0.20396172  |
| stats_o/std                    | 0.054139525 |
| test/episodes                  | 4970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.849144  |
| test/Q_plus_P                  | -39.849144  |
| test/reward_per_eps            | -40         |
| test/steps                     | 198800      |
| train/episodes                 | 19880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 795200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 497         |
| stats_o/mean                   | 0.2039671   |
| stats_o/std                    | 0.054114234 |
| test/episodes                  | 4980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.822083  |
| test/Q_plus_P                  | -39.822083  |
| test/reward_per_eps            | -40         |
| test/steps                     | 199200      |
| train/episodes                 | 19920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 796800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 498         |
| stats_o/mean                   | 0.20396812  |
| stats_o/std                    | 0.054091215 |
| test/episodes                  | 4990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.831005  |
| test/Q_plus_P                  | -39.831005  |
| test/reward_per_eps            | -40         |
| test/steps                     | 199600      |
| train/episodes                 | 19960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 798400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 499        |
| stats_o/mean                   | 0.2039683  |
| stats_o/std                    | 0.05406623 |
| test/episodes                  | 5000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.842216 |
| test/Q_plus_P                  | -39.842216 |
| test/reward_per_eps            | -40        |
| test/steps                     | 200000     |
| train/episodes                 | 20000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 800000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 500         |
| stats_o/mean                   | 0.20396562  |
| stats_o/std                    | 0.054041646 |
| test/episodes                  | 5010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.835976  |
| test/Q_plus_P                  | -39.835976  |
| test/reward_per_eps            | -40         |
| test/steps                     | 200400      |
| train/episodes                 | 20040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 801600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 501         |
| stats_o/mean                   | 0.20395742  |
| stats_o/std                    | 0.054017063 |
| test/episodes                  | 5020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.844383  |
| test/Q_plus_P                  | -39.844383  |
| test/reward_per_eps            | -40         |
| test/steps                     | 200800      |
| train/episodes                 | 20080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 803200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.20396002  |
| stats_o/std                    | 0.053993415 |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.846725  |
| test/Q_plus_P                  | -39.846725  |
| test/reward_per_eps            | -40         |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 503        |
| stats_o/mean                   | 0.20396239 |
| stats_o/std                    | 0.05396947 |
| test/episodes                  | 5040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.84917  |
| test/Q_plus_P                  | -39.84917  |
| test/reward_per_eps            | -40        |
| test/steps                     | 201600     |
| train/episodes                 | 20160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 806400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 504         |
| stats_o/mean                   | 0.20396082  |
| stats_o/std                    | 0.053946666 |
| test/episodes                  | 5050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.85681   |
| test/Q_plus_P                  | -39.85681   |
| test/reward_per_eps            | -40         |
| test/steps                     | 202000      |
| train/episodes                 | 20200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 808000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 505        |
| stats_o/mean                   | 0.20396523 |
| stats_o/std                    | 0.05392367 |
| test/episodes                  | 5060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.841694 |
| test/Q_plus_P                  | -39.841694 |
| test/reward_per_eps            | -40        |
| test/steps                     | 202400     |
| train/episodes                 | 20240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 809600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 506        |
| stats_o/mean                   | 0.20396805 |
| stats_o/std                    | 0.0538996  |
| test/episodes                  | 5070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.85993  |
| test/Q_plus_P                  | -39.85993  |
| test/reward_per_eps            | -40        |
| test/steps                     | 202800     |
| train/episodes                 | 20280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 811200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 507         |
| stats_o/mean                   | 0.20397009  |
| stats_o/std                    | 0.053876482 |
| test/episodes                  | 5080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.8583    |
| test/Q_plus_P                  | -39.8583    |
| test/reward_per_eps            | -40         |
| test/steps                     | 203200      |
| train/episodes                 | 20320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 812800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 508         |
| stats_o/mean                   | 0.20397198  |
| stats_o/std                    | 0.053854402 |
| test/episodes                  | 5090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.862396  |
| test/Q_plus_P                  | -39.862396  |
| test/reward_per_eps            | -40         |
| test/steps                     | 203600      |
| train/episodes                 | 20360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 814400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 509         |
| stats_o/mean                   | 0.20397682  |
| stats_o/std                    | 0.053832542 |
| test/episodes                  | 5100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.854336  |
| test/Q_plus_P                  | -39.854336  |
| test/reward_per_eps            | -40         |
| test/steps                     | 204000      |
| train/episodes                 | 20400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 816000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 510        |
| stats_o/mean                   | 0.20398024 |
| stats_o/std                    | 0.05380978 |
| test/episodes                  | 5110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.813995 |
| test/Q_plus_P                  | -39.813995 |
| test/reward_per_eps            | -40        |
| test/steps                     | 204400     |
| train/episodes                 | 20440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 817600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 511         |
| stats_o/mean                   | 0.20397083  |
| stats_o/std                    | 0.053840507 |
| test/episodes                  | 5120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.86464   |
| test/Q_plus_P                  | -39.86464   |
| test/reward_per_eps            | -40         |
| test/steps                     | 204800      |
| train/episodes                 | 20480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 819200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 512         |
| stats_o/mean                   | 0.20396835  |
| stats_o/std                    | 0.053817667 |
| test/episodes                  | 5130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.85945   |
| test/Q_plus_P                  | -39.85945   |
| test/reward_per_eps            | -40         |
| test/steps                     | 205200      |
| train/episodes                 | 20520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 820800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 513        |
| stats_o/mean                   | 0.20396797 |
| stats_o/std                    | 0.05379499 |
| test/episodes                  | 5140       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.87366  |
| test/Q_plus_P                  | -39.87366  |
| test/reward_per_eps            | -40        |
| test/steps                     | 205600     |
| train/episodes                 | 20560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 822400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 514        |
| stats_o/mean                   | 0.2039735  |
| stats_o/std                    | 0.05377402 |
| test/episodes                  | 5150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.894688 |
| test/Q_plus_P                  | -39.894688 |
| test/reward_per_eps            | -40        |
| test/steps                     | 206000     |
| train/episodes                 | 20600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 824000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 515         |
| stats_o/mean                   | 0.2039806   |
| stats_o/std                    | 0.053752795 |
| test/episodes                  | 5160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.889545  |
| test/Q_plus_P                  | -39.889545  |
| test/reward_per_eps            | -40         |
| test/steps                     | 206400      |
| train/episodes                 | 20640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 825600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 516        |
| stats_o/mean                   | 0.20398393 |
| stats_o/std                    | 0.0537323  |
| test/episodes                  | 5170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.890686 |
| test/Q_plus_P                  | -39.890686 |
| test/reward_per_eps            | -40        |
| test/steps                     | 206800     |
| train/episodes                 | 20680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 827200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 517        |
| stats_o/mean                   | 0.20397785 |
| stats_o/std                    | 0.05370983 |
| test/episodes                  | 5180       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.871742 |
| test/Q_plus_P                  | -39.871742 |
| test/reward_per_eps            | -40        |
| test/steps                     | 207200     |
| train/episodes                 | 20720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 828800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 518         |
| stats_o/mean                   | 0.20397186  |
| stats_o/std                    | 0.053686947 |
| test/episodes                  | 5190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.87882   |
| test/Q_plus_P                  | -39.87882   |
| test/reward_per_eps            | -40         |
| test/steps                     | 207600      |
| train/episodes                 | 20760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 830400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 519        |
| stats_o/mean                   | 0.20397179 |
| stats_o/std                    | 0.05366411 |
| test/episodes                  | 5200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.86857  |
| test/Q_plus_P                  | -39.86857  |
| test/reward_per_eps            | -40        |
| test/steps                     | 208000     |
| train/episodes                 | 20800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 832000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.20397502  |
| stats_o/std                    | 0.053641405 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.881554  |
| test/Q_plus_P                  | -39.881554  |
| test/reward_per_eps            | -40         |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 521        |
| stats_o/mean                   | 0.20397846 |
| stats_o/std                    | 0.05361876 |
| test/episodes                  | 5220       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.87571  |
| test/Q_plus_P                  | -39.87571  |
| test/reward_per_eps            | -40        |
| test/steps                     | 208800     |
| train/episodes                 | 20880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 835200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 522         |
| stats_o/mean                   | 0.2039826   |
| stats_o/std                    | 0.053596772 |
| test/episodes                  | 5230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.83896   |
| test/Q_plus_P                  | -39.83896   |
| test/reward_per_eps            | -40         |
| test/steps                     | 209200      |
| train/episodes                 | 20920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 836800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 523        |
| stats_o/mean                   | 0.20399025 |
| stats_o/std                    | 0.05357703 |
| test/episodes                  | 5240       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.874325 |
| test/Q_plus_P                  | -39.874325 |
| test/reward_per_eps            | -40        |
| test/steps                     | 209600     |
| train/episodes                 | 20960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 838400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 524        |
| stats_o/mean                   | 0.20398846 |
| stats_o/std                    | 0.05355356 |
| test/episodes                  | 5250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.853394 |
| test/Q_plus_P                  | -39.853394 |
| test/reward_per_eps            | -40        |
| test/steps                     | 210000     |
| train/episodes                 | 21000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 840000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 525         |
| stats_o/mean                   | 0.20398669  |
| stats_o/std                    | 0.053531405 |
| test/episodes                  | 5260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.87037   |
| test/Q_plus_P                  | -39.87037   |
| test/reward_per_eps            | -40         |
| test/steps                     | 210400      |
| train/episodes                 | 21040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 841600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 526        |
| stats_o/mean                   | 0.20399229 |
| stats_o/std                    | 0.05350925 |
| test/episodes                  | 5270       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.887573 |
| test/Q_plus_P                  | -39.887573 |
| test/reward_per_eps            | -40        |
| test/steps                     | 210800     |
| train/episodes                 | 21080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 843200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 527        |
| stats_o/mean                   | 0.20398822 |
| stats_o/std                    | 0.05348784 |
| test/episodes                  | 5280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.84739  |
| test/Q_plus_P                  | -39.84739  |
| test/reward_per_eps            | -40        |
| test/steps                     | 211200     |
| train/episodes                 | 21120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 844800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 528        |
| stats_o/mean                   | 0.20399164 |
| stats_o/std                    | 0.05346662 |
| test/episodes                  | 5290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.87936  |
| test/Q_plus_P                  | -39.87936  |
| test/reward_per_eps            | -40        |
| test/steps                     | 211600     |
| train/episodes                 | 21160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 846400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.20399106  |
| stats_o/std                    | 0.053444963 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.84888   |
| test/Q_plus_P                  | -39.84888   |
| test/reward_per_eps            | -40         |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 530         |
| stats_o/mean                   | 0.20398909  |
| stats_o/std                    | 0.053424302 |
| test/episodes                  | 5310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.878338  |
| test/Q_plus_P                  | -39.878338  |
| test/reward_per_eps            | -40         |
| test/steps                     | 212400      |
| train/episodes                 | 21240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 849600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 531         |
| stats_o/mean                   | 0.20399538  |
| stats_o/std                    | 0.053404566 |
| test/episodes                  | 5320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.888344  |
| test/Q_plus_P                  | -39.888344  |
| test/reward_per_eps            | -40         |
| test/steps                     | 212800      |
| train/episodes                 | 21280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 851200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.20399456  |
| stats_o/std                    | 0.053381346 |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.883907  |
| test/Q_plus_P                  | -39.883907  |
| test/reward_per_eps            | -40         |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 533       |
| stats_o/mean                   | 0.2039885 |
| stats_o/std                    | 0.0533602 |
| test/episodes                  | 5340      |
| test/info_is_success_max       | 0         |
| test/info_is_success_mean      | 0         |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.173    |
| test/info_shaping_reward_mean  | -0.173    |
| test/info_shaping_reward_min   | -0.173    |
| test/Q                         | -39.88312 |
| test/Q_plus_P                  | -39.88312 |
| test/reward_per_eps            | -40       |
| test/steps                     | 213600    |
| train/episodes                 | 21360     |
| train/info_is_success_max      | 0         |
| train/info_is_success_mean     | 0         |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.173    |
| train/info_shaping_reward_mean | -0.173    |
| train/info_shaping_reward_min  | -0.173    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -40       |
| train/steps                    | 854400    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 534        |
| stats_o/mean                   | 0.20399584 |
| stats_o/std                    | 0.05333863 |
| test/episodes                  | 5350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.87318  |
| test/Q_plus_P                  | -39.87318  |
| test/reward_per_eps            | -40        |
| test/steps                     | 214000     |
| train/episodes                 | 21400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 856000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 535        |
| stats_o/mean                   | 0.2039951  |
| stats_o/std                    | 0.05331717 |
| test/episodes                  | 5360       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.902103 |
| test/Q_plus_P                  | -39.902103 |
| test/reward_per_eps            | -40        |
| test/steps                     | 214400     |
| train/episodes                 | 21440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 857600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 536         |
| stats_o/mean                   | 0.20399046  |
| stats_o/std                    | 0.053294986 |
| test/episodes                  | 5370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.875988  |
| test/Q_plus_P                  | -39.875988  |
| test/reward_per_eps            | -40         |
| test/steps                     | 214800      |
| train/episodes                 | 21480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 859200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 537        |
| stats_o/mean                   | 0.20398842 |
| stats_o/std                    | 0.05327394 |
| test/episodes                  | 5380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.902065 |
| test/Q_plus_P                  | -39.902065 |
| test/reward_per_eps            | -40        |
| test/steps                     | 215200     |
| train/episodes                 | 21520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 860800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 538         |
| stats_o/mean                   | 0.20398718  |
| stats_o/std                    | 0.053252734 |
| test/episodes                  | 5390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.899086  |
| test/Q_plus_P                  | -39.899086  |
| test/reward_per_eps            | -40         |
| test/steps                     | 215600      |
| train/episodes                 | 21560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 862400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 539         |
| stats_o/mean                   | 0.20399055  |
| stats_o/std                    | 0.053231746 |
| test/episodes                  | 5400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.89492   |
| test/Q_plus_P                  | -39.89492   |
| test/reward_per_eps            | -40         |
| test/steps                     | 216000      |
| train/episodes                 | 21600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 864000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 540         |
| stats_o/mean                   | 0.20399477  |
| stats_o/std                    | 0.053245794 |
| test/episodes                  | 5410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.920006  |
| test/Q_plus_P                  | -39.920006  |
| test/reward_per_eps            | -40         |
| test/steps                     | 216400      |
| train/episodes                 | 21640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 865600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.20399497  |
| stats_o/std                    | 0.053224586 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.90207   |
| test/Q_plus_P                  | -39.90207   |
| test/reward_per_eps            | -40         |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 542         |
| stats_o/mean                   | 0.20399319  |
| stats_o/std                    | 0.053203054 |
| test/episodes                  | 5430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.908295  |
| test/Q_plus_P                  | -39.908295  |
| test/reward_per_eps            | -40         |
| test/steps                     | 217200      |
| train/episodes                 | 21720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 868800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 543        |
| stats_o/mean                   | 0.20399228 |
| stats_o/std                    | 0.05318168 |
| test/episodes                  | 5440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.847996 |
| test/Q_plus_P                  | -39.847996 |
| test/reward_per_eps            | -40        |
| test/steps                     | 217600     |
| train/episodes                 | 21760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 870400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 544        |
| stats_o/mean                   | 0.20399794 |
| stats_o/std                    | 0.05316033 |
| test/episodes                  | 5450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.910034 |
| test/Q_plus_P                  | -39.910034 |
| test/reward_per_eps            | -40        |
| test/steps                     | 218000     |
| train/episodes                 | 21800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 872000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 545         |
| stats_o/mean                   | 0.203991    |
| stats_o/std                    | 0.053138584 |
| test/episodes                  | 5460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.886276  |
| test/Q_plus_P                  | -39.886276  |
| test/reward_per_eps            | -40         |
| test/steps                     | 218400      |
| train/episodes                 | 21840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 873600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.20399538  |
| stats_o/std                    | 0.053115506 |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.88407   |
| test/Q_plus_P                  | -39.88407   |
| test/reward_per_eps            | -40         |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 547        |
| stats_o/mean                   | 0.20399195 |
| stats_o/std                    | 0.05309611 |
| test/episodes                  | 5480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.90436  |
| test/Q_plus_P                  | -39.90436  |
| test/reward_per_eps            | -40        |
| test/steps                     | 219200     |
| train/episodes                 | 21920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 876800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 548        |
| stats_o/mean                   | 0.20398894 |
| stats_o/std                    | 0.05307446 |
| test/episodes                  | 5490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.91209  |
| test/Q_plus_P                  | -39.91209  |
| test/reward_per_eps            | -40        |
| test/steps                     | 219600     |
| train/episodes                 | 21960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 878400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.20398964  |
| stats_o/std                    | 0.053053703 |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.901974  |
| test/Q_plus_P                  | -39.901974  |
| test/reward_per_eps            | -40         |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 550         |
| stats_o/mean                   | 0.20398998  |
| stats_o/std                    | 0.053033143 |
| test/episodes                  | 5510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.906242  |
| test/Q_plus_P                  | -39.906242  |
| test/reward_per_eps            | -40         |
| test/steps                     | 220400      |
| train/episodes                 | 22040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 881600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 551         |
| stats_o/mean                   | 0.20398787  |
| stats_o/std                    | 0.053012803 |
| test/episodes                  | 5520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.88212   |
| test/Q_plus_P                  | -39.88212   |
| test/reward_per_eps            | -40         |
| test/steps                     | 220800      |
| train/episodes                 | 22080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 883200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 552        |
| stats_o/mean                   | 0.20399019 |
| stats_o/std                    | 0.05299218 |
| test/episodes                  | 5530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.913387 |
| test/Q_plus_P                  | -39.913387 |
| test/reward_per_eps            | -40        |
| test/steps                     | 221200     |
| train/episodes                 | 22120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 884800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 553         |
| stats_o/mean                   | 0.20399313  |
| stats_o/std                    | 0.052971803 |
| test/episodes                  | 5540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.9428    |
| test/Q_plus_P                  | -39.9428    |
| test/reward_per_eps            | -40         |
| test/steps                     | 221600      |
| train/episodes                 | 22160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 886400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 554         |
| stats_o/mean                   | 0.20399456  |
| stats_o/std                    | 0.052951623 |
| test/episodes                  | 5550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.90543   |
| test/Q_plus_P                  | -39.90543   |
| test/reward_per_eps            | -40         |
| test/steps                     | 222000      |
| train/episodes                 | 22200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 888000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 555         |
| stats_o/mean                   | 0.20399103  |
| stats_o/std                    | 0.052931048 |
| test/episodes                  | 5560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.908222  |
| test/Q_plus_P                  | -39.908222  |
| test/reward_per_eps            | -40         |
| test/steps                     | 222400      |
| train/episodes                 | 22240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 889600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 556         |
| stats_o/mean                   | 0.20399708  |
| stats_o/std                    | 0.052910324 |
| test/episodes                  | 5570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.92481   |
| test/Q_plus_P                  | -39.92481   |
| test/reward_per_eps            | -40         |
| test/steps                     | 222800      |
| train/episodes                 | 22280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 891200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 557         |
| stats_o/mean                   | 0.20400113  |
| stats_o/std                    | 0.052890334 |
| test/episodes                  | 5580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.921036  |
| test/Q_plus_P                  | -39.921036  |
| test/reward_per_eps            | -40         |
| test/steps                     | 223200      |
| train/episodes                 | 22320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 892800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 558        |
| stats_o/mean                   | 0.20399742 |
| stats_o/std                    | 0.05287086 |
| test/episodes                  | 5590       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.91539  |
| test/Q_plus_P                  | -39.91539  |
| test/reward_per_eps            | -40        |
| test/steps                     | 223600     |
| train/episodes                 | 22360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 894400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 559         |
| stats_o/mean                   | 0.20399661  |
| stats_o/std                    | 0.052850332 |
| test/episodes                  | 5600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.918907  |
| test/Q_plus_P                  | -39.918907  |
| test/reward_per_eps            | -40         |
| test/steps                     | 224000      |
| train/episodes                 | 22400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 896000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 560         |
| stats_o/mean                   | 0.20399484  |
| stats_o/std                    | 0.052830376 |
| test/episodes                  | 5610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.91964   |
| test/Q_plus_P                  | -39.91964   |
| test/reward_per_eps            | -40         |
| test/steps                     | 224400      |
| train/episodes                 | 22440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 897600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 561         |
| stats_o/mean                   | 0.20399399  |
| stats_o/std                    | 0.052824944 |
| test/episodes                  | 5620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.922764  |
| test/Q_plus_P                  | -39.922764  |
| test/reward_per_eps            | -40         |
| test/steps                     | 224800      |
| train/episodes                 | 22480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 899200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.20399743  |
| stats_o/std                    | 0.052805524 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.929226  |
| test/Q_plus_P                  | -39.929226  |
| test/reward_per_eps            | -40         |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.20400454  |
| stats_o/std                    | 0.052786894 |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.934887  |
| test/Q_plus_P                  | -39.934887  |
| test/reward_per_eps            | -40         |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 564         |
| stats_o/mean                   | 0.20400298  |
| stats_o/std                    | 0.052766956 |
| test/episodes                  | 5650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.912777  |
| test/Q_plus_P                  | -39.912777  |
| test/reward_per_eps            | -40         |
| test/steps                     | 226000      |
| train/episodes                 | 22600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 904000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 565         |
| stats_o/mean                   | 0.20400213  |
| stats_o/std                    | 0.052747983 |
| test/episodes                  | 5660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.930336  |
| test/Q_plus_P                  | -39.930336  |
| test/reward_per_eps            | -40         |
| test/steps                     | 226400      |
| train/episodes                 | 22640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 905600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.2040028   |
| stats_o/std                    | 0.052727688 |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.925438  |
| test/Q_plus_P                  | -39.925438  |
| test/reward_per_eps            | -40         |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 567        |
| stats_o/mean                   | 0.20400009 |
| stats_o/std                    | 0.05270832 |
| test/episodes                  | 5680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.927017 |
| test/Q_plus_P                  | -39.927017 |
| test/reward_per_eps            | -40        |
| test/steps                     | 227200     |
| train/episodes                 | 22720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 908800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.20400093  |
| stats_o/std                    | 0.052688174 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.926205  |
| test/Q_plus_P                  | -39.926205  |
| test/reward_per_eps            | -40         |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.2039996   |
| stats_o/std                    | 0.052667424 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.91697   |
| test/Q_plus_P                  | -39.91697   |
| test/reward_per_eps            | -40         |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.20399779  |
| stats_o/std                    | 0.052647438 |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.95502   |
| test/Q_plus_P                  | -39.95502   |
| test/reward_per_eps            | -40         |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 571         |
| stats_o/mean                   | 0.20399688  |
| stats_o/std                    | 0.052627705 |
| test/episodes                  | 5720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.926197  |
| test/Q_plus_P                  | -39.926197  |
| test/reward_per_eps            | -40         |
| test/steps                     | 228800      |
| train/episodes                 | 22880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 915200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.204003    |
| stats_o/std                    | 0.052607264 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.93399   |
| test/Q_plus_P                  | -39.93399   |
| test/reward_per_eps            | -40         |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 573         |
| stats_o/mean                   | 0.20400229  |
| stats_o/std                    | 0.052588053 |
| test/episodes                  | 5740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.92683   |
| test/Q_plus_P                  | -39.92683   |
| test/reward_per_eps            | -40         |
| test/steps                     | 229600      |
| train/episodes                 | 22960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 918400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.20400192  |
| stats_o/std                    | 0.052568298 |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.926918  |
| test/Q_plus_P                  | -39.926918  |
| test/reward_per_eps            | -40         |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.20400032  |
| stats_o/std                    | 0.052547857 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.926968  |
| test/Q_plus_P                  | -39.926968  |
| test/reward_per_eps            | -40         |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.20399775  |
| stats_o/std                    | 0.052529324 |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.93085   |
| test/Q_plus_P                  | -39.93085   |
| test/reward_per_eps            | -40         |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 577        |
| stats_o/mean                   | 0.2039918  |
| stats_o/std                    | 0.05250917 |
| test/episodes                  | 5780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.924572 |
| test/Q_plus_P                  | -39.924572 |
| test/reward_per_eps            | -40        |
| test/steps                     | 231200     |
| train/episodes                 | 23120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 924800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 578         |
| stats_o/mean                   | 0.20399031  |
| stats_o/std                    | 0.052489117 |
| test/episodes                  | 5790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.932682  |
| test/Q_plus_P                  | -39.932682  |
| test/reward_per_eps            | -40         |
| test/steps                     | 231600      |
| train/episodes                 | 23160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 926400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 579         |
| stats_o/mean                   | 0.2039893   |
| stats_o/std                    | 0.052470602 |
| test/episodes                  | 5800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.933727  |
| test/Q_plus_P                  | -39.933727  |
| test/reward_per_eps            | -40         |
| test/steps                     | 232000      |
| train/episodes                 | 23200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 928000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 580        |
| stats_o/mean                   | 0.20398764 |
| stats_o/std                    | 0.05245183 |
| test/episodes                  | 5810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.935543 |
| test/Q_plus_P                  | -39.935543 |
| test/reward_per_eps            | -40        |
| test/steps                     | 232400     |
| train/episodes                 | 23240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 929600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 581        |
| stats_o/mean                   | 0.20399031 |
| stats_o/std                    | 0.05243374 |
| test/episodes                  | 5820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.92623  |
| test/Q_plus_P                  | -39.92623  |
| test/reward_per_eps            | -40        |
| test/steps                     | 232800     |
| train/episodes                 | 23280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 931200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 582        |
| stats_o/mean                   | 0.20399275 |
| stats_o/std                    | 0.05241518 |
| test/episodes                  | 5830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.924366 |
| test/Q_plus_P                  | -39.924366 |
| test/reward_per_eps            | -40        |
| test/steps                     | 233200     |
| train/episodes                 | 23320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 932800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 583         |
| stats_o/mean                   | 0.20398928  |
| stats_o/std                    | 0.052396737 |
| test/episodes                  | 5840        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.933983  |
| test/Q_plus_P                  | -39.933983  |
| test/reward_per_eps            | -40         |
| test/steps                     | 233600      |
| train/episodes                 | 23360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 934400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 584         |
| stats_o/mean                   | 0.204       |
| stats_o/std                    | 0.052377835 |
| test/episodes                  | 5850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.925247  |
| test/Q_plus_P                  | -39.925247  |
| test/reward_per_eps            | -40         |
| test/steps                     | 234000      |
| train/episodes                 | 23400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 936000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 585         |
| stats_o/mean                   | 0.20399782  |
| stats_o/std                    | 0.052358523 |
| test/episodes                  | 5860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.903465  |
| test/Q_plus_P                  | -39.903465  |
| test/reward_per_eps            | -40         |
| test/steps                     | 234400      |
| train/episodes                 | 23440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 937600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 586        |
| stats_o/mean                   | 0.2039975  |
| stats_o/std                    | 0.05234333 |
| test/episodes                  | 5870       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.876488 |
| test/Q_plus_P                  | -39.876488 |
| test/reward_per_eps            | -40        |
| test/steps                     | 234800     |
| train/episodes                 | 23480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 939200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 587         |
| stats_o/mean                   | 0.20400709  |
| stats_o/std                    | 0.052323293 |
| test/episodes                  | 5880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.928764  |
| test/Q_plus_P                  | -39.928764  |
| test/reward_per_eps            | -40         |
| test/steps                     | 235200      |
| train/episodes                 | 23520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 940800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 588        |
| stats_o/mean                   | 0.20400757 |
| stats_o/std                    | 0.05230571 |
| test/episodes                  | 5890       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.91752  |
| test/Q_plus_P                  | -39.91752  |
| test/reward_per_eps            | -40        |
| test/steps                     | 235600     |
| train/episodes                 | 23560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 942400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 589        |
| stats_o/mean                   | 0.20400672 |
| stats_o/std                    | 0.05228668 |
| test/episodes                  | 5900       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.942474 |
| test/Q_plus_P                  | -39.942474 |
| test/reward_per_eps            | -40        |
| test/steps                     | 236000     |
| train/episodes                 | 23600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 944000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.20400864  |
| stats_o/std                    | 0.052267548 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.935196  |
| test/Q_plus_P                  | -39.935196  |
| test/reward_per_eps            | -40         |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 591         |
| stats_o/mean                   | 0.20400178  |
| stats_o/std                    | 0.052250788 |
| test/episodes                  | 5920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.927372  |
| test/Q_plus_P                  | -39.927372  |
| test/reward_per_eps            | -40         |
| test/steps                     | 236800      |
| train/episodes                 | 23680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 947200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 592        |
| stats_o/mean                   | 0.20399632 |
| stats_o/std                    | 0.05223093 |
| test/episodes                  | 5930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.92161  |
| test/Q_plus_P                  | -39.92161  |
| test/reward_per_eps            | -40        |
| test/steps                     | 237200     |
| train/episodes                 | 23720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 948800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.2040009   |
| stats_o/std                    | 0.052213654 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.92613   |
| test/Q_plus_P                  | -39.92613   |
| test/reward_per_eps            | -40         |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 594         |
| stats_o/mean                   | 0.20400326  |
| stats_o/std                    | 0.052195765 |
| test/episodes                  | 5950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.921925  |
| test/Q_plus_P                  | -39.921925  |
| test/reward_per_eps            | -40         |
| test/steps                     | 238000      |
| train/episodes                 | 23800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 952000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 595         |
| stats_o/mean                   | 0.204001    |
| stats_o/std                    | 0.052176476 |
| test/episodes                  | 5960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.92636   |
| test/Q_plus_P                  | -39.92636   |
| test/reward_per_eps            | -40         |
| test/steps                     | 238400      |
| train/episodes                 | 23840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 953600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 596        |
| stats_o/mean                   | 0.20400192 |
| stats_o/std                    | 0.05215868 |
| test/episodes                  | 5970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.92261  |
| test/Q_plus_P                  | -39.92261  |
| test/reward_per_eps            | -40        |
| test/steps                     | 238800     |
| train/episodes                 | 23880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 955200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 597        |
| stats_o/mean                   | 0.20399551 |
| stats_o/std                    | 0.05214036 |
| test/episodes                  | 5980       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.920826 |
| test/Q_plus_P                  | -39.920826 |
| test/reward_per_eps            | -40        |
| test/steps                     | 239200     |
| train/episodes                 | 23920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 956800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.2039974   |
| stats_o/std                    | 0.052122205 |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -39.898373  |
| test/Q_plus_P                  | -39.898373  |
| test/reward_per_eps            | -40         |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.2039994  |
| stats_o/std                    | 0.0521044  |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -39.915882 |
| test/Q_plus_P                  | -39.915882 |
| test/reward_per_eps            | -40        |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
