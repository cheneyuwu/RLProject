Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPickAndPlace/config_TD3_BC_Init/seed_1
epoch: 19 policy initialization loss: 0.010032505728304386
epoch: 39 policy initialization loss: 0.0031896280124783516
epoch: 59 policy initialization loss: 0.0021191262640058994
epoch: 79 policy initialization loss: 0.001456502010114491
epoch: 99 policy initialization loss: 0.000796228414401412
epoch: 119 policy initialization loss: 0.0017868049908429384
epoch: 139 policy initialization loss: 0.0010575491469353437
epoch: 159 policy initialization loss: 0.0005170134245418012
epoch: 179 policy initialization loss: 0.0006654613534919918
epoch: 199 policy initialization loss: 0.0004809203092008829
epoch: 219 policy initialization loss: 0.00027765112463384867
epoch: 239 policy initialization loss: 0.0004672253271564841
epoch: 259 policy initialization loss: 0.00011692524276440963
epoch: 279 policy initialization loss: 0.00022653216728940606
epoch: 299 policy initialization loss: 0.00022611570602748543
epoch: 319 policy initialization loss: 0.0029550744220614433
epoch: 339 policy initialization loss: 0.0014431850286200643
epoch: 359 policy initialization loss: 0.00010441050108056515
epoch: 379 policy initialization loss: 7.507337431889027e-05
epoch: 399 policy initialization loss: 8.30892677186057e-05
epoch: 419 policy initialization loss: 2.6742352929431945e-05
epoch: 439 policy initialization loss: 0.0001105117698898539
epoch: 459 policy initialization loss: 0.0008360737701877952
epoch: 479 policy initialization loss: 0.002814612118527293
epoch: 499 policy initialization loss: 6.643426604568958e-05
epoch: 519 policy initialization loss: 0.00036123686004430056
epoch: 539 policy initialization loss: 5.372449231799692e-05
epoch: 559 policy initialization loss: 0.0003802182909566909
epoch: 579 policy initialization loss: 0.0002079538389807567
epoch: 599 policy initialization loss: 0.0001894132001325488
epoch: 619 policy initialization loss: 1.854700894909911e-05
epoch: 639 policy initialization loss: 0.00011783954687416553
epoch: 659 policy initialization loss: 3.942518014810048e-05
epoch: 679 policy initialization loss: 6.228339771041647e-05
epoch: 699 policy initialization loss: 6.311750621534884e-05
epoch: 719 policy initialization loss: 0.0003670291625894606
epoch: 739 policy initialization loss: 4.865023583988659e-05
epoch: 759 policy initialization loss: 0.00014701882901135832
epoch: 779 policy initialization loss: 0.000407949963118881
epoch: 799 policy initialization loss: 0.00016055867308750749
epoch: 819 policy initialization loss: 4.956352131557651e-05
epoch: 839 policy initialization loss: 4.281080327928066e-05
epoch: 859 policy initialization loss: 6.031765224179253e-05
epoch: 879 policy initialization loss: 8.832391176838428e-05
epoch: 899 policy initialization loss: 3.172239667037502e-05
epoch: 919 policy initialization loss: 8.724657527636737e-05
epoch: 939 policy initialization loss: 2.276390659972094e-05
epoch: 959 policy initialization loss: 3.513514093356207e-05
epoch: 979 policy initialization loss: 0.0009246727568097413
epoch: 999 policy initialization loss: 0.000282508903183043
epoch: 1019 policy initialization loss: 1.9765280740102753e-05
epoch: 1039 policy initialization loss: 1.7698148440103978e-05
epoch: 1059 policy initialization loss: 8.590788638684899e-06
epoch: 1079 policy initialization loss: 1.3552406016970053e-05
epoch: 1099 policy initialization loss: 2.2429325326811522e-05
epoch: 1119 policy initialization loss: 6.571780249942094e-05
epoch: 1139 policy initialization loss: 0.0005024591810069978
epoch: 1159 policy initialization loss: 5.606297781923786e-05
epoch: 1179 policy initialization loss: 2.3117878299672157e-05
epoch: 1199 policy initialization loss: 4.469427949516103e-05
epoch: 1219 policy initialization loss: 2.8855291020590812e-05
epoch: 1239 policy initialization loss: 4.926137626171112e-05
epoch: 1259 policy initialization loss: 0.00013856927398592234
epoch: 1279 policy initialization loss: 1.6660815163049847e-05
epoch: 1299 policy initialization loss: 0.0018458268605172634
epoch: 1319 policy initialization loss: 2.498506728443317e-05
epoch: 1339 policy initialization loss: 2.6555419026408345e-05
epoch: 1359 policy initialization loss: 1.1831067240564153e-05
epoch: 1379 policy initialization loss: 1.55234920384828e-05
epoch: 1399 policy initialization loss: 7.367492071352899e-05
epoch: 1419 policy initialization loss: 1.9217884982936084e-05
epoch: 1439 policy initialization loss: 0.001458799815736711
epoch: 1459 policy initialization loss: 1.9344188331160694e-05
epoch: 1479 policy initialization loss: 0.00011833511962322518
epoch: 1499 policy initialization loss: 8.785415047896095e-06
epoch: 1519 policy initialization loss: 1.2127162335673347e-05
epoch: 1539 policy initialization loss: 1.8256276234751567e-05
epoch: 1559 policy initialization loss: 0.00016421933833044022
epoch: 1579 policy initialization loss: 0.00014722271589562297
epoch: 1599 policy initialization loss: 1.6353978935512714e-05
epoch: 1619 policy initialization loss: 1.1676191206788644e-05
epoch: 1639 policy initialization loss: 0.00018470482609700412
epoch: 1659 policy initialization loss: 0.00018970199744217098
epoch: 1679 policy initialization loss: 9.952794061973691e-05
epoch: 1699 policy initialization loss: 4.707190964836627e-05
epoch: 1719 policy initialization loss: 7.0623195824737195e-06
epoch: 1739 policy initialization loss: 0.00017105278675444424
epoch: 1759 policy initialization loss: 1.168561811937252e-05
epoch: 1779 policy initialization loss: 1.2884315765404608e-05
epoch: 1799 policy initialization loss: 2.018628583755344e-05
epoch: 1819 policy initialization loss: 4.244349111104384e-05
epoch: 1839 policy initialization loss: 1.3416191904980224e-05
epoch: 1859 policy initialization loss: 0.0001737054844852537
epoch: 1879 policy initialization loss: 4.881572021986358e-05
epoch: 1899 policy initialization loss: 5.0510483561083674e-05
epoch: 1919 policy initialization loss: 0.00014698822633363307
epoch: 1939 policy initialization loss: 0.0016372173558920622
epoch: 1959 policy initialization loss: 3.899276634911075e-05
epoch: 1979 policy initialization loss: 1.9449438696028665e-05
epoch: 1999 policy initialization loss: 9.828427209868096e-06
Saving initial policy.
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.20409831  |
| stats_o/std                    | 0.047222413 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.359446   |
| test/Q_plus_P                  | -1.359446   |
| test/reward_per_eps            | -40         |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.20396799  |
| stats_o/std                    | 0.048545867 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.7835698  |
| test/Q_plus_P                  | -1.7835698  |
| test/reward_per_eps            | -40         |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 2           |
| stats_o/mean                   | 0.20333749  |
| stats_o/std                    | 0.047526922 |
| test/episodes                  | 30          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.0474708  |
| test/Q_plus_P                  | -2.0474708  |
| test/reward_per_eps            | -40         |
| test/steps                     | 1200        |
| train/episodes                 | 120         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 4800        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 3           |
| stats_o/mean                   | 0.20299266  |
| stats_o/std                    | 0.046196032 |
| test/episodes                  | 40          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -2.3743362  |
| test/Q_plus_P                  | -2.3743362  |
| test/reward_per_eps            | -40         |
| test/steps                     | 1600        |
| train/episodes                 | 160         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 6400        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 4          |
| stats_o/mean                   | 0.2031705  |
| stats_o/std                    | 0.04494725 |
| test/episodes                  | 50         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.784352  |
| test/Q_plus_P                  | -2.784352  |
| test/reward_per_eps            | -40        |
| test/steps                     | 2000       |
| train/episodes                 | 200        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 8000       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 5          |
| stats_o/mean                   | 0.20386872 |
| stats_o/std                    | 0.04413145 |
| test/episodes                  | 60         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.1649694 |
| test/Q_plus_P                  | -3.1649694 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2400       |
| train/episodes                 | 240        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 9600       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 6           |
| stats_o/mean                   | 0.20390247  |
| stats_o/std                    | 0.043360278 |
| test/episodes                  | 70          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -3.5965285  |
| test/Q_plus_P                  | -3.5965285  |
| test/reward_per_eps            | -40         |
| test/steps                     | 2800        |
| train/episodes                 | 280         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 11200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 7           |
| stats_o/mean                   | 0.20378704  |
| stats_o/std                    | 0.042877447 |
| test/episodes                  | 80          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -3.9850726  |
| test/Q_plus_P                  | -3.9850726  |
| test/reward_per_eps            | -40         |
| test/steps                     | 3200        |
| train/episodes                 | 320         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 12800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 8           |
| stats_o/mean                   | 0.20392504  |
| stats_o/std                    | 0.042482518 |
| test/episodes                  | 90          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -4.326242   |
| test/Q_plus_P                  | -4.326242   |
| test/reward_per_eps            | -40         |
| test/steps                     | 3600        |
| train/episodes                 | 360         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 14400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 9          |
| stats_o/mean                   | 0.20399353 |
| stats_o/std                    | 0.04215903 |
| test/episodes                  | 100        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -4.7603045 |
| test/Q_plus_P                  | -4.7603045 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4000       |
| train/episodes                 | 400        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 16000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 10          |
| stats_o/mean                   | 0.20385166  |
| stats_o/std                    | 0.041991588 |
| test/episodes                  | 110         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -5.1464415  |
| test/Q_plus_P                  | -5.1464415  |
| test/reward_per_eps            | -40         |
| test/steps                     | 4400        |
| train/episodes                 | 440         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 17600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 11         |
| stats_o/mean                   | 0.20390518 |
| stats_o/std                    | 0.04182335 |
| test/episodes                  | 120        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -5.5082593 |
| test/Q_plus_P                  | -5.5082593 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4800       |
| train/episodes                 | 480        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 19200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.2035418   |
| stats_o/std                    | 0.041652985 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -5.8870416  |
| test/Q_plus_P                  | -5.8870416  |
| test/reward_per_eps            | -40         |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 13         |
| stats_o/mean                   | 0.20388241 |
| stats_o/std                    | 0.04161092 |
| test/episodes                  | 140        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -6.314055  |
| test/Q_plus_P                  | -6.314055  |
| test/reward_per_eps            | -40        |
| test/steps                     | 5600       |
| train/episodes                 | 560        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 22400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 14          |
| stats_o/mean                   | 0.20356578  |
| stats_o/std                    | 0.041478418 |
| test/episodes                  | 150         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -6.689359   |
| test/Q_plus_P                  | -6.689359   |
| test/reward_per_eps            | -40         |
| test/steps                     | 6000        |
| train/episodes                 | 600         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 24000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 15          |
| stats_o/mean                   | 0.20367661  |
| stats_o/std                    | 0.041515294 |
| test/episodes                  | 160         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -7.0148554  |
| test/Q_plus_P                  | -7.0148554  |
| test/reward_per_eps            | -40         |
| test/steps                     | 6400        |
| train/episodes                 | 640         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 25600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 16          |
| stats_o/mean                   | 0.20369299  |
| stats_o/std                    | 0.041472346 |
| test/episodes                  | 170         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -7.4126515  |
| test/Q_plus_P                  | -7.4126515  |
| test/reward_per_eps            | -40         |
| test/steps                     | 6800        |
| train/episodes                 | 680         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 27200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 17         |
| stats_o/mean                   | 0.20356137 |
| stats_o/std                    | 0.04157164 |
| test/episodes                  | 180        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -7.755642  |
| test/Q_plus_P                  | -7.755642  |
| test/reward_per_eps            | -40        |
| test/steps                     | 7200       |
| train/episodes                 | 720        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 28800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 18          |
| stats_o/mean                   | 0.20343864  |
| stats_o/std                    | 0.041487724 |
| test/episodes                  | 190         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -8.176206   |
| test/Q_plus_P                  | -8.176206   |
| test/reward_per_eps            | -40         |
| test/steps                     | 7600        |
| train/episodes                 | 760         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 30400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.20354454 |
| stats_o/std                    | 0.04151065 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -8.510415  |
| test/Q_plus_P                  | -8.510415  |
| test/reward_per_eps            | -40        |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 20          |
| stats_o/mean                   | 0.20357628  |
| stats_o/std                    | 0.041511215 |
| test/episodes                  | 210         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -8.934586   |
| test/Q_plus_P                  | -8.934586   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8400        |
| train/episodes                 | 840         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 33600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 21         |
| stats_o/mean                   | 0.20353678 |
| stats_o/std                    | 0.04144411 |
| test/episodes                  | 220        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.237113  |
| test/Q_plus_P                  | -9.237113  |
| test/reward_per_eps            | -40        |
| test/steps                     | 8800       |
| train/episodes                 | 880        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 35200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 22         |
| stats_o/mean                   | 0.20349278 |
| stats_o/std                    | 0.04143529 |
| test/episodes                  | 230        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.557502  |
| test/Q_plus_P                  | -9.557502  |
| test/reward_per_eps            | -40        |
| test/steps                     | 9200       |
| train/episodes                 | 920        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 36800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 23          |
| stats_o/mean                   | 0.2035782   |
| stats_o/std                    | 0.041399814 |
| test/episodes                  | 240         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -9.891431   |
| test/Q_plus_P                  | -9.891431   |
| test/reward_per_eps            | -40         |
| test/steps                     | 9600        |
| train/episodes                 | 960         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 38400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.20349999  |
| stats_o/std                    | 0.041325904 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -10.305199  |
| test/Q_plus_P                  | -10.305199  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 25          |
| stats_o/mean                   | 0.20350157  |
| stats_o/std                    | 0.041342616 |
| test/episodes                  | 260         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -10.6523485 |
| test/Q_plus_P                  | -10.6523485 |
| test/reward_per_eps            | -40         |
| test/steps                     | 10400       |
| train/episodes                 | 1040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 41600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 26         |
| stats_o/mean                   | 0.2033769  |
| stats_o/std                    | 0.04133254 |
| test/episodes                  | 270        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -10.951516 |
| test/Q_plus_P                  | -10.951516 |
| test/reward_per_eps            | -40        |
| test/steps                     | 10800      |
| train/episodes                 | 1080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 43200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 27          |
| stats_o/mean                   | 0.20336847  |
| stats_o/std                    | 0.04133564  |
| test/episodes                  | 280         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -11.3181505 |
| test/Q_plus_P                  | -11.3181505 |
| test/reward_per_eps            | -40         |
| test/steps                     | 11200       |
| train/episodes                 | 1120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 44800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 28          |
| stats_o/mean                   | 0.20340206  |
| stats_o/std                    | 0.041312095 |
| test/episodes                  | 290         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -11.610161  |
| test/Q_plus_P                  | -11.610161  |
| test/reward_per_eps            | -40         |
| test/steps                     | 11600       |
| train/episodes                 | 1160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 46400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 29          |
| stats_o/mean                   | 0.20343944  |
| stats_o/std                    | 0.041301016 |
| test/episodes                  | 300         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -11.921961  |
| test/Q_plus_P                  | -11.921961  |
| test/reward_per_eps            | -40         |
| test/steps                     | 12000       |
| train/episodes                 | 1200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 48000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 30         |
| stats_o/mean                   | 0.20346755 |
| stats_o/std                    | 0.04130191 |
| test/episodes                  | 310        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -12.237658 |
| test/Q_plus_P                  | -12.237658 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12400      |
| train/episodes                 | 1240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 49600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 31         |
| stats_o/mean                   | 0.20346932 |
| stats_o/std                    | 0.0412971  |
| test/episodes                  | 320        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -12.585681 |
| test/Q_plus_P                  | -12.585681 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12800      |
| train/episodes                 | 1280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 51200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 32          |
| stats_o/mean                   | 0.20353277  |
| stats_o/std                    | 0.041293934 |
| test/episodes                  | 330         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -12.895801  |
| test/Q_plus_P                  | -12.895801  |
| test/reward_per_eps            | -40         |
| test/steps                     | 13200       |
| train/episodes                 | 1320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 52800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 33          |
| stats_o/mean                   | 0.20357043  |
| stats_o/std                    | 0.041305408 |
| test/episodes                  | 340         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -13.241458  |
| test/Q_plus_P                  | -13.241458  |
| test/reward_per_eps            | -40         |
| test/steps                     | 13600       |
| train/episodes                 | 1360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 54400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 34          |
| stats_o/mean                   | 0.20366561  |
| stats_o/std                    | 0.041320696 |
| test/episodes                  | 350         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -13.539338  |
| test/Q_plus_P                  | -13.539338  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14000       |
| train/episodes                 | 1400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 56000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.20375863 |
| stats_o/std                    | 0.04130422 |
| test/episodes                  | 360        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -13.876167 |
| test/Q_plus_P                  | -13.876167 |
| test/reward_per_eps            | -40        |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 36          |
| stats_o/mean                   | 0.2038314   |
| stats_o/std                    | 0.041309886 |
| test/episodes                  | 370         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -14.149973  |
| test/Q_plus_P                  | -14.149973  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14800       |
| train/episodes                 | 1480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 59200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.20379119  |
| stats_o/std                    | 0.041311055 |
| test/episodes                  | 380         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -14.477808  |
| test/Q_plus_P                  | -14.477808  |
| test/reward_per_eps            | -40         |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 38          |
| stats_o/mean                   | 0.20378758  |
| stats_o/std                    | 0.041634973 |
| test/episodes                  | 390         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -14.780354  |
| test/Q_plus_P                  | -14.780354  |
| test/reward_per_eps            | -40         |
| test/steps                     | 15600       |
| train/episodes                 | 1560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 62400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 39          |
| stats_o/mean                   | 0.20386739  |
| stats_o/std                    | 0.041651297 |
| test/episodes                  | 400         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.935      |
| test/Q                         | -16.371239  |
| test/Q_plus_P                  | -16.371239  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16000       |
| train/episodes                 | 1600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 64000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.20387289  |
| stats_o/std                    | 0.044849075 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.214      |
| test/Q                         | -14.801343  |
| test/Q_plus_P                  | -14.801343  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.195      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.203916    |
| stats_o/std                    | 0.045393847 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -15.447916  |
| test/Q_plus_P                  | -15.447916  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 42          |
| stats_o/mean                   | 0.20401375  |
| stats_o/std                    | 0.047337078 |
| test/episodes                  | 430         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.19       |
| test/info_shaping_reward_min   | -0.498      |
| test/Q                         | -15.980779  |
| test/Q_plus_P                  | -15.980779  |
| test/reward_per_eps            | -40         |
| test/steps                     | 17200       |
| train/episodes                 | 1720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.18       |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 68800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 43          |
| stats_o/mean                   | 0.2038699   |
| stats_o/std                    | 0.048186917 |
| test/episodes                  | 440         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.175      |
| test/info_shaping_reward_min   | -0.2        |
| test/Q                         | -15.899217  |
| test/Q_plus_P                  | -15.899217  |
| test/reward_per_eps            | -40         |
| test/steps                     | 17600       |
| train/episodes                 | 1760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 70400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 44          |
| stats_o/mean                   | 0.20400353  |
| stats_o/std                    | 0.049529206 |
| test/episodes                  | 450         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.175      |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -16.183952  |
| test/Q_plus_P                  | -16.183952  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18000       |
| train/episodes                 | 1800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.191      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 72000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.20386225  |
| stats_o/std                    | 0.050757915 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -16.995913  |
| test/Q_plus_P                  | -16.995913  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 46          |
| stats_o/mean                   | 0.20392267  |
| stats_o/std                    | 0.051353112 |
| test/episodes                  | 470         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.19       |
| test/Q                         | -17.031256  |
| test/Q_plus_P                  | -17.031256  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18800       |
| train/episodes                 | 1880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.191      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 75200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 47          |
| stats_o/mean                   | 0.20401558  |
| stats_o/std                    | 0.053392153 |
| test/episodes                  | 480         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.19       |
| test/info_shaping_reward_min   | -0.502      |
| test/Q                         | -17.1591    |
| test/Q_plus_P                  | -17.1591    |
| test/reward_per_eps            | -40         |
| test/steps                     | 19200       |
| train/episodes                 | 1920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.192      |
| train/info_shaping_reward_min  | -0.3        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 76800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 48         |
| stats_o/mean                   | 0.2041655  |
| stats_o/std                    | 0.05666559 |
| test/episodes                  | 490        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -0.504     |
| test/Q                         | -17.48948  |
| test/Q_plus_P                  | -17.48948  |
| test/reward_per_eps            | -40        |
| test/steps                     | 19600      |
| train/episodes                 | 1960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.213     |
| train/info_shaping_reward_min  | -0.419     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 78400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 49          |
| stats_o/mean                   | 0.20416264  |
| stats_o/std                    | 0.057403296 |
| test/episodes                  | 500         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.19       |
| test/Q                         | -17.945599  |
| test/Q_plus_P                  | -17.945599  |
| test/reward_per_eps            | -40         |
| test/steps                     | 20000       |
| train/episodes                 | 2000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.186      |
| train/info_shaping_reward_min  | -0.3        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 80000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.20416303 |
| stats_o/std                    | 0.0590693  |
| test/episodes                  | 510        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.611     |
| test/Q                         | -18.508759 |
| test/Q_plus_P                  | -18.508759 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.204     |
| train/info_shaping_reward_min  | -0.451     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 51          |
| stats_o/mean                   | 0.20416875  |
| stats_o/std                    | 0.059065476 |
| test/episodes                  | 520         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.175      |
| test/info_shaping_reward_min   | -0.191      |
| test/Q                         | -18.313719  |
| test/Q_plus_P                  | -18.313719  |
| test/reward_per_eps            | -40         |
| test/steps                     | 20800       |
| train/episodes                 | 2080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 83200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.2041374  |
| stats_o/std                    | 0.05969759 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -18.528906 |
| test/Q_plus_P                  | -18.528906 |
| test/reward_per_eps            | -40        |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.311     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.20415545  |
| stats_o/std                    | 0.060268935 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -18.676224  |
| test/Q_plus_P                  | -18.676224  |
| test/reward_per_eps            | -40         |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.183      |
| train/info_shaping_reward_min  | -0.302      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 54          |
| stats_o/mean                   | 0.20414518  |
| stats_o/std                    | 0.060201522 |
| test/episodes                  | 550         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -18.921406  |
| test/Q_plus_P                  | -18.921406  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22000       |
| train/episodes                 | 2200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.181      |
| train/info_shaping_reward_min  | -0.298      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 88000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 55          |
| stats_o/mean                   | 0.20412685  |
| stats_o/std                    | 0.060302306 |
| test/episodes                  | 560         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.368721  |
| test/Q_plus_P                  | -19.368721  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22400       |
| train/episodes                 | 2240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 89600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 56          |
| stats_o/mean                   | 0.20420629  |
| stats_o/std                    | 0.060471192 |
| test/episodes                  | 570         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -18.724669  |
| test/Q_plus_P                  | -18.724669  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22800       |
| train/episodes                 | 2280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 91200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 57          |
| stats_o/mean                   | 0.20419446  |
| stats_o/std                    | 0.060969774 |
| test/episodes                  | 580         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.175      |
| test/info_shaping_reward_min   | -0.201      |
| test/Q                         | -19.439976  |
| test/Q_plus_P                  | -19.439976  |
| test/reward_per_eps            | -40         |
| test/steps                     | 23200       |
| train/episodes                 | 2320        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00375     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 92800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.20423499  |
| stats_o/std                    | 0.060863722 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.175684  |
| test/Q_plus_P                  | -19.175684  |
| test/reward_per_eps            | -40         |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 59          |
| stats_o/mean                   | 0.20426905  |
| stats_o/std                    | 0.060759407 |
| test/episodes                  | 600         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.111      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -19.955326  |
| test/Q_plus_P                  | -19.955326  |
| test/reward_per_eps            | -40         |
| test/steps                     | 24000       |
| train/episodes                 | 2400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 96000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 60          |
| stats_o/mean                   | 0.20432213  |
| stats_o/std                    | 0.060772028 |
| test/episodes                  | 610         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.676817  |
| test/Q_plus_P                  | -19.676817  |
| test/reward_per_eps            | -40         |
| test/steps                     | 24400       |
| train/episodes                 | 2440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 97600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 61         |
| stats_o/mean                   | 0.20430288 |
| stats_o/std                    | 0.06067095 |
| test/episodes                  | 620        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.141788 |
| test/Q_plus_P                  | -20.141788 |
| test/reward_per_eps            | -40        |
| test/steps                     | 24800      |
| train/episodes                 | 2480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 99200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 62          |
| stats_o/mean                   | 0.20430106  |
| stats_o/std                    | 0.060773097 |
| test/episodes                  | 630         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.134      |
| test/info_shaping_reward_mean  | -0.171      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -20.250368  |
| test/Q_plus_P                  | -20.250368  |
| test/reward_per_eps            | -40         |
| test/steps                     | 25200       |
| train/episodes                 | 2520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 100800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 63          |
| stats_o/mean                   | 0.20427406  |
| stats_o/std                    | 0.060840372 |
| test/episodes                  | 640         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.999378  |
| test/Q_plus_P                  | -19.999378  |
| test/reward_per_eps            | -40         |
| test/steps                     | 25600       |
| train/episodes                 | 2560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 102400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 64          |
| stats_o/mean                   | 0.20423248  |
| stats_o/std                    | 0.060810946 |
| test/episodes                  | 650         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.255974  |
| test/Q_plus_P                  | -19.255974  |
| test/reward_per_eps            | -40         |
| test/steps                     | 26000       |
| train/episodes                 | 2600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 104000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 65         |
| stats_o/mean                   | 0.20415822 |
| stats_o/std                    | 0.06082392 |
| test/episodes                  | 660        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -19.827759 |
| test/Q_plus_P                  | -19.827759 |
| test/reward_per_eps            | -40        |
| test/steps                     | 26400      |
| train/episodes                 | 2640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 105600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 66         |
| stats_o/mean                   | 0.20417881 |
| stats_o/std                    | 0.06094486 |
| test/episodes                  | 670        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -19.739763 |
| test/Q_plus_P                  | -19.739763 |
| test/reward_per_eps            | -40        |
| test/steps                     | 26800      |
| train/episodes                 | 2680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 107200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 67          |
| stats_o/mean                   | 0.20416091  |
| stats_o/std                    | 0.061131302 |
| test/episodes                  | 680         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.196867  |
| test/Q_plus_P                  | -19.196867  |
| test/reward_per_eps            | -40         |
| test/steps                     | 27200       |
| train/episodes                 | 2720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 108800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 68          |
| stats_o/mean                   | 0.20416258  |
| stats_o/std                    | 0.061446935 |
| test/episodes                  | 690         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.540272  |
| test/Q_plus_P                  | -19.540272  |
| test/reward_per_eps            | -40         |
| test/steps                     | 27600       |
| train/episodes                 | 2760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 110400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 69          |
| stats_o/mean                   | 0.20420171  |
| stats_o/std                    | 0.061621618 |
| test/episodes                  | 700         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.062843  |
| test/Q_plus_P                  | -19.062843  |
| test/reward_per_eps            | -40         |
| test/steps                     | 28000       |
| train/episodes                 | 2800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 112000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 70          |
| stats_o/mean                   | 0.20418848  |
| stats_o/std                    | 0.061909076 |
| test/episodes                  | 710         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.076675  |
| test/Q_plus_P                  | -19.076675  |
| test/reward_per_eps            | -40         |
| test/steps                     | 28400       |
| train/episodes                 | 2840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 113600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 71         |
| stats_o/mean                   | 0.20421271 |
| stats_o/std                    | 0.06215659 |
| test/episodes                  | 720        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -19.084648 |
| test/Q_plus_P                  | -19.084648 |
| test/reward_per_eps            | -40        |
| test/steps                     | 28800      |
| train/episodes                 | 2880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 115200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 72          |
| stats_o/mean                   | 0.20421305  |
| stats_o/std                    | 0.062519945 |
| test/episodes                  | 730         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.088966  |
| test/Q_plus_P                  | -19.088966  |
| test/reward_per_eps            | -40         |
| test/steps                     | 29200       |
| train/episodes                 | 2920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 116800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.2042446  |
| stats_o/std                    | 0.06270123 |
| test/episodes                  | 740        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -18.699856 |
| test/Q_plus_P                  | -18.699856 |
| test/reward_per_eps            | -40        |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 74         |
| stats_o/mean                   | 0.20418087 |
| stats_o/std                    | 0.06299878 |
| test/episodes                  | 750        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -18.889921 |
| test/Q_plus_P                  | -18.889921 |
| test/reward_per_eps            | -40        |
| test/steps                     | 30000      |
| train/episodes                 | 3000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 120000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 75          |
| stats_o/mean                   | 0.2041795   |
| stats_o/std                    | 0.063298814 |
| test/episodes                  | 760         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.360727  |
| test/Q_plus_P                  | -19.360727  |
| test/reward_per_eps            | -40         |
| test/steps                     | 30400       |
| train/episodes                 | 3040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 121600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 76         |
| stats_o/mean                   | 0.20415927 |
| stats_o/std                    | 0.06353402 |
| test/episodes                  | 770        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -19.778715 |
| test/Q_plus_P                  | -19.778715 |
| test/reward_per_eps            | -40        |
| test/steps                     | 30800      |
| train/episodes                 | 3080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 123200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 77          |
| stats_o/mean                   | 0.20417342  |
| stats_o/std                    | 0.063631274 |
| test/episodes                  | 780         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.930874  |
| test/Q_plus_P                  | -19.930874  |
| test/reward_per_eps            | -40         |
| test/steps                     | 31200       |
| train/episodes                 | 3120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 124800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 78         |
| stats_o/mean                   | 0.2041588  |
| stats_o/std                    | 0.06384592 |
| test/episodes                  | 790        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -19.186146 |
| test/Q_plus_P                  | -19.186146 |
| test/reward_per_eps            | -40        |
| test/steps                     | 31600      |
| train/episodes                 | 3160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 126400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 79         |
| stats_o/mean                   | 0.20416003 |
| stats_o/std                    | 0.06391349 |
| test/episodes                  | 800        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -18.888893 |
| test/Q_plus_P                  | -18.888893 |
| test/reward_per_eps            | -40        |
| test/steps                     | 32000      |
| train/episodes                 | 3200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 128000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.20416923  |
| stats_o/std                    | 0.063994594 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.770779  |
| test/Q_plus_P                  | -19.770779  |
| test/reward_per_eps            | -40         |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 81         |
| stats_o/mean                   | 0.20419706 |
| stats_o/std                    | 0.06408325 |
| test/episodes                  | 820        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.229216 |
| test/Q_plus_P                  | -20.229216 |
| test/reward_per_eps            | -40        |
| test/steps                     | 32800      |
| train/episodes                 | 3280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 131200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.20418131  |
| stats_o/std                    | 0.064200394 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -20.52724   |
| test/Q_plus_P                  | -20.52724   |
| test/reward_per_eps            | -40         |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 83         |
| stats_o/mean                   | 0.20423284 |
| stats_o/std                    | 0.06424214 |
| test/episodes                  | 840        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.089418 |
| test/Q_plus_P                  | -20.089418 |
| test/reward_per_eps            | -40        |
| test/steps                     | 33600      |
| train/episodes                 | 3360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 134400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 84         |
| stats_o/mean                   | 0.20420693 |
| stats_o/std                    | 0.06428618 |
| test/episodes                  | 850        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.43183  |
| test/Q_plus_P                  | -20.43183  |
| test/reward_per_eps            | -40        |
| test/steps                     | 34000      |
| train/episodes                 | 3400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 136000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.20424797  |
| stats_o/std                    | 0.064273976 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -20.781733  |
| test/Q_plus_P                  | -20.781733  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 86          |
| stats_o/mean                   | 0.20429249  |
| stats_o/std                    | 0.064223185 |
| test/episodes                  | 870         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -20.280367  |
| test/Q_plus_P                  | -20.280367  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34800       |
| train/episodes                 | 3480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 139200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 87          |
| stats_o/mean                   | 0.2042943   |
| stats_o/std                    | 0.064230844 |
| test/episodes                  | 880         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -21.515366  |
| test/Q_plus_P                  | -21.515366  |
| test/reward_per_eps            | -40         |
| test/steps                     | 35200       |
| train/episodes                 | 3520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 140800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 88         |
| stats_o/mean                   | 0.20430738 |
| stats_o/std                    | 0.06422401 |
| test/episodes                  | 890        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.91045  |
| test/Q_plus_P                  | -20.91045  |
| test/reward_per_eps            | -40        |
| test/steps                     | 35600      |
| train/episodes                 | 3560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 142400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 89          |
| stats_o/mean                   | 0.20429629  |
| stats_o/std                    | 0.064223915 |
| test/episodes                  | 900         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -21.967815  |
| test/Q_plus_P                  | -21.967815  |
| test/reward_per_eps            | -40         |
| test/steps                     | 36000       |
| train/episodes                 | 3600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 144000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 90         |
| stats_o/mean                   | 0.20432833 |
| stats_o/std                    | 0.06419591 |
| test/episodes                  | 910        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -22.045012 |
| test/Q_plus_P                  | -22.045012 |
| test/reward_per_eps            | -40        |
| test/steps                     | 36400      |
| train/episodes                 | 3640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 145600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 91         |
| stats_o/mean                   | 0.20430715 |
| stats_o/std                    | 0.06416259 |
| test/episodes                  | 920        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -21.905725 |
| test/Q_plus_P                  | -21.905725 |
| test/reward_per_eps            | -40        |
| test/steps                     | 36800      |
| train/episodes                 | 3680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 147200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 92          |
| stats_o/mean                   | 0.2042932   |
| stats_o/std                    | 0.064161085 |
| test/episodes                  | 930         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -21.562822  |
| test/Q_plus_P                  | -21.562822  |
| test/reward_per_eps            | -40         |
| test/steps                     | 37200       |
| train/episodes                 | 3720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 148800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.20426013  |
| stats_o/std                    | 0.064129956 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -22.41074   |
| test/Q_plus_P                  | -22.41074   |
| test/reward_per_eps            | -40         |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 94         |
| stats_o/mean                   | 0.20430285 |
| stats_o/std                    | 0.0641155  |
| test/episodes                  | 950        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -22.562925 |
| test/Q_plus_P                  | -22.562925 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38000      |
| train/episodes                 | 3800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 152000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 95         |
| stats_o/mean                   | 0.20428309 |
| stats_o/std                    | 0.06407496 |
| test/episodes                  | 960        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -22.308882 |
| test/Q_plus_P                  | -22.308882 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38400      |
| train/episodes                 | 3840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 153600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 96         |
| stats_o/mean                   | 0.20423006 |
| stats_o/std                    | 0.0640137  |
| test/episodes                  | 970        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -22.817455 |
| test/Q_plus_P                  | -22.817455 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38800      |
| train/episodes                 | 3880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 155200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 97          |
| stats_o/mean                   | 0.20424062  |
| stats_o/std                    | 0.064024426 |
| test/episodes                  | 980         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -22.090334  |
| test/Q_plus_P                  | -22.090334  |
| test/reward_per_eps            | -40         |
| test/steps                     | 39200       |
| train/episodes                 | 3920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 156800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 98         |
| stats_o/mean                   | 0.20423035 |
| stats_o/std                    | 0.06406484 |
| test/episodes                  | 990        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -23.475344 |
| test/Q_plus_P                  | -23.475344 |
| test/reward_per_eps            | -40        |
| test/steps                     | 39600      |
| train/episodes                 | 3960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 158400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.20422664 |
| stats_o/std                    | 0.06464748 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.144     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -23.41149  |
| test/Q_plus_P                  | -23.41149  |
| test/reward_per_eps            | -40        |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.291     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 100         |
| stats_o/mean                   | 0.20424563  |
| stats_o/std                    | 0.064911924 |
| test/episodes                  | 1010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -23.598494  |
| test/Q_plus_P                  | -23.598494  |
| test/reward_per_eps            | -40         |
| test/steps                     | 40400       |
| train/episodes                 | 4040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 161600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 101        |
| stats_o/mean                   | 0.20423244 |
| stats_o/std                    | 0.06515125 |
| test/episodes                  | 1020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.125     |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -23.275263 |
| test/Q_plus_P                  | -23.275263 |
| test/reward_per_eps            | -40        |
| test/steps                     | 40800      |
| train/episodes                 | 4080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.299     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 163200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 102         |
| stats_o/mean                   | 0.20424081  |
| stats_o/std                    | 0.065353855 |
| test/episodes                  | 1030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -23.890175  |
| test/Q_plus_P                  | -23.890175  |
| test/reward_per_eps            | -40         |
| test/steps                     | 41200       |
| train/episodes                 | 4120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.225      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 164800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 103        |
| stats_o/mean                   | 0.20421554 |
| stats_o/std                    | 0.06529334 |
| test/episodes                  | 1040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.217     |
| test/Q                         | -24.642107 |
| test/Q_plus_P                  | -24.642107 |
| test/reward_per_eps            | -40        |
| test/steps                     | 41600      |
| train/episodes                 | 4160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 166400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 104         |
| stats_o/mean                   | 0.20419484  |
| stats_o/std                    | 0.065704644 |
| test/episodes                  | 1050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.177      |
| test/info_shaping_reward_min   | -0.205      |
| test/Q                         | -24.408327  |
| test/Q_plus_P                  | -24.408327  |
| test/reward_per_eps            | -40         |
| test/steps                     | 42000       |
| train/episodes                 | 4200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.189      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 168000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 105        |
| stats_o/mean                   | 0.20419827 |
| stats_o/std                    | 0.0659894  |
| test/episodes                  | 1060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.187     |
| test/info_shaping_reward_min   | -0.505     |
| test/Q                         | -24.405577 |
| test/Q_plus_P                  | -24.405577 |
| test/reward_per_eps            | -40        |
| test/steps                     | 42400      |
| train/episodes                 | 4240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 169600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 106        |
| stats_o/mean                   | 0.20420462 |
| stats_o/std                    | 0.06628105 |
| test/episodes                  | 1070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.182     |
| test/info_shaping_reward_min   | -0.502     |
| test/Q                         | -24.031538 |
| test/Q_plus_P                  | -24.031538 |
| test/reward_per_eps            | -40        |
| test/steps                     | 42800      |
| train/episodes                 | 4280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 171200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 107         |
| stats_o/mean                   | 0.20420572  |
| stats_o/std                    | 0.066445634 |
| test/episodes                  | 1080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.201      |
| test/Q                         | -24.241905  |
| test/Q_plus_P                  | -24.241905  |
| test/reward_per_eps            | -40         |
| test/steps                     | 43200       |
| train/episodes                 | 4320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 172800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 108         |
| stats_o/mean                   | 0.20423073  |
| stats_o/std                    | 0.066689074 |
| test/episodes                  | 1090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.175      |
| test/info_shaping_reward_min   | -0.193      |
| test/Q                         | -24.594423  |
| test/Q_plus_P                  | -24.594423  |
| test/reward_per_eps            | -40         |
| test/steps                     | 43600       |
| train/episodes                 | 4360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 174400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 109        |
| stats_o/mean                   | 0.20425662 |
| stats_o/std                    | 0.06679408 |
| test/episodes                  | 1100       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.181     |
| test/info_shaping_reward_min   | -0.583     |
| test/Q                         | -25.128237 |
| test/Q_plus_P                  | -25.128237 |
| test/reward_per_eps            | -40        |
| test/steps                     | 44000      |
| train/episodes                 | 4400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 176000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 110        |
| stats_o/mean                   | 0.2042491  |
| stats_o/std                    | 0.06711868 |
| test/episodes                  | 1110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -25.699688 |
| test/Q_plus_P                  | -25.699688 |
| test/reward_per_eps            | -40        |
| test/steps                     | 44400      |
| train/episodes                 | 4440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.291     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 177600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 111         |
| stats_o/mean                   | 0.20425382  |
| stats_o/std                    | 0.067258134 |
| test/episodes                  | 1120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -25.051031  |
| test/Q_plus_P                  | -25.051031  |
| test/reward_per_eps            | -40         |
| test/steps                     | 44800       |
| train/episodes                 | 4480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.187      |
| train/info_shaping_reward_min  | -0.305      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 179200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 112        |
| stats_o/mean                   | 0.20422125 |
| stats_o/std                    | 0.06771647 |
| test/episodes                  | 1130       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.207     |
| test/info_shaping_reward_min   | -0.798     |
| test/Q                         | -26.377047 |
| test/Q_plus_P                  | -26.377047 |
| test/reward_per_eps            | -40        |
| test/steps                     | 45200      |
| train/episodes                 | 4520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 180800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 113        |
| stats_o/mean                   | 0.20424713 |
| stats_o/std                    | 0.06809    |
| test/episodes                  | 1140       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.186     |
| test/info_shaping_reward_min   | -0.559     |
| test/Q                         | -25.144234 |
| test/Q_plus_P                  | -25.144234 |
| test/reward_per_eps            | -40        |
| test/steps                     | 45600      |
| train/episodes                 | 4560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 182400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 114        |
| stats_o/mean                   | 0.20424078 |
| stats_o/std                    | 0.06845985 |
| test/episodes                  | 1150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.21      |
| test/Q                         | -25.59127  |
| test/Q_plus_P                  | -25.59127  |
| test/reward_per_eps            | -40        |
| test/steps                     | 46000      |
| train/episodes                 | 4600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.29      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 184000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 115        |
| stats_o/mean                   | 0.20424555 |
| stats_o/std                    | 0.06882883 |
| test/episodes                  | 1160       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.146     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -25.589039 |
| test/Q_plus_P                  | -25.589039 |
| test/reward_per_eps            | -40        |
| test/steps                     | 46400      |
| train/episodes                 | 4640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.282     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 185600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 116        |
| stats_o/mean                   | 0.20424433 |
| stats_o/std                    | 0.06926689 |
| test/episodes                  | 1170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0846    |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -25.77459  |
| test/Q_plus_P                  | -25.77459  |
| test/reward_per_eps            | -40        |
| test/steps                     | 46800      |
| train/episodes                 | 4680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 187200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 117         |
| stats_o/mean                   | 0.20422739  |
| stats_o/std                    | 0.069451556 |
| test/episodes                  | 1180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.206      |
| test/Q                         | -25.807436  |
| test/Q_plus_P                  | -25.807436  |
| test/reward_per_eps            | -40         |
| test/steps                     | 47200       |
| train/episodes                 | 4720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.183      |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 188800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.20425864  |
| stats_o/std                    | 0.069546364 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.204      |
| test/Q                         | -26.357397  |
| test/Q_plus_P                  | -26.357397  |
| test/reward_per_eps            | -40         |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 119        |
| stats_o/mean                   | 0.2042717  |
| stats_o/std                    | 0.06969997 |
| test/episodes                  | 1200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -0.333     |
| test/Q                         | -26.199028 |
| test/Q_plus_P                  | -26.199028 |
| test/reward_per_eps            | -40        |
| test/steps                     | 48000      |
| train/episodes                 | 4800       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0075     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 192000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 120        |
| stats_o/mean                   | 0.20429127 |
| stats_o/std                    | 0.06989375 |
| test/episodes                  | 1210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -26.665495 |
| test/Q_plus_P                  | -26.665495 |
| test/reward_per_eps            | -40        |
| test/steps                     | 48400      |
| train/episodes                 | 4840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 193600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.20432079 |
| stats_o/std                    | 0.06993092 |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.095     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.219     |
| test/Q                         | -26.479195 |
| test/Q_plus_P                  | -26.479195 |
| test/reward_per_eps            | -40        |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 122         |
| stats_o/mean                   | 0.20429869  |
| stats_o/std                    | 0.070339374 |
| test/episodes                  | 1230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.211      |
| test/Q                         | -26.685034  |
| test/Q_plus_P                  | -26.685034  |
| test/reward_per_eps            | -40         |
| test/steps                     | 49200       |
| train/episodes                 | 4920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 196800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 123        |
| stats_o/mean                   | 0.20432445 |
| stats_o/std                    | 0.07042134 |
| test/episodes                  | 1240       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0874    |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -26.669924 |
| test/Q_plus_P                  | -26.669924 |
| test/reward_per_eps            | -40        |
| test/steps                     | 49600      |
| train/episodes                 | 4960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 198400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 124        |
| stats_o/mean                   | 0.20435311 |
| stats_o/std                    | 0.07043672 |
| test/episodes                  | 1250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.126     |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -27.04152  |
| test/Q_plus_P                  | -27.04152  |
| test/reward_per_eps            | -40        |
| test/steps                     | 50000      |
| train/episodes                 | 5000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 200000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 125        |
| stats_o/mean                   | 0.20439576 |
| stats_o/std                    | 0.07069376 |
| test/episodes                  | 1260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.228     |
| test/Q                         | -27.32818  |
| test/Q_plus_P                  | -27.32818  |
| test/reward_per_eps            | -40        |
| test/steps                     | 50400      |
| train/episodes                 | 5040       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00187    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 201600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 126         |
| stats_o/mean                   | 0.20437962  |
| stats_o/std                    | 0.070874274 |
| test/episodes                  | 1270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.104      |
| test/info_shaping_reward_mean  | -0.168      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -27.53315   |
| test/Q_plus_P                  | -27.53315   |
| test/reward_per_eps            | -40         |
| test/steps                     | 50800       |
| train/episodes                 | 5080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 203200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 127        |
| stats_o/mean                   | 0.20439605 |
| stats_o/std                    | 0.07086687 |
| test/episodes                  | 1280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.147     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.217     |
| test/Q                         | -27.380264 |
| test/Q_plus_P                  | -27.380264 |
| test/reward_per_eps            | -40        |
| test/steps                     | 51200      |
| train/episodes                 | 5120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 204800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.20439078 |
| stats_o/std                    | 0.07115546 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -27.158152 |
| test/Q_plus_P                  | -27.158152 |
| test/reward_per_eps            | -40        |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.362     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 129         |
| stats_o/mean                   | 0.20442086  |
| stats_o/std                    | 0.071335874 |
| test/episodes                  | 1300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -28.177113  |
| test/Q_plus_P                  | -28.177113  |
| test/reward_per_eps            | -40         |
| test/steps                     | 52000       |
| train/episodes                 | 5200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.19       |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 208000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 130        |
| stats_o/mean                   | 0.20445462 |
| stats_o/std                    | 0.07149415 |
| test/episodes                  | 1310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.185     |
| test/info_shaping_reward_min   | -0.403     |
| test/Q                         | -28.551918 |
| test/Q_plus_P                  | -28.551918 |
| test/reward_per_eps            | -40        |
| test/steps                     | 52400      |
| train/episodes                 | 5240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 209600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 131         |
| stats_o/mean                   | 0.20444241  |
| stats_o/std                    | 0.071588166 |
| test/episodes                  | 1320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.181      |
| test/info_shaping_reward_min   | -0.327      |
| test/Q                         | -28.293373  |
| test/Q_plus_P                  | -28.293373  |
| test/reward_per_eps            | -40         |
| test/steps                     | 52800       |
| train/episodes                 | 5280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.186      |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 211200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 132        |
| stats_o/mean                   | 0.2044338  |
| stats_o/std                    | 0.07170599 |
| test/episodes                  | 1330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -28.228151 |
| test/Q_plus_P                  | -28.228151 |
| test/reward_per_eps            | -40        |
| test/steps                     | 53200      |
| train/episodes                 | 5320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.293     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 212800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 133         |
| stats_o/mean                   | 0.20442039  |
| stats_o/std                    | 0.071689926 |
| test/episodes                  | 1340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.195      |
| test/info_shaping_reward_min   | -0.357      |
| test/Q                         | -28.785269  |
| test/Q_plus_P                  | -28.785269  |
| test/reward_per_eps            | -40         |
| test/steps                     | 53600       |
| train/episodes                 | 5360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.21       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 214400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 134        |
| stats_o/mean                   | 0.20443039 |
| stats_o/std                    | 0.07213039 |
| test/episodes                  | 1350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -28.165434 |
| test/Q_plus_P                  | -28.165434 |
| test/reward_per_eps            | -40        |
| test/steps                     | 54000      |
| train/episodes                 | 5400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.502     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 216000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 135        |
| stats_o/mean                   | 0.20437498 |
| stats_o/std                    | 0.07204553 |
| test/episodes                  | 1360       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.182     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -28.203213 |
| test/Q_plus_P                  | -28.203213 |
| test/reward_per_eps            | -40        |
| test/steps                     | 54400      |
| train/episodes                 | 5440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 217600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 136        |
| stats_o/mean                   | 0.20434877 |
| stats_o/std                    | 0.07226934 |
| test/episodes                  | 1370       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.135     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -28.68149  |
| test/Q_plus_P                  | -28.68149  |
| test/reward_per_eps            | -40        |
| test/steps                     | 54800      |
| train/episodes                 | 5480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.197     |
| train/info_shaping_reward_min  | -0.425     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 219200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 137         |
| stats_o/mean                   | 0.20434068  |
| stats_o/std                    | 0.072187714 |
| test/episodes                  | 1380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -28.631357  |
| test/Q_plus_P                  | -28.631357  |
| test/reward_per_eps            | -40         |
| test/steps                     | 55200       |
| train/episodes                 | 5520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 220800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.2043508  |
| stats_o/std                    | 0.07224157 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -28.750826 |
| test/Q_plus_P                  | -28.750826 |
| test/reward_per_eps            | -40        |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.288     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.2043451  |
| stats_o/std                    | 0.07242088 |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.147     |
| test/info_shaping_reward_mean  | -0.189     |
| test/info_shaping_reward_min   | -0.674     |
| test/Q                         | -28.822931 |
| test/Q_plus_P                  | -28.822931 |
| test/reward_per_eps            | -40        |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.307     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 140         |
| stats_o/mean                   | 0.20436521  |
| stats_o/std                    | 0.072628565 |
| test/episodes                  | 1410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.178      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -29.417706  |
| test/Q_plus_P                  | -29.417706  |
| test/reward_per_eps            | -40         |
| test/steps                     | 56400       |
| train/episodes                 | 5640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.204      |
| train/info_shaping_reward_min  | -0.491      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 225600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.20435594 |
| stats_o/std                    | 0.07270309 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.177     |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -29.285435 |
| test/Q_plus_P                  | -29.285435 |
| test/reward_per_eps            | -40        |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 142        |
| stats_o/mean                   | 0.20434731 |
| stats_o/std                    | 0.07289124 |
| test/episodes                  | 1430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.224     |
| test/Q                         | -29.475557 |
| test/Q_plus_P                  | -29.475557 |
| test/reward_per_eps            | -40        |
| test/steps                     | 57200      |
| train/episodes                 | 5720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.302     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 228800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 143        |
| stats_o/mean                   | 0.20434847 |
| stats_o/std                    | 0.07305591 |
| test/episodes                  | 1440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.13      |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -29.353445 |
| test/Q_plus_P                  | -29.353445 |
| test/reward_per_eps            | -40        |
| test/steps                     | 57600      |
| train/episodes                 | 5760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 230400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.20434786  |
| stats_o/std                    | 0.073105104 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.129      |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -29.531187  |
| test/Q_plus_P                  | -29.531187  |
| test/reward_per_eps            | -40         |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 145         |
| stats_o/mean                   | 0.20431644  |
| stats_o/std                    | 0.073522866 |
| test/episodes                  | 1460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.124      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.202      |
| test/Q                         | -29.502075  |
| test/Q_plus_P                  | -29.502075  |
| test/reward_per_eps            | -40         |
| test/steps                     | 58400       |
| train/episodes                 | 5840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.2        |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 233600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 146        |
| stats_o/mean                   | 0.20431107 |
| stats_o/std                    | 0.07348995 |
| test/episodes                  | 1470       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.121     |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -29.651855 |
| test/Q_plus_P                  | -29.651855 |
| test/reward_per_eps            | -40        |
| test/steps                     | 58800      |
| train/episodes                 | 5880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 235200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 147        |
| stats_o/mean                   | 0.20431152 |
| stats_o/std                    | 0.07347793 |
| test/episodes                  | 1480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.129     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.202     |
| test/Q                         | -29.7498   |
| test/Q_plus_P                  | -29.7498   |
| test/reward_per_eps            | -40        |
| test/steps                     | 59200      |
| train/episodes                 | 5920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 236800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 148        |
| stats_o/mean                   | 0.20438254 |
| stats_o/std                    | 0.07396491 |
| test/episodes                  | 1490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.117     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -29.71126  |
| test/Q_plus_P                  | -29.71126  |
| test/reward_per_eps            | -40        |
| test/steps                     | 59600      |
| train/episodes                 | 5960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 238400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 149        |
| stats_o/mean                   | 0.20440182 |
| stats_o/std                    | 0.07407019 |
| test/episodes                  | 1500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.177     |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -29.973705 |
| test/Q_plus_P                  | -29.973705 |
| test/reward_per_eps            | -40        |
| test/steps                     | 60000      |
| train/episodes                 | 6000       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00125    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 240000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 150        |
| stats_o/mean                   | 0.20443557 |
| stats_o/std                    | 0.07443455 |
| test/episodes                  | 1510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -30.552214 |
| test/Q_plus_P                  | -30.552214 |
| test/reward_per_eps            | -40        |
| test/steps                     | 60400      |
| train/episodes                 | 6040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.284     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 241600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 151        |
| stats_o/mean                   | 0.20440708 |
| stats_o/std                    | 0.07496603 |
| test/episodes                  | 1520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.723     |
| test/Q                         | -30.735659 |
| test/Q_plus_P                  | -30.735659 |
| test/reward_per_eps            | -40        |
| test/steps                     | 60800      |
| train/episodes                 | 6080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.194     |
| train/info_shaping_reward_min  | -0.367     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 243200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 152         |
| stats_o/mean                   | 0.20432906  |
| stats_o/std                    | 0.075492054 |
| test/episodes                  | 1530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.11       |
| test/info_shaping_reward_mean  | -0.168      |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -29.794897  |
| test/Q_plus_P                  | -29.794897  |
| test/reward_per_eps            | -40         |
| test/steps                     | 61200       |
| train/episodes                 | 6120        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.01        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.285      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 244800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 153        |
| stats_o/mean                   | 0.20435272 |
| stats_o/std                    | 0.07582538 |
| test/episodes                  | 1540       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.19      |
| test/info_shaping_reward_min   | -0.399     |
| test/Q                         | -30.698502 |
| test/Q_plus_P                  | -30.698502 |
| test/reward_per_eps            | -40        |
| test/steps                     | 61600      |
| train/episodes                 | 6160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 246400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.20434959 |
| stats_o/std                    | 0.07625736 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.183     |
| test/info_shaping_reward_min   | -0.37      |
| test/Q                         | -30.651802 |
| test/Q_plus_P                  | -30.651802 |
| test/reward_per_eps            | -40        |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.20429297 |
| stats_o/std                    | 0.07632321 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.124     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.329     |
| test/Q                         | -30.346333 |
| test/Q_plus_P                  | -30.346333 |
| test/reward_per_eps            | -40        |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00437    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 156         |
| stats_o/mean                   | 0.20430945  |
| stats_o/std                    | 0.076421626 |
| test/episodes                  | 1570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -30.638283  |
| test/Q_plus_P                  | -30.638283  |
| test/reward_per_eps            | -40         |
| test/steps                     | 62800       |
| train/episodes                 | 6280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.225      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 251200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 157        |
| stats_o/mean                   | 0.20426817 |
| stats_o/std                    | 0.07653757 |
| test/episodes                  | 1580       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.128     |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -30.44088  |
| test/Q_plus_P                  | -30.44088  |
| test/reward_per_eps            | -40        |
| test/steps                     | 63200      |
| train/episodes                 | 6320       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00813    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.12      |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 252800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 158        |
| stats_o/mean                   | 0.20426306 |
| stats_o/std                    | 0.07656638 |
| test/episodes                  | 1590       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.724     |
| test/Q                         | -30.587906 |
| test/Q_plus_P                  | -30.587906 |
| test/reward_per_eps            | -40        |
| test/steps                     | 63600      |
| train/episodes                 | 6360       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00313    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 254400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 159        |
| stats_o/mean                   | 0.20419472 |
| stats_o/std                    | 0.0767999  |
| test/episodes                  | 1600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0575     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0259    |
| test/info_shaping_reward_mean  | -0.158     |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -30.485563 |
| test/Q_plus_P                  | -30.485563 |
| test/reward_per_eps            | -37.7      |
| test/steps                     | 64000      |
| train/episodes                 | 6400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 256000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 160        |
| stats_o/mean                   | 0.20415913 |
| stats_o/std                    | 0.07696137 |
| test/episodes                  | 1610       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0681    |
| test/info_shaping_reward_mean  | -0.181     |
| test/info_shaping_reward_min   | -0.448     |
| test/Q                         | -30.686932 |
| test/Q_plus_P                  | -30.686932 |
| test/reward_per_eps            | -40        |
| test/steps                     | 64400      |
| train/episodes                 | 6440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 257600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 161        |
| stats_o/mean                   | 0.20414019 |
| stats_o/std                    | 0.07703465 |
| test/episodes                  | 1620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.123     |
| test/info_shaping_reward_mean  | -0.201     |
| test/info_shaping_reward_min   | -0.682     |
| test/Q                         | -30.829073 |
| test/Q_plus_P                  | -30.829073 |
| test/reward_per_eps            | -40        |
| test/steps                     | 64800      |
| train/episodes                 | 6480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.192     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 259200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 162         |
| stats_o/mean                   | 0.20413029  |
| stats_o/std                    | 0.077209584 |
| test/episodes                  | 1630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.128      |
| test/info_shaping_reward_mean  | -0.175      |
| test/info_shaping_reward_min   | -0.712      |
| test/Q                         | -31.059341  |
| test/Q_plus_P                  | -31.059341  |
| test/reward_per_eps            | -40         |
| test/steps                     | 65200       |
| train/episodes                 | 6520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.186      |
| train/info_shaping_reward_min  | -0.367      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 260800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 163        |
| stats_o/mean                   | 0.20416677 |
| stats_o/std                    | 0.07743586 |
| test/episodes                  | 1640       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.125     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -30.710669 |
| test/Q_plus_P                  | -30.710669 |
| test/reward_per_eps            | -40        |
| test/steps                     | 65600      |
| train/episodes                 | 6560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 262400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 164        |
| stats_o/mean                   | 0.2041963  |
| stats_o/std                    | 0.07774521 |
| test/episodes                  | 1650       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.214     |
| test/info_shaping_reward_min   | -0.663     |
| test/Q                         | -31.299185 |
| test/Q_plus_P                  | -31.299185 |
| test/reward_per_eps            | -40        |
| test/steps                     | 66000      |
| train/episodes                 | 6600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.301     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 264000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 165        |
| stats_o/mean                   | 0.20418605 |
| stats_o/std                    | 0.07802707 |
| test/episodes                  | 1660       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.132     |
| test/info_shaping_reward_mean  | -0.204     |
| test/info_shaping_reward_min   | -0.516     |
| test/Q                         | -32.090862 |
| test/Q_plus_P                  | -32.090862 |
| test/reward_per_eps            | -40        |
| test/steps                     | 66400      |
| train/episodes                 | 6640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.194     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 265600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 166         |
| stats_o/mean                   | 0.20417085  |
| stats_o/std                    | 0.078245275 |
| test/episodes                  | 1670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.215      |
| test/Q                         | -31.333635  |
| test/Q_plus_P                  | -31.333635  |
| test/reward_per_eps            | -40         |
| test/steps                     | 66800       |
| train/episodes                 | 6680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 267200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 167         |
| stats_o/mean                   | 0.20416145  |
| stats_o/std                    | 0.078430414 |
| test/episodes                  | 1680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0786     |
| test/info_shaping_reward_mean  | -0.167      |
| test/info_shaping_reward_min   | -0.2        |
| test/Q                         | -31.313196  |
| test/Q_plus_P                  | -31.313196  |
| test/reward_per_eps            | -40         |
| test/steps                     | 67200       |
| train/episodes                 | 6720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.186      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 268800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 168        |
| stats_o/mean                   | 0.20415846 |
| stats_o/std                    | 0.07866453 |
| test/episodes                  | 1690       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.135     |
| test/info_shaping_reward_mean  | -0.195     |
| test/info_shaping_reward_min   | -0.572     |
| test/Q                         | -31.531792 |
| test/Q_plus_P                  | -31.531792 |
| test/reward_per_eps            | -40        |
| test/steps                     | 67600      |
| train/episodes                 | 6760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.366     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 270400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.20411503 |
| stats_o/std                    | 0.07874945 |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.207     |
| test/info_shaping_reward_min   | -0.505     |
| test/Q                         | -31.868942 |
| test/Q_plus_P                  | -31.868942 |
| test/reward_per_eps            | -40        |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 170        |
| stats_o/mean                   | 0.20407459 |
| stats_o/std                    | 0.07896558 |
| test/episodes                  | 1710       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.11      |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -31.35595  |
| test/Q_plus_P                  | -31.35595  |
| test/reward_per_eps            | -40        |
| test/steps                     | 68400      |
| train/episodes                 | 6840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 273600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 171        |
| stats_o/mean                   | 0.20403737 |
| stats_o/std                    | 0.07947741 |
| test/episodes                  | 1720       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.185     |
| test/info_shaping_reward_min   | -0.277     |
| test/Q                         | -31.448536 |
| test/Q_plus_P                  | -31.448536 |
| test/reward_per_eps            | -40        |
| test/steps                     | 68800      |
| train/episodes                 | 6880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.203     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 275200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 172        |
| stats_o/mean                   | 0.20404501 |
| stats_o/std                    | 0.07989869 |
| test/episodes                  | 1730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.191     |
| test/info_shaping_reward_min   | -0.616     |
| test/Q                         | -31.77843  |
| test/Q_plus_P                  | -31.77843  |
| test/reward_per_eps            | -40        |
| test/steps                     | 69200      |
| train/episodes                 | 6920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.192     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 276800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 173        |
| stats_o/mean                   | 0.2040207  |
| stats_o/std                    | 0.08019062 |
| test/episodes                  | 1740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.206     |
| test/info_shaping_reward_min   | -0.749     |
| test/Q                         | -31.970797 |
| test/Q_plus_P                  | -31.970797 |
| test/reward_per_eps            | -40        |
| test/steps                     | 69600      |
| train/episodes                 | 6960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.526     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 278400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 174         |
| stats_o/mean                   | 0.20403953  |
| stats_o/std                    | 0.080685675 |
| test/episodes                  | 1750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0919     |
| test/info_shaping_reward_mean  | -0.199      |
| test/info_shaping_reward_min   | -0.674      |
| test/Q                         | -30.987768  |
| test/Q_plus_P                  | -30.987768  |
| test/reward_per_eps            | -40         |
| test/steps                     | 70000       |
| train/episodes                 | 7000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.213      |
| train/info_shaping_reward_min  | -0.422      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 280000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 175        |
| stats_o/mean                   | 0.20405395 |
| stats_o/std                    | 0.08096129 |
| test/episodes                  | 1760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.131     |
| test/info_shaping_reward_mean  | -0.18      |
| test/info_shaping_reward_min   | -0.432     |
| test/Q                         | -31.557774 |
| test/Q_plus_P                  | -31.557774 |
| test/reward_per_eps            | -40        |
| test/steps                     | 70400      |
| train/episodes                 | 7040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.213     |
| train/info_shaping_reward_min  | -0.438     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 281600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 176        |
| stats_o/mean                   | 0.2040604  |
| stats_o/std                    | 0.0814707  |
| test/episodes                  | 1770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.188     |
| test/info_shaping_reward_min   | -0.432     |
| test/Q                         | -31.517292 |
| test/Q_plus_P                  | -31.517292 |
| test/reward_per_eps            | -40        |
| test/steps                     | 70800      |
| train/episodes                 | 7080       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.212     |
| train/info_shaping_reward_min  | -0.421     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 283200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 177        |
| stats_o/mean                   | 0.20407486 |
| stats_o/std                    | 0.08176392 |
| test/episodes                  | 1780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -0.421     |
| test/Q                         | -31.758717 |
| test/Q_plus_P                  | -31.758717 |
| test/reward_per_eps            | -40        |
| test/steps                     | 71200      |
| train/episodes                 | 7120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.508     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 284800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.20406346 |
| stats_o/std                    | 0.08213135 |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.196     |
| test/info_shaping_reward_min   | -0.743     |
| test/Q                         | -31.266884 |
| test/Q_plus_P                  | -31.266884 |
| test/reward_per_eps            | -40        |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.205     |
| train/info_shaping_reward_min  | -0.368     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 179         |
| stats_o/mean                   | 0.2041068   |
| stats_o/std                    | 0.082449876 |
| test/episodes                  | 1800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.196      |
| test/info_shaping_reward_min   | -0.372      |
| test/Q                         | -31.914236  |
| test/Q_plus_P                  | -31.914236  |
| test/reward_per_eps            | -40         |
| test/steps                     | 72000       |
| train/episodes                 | 7200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.212      |
| train/info_shaping_reward_min  | -0.495      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 288000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 180        |
| stats_o/mean                   | 0.2041361  |
| stats_o/std                    | 0.08269842 |
| test/episodes                  | 1810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.209     |
| test/Q                         | -31.818428 |
| test/Q_plus_P                  | -31.818428 |
| test/reward_per_eps            | -40        |
| test/steps                     | 72400      |
| train/episodes                 | 7240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.463     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 289600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 181         |
| stats_o/mean                   | 0.20411712  |
| stats_o/std                    | 0.082779214 |
| test/episodes                  | 1820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.139      |
| test/info_shaping_reward_mean  | -0.183      |
| test/info_shaping_reward_min   | -0.488      |
| test/Q                         | -31.678896  |
| test/Q_plus_P                  | -31.678896  |
| test/reward_per_eps            | -40         |
| test/steps                     | 72800       |
| train/episodes                 | 7280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.205      |
| train/info_shaping_reward_min  | -0.387      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 291200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 182        |
| stats_o/mean                   | 0.20411171 |
| stats_o/std                    | 0.08281974 |
| test/episodes                  | 1830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0929    |
| test/info_shaping_reward_mean  | -0.202     |
| test/info_shaping_reward_min   | -0.735     |
| test/Q                         | -30.961407 |
| test/Q_plus_P                  | -30.961407 |
| test/reward_per_eps            | -40        |
| test/steps                     | 73200      |
| train/episodes                 | 7320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.302     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 292800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 183        |
| stats_o/mean                   | 0.20410547 |
| stats_o/std                    | 0.08280262 |
| test/episodes                  | 1840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.143     |
| test/info_shaping_reward_mean  | -0.203     |
| test/info_shaping_reward_min   | -0.739     |
| test/Q                         | -32.16876  |
| test/Q_plus_P                  | -32.16876  |
| test/reward_per_eps            | -40        |
| test/steps                     | 73600      |
| train/episodes                 | 7360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 294400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 184        |
| stats_o/mean                   | 0.20410208 |
| stats_o/std                    | 0.08289317 |
| test/episodes                  | 1850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -31.825415 |
| test/Q_plus_P                  | -31.825415 |
| test/reward_per_eps            | -40        |
| test/steps                     | 74000      |
| train/episodes                 | 7400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 296000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 185         |
| stats_o/mean                   | 0.2041137   |
| stats_o/std                    | 0.082915805 |
| test/episodes                  | 1860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.177      |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -30.748535  |
| test/Q_plus_P                  | -30.748535  |
| test/reward_per_eps            | -40         |
| test/steps                     | 74400       |
| train/episodes                 | 7440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.277      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 297600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 186        |
| stats_o/mean                   | 0.20411831 |
| stats_o/std                    | 0.08295924 |
| test/episodes                  | 1870       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0961    |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -31.442633 |
| test/Q_plus_P                  | -31.442633 |
| test/reward_per_eps            | -40        |
| test/steps                     | 74800      |
| train/episodes                 | 7480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.278     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 299200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.20410866 |
| stats_o/std                    | 0.08306494 |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0793    |
| test/info_shaping_reward_mean  | -0.181     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -32.499023 |
| test/Q_plus_P                  | -32.499023 |
| test/reward_per_eps            | -40        |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 188        |
| stats_o/mean                   | 0.2041071  |
| stats_o/std                    | 0.08312576 |
| test/episodes                  | 1890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0325     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0462    |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -31.686024 |
| test/Q_plus_P                  | -31.686024 |
| test/reward_per_eps            | -38.7      |
| test/steps                     | 75600      |
| train/episodes                 | 7560       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00375    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.277     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 302400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 189        |
| stats_o/mean                   | 0.20406717 |
| stats_o/std                    | 0.08307648 |
| test/episodes                  | 1900       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0925    |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -31.681797 |
| test/Q_plus_P                  | -31.681797 |
| test/reward_per_eps            | -40        |
| test/steps                     | 76000      |
| train/episodes                 | 7600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 304000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 190        |
| stats_o/mean                   | 0.20405251 |
| stats_o/std                    | 0.083043   |
| test/episodes                  | 1910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.141     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -32.494583 |
| test/Q_plus_P                  | -32.494583 |
| test/reward_per_eps            | -40        |
| test/steps                     | 76400      |
| train/episodes                 | 7640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 305600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 191        |
| stats_o/mean                   | 0.20404556 |
| stats_o/std                    | 0.08306987 |
| test/episodes                  | 1920       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -33.266304 |
| test/Q_plus_P                  | -33.266304 |
| test/reward_per_eps            | -40        |
| test/steps                     | 76800      |
| train/episodes                 | 7680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 307200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 192        |
| stats_o/mean                   | 0.20403957 |
| stats_o/std                    | 0.08304677 |
| test/episodes                  | 1930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.139     |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -32.822845 |
| test/Q_plus_P                  | -32.822845 |
| test/reward_per_eps            | -40        |
| test/steps                     | 77200      |
| train/episodes                 | 7720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 308800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 193        |
| stats_o/mean                   | 0.20402092 |
| stats_o/std                    | 0.08298293 |
| test/episodes                  | 1940       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.13      |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -32.552624 |
| test/Q_plus_P                  | -32.552624 |
| test/reward_per_eps            | -40        |
| test/steps                     | 77600      |
| train/episodes                 | 7760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 310400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 194        |
| stats_o/mean                   | 0.20402822 |
| stats_o/std                    | 0.08303851 |
| test/episodes                  | 1950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -32.713497 |
| test/Q_plus_P                  | -32.713497 |
| test/reward_per_eps            | -40        |
| test/steps                     | 78000      |
| train/episodes                 | 7800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 312000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 195         |
| stats_o/mean                   | 0.2040213   |
| stats_o/std                    | 0.082981855 |
| test/episodes                  | 1960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.119      |
| test/info_shaping_reward_mean  | -0.168      |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -32.857388  |
| test/Q_plus_P                  | -32.857388  |
| test/reward_per_eps            | -40         |
| test/steps                     | 78400       |
| train/episodes                 | 7840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 313600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 196        |
| stats_o/mean                   | 0.20402372 |
| stats_o/std                    | 0.08304932 |
| test/episodes                  | 1970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.131     |
| test/info_shaping_reward_mean  | -0.202     |
| test/info_shaping_reward_min   | -0.535     |
| test/Q                         | -31.49005  |
| test/Q_plus_P                  | -31.49005  |
| test/reward_per_eps            | -40        |
| test/steps                     | 78800      |
| train/episodes                 | 7880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.265     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 315200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 197        |
| stats_o/mean                   | 0.2040201  |
| stats_o/std                    | 0.08307191 |
| test/episodes                  | 1980       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.111     |
| test/info_shaping_reward_mean  | -0.179     |
| test/info_shaping_reward_min   | -0.635     |
| test/Q                         | -32.099087 |
| test/Q_plus_P                  | -32.099087 |
| test/reward_per_eps            | -40        |
| test/steps                     | 79200      |
| train/episodes                 | 7920       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00688    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 316800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 198        |
| stats_o/mean                   | 0.20399944 |
| stats_o/std                    | 0.08304883 |
| test/episodes                  | 1990       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.1       |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -32.59525  |
| test/Q_plus_P                  | -32.59525  |
| test/reward_per_eps            | -40        |
| test/steps                     | 79600      |
| train/episodes                 | 7960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.267     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 318400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.20398855 |
| stats_o/std                    | 0.08308029 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.19      |
| test/info_shaping_reward_min   | -0.721     |
| test/Q                         | -33.47996  |
| test/Q_plus_P                  | -33.47996  |
| test/reward_per_eps            | -40        |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00125    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 200        |
| stats_o/mean                   | 0.20400171 |
| stats_o/std                    | 0.08305806 |
| test/episodes                  | 2010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.147     |
| test/info_shaping_reward_mean  | -0.185     |
| test/info_shaping_reward_min   | -0.463     |
| test/Q                         | -33.166145 |
| test/Q_plus_P                  | -33.166145 |
| test/reward_per_eps            | -40        |
| test/steps                     | 80400      |
| train/episodes                 | 8040       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 321600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 201        |
| stats_o/mean                   | 0.20398356 |
| stats_o/std                    | 0.08311619 |
| test/episodes                  | 2020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.118     |
| test/info_shaping_reward_mean  | -0.19      |
| test/info_shaping_reward_min   | -0.616     |
| test/Q                         | -32.892094 |
| test/Q_plus_P                  | -32.892094 |
| test/reward_per_eps            | -40        |
| test/steps                     | 80800      |
| train/episodes                 | 8080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 323200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 202        |
| stats_o/mean                   | 0.20396987 |
| stats_o/std                    | 0.08308574 |
| test/episodes                  | 2030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.107     |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -32.34262  |
| test/Q_plus_P                  | -32.34262  |
| test/reward_per_eps            | -40        |
| test/steps                     | 81200      |
| train/episodes                 | 8120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 324800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 203        |
| stats_o/mean                   | 0.20397815 |
| stats_o/std                    | 0.08315237 |
| test/episodes                  | 2040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0475     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0292    |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -32.721134 |
| test/Q_plus_P                  | -32.721134 |
| test/reward_per_eps            | -38.1      |
| test/steps                     | 81600      |
| train/episodes                 | 8160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 326400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 204         |
| stats_o/mean                   | 0.2039478   |
| stats_o/std                    | 0.083144754 |
| test/episodes                  | 2050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.101      |
| test/info_shaping_reward_mean  | -0.17       |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -33.49923   |
| test/Q_plus_P                  | -33.49923   |
| test/reward_per_eps            | -40         |
| test/steps                     | 82000       |
| train/episodes                 | 8200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 328000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 205        |
| stats_o/mean                   | 0.20392711 |
| stats_o/std                    | 0.08320784 |
| test/episodes                  | 2060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.136     |
| test/info_shaping_reward_mean  | -0.18      |
| test/info_shaping_reward_min   | -0.453     |
| test/Q                         | -32.640408 |
| test/Q_plus_P                  | -32.640408 |
| test/reward_per_eps            | -40        |
| test/steps                     | 82400      |
| train/episodes                 | 8240       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.005      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.108     |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 329600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 206        |
| stats_o/mean                   | 0.2039381  |
| stats_o/std                    | 0.08333498 |
| test/episodes                  | 2070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0615    |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.398     |
| test/Q                         | -32.21874  |
| test/Q_plus_P                  | -32.21874  |
| test/reward_per_eps            | -40        |
| test/steps                     | 82800      |
| train/episodes                 | 8280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.291     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 331200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.2039293  |
| stats_o/std                    | 0.08345557 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0885    |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.347     |
| test/Q                         | -32.548737 |
| test/Q_plus_P                  | -32.548737 |
| test/reward_per_eps            | -40        |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0269     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.121     |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 208        |
| stats_o/mean                   | 0.20391218 |
| stats_o/std                    | 0.08351656 |
| test/episodes                  | 2090       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.143     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -32.689785 |
| test/Q_plus_P                  | -32.689785 |
| test/reward_per_eps            | -40        |
| test/steps                     | 83600      |
| train/episodes                 | 8360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 334400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 209         |
| stats_o/mean                   | 0.2039197   |
| stats_o/std                    | 0.083544895 |
| test/episodes                  | 2100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0889     |
| test/info_shaping_reward_mean  | -0.169      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -32.74054   |
| test/Q_plus_P                  | -32.74054   |
| test/reward_per_eps            | -40         |
| test/steps                     | 84000       |
| train/episodes                 | 8400        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00813     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 336000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 210        |
| stats_o/mean                   | 0.20393841 |
| stats_o/std                    | 0.08360796 |
| test/episodes                  | 2110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.133     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.337     |
| test/Q                         | -32.842564 |
| test/Q_plus_P                  | -32.842564 |
| test/reward_per_eps            | -40        |
| test/steps                     | 84400      |
| train/episodes                 | 8440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 337600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 211        |
| stats_o/mean                   | 0.20393091 |
| stats_o/std                    | 0.08360004 |
| test/episodes                  | 2120       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.188     |
| test/info_shaping_reward_min   | -0.58      |
| test/Q                         | -32.297    |
| test/Q_plus_P                  | -32.297    |
| test/reward_per_eps            | -40        |
| test/steps                     | 84800      |
| train/episodes                 | 8480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 339200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 212        |
| stats_o/mean                   | 0.20392998 |
| stats_o/std                    | 0.08366796 |
| test/episodes                  | 2130       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.112     |
| test/info_shaping_reward_mean  | -0.177     |
| test/info_shaping_reward_min   | -0.405     |
| test/Q                         | -32.201588 |
| test/Q_plus_P                  | -32.201588 |
| test/reward_per_eps            | -40        |
| test/steps                     | 85200      |
| train/episodes                 | 8520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 340800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 213        |
| stats_o/mean                   | 0.20393378 |
| stats_o/std                    | 0.08365297 |
| test/episodes                  | 2140       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -33.187466 |
| test/Q_plus_P                  | -33.187466 |
| test/reward_per_eps            | -40        |
| test/steps                     | 85600      |
| train/episodes                 | 8560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 342400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 214        |
| stats_o/mean                   | 0.20391196 |
| stats_o/std                    | 0.08362647 |
| test/episodes                  | 2150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0731    |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -31.7651   |
| test/Q_plus_P                  | -31.7651   |
| test/reward_per_eps            | -40        |
| test/steps                     | 86000      |
| train/episodes                 | 8600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 344000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 215        |
| stats_o/mean                   | 0.20390181 |
| stats_o/std                    | 0.08355674 |
| test/episodes                  | 2160       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.199     |
| test/info_shaping_reward_min   | -0.506     |
| test/Q                         | -32.02427  |
| test/Q_plus_P                  | -32.02427  |
| test/reward_per_eps            | -40        |
| test/steps                     | 86400      |
| train/episodes                 | 8640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 345600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 216        |
| stats_o/mean                   | 0.20390308 |
| stats_o/std                    | 0.08351516 |
| test/episodes                  | 2170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -33.303696 |
| test/Q_plus_P                  | -33.303696 |
| test/reward_per_eps            | -40        |
| test/steps                     | 86800      |
| train/episodes                 | 8680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 347200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 217         |
| stats_o/mean                   | 0.20389004  |
| stats_o/std                    | 0.083467714 |
| test/episodes                  | 2180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -33.34978   |
| test/Q_plus_P                  | -33.34978   |
| test/reward_per_eps            | -40         |
| test/steps                     | 87200       |
| train/episodes                 | 8720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 348800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 218        |
| stats_o/mean                   | 0.20390218 |
| stats_o/std                    | 0.08344305 |
| test/episodes                  | 2190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -32.908672 |
| test/Q_plus_P                  | -32.908672 |
| test/reward_per_eps            | -40        |
| test/steps                     | 87600      |
| train/episodes                 | 8760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 350400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 219        |
| stats_o/mean                   | 0.20390143 |
| stats_o/std                    | 0.08338718 |
| test/episodes                  | 2200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -32.65789  |
| test/Q_plus_P                  | -32.65789  |
| test/reward_per_eps            | -40        |
| test/steps                     | 88000      |
| train/episodes                 | 8800       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00187    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 352000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 220        |
| stats_o/mean                   | 0.20389286 |
| stats_o/std                    | 0.08333272 |
| test/episodes                  | 2210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.132     |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -32.95847  |
| test/Q_plus_P                  | -32.95847  |
| test/reward_per_eps            | -40        |
| test/steps                     | 88400      |
| train/episodes                 | 8840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 353600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 221        |
| stats_o/mean                   | 0.20391205 |
| stats_o/std                    | 0.083315   |
| test/episodes                  | 2220       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.114     |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -33.42243  |
| test/Q_plus_P                  | -33.42243  |
| test/reward_per_eps            | -40        |
| test/steps                     | 88800      |
| train/episodes                 | 8880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 355200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.2039304  |
| stats_o/std                    | 0.08329426 |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.179     |
| test/info_shaping_reward_min   | -0.422     |
| test/Q                         | -32.81117  |
| test/Q_plus_P                  | -32.81117  |
| test/reward_per_eps            | -40        |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 223        |
| stats_o/mean                   | 0.20392923 |
| stats_o/std                    | 0.08329947 |
| test/episodes                  | 2240       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -33.18977  |
| test/Q_plus_P                  | -33.18977  |
| test/reward_per_eps            | -40        |
| test/steps                     | 89600      |
| train/episodes                 | 8960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 358400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.20393206 |
| stats_o/std                    | 0.08324718 |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -33.63035  |
| test/Q_plus_P                  | -33.63035  |
| test/reward_per_eps            | -40        |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 225       |
| stats_o/mean                   | 0.2039296 |
| stats_o/std                    | 0.0831802 |
| test/episodes                  | 2260      |
| test/info_is_success_max       | 0         |
| test/info_is_success_mean      | 0         |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.157    |
| test/info_shaping_reward_mean  | -0.175    |
| test/info_shaping_reward_min   | -0.362    |
| test/Q                         | -33.54498 |
| test/Q_plus_P                  | -33.54498 |
| test/reward_per_eps            | -40       |
| test/steps                     | 90400     |
| train/episodes                 | 9040      |
| train/info_is_success_max      | 0         |
| train/info_is_success_mean     | 0         |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.16     |
| train/info_shaping_reward_mean | -0.172    |
| train/info_shaping_reward_min  | -0.177    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -40       |
| train/steps                    | 361600    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 226        |
| stats_o/mean                   | 0.20392248 |
| stats_o/std                    | 0.08309537 |
| test/episodes                  | 2270       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.141     |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -33.96804  |
| test/Q_plus_P                  | -33.96804  |
| test/reward_per_eps            | -40        |
| test/steps                     | 90800      |
| train/episodes                 | 9080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 363200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 227         |
| stats_o/mean                   | 0.20390628  |
| stats_o/std                    | 0.083026916 |
| test/episodes                  | 2280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -33.311462  |
| test/Q_plus_P                  | -33.311462  |
| test/reward_per_eps            | -40         |
| test/steps                     | 91200       |
| train/episodes                 | 9120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 364800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 228        |
| stats_o/mean                   | 0.20391402 |
| stats_o/std                    | 0.08294998 |
| test/episodes                  | 2290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.135     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -33.789494 |
| test/Q_plus_P                  | -33.789494 |
| test/reward_per_eps            | -40        |
| test/steps                     | 91600      |
| train/episodes                 | 9160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 366400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 229        |
| stats_o/mean                   | 0.20392595 |
| stats_o/std                    | 0.08287617 |
| test/episodes                  | 2300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0616    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -32.727894 |
| test/Q_plus_P                  | -32.727894 |
| test/reward_per_eps            | -40        |
| test/steps                     | 92000      |
| train/episodes                 | 9200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 368000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 230        |
| stats_o/mean                   | 0.20393455 |
| stats_o/std                    | 0.0827977  |
| test/episodes                  | 2310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.132     |
| test/info_shaping_reward_mean  | -0.178     |
| test/info_shaping_reward_min   | -0.407     |
| test/Q                         | -34.28307  |
| test/Q_plus_P                  | -34.28307  |
| test/reward_per_eps            | -40        |
| test/steps                     | 92400      |
| train/episodes                 | 9240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 369600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 231        |
| stats_o/mean                   | 0.20390832 |
| stats_o/std                    | 0.08275877 |
| test/episodes                  | 2320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.04       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0374    |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -33.76653  |
| test/Q_plus_P                  | -33.76653  |
| test/reward_per_eps            | -38.4      |
| test/steps                     | 92800      |
| train/episodes                 | 9280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 371200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 232        |
| stats_o/mean                   | 0.20391741 |
| stats_o/std                    | 0.08268798 |
| test/episodes                  | 2330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.203     |
| test/Q                         | -33.487526 |
| test/Q_plus_P                  | -33.487526 |
| test/reward_per_eps            | -40        |
| test/steps                     | 93200      |
| train/episodes                 | 9320       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0025     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 372800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 233        |
| stats_o/mean                   | 0.20393424 |
| stats_o/std                    | 0.08263953 |
| test/episodes                  | 2340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.178     |
| test/info_shaping_reward_min   | -0.219     |
| test/Q                         | -34.708317 |
| test/Q_plus_P                  | -34.708317 |
| test/reward_per_eps            | -40        |
| test/steps                     | 93600      |
| train/episodes                 | 9360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 374400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.20393425 |
| stats_o/std                    | 0.08266197 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.186     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -34.356247 |
| test/Q_plus_P                  | -34.356247 |
| test/reward_per_eps            | -40        |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 235        |
| stats_o/mean                   | 0.2039416  |
| stats_o/std                    | 0.08265097 |
| test/episodes                  | 2360       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -34.01677  |
| test/Q_plus_P                  | -34.01677  |
| test/reward_per_eps            | -40        |
| test/steps                     | 94400      |
| train/episodes                 | 9440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 377600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 236        |
| stats_o/mean                   | 0.20394775 |
| stats_o/std                    | 0.08259624 |
| test/episodes                  | 2370       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.196     |
| test/info_shaping_reward_min   | -0.532     |
| test/Q                         | -33.963184 |
| test/Q_plus_P                  | -33.963184 |
| test/reward_per_eps            | -40        |
| test/steps                     | 94800      |
| train/episodes                 | 9480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.288     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 379200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 237        |
| stats_o/mean                   | 0.20395297 |
| stats_o/std                    | 0.08264185 |
| test/episodes                  | 2380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.186     |
| test/info_shaping_reward_min   | -0.457     |
| test/Q                         | -33.787735 |
| test/Q_plus_P                  | -33.787735 |
| test/reward_per_eps            | -40        |
| test/steps                     | 95200      |
| train/episodes                 | 9520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 380800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 238        |
| stats_o/mean                   | 0.20395155 |
| stats_o/std                    | 0.08281786 |
| test/episodes                  | 2390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -33.394886 |
| test/Q_plus_P                  | -33.394886 |
| test/reward_per_eps            | -40        |
| test/steps                     | 95600      |
| train/episodes                 | 9560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.296     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 382400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 239        |
| stats_o/mean                   | 0.20396334 |
| stats_o/std                    | 0.08289832 |
| test/episodes                  | 2400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.199     |
| test/info_shaping_reward_min   | -0.476     |
| test/Q                         | -32.694828 |
| test/Q_plus_P                  | -32.694828 |
| test/reward_per_eps            | -40        |
| test/steps                     | 96000      |
| train/episodes                 | 9600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 384000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 240        |
| stats_o/mean                   | 0.20392925 |
| stats_o/std                    | 0.08291498 |
| test/episodes                  | 2410       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.188     |
| test/info_shaping_reward_min   | -0.479     |
| test/Q                         | -33.704926 |
| test/Q_plus_P                  | -33.704926 |
| test/reward_per_eps            | -40        |
| test/steps                     | 96400      |
| train/episodes                 | 9640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 385600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 241         |
| stats_o/mean                   | 0.2039324   |
| stats_o/std                    | 0.082952745 |
| test/episodes                  | 2420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.19       |
| test/info_shaping_reward_min   | -0.502      |
| test/Q                         | -33.582027  |
| test/Q_plus_P                  | -33.582027  |
| test/reward_per_eps            | -40         |
| test/steps                     | 96800       |
| train/episodes                 | 9680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.18       |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 387200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 242        |
| stats_o/mean                   | 0.20391478 |
| stats_o/std                    | 0.08323488 |
| test/episodes                  | 2430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.101     |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -34.27403  |
| test/Q_plus_P                  | -34.27403  |
| test/reward_per_eps            | -40        |
| test/steps                     | 97200      |
| train/episodes                 | 9720       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.278     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 388800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 243        |
| stats_o/mean                   | 0.20391598 |
| stats_o/std                    | 0.08345413 |
| test/episodes                  | 2440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -0.549     |
| test/Q                         | -33.531876 |
| test/Q_plus_P                  | -33.531876 |
| test/reward_per_eps            | -40        |
| test/steps                     | 97600      |
| train/episodes                 | 9760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.278     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 390400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 244         |
| stats_o/mean                   | 0.2039181   |
| stats_o/std                    | 0.083604164 |
| test/episodes                  | 2450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.194      |
| test/info_shaping_reward_min   | -0.542      |
| test/Q                         | -34.315353  |
| test/Q_plus_P                  | -34.315353  |
| test/reward_per_eps            | -40         |
| test/steps                     | 98000       |
| train/episodes                 | 9800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.188      |
| train/info_shaping_reward_min  | -0.295      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 392000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 245         |
| stats_o/mean                   | 0.20389704  |
| stats_o/std                    | 0.083693475 |
| test/episodes                  | 2460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.588      |
| test/Q                         | -33.146873  |
| test/Q_plus_P                  | -33.146873  |
| test/reward_per_eps            | -40         |
| test/steps                     | 98400       |
| train/episodes                 | 9840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 393600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 246        |
| stats_o/mean                   | 0.20387977 |
| stats_o/std                    | 0.08391142 |
| test/episodes                  | 2470       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.128     |
| test/info_shaping_reward_mean  | -0.182     |
| test/info_shaping_reward_min   | -0.492     |
| test/Q                         | -32.227222 |
| test/Q_plus_P                  | -32.227222 |
| test/reward_per_eps            | -40        |
| test/steps                     | 98800      |
| train/episodes                 | 9880       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.01       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 395200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 247         |
| stats_o/mean                   | 0.20388351  |
| stats_o/std                    | 0.084264114 |
| test/episodes                  | 2480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.175      |
| test/info_shaping_reward_min   | -0.199      |
| test/Q                         | -34.564316  |
| test/Q_plus_P                  | -34.564316  |
| test/reward_per_eps            | -40         |
| test/steps                     | 99200       |
| train/episodes                 | 9920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 396800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.20387895 |
| stats_o/std                    | 0.08437685 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.147     |
| test/info_shaping_reward_mean  | -0.203     |
| test/info_shaping_reward_min   | -0.58      |
| test/Q                         | -32.880436 |
| test/Q_plus_P                  | -32.880436 |
| test/reward_per_eps            | -40        |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.301     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 249         |
| stats_o/mean                   | 0.20388119  |
| stats_o/std                    | 0.084501915 |
| test/episodes                  | 2500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.134      |
| test/info_shaping_reward_mean  | -0.203      |
| test/info_shaping_reward_min   | -0.563      |
| test/Q                         | -34.589523  |
| test/Q_plus_P                  | -34.589523  |
| test/reward_per_eps            | -40         |
| test/steps                     | 100000      |
| train/episodes                 | 10000       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.015       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.181      |
| train/info_shaping_reward_min  | -0.294      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 400000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 250        |
| stats_o/mean                   | 0.20389605 |
| stats_o/std                    | 0.08458979 |
| test/episodes                  | 2510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.186     |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -34.36816  |
| test/Q_plus_P                  | -34.36816  |
| test/reward_per_eps            | -40        |
| test/steps                     | 100400     |
| train/episodes                 | 10040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.3       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 401600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 251        |
| stats_o/mean                   | 0.20392446 |
| stats_o/std                    | 0.08500352 |
| test/episodes                  | 2520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.197     |
| test/info_shaping_reward_min   | -0.525     |
| test/Q                         | -34.053898 |
| test/Q_plus_P                  | -34.053898 |
| test/reward_per_eps            | -40        |
| test/steps                     | 100800     |
| train/episodes                 | 10080      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.112     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.278     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 403200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 252        |
| stats_o/mean                   | 0.20392062 |
| stats_o/std                    | 0.08521147 |
| test/episodes                  | 2530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0705    |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -34.18246  |
| test/Q_plus_P                  | -34.18246  |
| test/reward_per_eps            | -40        |
| test/steps                     | 101200     |
| train/episodes                 | 10120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 404800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.20392616  |
| stats_o/std                    | 0.085553624 |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0952     |
| test/info_shaping_reward_mean  | -0.177      |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -33.08759   |
| test/Q_plus_P                  | -33.08759   |
| test/reward_per_eps            | -40         |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.000625    |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.192      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 254        |
| stats_o/mean                   | 0.20393306 |
| stats_o/std                    | 0.08576399 |
| test/episodes                  | 2550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0924    |
| test/info_shaping_reward_mean  | -0.187     |
| test/info_shaping_reward_min   | -0.467     |
| test/Q                         | -32.698944 |
| test/Q_plus_P                  | -32.698944 |
| test/reward_per_eps            | -40        |
| test/steps                     | 102000     |
| train/episodes                 | 10200      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0025     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 408000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 255         |
| stats_o/mean                   | 0.20393363  |
| stats_o/std                    | 0.085891835 |
| test/episodes                  | 2560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.12       |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.574      |
| test/Q                         | -33.489693  |
| test/Q_plus_P                  | -33.489693  |
| test/reward_per_eps            | -40         |
| test/steps                     | 102400      |
| train/episodes                 | 10240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 409600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 256        |
| stats_o/mean                   | 0.20391566 |
| stats_o/std                    | 0.08605544 |
| test/episodes                  | 2570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.103     |
| test/info_shaping_reward_mean  | -0.185     |
| test/info_shaping_reward_min   | -0.557     |
| test/Q                         | -33.051796 |
| test/Q_plus_P                  | -33.051796 |
| test/reward_per_eps            | -40        |
| test/steps                     | 102800     |
| train/episodes                 | 10280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 411200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 257        |
| stats_o/mean                   | 0.2039246  |
| stats_o/std                    | 0.0863454  |
| test/episodes                  | 2580       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.196     |
| test/info_shaping_reward_min   | -0.415     |
| test/Q                         | -34.371838 |
| test/Q_plus_P                  | -34.371838 |
| test/reward_per_eps            | -40        |
| test/steps                     | 103200     |
| train/episodes                 | 10320      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 412800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 258        |
| stats_o/mean                   | 0.20391943 |
| stats_o/std                    | 0.08662203 |
| test/episodes                  | 2590       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0822    |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.226     |
| test/Q                         | -32.793667 |
| test/Q_plus_P                  | -32.793667 |
| test/reward_per_eps            | -40        |
| test/steps                     | 103600     |
| train/episodes                 | 10360      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00688    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.368     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 414400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 259         |
| stats_o/mean                   | 0.2039134   |
| stats_o/std                    | 0.086953185 |
| test/episodes                  | 2600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.126      |
| test/info_shaping_reward_mean  | -0.168      |
| test/info_shaping_reward_min   | -0.198      |
| test/Q                         | -32.9448    |
| test/Q_plus_P                  | -32.9448    |
| test/reward_per_eps            | -40         |
| test/steps                     | 104000      |
| train/episodes                 | 10400       |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0238      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0882     |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39         |
| train/steps                    | 416000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 260        |
| stats_o/mean                   | 0.2039215  |
| stats_o/std                    | 0.08711079 |
| test/episodes                  | 2610       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0743    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -32.637684 |
| test/Q_plus_P                  | -32.637684 |
| test/reward_per_eps            | -40        |
| test/steps                     | 104400     |
| train/episodes                 | 10440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 417600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 261        |
| stats_o/mean                   | 0.20391478 |
| stats_o/std                    | 0.08722775 |
| test/episodes                  | 2620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.068     |
| test/info_shaping_reward_mean  | -0.183     |
| test/info_shaping_reward_min   | -0.324     |
| test/Q                         | -32.26915  |
| test/Q_plus_P                  | -32.26915  |
| test/reward_per_eps            | -40        |
| test/steps                     | 104800     |
| train/episodes                 | 10480      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00688    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 419200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.20393468  |
| stats_o/std                    | 0.087438576 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0569     |
| test/info_shaping_reward_mean  | -0.158      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -31.91352   |
| test/Q_plus_P                  | -31.91352   |
| test/reward_per_eps            | -40         |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00125     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.102      |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 263        |
| stats_o/mean                   | 0.20395386 |
| stats_o/std                    | 0.0875657  |
| test/episodes                  | 2640       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0961    |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.277     |
| test/Q                         | -33.003777 |
| test/Q_plus_P                  | -33.003777 |
| test/reward_per_eps            | -40        |
| test/steps                     | 105600     |
| train/episodes                 | 10560      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 422400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 264        |
| stats_o/mean                   | 0.20393828 |
| stats_o/std                    | 0.08772376 |
| test/episodes                  | 2650       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0636    |
| test/info_shaping_reward_mean  | -0.156     |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -32.114086 |
| test/Q_plus_P                  | -32.114086 |
| test/reward_per_eps            | -40        |
| test/steps                     | 106000     |
| train/episodes                 | 10600      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0112     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.108     |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 424000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 265        |
| stats_o/mean                   | 0.20392677 |
| stats_o/std                    | 0.08799656 |
| test/episodes                  | 2660       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.125     |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.313     |
| test/Q                         | -32.657032 |
| test/Q_plus_P                  | -32.657032 |
| test/reward_per_eps            | -40        |
| test/steps                     | 106400     |
| train/episodes                 | 10640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.11      |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 425600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 266        |
| stats_o/mean                   | 0.2039267  |
| stats_o/std                    | 0.08808082 |
| test/episodes                  | 2670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0997    |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -33.35223  |
| test/Q_plus_P                  | -33.35223  |
| test/reward_per_eps            | -40        |
| test/steps                     | 106800     |
| train/episodes                 | 10680      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0125     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.111     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 427200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 267        |
| stats_o/mean                   | 0.20392114 |
| stats_o/std                    | 0.08840391 |
| test/episodes                  | 2680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0647    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -32.377274 |
| test/Q_plus_P                  | -32.377274 |
| test/reward_per_eps            | -40        |
| test/steps                     | 107200     |
| train/episodes                 | 10720      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.122     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 428800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 268        |
| stats_o/mean                   | 0.20392051 |
| stats_o/std                    | 0.08851616 |
| test/episodes                  | 2690       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0854    |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.35      |
| test/Q                         | -32.598392 |
| test/Q_plus_P                  | -32.598392 |
| test/reward_per_eps            | -40        |
| test/steps                     | 107600     |
| train/episodes                 | 10760      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 430400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 269        |
| stats_o/mean                   | 0.20393497 |
| stats_o/std                    | 0.08865747 |
| test/episodes                  | 2700       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0816    |
| test/info_shaping_reward_mean  | -0.178     |
| test/info_shaping_reward_min   | -0.438     |
| test/Q                         | -34.212997 |
| test/Q_plus_P                  | -34.212997 |
| test/reward_per_eps            | -40        |
| test/steps                     | 108000     |
| train/episodes                 | 10800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 432000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 270        |
| stats_o/mean                   | 0.20389344 |
| stats_o/std                    | 0.08882886 |
| test/episodes                  | 2710       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -34.116302 |
| test/Q_plus_P                  | -34.116302 |
| test/reward_per_eps            | -40        |
| test/steps                     | 108400     |
| train/episodes                 | 10840      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 433600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 271        |
| stats_o/mean                   | 0.20389995 |
| stats_o/std                    | 0.08908452 |
| test/episodes                  | 2720       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0689    |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -33.525963 |
| test/Q_plus_P                  | -33.525963 |
| test/reward_per_eps            | -40        |
| test/steps                     | 108800     |
| train/episodes                 | 10880      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0163     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.108     |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 435200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 272        |
| stats_o/mean                   | 0.20388317 |
| stats_o/std                    | 0.08933483 |
| test/episodes                  | 2730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0541    |
| test/info_shaping_reward_mean  | -0.179     |
| test/info_shaping_reward_min   | -0.453     |
| test/Q                         | -34.055218 |
| test/Q_plus_P                  | -34.055218 |
| test/reward_per_eps            | -40        |
| test/steps                     | 109200     |
| train/episodes                 | 10920      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0294     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.105     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 436800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 273         |
| stats_o/mean                   | 0.20387685  |
| stats_o/std                    | 0.089529485 |
| test/episodes                  | 2740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.065       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0442     |
| test/info_shaping_reward_mean  | -0.161      |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -33.480923  |
| test/Q_plus_P                  | -33.480923  |
| test/reward_per_eps            | -37.4       |
| test/steps                     | 109600      |
| train/episodes                 | 10960       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00125     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.19       |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 438400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 274        |
| stats_o/mean                   | 0.20386435 |
| stats_o/std                    | 0.08981568 |
| test/episodes                  | 2750       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0901    |
| test/info_shaping_reward_mean  | -0.189     |
| test/info_shaping_reward_min   | -0.403     |
| test/Q                         | -32.73097  |
| test/Q_plus_P                  | -32.73097  |
| test/reward_per_eps            | -40        |
| test/steps                     | 110000     |
| train/episodes                 | 11000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.374     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 440000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 275         |
| stats_o/mean                   | 0.20388222  |
| stats_o/std                    | 0.089979485 |
| test/episodes                  | 2760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.025       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0328     |
| test/info_shaping_reward_mean  | -0.171      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -34.342648  |
| test/Q_plus_P                  | -34.342648  |
| test/reward_per_eps            | -39         |
| test/steps                     | 110400      |
| train/episodes                 | 11040       |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0219      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.101      |
| train/info_shaping_reward_mean | -0.183      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.1       |
| train/steps                    | 441600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 276        |
| stats_o/mean                   | 0.20386477 |
| stats_o/std                    | 0.0901857  |
| test/episodes                  | 2770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0947    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -33.167202 |
| test/Q_plus_P                  | -33.167202 |
| test/reward_per_eps            | -40        |
| test/steps                     | 110800     |
| train/episodes                 | 11080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.106     |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 443200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 277        |
| stats_o/mean                   | 0.20385107 |
| stats_o/std                    | 0.09028267 |
| test/episodes                  | 2780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0724    |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -33.515396 |
| test/Q_plus_P                  | -33.515396 |
| test/reward_per_eps            | -40        |
| test/steps                     | 111200     |
| train/episodes                 | 11120      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.025      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0907    |
| train/info_shaping_reward_mean | -0.167     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 444800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 278        |
| stats_o/mean                   | 0.20386554 |
| stats_o/std                    | 0.09040503 |
| test/episodes                  | 2790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0725     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.012     |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.63      |
| test/Q                         | -32.660694 |
| test/Q_plus_P                  | -32.660694 |
| test/reward_per_eps            | -37.1      |
| test/steps                     | 111600     |
| train/episodes                 | 11160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 446400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 279        |
| stats_o/mean                   | 0.2038808  |
| stats_o/std                    | 0.09064103 |
| test/episodes                  | 2800       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.107     |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -34.484737 |
| test/Q_plus_P                  | -34.484737 |
| test/reward_per_eps            | -40        |
| test/steps                     | 112000     |
| train/episodes                 | 11200      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 448000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 280        |
| stats_o/mean                   | 0.20388292 |
| stats_o/std                    | 0.09074367 |
| test/episodes                  | 2810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0661    |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.223     |
| test/Q                         | -33.294662 |
| test/Q_plus_P                  | -33.294662 |
| test/reward_per_eps            | -40        |
| test/steps                     | 112400     |
| train/episodes                 | 11240      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0175     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.1       |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.288     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 449600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.20389296 |
| stats_o/std                    | 0.09075931 |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0765    |
| test/info_shaping_reward_mean  | -0.181     |
| test/info_shaping_reward_min   | -0.38      |
| test/Q                         | -33.942192 |
| test/Q_plus_P                  | -33.942192 |
| test/reward_per_eps            | -40        |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0075     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.20386642 |
| stats_o/std                    | 0.09083185 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.103     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -34.24172  |
| test/Q_plus_P                  | -34.24172  |
| test/reward_per_eps            | -40        |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0925    |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.20386049 |
| stats_o/std                    | 0.09086562 |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0964    |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.432     |
| test/Q                         | -32.928104 |
| test/Q_plus_P                  | -32.928104 |
| test/reward_per_eps            | -40        |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00562    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.113     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 284         |
| stats_o/mean                   | 0.20387682  |
| stats_o/std                    | 0.090995386 |
| test/episodes                  | 2850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.175      |
| test/info_shaping_reward_min   | -0.217      |
| test/Q                         | -33.550617  |
| test/Q_plus_P                  | -33.550617  |
| test/reward_per_eps            | -40         |
| test/steps                     | 114000      |
| train/episodes                 | 11400       |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0144      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.183      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 456000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 285        |
| stats_o/mean                   | 0.20385768 |
| stats_o/std                    | 0.09113642 |
| test/episodes                  | 2860       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0728    |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.374     |
| test/Q                         | -33.60065  |
| test/Q_plus_P                  | -33.60065  |
| test/reward_per_eps            | -40        |
| test/steps                     | 114400     |
| train/episodes                 | 11440      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00125    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.108     |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 457600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 286        |
| stats_o/mean                   | 0.2038474  |
| stats_o/std                    | 0.09124709 |
| test/episodes                  | 2870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.02       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0426    |
| test/info_shaping_reward_mean  | -0.161     |
| test/info_shaping_reward_min   | -0.331     |
| test/Q                         | -32.93467  |
| test/Q_plus_P                  | -32.93467  |
| test/reward_per_eps            | -39.2      |
| test/steps                     | 114800     |
| train/episodes                 | 11480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.115     |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 459200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.20383878 |
| stats_o/std                    | 0.0914256  |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0475     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0478    |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -33.654053 |
| test/Q_plus_P                  | -33.654053 |
| test/reward_per_eps            | -38.1      |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.297     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 288        |
| stats_o/mean                   | 0.2038096  |
| stats_o/std                    | 0.09168639 |
| test/episodes                  | 2890       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.121     |
| test/info_shaping_reward_mean  | -0.2       |
| test/info_shaping_reward_min   | -0.681     |
| test/Q                         | -32.37506  |
| test/Q_plus_P                  | -32.37506  |
| test/reward_per_eps            | -40        |
| test/steps                     | 115600     |
| train/episodes                 | 11560      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0025     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0998    |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 462400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 289        |
| stats_o/mean                   | 0.20381853 |
| stats_o/std                    | 0.0918718  |
| test/episodes                  | 2900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0375     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00997   |
| test/info_shaping_reward_mean  | -0.161     |
| test/info_shaping_reward_min   | -0.222     |
| test/Q                         | -33.54273  |
| test/Q_plus_P                  | -33.54273  |
| test/reward_per_eps            | -38.5      |
| test/steps                     | 116000     |
| train/episodes                 | 11600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.129     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 464000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 290         |
| stats_o/mean                   | 0.20383349  |
| stats_o/std                    | 0.091979295 |
| test/episodes                  | 2910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0954     |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.502      |
| test/Q                         | -34.47612   |
| test/Q_plus_P                  | -34.47612   |
| test/reward_per_eps            | -40         |
| test/steps                     | 116400      |
| train/episodes                 | 11640       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0138      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.18       |
| train/info_shaping_reward_min  | -0.284      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 465600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.2038348   |
| stats_o/std                    | 0.092183515 |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0075      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0369     |
| test/info_shaping_reward_mean  | -0.145      |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -32.42949   |
| test/Q_plus_P                  | -32.42949   |
| test/reward_per_eps            | -39.7       |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.015       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.119      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 292        |
| stats_o/mean                   | 0.20383306 |
| stats_o/std                    | 0.0923489  |
| test/episodes                  | 2930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0717    |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.332     |
| test/Q                         | -34.31665  |
| test/Q_plus_P                  | -34.31665  |
| test/reward_per_eps            | -40        |
| test/steps                     | 117200     |
| train/episodes                 | 11720      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.104     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 468800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.203825    |
| stats_o/std                    | 0.092418626 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0842     |
| test/info_shaping_reward_mean  | -0.189      |
| test/info_shaping_reward_min   | -0.513      |
| test/Q                         | -33.010777  |
| test/Q_plus_P                  | -33.010777  |
| test/reward_per_eps            | -40         |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00125     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.184      |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.20384918  |
| stats_o/std                    | 0.092553645 |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.101      |
| test/info_shaping_reward_mean  | -0.181      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -34.201324  |
| test/Q_plus_P                  | -34.201324  |
| test/reward_per_eps            | -40         |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.117      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 295        |
| stats_o/mean                   | 0.20383637 |
| stats_o/std                    | 0.0927291  |
| test/episodes                  | 2960       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -33.848885 |
| test/Q_plus_P                  | -33.848885 |
| test/reward_per_eps            | -40        |
| test/steps                     | 118400     |
| train/episodes                 | 11840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 473600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 296        |
| stats_o/mean                   | 0.20380792 |
| stats_o/std                    | 0.09307237 |
| test/episodes                  | 2970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0885    |
| test/info_shaping_reward_mean  | -0.179     |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -34.768814 |
| test/Q_plus_P                  | -34.768814 |
| test/reward_per_eps            | -40        |
| test/steps                     | 118800     |
| train/episodes                 | 11880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.103     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.285     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 475200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 297        |
| stats_o/mean                   | 0.2038109  |
| stats_o/std                    | 0.09330039 |
| test/episodes                  | 2980       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.189     |
| test/info_shaping_reward_min   | -0.353     |
| test/Q                         | -33.51087  |
| test/Q_plus_P                  | -33.51087  |
| test/reward_per_eps            | -40        |
| test/steps                     | 119200     |
| train/episodes                 | 11920      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0319     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0987    |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.374     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 476800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 298         |
| stats_o/mean                   | 0.20380591  |
| stats_o/std                    | 0.093402736 |
| test/episodes                  | 2990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.06        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0455     |
| test/info_shaping_reward_mean  | -0.188      |
| test/info_shaping_reward_min   | -0.344      |
| test/Q                         | -33.2626    |
| test/Q_plus_P                  | -33.2626    |
| test/reward_per_eps            | -37.6       |
| test/steps                     | 119600      |
| train/episodes                 | 11960       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0138      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0989     |
| train/info_shaping_reward_mean | -0.17       |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 478400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 299        |
| stats_o/mean                   | 0.20380543 |
| stats_o/std                    | 0.09350003 |
| test/episodes                  | 3000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0601    |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -33.261154 |
| test/Q_plus_P                  | -33.261154 |
| test/reward_per_eps            | -40        |
| test/steps                     | 120000     |
| train/episodes                 | 12000      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.107     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 480000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 300        |
| stats_o/mean                   | 0.20378979 |
| stats_o/std                    | 0.09370167 |
| test/episodes                  | 3010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0686    |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -33.16731  |
| test/Q_plus_P                  | -33.16731  |
| test/reward_per_eps            | -40        |
| test/steps                     | 120400     |
| train/episodes                 | 12040      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0125     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.121     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.31      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 481600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.20381698 |
| stats_o/std                    | 0.09390213 |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.015      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0261    |
| test/info_shaping_reward_mean  | -0.161     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -32.80835  |
| test/Q_plus_P                  | -32.80835  |
| test/reward_per_eps            | -39.4      |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0225     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.111     |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 302        |
| stats_o/mean                   | 0.20383085 |
| stats_o/std                    | 0.09405672 |
| test/episodes                  | 3030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.182     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -34.134693 |
| test/Q_plus_P                  | -34.134693 |
| test/reward_per_eps            | -40        |
| test/steps                     | 121200     |
| train/episodes                 | 12120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.294     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 484800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 303        |
| stats_o/mean                   | 0.20385742 |
| stats_o/std                    | 0.09429077 |
| test/episodes                  | 3040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0521    |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -33.746315 |
| test/Q_plus_P                  | -33.746315 |
| test/reward_per_eps            | -40        |
| test/steps                     | 121600     |
| train/episodes                 | 12160      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0119     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0946    |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 486400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 304        |
| stats_o/mean                   | 0.20385855 |
| stats_o/std                    | 0.09443125 |
| test/episodes                  | 3050       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0858    |
| test/info_shaping_reward_mean  | -0.19      |
| test/info_shaping_reward_min   | -0.451     |
| test/Q                         | -33.692104 |
| test/Q_plus_P                  | -33.692104 |
| test/reward_per_eps            | -40        |
| test/steps                     | 122000     |
| train/episodes                 | 12200      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0138     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.105     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.308     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 488000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.20383564 |
| stats_o/std                    | 0.09450449 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0643    |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -32.904488 |
| test/Q_plus_P                  | -32.904488 |
| test/reward_per_eps            | -40        |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0406     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.107     |
| train/info_shaping_reward_mean | -0.167     |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 306        |
| stats_o/mean                   | 0.20382106 |
| stats_o/std                    | 0.09470882 |
| test/episodes                  | 3070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0579    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -33.781334 |
| test/Q_plus_P                  | -33.781334 |
| test/reward_per_eps            | -40        |
| test/steps                     | 122800     |
| train/episodes                 | 12280      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0156     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0971    |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 491200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.20382118 |
| stats_o/std                    | 0.09488436 |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0725     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0365    |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -32.69942  |
| test/Q_plus_P                  | -32.69942  |
| test/reward_per_eps            | -37.1      |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.122     |
| train/info_shaping_reward_mean | -0.196     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 308        |
| stats_o/mean                   | 0.20381299 |
| stats_o/std                    | 0.09488218 |
| test/episodes                  | 3090       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0905    |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -33.142742 |
| test/Q_plus_P                  | -33.142742 |
| test/reward_per_eps            | -40        |
| test/steps                     | 123600     |
| train/episodes                 | 12360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 494400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 309         |
| stats_o/mean                   | 0.20376772  |
| stats_o/std                    | 0.095069125 |
| test/episodes                  | 3100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.635      |
| test/Q                         | -33.842327  |
| test/Q_plus_P                  | -33.842327  |
| test/reward_per_eps            | -40         |
| test/steps                     | 124000      |
| train/episodes                 | 12400       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0163      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.301      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 496000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 310        |
| stats_o/mean                   | 0.20377003 |
| stats_o/std                    | 0.09516105 |
| test/episodes                  | 3110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0025     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0448    |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -0.453     |
| test/Q                         | -32.95572  |
| test/Q_plus_P                  | -32.95572  |
| test/reward_per_eps            | -39.9      |
| test/steps                     | 124400     |
| train/episodes                 | 12440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 497600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.20375395 |
| stats_o/std                    | 0.09519508 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.14      |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.216     |
| test/Q                         | -33.66644  |
| test/Q_plus_P                  | -33.66644  |
| test/reward_per_eps            | -40        |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 312        |
| stats_o/mean                   | 0.20376498 |
| stats_o/std                    | 0.09533207 |
| test/episodes                  | 3130       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.11      |
| test/info_shaping_reward_mean  | -0.188     |
| test/info_shaping_reward_min   | -0.444     |
| test/Q                         | -33.96387  |
| test/Q_plus_P                  | -33.96387  |
| test/reward_per_eps            | -40        |
| test/steps                     | 125200     |
| train/episodes                 | 12520      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0144     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 500800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.20376238  |
| stats_o/std                    | 0.095293224 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.179      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -34.28883   |
| test/Q_plus_P                  | -34.28883   |
| test/reward_per_eps            | -40         |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.205      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 314        |
| stats_o/mean                   | 0.20377703 |
| stats_o/std                    | 0.09531441 |
| test/episodes                  | 3150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0025     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0469    |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -32.868023 |
| test/Q_plus_P                  | -32.868023 |
| test/reward_per_eps            | -39.9      |
| test/steps                     | 126000     |
| train/episodes                 | 12600      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0025     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.103     |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 504000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 315        |
| stats_o/mean                   | 0.20380183 |
| stats_o/std                    | 0.09542111 |
| test/episodes                  | 3160       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0944    |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -33.302853 |
| test/Q_plus_P                  | -33.302853 |
| test/reward_per_eps            | -40        |
| test/steps                     | 126400     |
| train/episodes                 | 12640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.1       |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 505600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.20382178  |
| stats_o/std                    | 0.095474266 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.105       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0207     |
| test/info_shaping_reward_mean  | -0.159      |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -33.221573  |
| test/Q_plus_P                  | -33.221573  |
| test/reward_per_eps            | -35.8       |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0989     |
| train/info_shaping_reward_mean | -0.161      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 317         |
| stats_o/mean                   | 0.20384619  |
| stats_o/std                    | 0.095510915 |
| test/episodes                  | 3180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.192      |
| test/info_shaping_reward_min   | -0.524      |
| test/Q                         | -34.387177  |
| test/Q_plus_P                  | -34.387177  |
| test/reward_per_eps            | -40         |
| test/steps                     | 127200      |
| train/episodes                 | 12720       |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0187      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.102      |
| train/info_shaping_reward_mean | -0.167      |
| train/info_shaping_reward_min  | -0.206      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 508800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 318        |
| stats_o/mean                   | 0.20384723 |
| stats_o/std                    | 0.09554601 |
| test/episodes                  | 3190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0559    |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -33.918457 |
| test/Q_plus_P                  | -33.918457 |
| test/reward_per_eps            | -40        |
| test/steps                     | 127600     |
| train/episodes                 | 12760      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.105     |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 510400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 319         |
| stats_o/mean                   | 0.20385626  |
| stats_o/std                    | 0.095612265 |
| test/episodes                  | 3200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0807     |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -34.01878   |
| test/Q_plus_P                  | -34.01878   |
| test/reward_per_eps            | -40         |
| test/steps                     | 128000      |
| train/episodes                 | 12800       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00125     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.103      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 512000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 320        |
| stats_o/mean                   | 0.20385805 |
| stats_o/std                    | 0.09564472 |
| test/episodes                  | 3210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.144     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -33.788296 |
| test/Q_plus_P                  | -33.788296 |
| test/reward_per_eps            | -40        |
| test/steps                     | 128400     |
| train/episodes                 | 12840      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0106     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0983    |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 513600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.20386094  |
| stats_o/std                    | 0.095664494 |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0717     |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.587      |
| test/Q                         | -33.86298   |
| test/Q_plus_P                  | -33.86298   |
| test/reward_per_eps            | -40         |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0187      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.099      |
| train/info_shaping_reward_mean | -0.164      |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 322         |
| stats_o/mean                   | 0.20387287  |
| stats_o/std                    | 0.095708884 |
| test/episodes                  | 3230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0025      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0499     |
| test/info_shaping_reward_mean  | -0.161      |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -33.367573  |
| test/Q_plus_P                  | -33.367573  |
| test/reward_per_eps            | -39.9       |
| test/steps                     | 129200      |
| train/episodes                 | 12920       |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0281      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0897     |
| train/info_shaping_reward_mean | -0.161      |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.9       |
| train/steps                    | 516800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 323        |
| stats_o/mean                   | 0.20385586 |
| stats_o/std                    | 0.0957994  |
| test/episodes                  | 3240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0475     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0315    |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -33.436066 |
| test/Q_plus_P                  | -33.436066 |
| test/reward_per_eps            | -38.1      |
| test/steps                     | 129600     |
| train/episodes                 | 12960      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00125    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.105     |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 518400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.20383917  |
| stats_o/std                    | 0.095836766 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0787     |
| test/info_shaping_reward_mean  | -0.168      |
| test/info_shaping_reward_min   | -0.205      |
| test/Q                         | -33.834793  |
| test/Q_plus_P                  | -33.834793  |
| test/reward_per_eps            | -40         |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0075      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.17       |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 325        |
| stats_o/mean                   | 0.2038537  |
| stats_o/std                    | 0.09594178 |
| test/episodes                  | 3260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.117     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -34.47078  |
| test/Q_plus_P                  | -34.47078  |
| test/reward_per_eps            | -40        |
| test/steps                     | 130400     |
| train/episodes                 | 13040      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0979    |
| train/info_shaping_reward_mean | -0.164     |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 521600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 326        |
| stats_o/mean                   | 0.20387413 |
| stats_o/std                    | 0.09604021 |
| test/episodes                  | 3270       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0834    |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -33.825912 |
| test/Q_plus_P                  | -33.825912 |
| test/reward_per_eps            | -40        |
| test/steps                     | 130800     |
| train/episodes                 | 13080      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0992    |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 523200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 327        |
| stats_o/mean                   | 0.2038694  |
| stats_o/std                    | 0.09610956 |
| test/episodes                  | 3280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0365    |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.348     |
| test/Q                         | -32.396473 |
| test/Q_plus_P                  | -32.396473 |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 131200     |
| train/episodes                 | 13120      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0175     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.115     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 524800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 328        |
| stats_o/mean                   | 0.2038966  |
| stats_o/std                    | 0.09627821 |
| test/episodes                  | 3290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0642    |
| test/info_shaping_reward_mean  | -0.158     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -32.794483 |
| test/Q_plus_P                  | -32.794483 |
| test/reward_per_eps            | -40        |
| test/steps                     | 131600     |
| train/episodes                 | 13160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.113     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.282     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 526400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 329        |
| stats_o/mean                   | 0.20390706 |
| stats_o/std                    | 0.09642407 |
| test/episodes                  | 3300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.065      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00971   |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -32.93532  |
| test/Q_plus_P                  | -32.93532  |
| test/reward_per_eps            | -37.4      |
| test/steps                     | 132000     |
| train/episodes                 | 13200      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.101     |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 528000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.20392093  |
| stats_o/std                    | 0.096635714 |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0705     |
| test/info_shaping_reward_mean  | -0.167      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -33.960987  |
| test/Q_plus_P                  | -33.960987  |
| test/reward_per_eps            | -40         |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0163      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0973     |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 331         |
| stats_o/mean                   | 0.20393322  |
| stats_o/std                    | 0.096663825 |
| test/episodes                  | 3320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.06       |
| test/info_shaping_reward_mean  | -0.171      |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -34.347733  |
| test/Q_plus_P                  | -34.347733  |
| test/reward_per_eps            | -40         |
| test/steps                     | 132800      |
| train/episodes                 | 13280       |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0288      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0675     |
| train/info_shaping_reward_mean | -0.162      |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.9       |
| train/steps                    | 531200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 332         |
| stats_o/mean                   | 0.20394455  |
| stats_o/std                    | 0.096745014 |
| test/episodes                  | 3330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0625      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0226     |
| test/info_shaping_reward_mean  | -0.163      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -33.827682  |
| test/Q_plus_P                  | -33.827682  |
| test/reward_per_eps            | -37.5       |
| test/steps                     | 133200      |
| train/episodes                 | 13320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.167      |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 532800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 333        |
| stats_o/mean                   | 0.20395824 |
| stats_o/std                    | 0.09691229 |
| test/episodes                  | 3340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0753    |
| test/info_shaping_reward_mean  | -0.161     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -33.42999  |
| test/Q_plus_P                  | -33.42999  |
| test/reward_per_eps            | -40        |
| test/steps                     | 133600     |
| train/episodes                 | 13360      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.03       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0705    |
| train/info_shaping_reward_mean | -0.159     |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 534400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 334        |
| stats_o/mean                   | 0.20394745 |
| stats_o/std                    | 0.09699706 |
| test/episodes                  | 3350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0604    |
| test/info_shaping_reward_mean  | -0.157     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -32.766727 |
| test/Q_plus_P                  | -32.766727 |
| test/reward_per_eps            | -40        |
| test/steps                     | 134000     |
| train/episodes                 | 13400      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0978    |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 536000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.20395733 |
| stats_o/std                    | 0.09710112 |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0735    |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -34.200783 |
| test/Q_plus_P                  | -34.200783 |
| test/reward_per_eps            | -40        |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0194     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.097     |
| train/info_shaping_reward_mean | -0.164     |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.2039569   |
| stats_o/std                    | 0.097177885 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -34.58621   |
| test/Q_plus_P                  | -34.58621   |
| test/reward_per_eps            | -40         |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0163      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0807     |
| train/info_shaping_reward_mean | -0.162      |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 337        |
| stats_o/mean                   | 0.20397192 |
| stats_o/std                    | 0.09723752 |
| test/episodes                  | 3380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.147     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.532     |
| test/Q                         | -34.499878 |
| test/Q_plus_P                  | -34.499878 |
| test/reward_per_eps            | -40        |
| test/steps                     | 135200     |
| train/episodes                 | 13520      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0931    |
| train/info_shaping_reward_mean | -0.162     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 540800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.20398854 |
| stats_o/std                    | 0.09738711 |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0553    |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -33.15103  |
| test/Q_plus_P                  | -33.15103  |
| test/reward_per_eps            | -40        |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0194     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.20398557  |
| stats_o/std                    | 0.097426444 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.053      |
| test/info_shaping_reward_mean  | -0.162      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -33.399067  |
| test/Q_plus_P                  | -33.399067  |
| test/reward_per_eps            | -40         |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0231      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.116      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.1       |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 340        |
| stats_o/mean                   | 0.20399739 |
| stats_o/std                    | 0.09756844 |
| test/episodes                  | 3410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.133      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0237    |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.202     |
| test/Q                         | -31.474669 |
| test/Q_plus_P                  | -31.474669 |
| test/reward_per_eps            | -34.7      |
| test/steps                     | 136400     |
| train/episodes                 | 13640      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0206     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0922    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.284     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 545600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 341        |
| stats_o/mean                   | 0.20398465 |
| stats_o/std                    | 0.09774134 |
| test/episodes                  | 3420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.128     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -34.05538  |
| test/Q_plus_P                  | -34.05538  |
| test/reward_per_eps            | -40        |
| test/steps                     | 136800     |
| train/episodes                 | 13680      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0169     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0868    |
| train/info_shaping_reward_mean | -0.162     |
| train/info_shaping_reward_min  | -0.203     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 547200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 342        |
| stats_o/mean                   | 0.20398104 |
| stats_o/std                    | 0.09778173 |
| test/episodes                  | 3430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0775    |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -34.726543 |
| test/Q_plus_P                  | -34.726543 |
| test/reward_per_eps            | -40        |
| test/steps                     | 137200     |
| train/episodes                 | 13720      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0462     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0955    |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.277     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.1      |
| train/steps                    | 548800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 343        |
| stats_o/mean                   | 0.20397608 |
| stats_o/std                    | 0.09784987 |
| test/episodes                  | 3440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.113      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.018     |
| test/info_shaping_reward_mean  | -0.155     |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -32.40904  |
| test/Q_plus_P                  | -32.40904  |
| test/reward_per_eps            | -35.5      |
| test/steps                     | 137600     |
| train/episodes                 | 13760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 550400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 344        |
| stats_o/mean                   | 0.20397691 |
| stats_o/std                    | 0.09790021 |
| test/episodes                  | 3450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.144     |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.661     |
| test/Q                         | -34.598526 |
| test/Q_plus_P                  | -34.598526 |
| test/reward_per_eps            | -40        |
| test/steps                     | 138000     |
| train/episodes                 | 13800      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.02       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0792    |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 552000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.20398147  |
| stats_o/std                    | 0.097949736 |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.055       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.02       |
| test/info_shaping_reward_mean  | -0.15       |
| test/info_shaping_reward_min   | -0.209      |
| test/Q                         | -32.267357  |
| test/Q_plus_P                  | -32.267357  |
| test/reward_per_eps            | -37.8       |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0294      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.115      |
| train/info_shaping_reward_mean | -0.163      |
| train/info_shaping_reward_min  | -0.203      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.8       |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 346        |
| stats_o/mean                   | 0.20398097 |
| stats_o/std                    | 0.09803374 |
| test/episodes                  | 3470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0475     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0189    |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -34.041042 |
| test/Q_plus_P                  | -34.041042 |
| test/reward_per_eps            | -38.1      |
| test/steps                     | 138800     |
| train/episodes                 | 13880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.107     |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 555200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 347         |
| stats_o/mean                   | 0.20397201  |
| stats_o/std                    | 0.098141015 |
| test/episodes                  | 3480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.14       |
| test/info_shaping_reward_mean  | -0.183      |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -34.837025  |
| test/Q_plus_P                  | -34.837025  |
| test/reward_per_eps            | -40         |
| test/steps                     | 139200      |
| train/episodes                 | 13920       |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0281      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0742     |
| train/info_shaping_reward_mean | -0.166      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.9       |
| train/steps                    | 556800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 348        |
| stats_o/mean                   | 0.20397358 |
| stats_o/std                    | 0.09821823 |
| test/episodes                  | 3490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0725     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.043     |
| test/info_shaping_reward_mean  | -0.15      |
| test/info_shaping_reward_min   | -0.214     |
| test/Q                         | -32.103016 |
| test/Q_plus_P                  | -32.103016 |
| test/reward_per_eps            | -37.1      |
| test/steps                     | 139600     |
| train/episodes                 | 13960      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0106     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.115     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.28      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 558400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 349        |
| stats_o/mean                   | 0.2039774  |
| stats_o/std                    | 0.09833262 |
| test/episodes                  | 3500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.165      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.027     |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -32.11618  |
| test/Q_plus_P                  | -32.11618  |
| test/reward_per_eps            | -33.4      |
| test/steps                     | 140000     |
| train/episodes                 | 14000      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.055      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0724    |
| train/info_shaping_reward_mean | -0.156     |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 560000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 350        |
| stats_o/mean                   | 0.20399266 |
| stats_o/std                    | 0.09848582 |
| test/episodes                  | 3510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0675     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.033     |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.345     |
| test/Q                         | -32.70843  |
| test/Q_plus_P                  | -32.70843  |
| test/reward_per_eps            | -37.3      |
| test/steps                     | 140400     |
| train/episodes                 | 14040      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0569     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0903    |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.7      |
| train/steps                    | 561600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 351        |
| stats_o/mean                   | 0.20400055 |
| stats_o/std                    | 0.09854263 |
| test/episodes                  | 3520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0509    |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.222     |
| test/Q                         | -33.728172 |
| test/Q_plus_P                  | -33.728172 |
| test/reward_per_eps            | -40        |
| test/steps                     | 140800     |
| train/episodes                 | 14080      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0325     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.089     |
| train/info_shaping_reward_mean | -0.167     |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 563200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 352        |
| stats_o/mean                   | 0.20399606 |
| stats_o/std                    | 0.09862189 |
| test/episodes                  | 3530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0521    |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -34.515343 |
| test/Q_plus_P                  | -34.515343 |
| test/reward_per_eps            | -40        |
| test/steps                     | 141200     |
| train/episodes                 | 14120      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0175     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0981    |
| train/info_shaping_reward_mean | -0.167     |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 564800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 353        |
| stats_o/mean                   | 0.20399857 |
| stats_o/std                    | 0.09869706 |
| test/episodes                  | 3540       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.104     |
| test/info_shaping_reward_mean  | -0.19      |
| test/info_shaping_reward_min   | -0.637     |
| test/Q                         | -34.93314  |
| test/Q_plus_P                  | -34.93314  |
| test/reward_per_eps            | -40        |
| test/steps                     | 141600     |
| train/episodes                 | 14160      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0381     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0807    |
| train/info_shaping_reward_mean | -0.159     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 566400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 354        |
| stats_o/mean                   | 0.20399646 |
| stats_o/std                    | 0.09870098 |
| test/episodes                  | 3550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0655    |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.23      |
| test/Q                         | -34.10113  |
| test/Q_plus_P                  | -34.10113  |
| test/reward_per_eps            | -40        |
| test/steps                     | 142000     |
| train/episodes                 | 14200      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0238     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.118     |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 568000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.20400646  |
| stats_o/std                    | 0.098811775 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.09        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0191     |
| test/info_shaping_reward_mean  | -0.148      |
| test/info_shaping_reward_min   | -0.21       |
| test/Q                         | -31.500593  |
| test/Q_plus_P                  | -31.500593  |
| test/reward_per_eps            | -36.4       |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0387      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0877     |
| train/info_shaping_reward_mean | -0.162      |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.5       |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 356         |
| stats_o/mean                   | 0.20401041  |
| stats_o/std                    | 0.098871425 |
| test/episodes                  | 3570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.133      |
| test/info_shaping_reward_mean  | -0.196      |
| test/info_shaping_reward_min   | -0.65       |
| test/Q                         | -34.85678   |
| test/Q_plus_P                  | -34.85678   |
| test/reward_per_eps            | -40         |
| test/steps                     | 142800      |
| train/episodes                 | 14280       |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0287      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0781     |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.277      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.9       |
| train/steps                    | 571200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 357        |
| stats_o/mean                   | 0.20404051 |
| stats_o/std                    | 0.09902188 |
| test/episodes                  | 3580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0675     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0313    |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -31.396982 |
| test/Q_plus_P                  | -31.396982 |
| test/reward_per_eps            | -37.3      |
| test/steps                     | 143200     |
| train/episodes                 | 14320      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0125     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.103     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.267     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 572800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 358        |
| stats_o/mean                   | 0.2040394  |
| stats_o/std                    | 0.09912943 |
| test/episodes                  | 3590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.065      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0167    |
| test/info_shaping_reward_mean  | -0.152     |
| test/info_shaping_reward_min   | -0.217     |
| test/Q                         | -31.679193 |
| test/Q_plus_P                  | -31.679193 |
| test/reward_per_eps            | -37.4      |
| test/steps                     | 143600     |
| train/episodes                 | 14360      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0619     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0771    |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.293     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 574400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.20404054  |
| stats_o/std                    | 0.099152625 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.05        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0123     |
| test/info_shaping_reward_mean  | -0.192      |
| test/info_shaping_reward_min   | -1.21       |
| test/Q                         | -33.781918  |
| test/Q_plus_P                  | -33.781918  |
| test/reward_per_eps            | -38         |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0406      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.115      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.4       |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 360        |
| stats_o/mean                   | 0.20404096 |
| stats_o/std                    | 0.09924466 |
| test/episodes                  | 3610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.07       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0418    |
| test/info_shaping_reward_mean  | -0.145     |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -31.290033 |
| test/Q_plus_P                  | -31.290033 |
| test/reward_per_eps            | -37.2      |
| test/steps                     | 144400     |
| train/episodes                 | 14440      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.116     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 577600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 361         |
| stats_o/mean                   | 0.20404314  |
| stats_o/std                    | 0.099264115 |
| test/episodes                  | 3620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.193      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -35.42121   |
| test/Q_plus_P                  | -35.42121   |
| test/reward_per_eps            | -40         |
| test/steps                     | 144800      |
| train/episodes                 | 14480       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0163      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.184      |
| train/info_shaping_reward_min  | -0.3        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 579200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 362         |
| stats_o/mean                   | 0.20402071  |
| stats_o/std                    | 0.099357136 |
| test/episodes                  | 3630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0909     |
| test/info_shaping_reward_mean  | -0.179      |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -34.162514  |
| test/Q_plus_P                  | -34.162514  |
| test/reward_per_eps            | -40         |
| test/steps                     | 145200      |
| train/episodes                 | 14520       |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0325      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.108      |
| train/info_shaping_reward_mean | -0.188      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.7       |
| train/steps                    | 580800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.2040233   |
| stats_o/std                    | 0.099477224 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0698     |
| test/info_shaping_reward_mean  | -0.178      |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -34.018784  |
| test/Q_plus_P                  | -34.018784  |
| test/reward_per_eps            | -40         |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.03        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.105      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.276      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.8       |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 364        |
| stats_o/mean                   | 0.20403871 |
| stats_o/std                    | 0.0995543  |
| test/episodes                  | 3650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.06       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0203    |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -33.916767 |
| test/Q_plus_P                  | -33.916767 |
| test/reward_per_eps            | -37.6      |
| test/steps                     | 146000     |
| train/episodes                 | 14600      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0169     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.111     |
| train/info_shaping_reward_mean | -0.192     |
| train/info_shaping_reward_min  | -0.309     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 584000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.20404531  |
| stats_o/std                    | 0.099668175 |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.122       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0414     |
| test/info_shaping_reward_mean  | -0.145      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -30.884897  |
| test/Q_plus_P                  | -30.884897  |
| test/reward_per_eps            | -35.1       |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.015       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.104      |
| train/info_shaping_reward_mean | -0.17       |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 366        |
| stats_o/mean                   | 0.20404057 |
| stats_o/std                    | 0.09979741 |
| test/episodes                  | 3670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0647    |
| test/info_shaping_reward_mean  | -0.158     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -33.45549  |
| test/Q_plus_P                  | -33.45549  |
| test/reward_per_eps            | -40        |
| test/steps                     | 146800     |
| train/episodes                 | 14680      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0144     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0766    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 587200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 367        |
| stats_o/mean                   | 0.20404391 |
| stats_o/std                    | 0.09994575 |
| test/episodes                  | 3680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0539    |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -33.484673 |
| test/Q_plus_P                  | -33.484673 |
| test/reward_per_eps            | -40        |
| test/steps                     | 147200     |
| train/episodes                 | 14720      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0387     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0594    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.301     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 588800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 368         |
| stats_o/mean                   | 0.2040628   |
| stats_o/std                    | 0.099988416 |
| test/episodes                  | 3690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.065       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0379     |
| test/info_shaping_reward_mean  | -0.151      |
| test/info_shaping_reward_min   | -0.209      |
| test/Q                         | -31.60483   |
| test/Q_plus_P                  | -31.60483   |
| test/reward_per_eps            | -37.4       |
| test/steps                     | 147600      |
| train/episodes                 | 14760       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.01        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0941     |
| train/info_shaping_reward_mean | -0.163      |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 590400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 369         |
| stats_o/mean                   | 0.20408203  |
| stats_o/std                    | 0.100147635 |
| test/episodes                  | 3700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.145       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0119     |
| test/info_shaping_reward_mean  | -0.146      |
| test/info_shaping_reward_min   | -0.355      |
| test/Q                         | -30.27769   |
| test/Q_plus_P                  | -30.27769   |
| test/reward_per_eps            | -34.2       |
| test/steps                     | 148000      |
| train/episodes                 | 14800       |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.055       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.077      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.377      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.8       |
| train/steps                    | 592000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 370        |
| stats_o/mean                   | 0.20407806 |
| stats_o/std                    | 0.1002434  |
| test/episodes                  | 3710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.055      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0246    |
| test/info_shaping_reward_mean  | -0.155     |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -31.23275  |
| test/Q_plus_P                  | -31.23275  |
| test/reward_per_eps            | -37.8      |
| test/steps                     | 148400     |
| train/episodes                 | 14840      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00125    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.379     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 593600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 371         |
| stats_o/mean                   | 0.20407647  |
| stats_o/std                    | 0.100348786 |
| test/episodes                  | 3720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.206      |
| test/info_shaping_reward_min   | -0.558      |
| test/Q                         | -34.593067  |
| test/Q_plus_P                  | -34.593067  |
| test/reward_per_eps            | -40         |
| test/steps                     | 148800      |
| train/episodes                 | 14880       |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0369      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0878     |
| train/info_shaping_reward_mean | -0.169      |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.5       |
| train/steps                    | 595200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 372        |
| stats_o/mean                   | 0.20411593 |
| stats_o/std                    | 0.1005774  |
| test/episodes                  | 3730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.182      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00907   |
| test/info_shaping_reward_mean  | -0.12      |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -27.509947 |
| test/Q_plus_P                  | -27.509947 |
| test/reward_per_eps            | -32.7      |
| test/steps                     | 149200     |
| train/episodes                 | 14920      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0138     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0938    |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.297     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 596800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 373        |
| stats_o/mean                   | 0.20410052 |
| stats_o/std                    | 0.10057469 |
| test/episodes                  | 3740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.138      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0395    |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.335     |
| test/Q                         | -31.706528 |
| test/Q_plus_P                  | -31.706528 |
| test/reward_per_eps            | -34.5      |
| test/steps                     | 149600     |
| train/episodes                 | 14960      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0869     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0881    |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.5      |
| train/steps                    | 598400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.2041053  |
| stats_o/std                    | 0.1006507  |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.055      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0228    |
| test/info_shaping_reward_mean  | -0.146     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -30.522367 |
| test/Q_plus_P                  | -30.522367 |
| test/reward_per_eps            | -37.8      |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0138     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.20410134  |
| stats_o/std                    | 0.100689344 |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.089      |
| test/info_shaping_reward_mean  | -0.165      |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -34.064354  |
| test/Q_plus_P                  | -34.064354  |
| test/reward_per_eps            | -40         |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.025       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0917     |
| train/info_shaping_reward_mean | -0.165      |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39         |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 376         |
| stats_o/mean                   | 0.20409422  |
| stats_o/std                    | 0.100820474 |
| test/episodes                  | 3770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0325      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0315     |
| test/info_shaping_reward_mean  | -0.158      |
| test/info_shaping_reward_min   | -0.218      |
| test/Q                         | -32.75035   |
| test/Q_plus_P                  | -32.75035   |
| test/reward_per_eps            | -38.7       |
| test/steps                     | 150800      |
| train/episodes                 | 15080       |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0319      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.078      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.7       |
| train/steps                    | 603200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.20406553  |
| stats_o/std                    | 0.100987695 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0575      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0189     |
| test/info_shaping_reward_mean  | -0.184      |
| test/info_shaping_reward_min   | -0.572      |
| test/Q                         | -33.096382  |
| test/Q_plus_P                  | -33.096382  |
| test/reward_per_eps            | -37.7       |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0375      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.062      |
| train/info_shaping_reward_mean | -0.168      |
| train/info_shaping_reward_min  | -0.299      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.5       |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 378        |
| stats_o/mean                   | 0.20405604 |
| stats_o/std                    | 0.10106073 |
| test/episodes                  | 3790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0075     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0262    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -33.36673  |
| test/Q_plus_P                  | -33.36673  |
| test/reward_per_eps            | -39.7      |
| test/steps                     | 151600     |
| train/episodes                 | 15160      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0594     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0743    |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.36      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.6      |
| train/steps                    | 606400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 379        |
| stats_o/mean                   | 0.20404643 |
| stats_o/std                    | 0.10118889 |
| test/episodes                  | 3800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0323    |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.328     |
| test/Q                         | -29.634798 |
| test/Q_plus_P                  | -29.634798 |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 152000     |
| train/episodes                 | 15200      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.104      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0609    |
| train/info_shaping_reward_mean | -0.162     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.8      |
| train/steps                    | 608000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 380        |
| stats_o/mean                   | 0.20403343 |
| stats_o/std                    | 0.1012831  |
| test/episodes                  | 3810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.133      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0348    |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -1.08      |
| test/Q                         | -30.961567 |
| test/Q_plus_P                  | -30.961567 |
| test/reward_per_eps            | -34.7      |
| test/steps                     | 152400     |
| train/episodes                 | 15240      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0238     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0827    |
| train/info_shaping_reward_mean | -0.204     |
| train/info_shaping_reward_min  | -0.465     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 609600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 381        |
| stats_o/mean                   | 0.2040331  |
| stats_o/std                    | 0.10137393 |
| test/episodes                  | 3820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.1        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0269    |
| test/info_shaping_reward_mean  | -0.152     |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -30.448809 |
| test/Q_plus_P                  | -30.448809 |
| test/reward_per_eps            | -36        |
| test/steps                     | 152800     |
| train/episodes                 | 15280      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0381     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0697    |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 611200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.20402165 |
| stats_o/std                    | 0.10144942 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.225      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.013     |
| test/info_shaping_reward_mean  | -0.13      |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -27.418894 |
| test/Q_plus_P                  | -27.418894 |
| test/reward_per_eps            | -31        |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0381     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.111     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.414     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.20401396  |
| stats_o/std                    | 0.101559885 |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.115       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00813    |
| test/info_shaping_reward_mean  | -0.131      |
| test/info_shaping_reward_min   | -0.204      |
| test/Q                         | -28.853125  |
| test/Q_plus_P                  | -28.853125  |
| test/reward_per_eps            | -35.4       |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0488      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0857     |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38         |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 384         |
| stats_o/mean                   | 0.20401394  |
| stats_o/std                    | 0.101695254 |
| test/episodes                  | 3850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0573     |
| test/info_shaping_reward_mean  | -0.154      |
| test/info_shaping_reward_min   | -0.215      |
| test/Q                         | -31.309364  |
| test/Q_plus_P                  | -31.309364  |
| test/reward_per_eps            | -40         |
| test/steps                     | 154000      |
| train/episodes                 | 15400       |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.045       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0601     |
| train/info_shaping_reward_mean | -0.198      |
| train/info_shaping_reward_min  | -0.467      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.2       |
| train/steps                    | 616000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 385        |
| stats_o/mean                   | 0.20400824 |
| stats_o/std                    | 0.10172153 |
| test/episodes                  | 3860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00613   |
| test/info_shaping_reward_mean  | -0.137     |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -28.502192 |
| test/Q_plus_P                  | -28.502192 |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 154400     |
| train/episodes                 | 15440      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0919     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0495    |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.3      |
| train/steps                    | 617600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 386        |
| stats_o/mean                   | 0.20401289 |
| stats_o/std                    | 0.10178321 |
| test/episodes                  | 3870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.107      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0292    |
| test/info_shaping_reward_mean  | -0.15      |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -29.886154 |
| test/Q_plus_P                  | -29.886154 |
| test/reward_per_eps            | -35.7      |
| test/steps                     | 154800     |
| train/episodes                 | 15480      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.055      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0786    |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 619200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 387        |
| stats_o/mean                   | 0.203993   |
| stats_o/std                    | 0.10188348 |
| test/episodes                  | 3880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0725     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00663   |
| test/info_shaping_reward_mean  | -0.152     |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -29.416065 |
| test/Q_plus_P                  | -29.416065 |
| test/reward_per_eps            | -37.1      |
| test/steps                     | 155200     |
| train/episodes                 | 15520      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0625     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0562    |
| train/info_shaping_reward_mean | -0.156     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 620800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 388         |
| stats_o/mean                   | 0.20395863  |
| stats_o/std                    | 0.101992704 |
| test/episodes                  | 3890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0582     |
| test/info_shaping_reward_mean  | -0.159      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -31.406239  |
| test/Q_plus_P                  | -31.406239  |
| test/reward_per_eps            | -40         |
| test/steps                     | 155600      |
| train/episodes                 | 15560       |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0456      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0562     |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.2       |
| train/steps                    | 622400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 389        |
| stats_o/mean                   | 0.20394568 |
| stats_o/std                    | 0.10205399 |
| test/episodes                  | 3900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0775     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00735   |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.215     |
| test/Q                         | -27.74797  |
| test/Q_plus_P                  | -27.74797  |
| test/reward_per_eps            | -36.9      |
| test/steps                     | 156000     |
| train/episodes                 | 15600      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0375     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0838    |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 624000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 390        |
| stats_o/mean                   | 0.20397322 |
| stats_o/std                    | 0.1022052  |
| test/episodes                  | 3910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0454    |
| test/info_shaping_reward_mean  | -0.151     |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -28.591038 |
| test/Q_plus_P                  | -28.591038 |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 156400     |
| train/episodes                 | 15640      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0212     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0936    |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 625600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 391        |
| stats_o/mean                   | 0.20396055 |
| stats_o/std                    | 0.10230399 |
| test/episodes                  | 3920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0173    |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -31.007236 |
| test/Q_plus_P                  | -31.007236 |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 156800     |
| train/episodes                 | 15680      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0631     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0897    |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.371     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 627200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 392        |
| stats_o/mean                   | 0.20395292 |
| stats_o/std                    | 0.10235879 |
| test/episodes                  | 3930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.005      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0395    |
| test/info_shaping_reward_mean  | -0.15      |
| test/info_shaping_reward_min   | -0.236     |
| test/Q                         | -29.344988 |
| test/Q_plus_P                  | -29.344988 |
| test/reward_per_eps            | -39.8      |
| test/steps                     | 157200     |
| train/episodes                 | 15720      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0312     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0989    |
| train/info_shaping_reward_mean | -0.164     |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 628800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 393         |
| stats_o/mean                   | 0.20391825  |
| stats_o/std                    | 0.102465875 |
| test/episodes                  | 3940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0675      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0416     |
| test/info_shaping_reward_mean  | -0.16       |
| test/info_shaping_reward_min   | -0.215      |
| test/Q                         | -31.008677  |
| test/Q_plus_P                  | -31.008677  |
| test/reward_per_eps            | -37.3       |
| test/steps                     | 157600      |
| train/episodes                 | 15760       |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0425      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0891     |
| train/info_shaping_reward_mean | -0.17       |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.3       |
| train/steps                    | 630400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 394        |
| stats_o/mean                   | 0.20389943 |
| stats_o/std                    | 0.1025254  |
| test/episodes                  | 3950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0648    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.362     |
| test/Q                         | -31.41393  |
| test/Q_plus_P                  | -31.41393  |
| test/reward_per_eps            | -40        |
| test/steps                     | 158000     |
| train/episodes                 | 15800      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0344     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0762    |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 632000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 395        |
| stats_o/mean                   | 0.2038836  |
| stats_o/std                    | 0.10257361 |
| test/episodes                  | 3960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.133      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0378    |
| test/info_shaping_reward_mean  | -0.125     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -25.435192 |
| test/Q_plus_P                  | -25.435192 |
| test/reward_per_eps            | -34.7      |
| test/steps                     | 158400     |
| train/episodes                 | 15840      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.102      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0578    |
| train/info_shaping_reward_mean | -0.157     |
| train/info_shaping_reward_min  | -0.277     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.9      |
| train/steps                    | 633600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 396        |
| stats_o/mean                   | 0.20386164 |
| stats_o/std                    | 0.10263869 |
| test/episodes                  | 3970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.107     |
| test/info_shaping_reward_mean  | -0.186     |
| test/info_shaping_reward_min   | -0.806     |
| test/Q                         | -32.487118 |
| test/Q_plus_P                  | -32.487118 |
| test/reward_per_eps            | -40        |
| test/steps                     | 158800     |
| train/episodes                 | 15880      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0631     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0976    |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 635200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.20386833  |
| stats_o/std                    | 0.102712154 |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.075       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0316     |
| test/info_shaping_reward_mean  | -0.139      |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -24.162786  |
| test/Q_plus_P                  | -24.162786  |
| test/reward_per_eps            | -37         |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.065       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0779     |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.4       |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 398        |
| stats_o/mean                   | 0.2038537  |
| stats_o/std                    | 0.10271546 |
| test/episodes                  | 3990       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0616    |
| test/info_shaping_reward_mean  | -0.187     |
| test/info_shaping_reward_min   | -0.605     |
| test/Q                         | -31.124052 |
| test/Q_plus_P                  | -31.124052 |
| test/reward_per_eps            | -40        |
| test/steps                     | 159600     |
| train/episodes                 | 15960      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0563     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.104     |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 638400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 399        |
| stats_o/mean                   | 0.20384355 |
| stats_o/std                    | 0.10277871 |
| test/episodes                  | 4000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.168      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.011     |
| test/info_shaping_reward_mean  | -0.157     |
| test/info_shaping_reward_min   | -0.528     |
| test/Q                         | -28.052412 |
| test/Q_plus_P                  | -28.052412 |
| test/reward_per_eps            | -33.3      |
| test/steps                     | 160000     |
| train/episodes                 | 16000      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.128      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.062     |
| train/info_shaping_reward_mean | -0.164     |
| train/info_shaping_reward_min  | -0.297     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.9      |
| train/steps                    | 640000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 400        |
| stats_o/mean                   | 0.20382288 |
| stats_o/std                    | 0.10285072 |
| test/episodes                  | 4010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.198      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0223    |
| test/info_shaping_reward_mean  | -0.152     |
| test/info_shaping_reward_min   | -0.509     |
| test/Q                         | -26.935806 |
| test/Q_plus_P                  | -26.935806 |
| test/reward_per_eps            | -32.1      |
| test/steps                     | 160400     |
| train/episodes                 | 16040      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0713     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0741    |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.278     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.1      |
| train/steps                    | 641600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 401        |
| stats_o/mean                   | 0.20380571 |
| stats_o/std                    | 0.10295246 |
| test/episodes                  | 4020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.142      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0224    |
| test/info_shaping_reward_mean  | -0.143     |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -26.319195 |
| test/Q_plus_P                  | -26.319195 |
| test/reward_per_eps            | -34.3      |
| test/steps                     | 160800     |
| train/episodes                 | 16080      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0869     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0631    |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.5      |
| train/steps                    | 643200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.20379883 |
| stats_o/std                    | 0.10301215 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.005      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0337    |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.217     |
| test/Q                         | -30.83708  |
| test/Q_plus_P                  | -30.83708  |
| test/reward_per_eps            | -39.8      |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.132      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0459    |
| train/info_shaping_reward_mean | -0.152     |
| train/info_shaping_reward_min  | -0.285     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.7      |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 403        |
| stats_o/mean                   | 0.20378202 |
| stats_o/std                    | 0.10304947 |
| test/episodes                  | 4040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.07       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0085    |
| test/info_shaping_reward_mean  | -0.152     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -30.206354 |
| test/Q_plus_P                  | -30.206354 |
| test/reward_per_eps            | -37.2      |
| test/steps                     | 161600     |
| train/episodes                 | 16160      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.109      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0611    |
| train/info_shaping_reward_mean | -0.152     |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.6      |
| train/steps                    | 646400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.20376319  |
| stats_o/std                    | 0.103095256 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.125       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0354     |
| test/info_shaping_reward_mean  | -0.179      |
| test/info_shaping_reward_min   | -0.74       |
| test/Q                         | -27.278545  |
| test/Q_plus_P                  | -27.278545  |
| test/reward_per_eps            | -35         |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.03        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.094      |
| train/info_shaping_reward_mean | -0.165      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.8       |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.20374605  |
| stats_o/std                    | 0.103100464 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.065       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0341     |
| test/info_shaping_reward_mean  | -0.155      |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -29.049341  |
| test/Q_plus_P                  | -29.049341  |
| test/reward_per_eps            | -37.4       |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0419      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.109      |
| train/info_shaping_reward_mean | -0.167      |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.3       |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 406         |
| stats_o/mean                   | 0.20372583  |
| stats_o/std                    | 0.103193715 |
| test/episodes                  | 4070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.355       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00285    |
| test/info_shaping_reward_mean  | -0.131      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -19.696936  |
| test/Q_plus_P                  | -19.696936  |
| test/reward_per_eps            | -25.8       |
| test/steps                     | 162800      |
| train/episodes                 | 16280       |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0531      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0633     |
| train/info_shaping_reward_mean | -0.165      |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.9       |
| train/steps                    | 651200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 407        |
| stats_o/mean                   | 0.20371588 |
| stats_o/std                    | 0.10324485 |
| test/episodes                  | 4080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.138      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0213    |
| test/info_shaping_reward_mean  | -0.204     |
| test/info_shaping_reward_min   | -0.821     |
| test/Q                         | -25.69262  |
| test/Q_plus_P                  | -25.69262  |
| test/reward_per_eps            | -34.5      |
| test/steps                     | 163200     |
| train/episodes                 | 16320      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0644     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0651    |
| train/info_shaping_reward_mean | -0.162     |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.4      |
| train/steps                    | 652800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 408        |
| stats_o/mean                   | 0.20369719 |
| stats_o/std                    | 0.10330209 |
| test/episodes                  | 4090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.27       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0083    |
| test/info_shaping_reward_mean  | -0.131     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -24.8372   |
| test/Q_plus_P                  | -24.8372   |
| test/reward_per_eps            | -29.2      |
| test/steps                     | 163600     |
| train/episodes                 | 16360      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0863     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0839    |
| train/info_shaping_reward_mean | -0.154     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.5      |
| train/steps                    | 654400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 409        |
| stats_o/mean                   | 0.20369163 |
| stats_o/std                    | 0.10332165 |
| test/episodes                  | 4100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.115      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0238    |
| test/info_shaping_reward_mean  | -0.142     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -25.709745 |
| test/Q_plus_P                  | -25.709745 |
| test/reward_per_eps            | -35.4      |
| test/steps                     | 164000     |
| train/episodes                 | 16400      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.181      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0486    |
| train/info_shaping_reward_mean | -0.143     |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.8      |
| train/steps                    | 656000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 410        |
| stats_o/mean                   | 0.20366183 |
| stats_o/std                    | 0.1034444  |
| test/episodes                  | 4110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0575     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.038     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.541     |
| test/Q                         | -27.471294 |
| test/Q_plus_P                  | -27.471294 |
| test/reward_per_eps            | -37.7      |
| test/steps                     | 164400     |
| train/episodes                 | 16440      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0781     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0667    |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.381     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.9      |
| train/steps                    | 657600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 411        |
| stats_o/mean                   | 0.2036431  |
| stats_o/std                    | 0.10345331 |
| test/episodes                  | 4120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.065      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0431    |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -29.04125  |
| test/Q_plus_P                  | -29.04125  |
| test/reward_per_eps            | -37.4      |
| test/steps                     | 164800     |
| train/episodes                 | 16480      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0475     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0538    |
| train/info_shaping_reward_mean | -0.162     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.1      |
| train/steps                    | 659200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 412        |
| stats_o/mean                   | 0.20362781 |
| stats_o/std                    | 0.10348249 |
| test/episodes                  | 4130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0475     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.027     |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.553     |
| test/Q                         | -26.612171 |
| test/Q_plus_P                  | -26.612171 |
| test/reward_per_eps            | -38.1      |
| test/steps                     | 165200     |
| train/episodes                 | 16520      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0506     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0831    |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38        |
| train/steps                    | 660800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.20359455  |
| stats_o/std                    | 0.103560336 |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0509     |
| test/info_shaping_reward_mean  | -0.151      |
| test/info_shaping_reward_min   | -0.204      |
| test/Q                         | -27.118406  |
| test/Q_plus_P                  | -27.118406  |
| test/reward_per_eps            | -40         |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.119       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0731     |
| train/info_shaping_reward_mean | -0.164      |
| train/info_shaping_reward_min  | -0.295      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.2       |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 414        |
| stats_o/mean                   | 0.20357117 |
| stats_o/std                    | 0.10363101 |
| test/episodes                  | 4150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.185      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.012     |
| test/info_shaping_reward_mean  | -0.144     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -25.98162  |
| test/Q_plus_P                  | -25.98162  |
| test/reward_per_eps            | -32.6      |
| test/steps                     | 166000     |
| train/episodes                 | 16600      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.148      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0471    |
| train/info_shaping_reward_mean | -0.156     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.1      |
| train/steps                    | 664000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 415        |
| stats_o/mean                   | 0.20355523 |
| stats_o/std                    | 0.10370773 |
| test/episodes                  | 4160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.335      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00737   |
| test/info_shaping_reward_mean  | -0.109     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -19.213285 |
| test/Q_plus_P                  | -19.213285 |
| test/reward_per_eps            | -26.6      |
| test/steps                     | 166400     |
| train/episodes                 | 16640      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0625     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0507    |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 665600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 416        |
| stats_o/mean                   | 0.20353855 |
| stats_o/std                    | 0.10381252 |
| test/episodes                  | 4170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.205      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00508   |
| test/info_shaping_reward_mean  | -0.136     |
| test/info_shaping_reward_min   | -0.211     |
| test/Q                         | -23.543818 |
| test/Q_plus_P                  | -23.543818 |
| test/reward_per_eps            | -31.8      |
| test/steps                     | 166800     |
| train/episodes                 | 16680      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0462     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0587    |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.1      |
| train/steps                    | 667200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 417         |
| stats_o/mean                   | 0.20352705  |
| stats_o/std                    | 0.103942655 |
| test/episodes                  | 4180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.19        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0277     |
| test/info_shaping_reward_mean  | -0.144      |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -23.738256  |
| test/Q_plus_P                  | -23.738256  |
| test/reward_per_eps            | -32.4       |
| test/steps                     | 167200      |
| train/episodes                 | 16720       |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.07        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.045      |
| train/info_shaping_reward_mean | -0.151      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.2       |
| train/steps                    | 668800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.20351875  |
| stats_o/std                    | 0.104061596 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.1         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0217     |
| test/info_shaping_reward_mean  | -0.161      |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -26.476429  |
| test/Q_plus_P                  | -26.476429  |
| test/reward_per_eps            | -36         |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.193       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.027      |
| train/info_shaping_reward_mean | -0.146      |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.3       |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 419         |
| stats_o/mean                   | 0.20351592  |
| stats_o/std                    | 0.104187876 |
| test/episodes                  | 4200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.27        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0174     |
| test/info_shaping_reward_mean  | -0.118      |
| test/info_shaping_reward_min   | -0.213      |
| test/Q                         | -19.84981   |
| test/Q_plus_P                  | -19.84981   |
| test/reward_per_eps            | -29.2       |
| test/steps                     | 168000      |
| train/episodes                 | 16800       |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.07        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0463     |
| train/info_shaping_reward_mean | -0.166      |
| train/info_shaping_reward_min  | -0.288      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.2       |
| train/steps                    | 672000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 420         |
| stats_o/mean                   | 0.20348306  |
| stats_o/std                    | 0.104304306 |
| test/episodes                  | 4210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.168       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0154     |
| test/info_shaping_reward_mean  | -0.136      |
| test/info_shaping_reward_min   | -0.212      |
| test/Q                         | -24.261574  |
| test/Q_plus_P                  | -24.261574  |
| test/reward_per_eps            | -33.3       |
| test/steps                     | 168400      |
| train/episodes                 | 16840       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.1         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0424     |
| train/info_shaping_reward_mean | -0.153      |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36         |
| train/steps                    | 673600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 421        |
| stats_o/mean                   | 0.20347622 |
| stats_o/std                    | 0.10443313 |
| test/episodes                  | 4220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.06       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.025     |
| test/info_shaping_reward_mean  | -0.135     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -24.357954 |
| test/Q_plus_P                  | -24.357954 |
| test/reward_per_eps            | -37.6      |
| test/steps                     | 168800     |
| train/episodes                 | 16880      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0675     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0613    |
| train/info_shaping_reward_mean | -0.159     |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.3      |
| train/steps                    | 675200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.20345965  |
| stats_o/std                    | 0.104551114 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.16        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0315     |
| test/info_shaping_reward_mean  | -0.14       |
| test/info_shaping_reward_min   | -0.193      |
| test/Q                         | -22.32398   |
| test/Q_plus_P                  | -22.32398   |
| test/reward_per_eps            | -33.6       |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.0769      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0506     |
| train/info_shaping_reward_mean | -0.169      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.9       |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.20346145  |
| stats_o/std                    | 0.104647115 |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.165       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00758    |
| test/info_shaping_reward_mean  | -0.133      |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -23.400772  |
| test/Q_plus_P                  | -23.400772  |
| test/reward_per_eps            | -33.4       |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.163       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0262     |
| train/info_shaping_reward_mean | -0.138      |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.5       |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 424         |
| stats_o/mean                   | 0.20345277  |
| stats_o/std                    | 0.104702145 |
| test/episodes                  | 4250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.18        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00676    |
| test/info_shaping_reward_mean  | -0.148      |
| test/info_shaping_reward_min   | -0.541      |
| test/Q                         | -20.006445  |
| test/Q_plus_P                  | -20.006445  |
| test/reward_per_eps            | -32.8       |
| test/steps                     | 170000      |
| train/episodes                 | 17000       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.0831      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0497     |
| train/info_shaping_reward_mean | -0.163      |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.7       |
| train/steps                    | 680000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 425        |
| stats_o/mean                   | 0.20343755 |
| stats_o/std                    | 0.10477999 |
| test/episodes                  | 4260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.285      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0173    |
| test/info_shaping_reward_mean  | -0.114     |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -16.912565 |
| test/Q_plus_P                  | -16.912565 |
| test/reward_per_eps            | -28.6      |
| test/steps                     | 170400     |
| train/episodes                 | 17040      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.12       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0506    |
| train/info_shaping_reward_mean | -0.148     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.2      |
| train/steps                    | 681600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.20340198  |
| stats_o/std                    | 0.104898185 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.223       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0218     |
| test/info_shaping_reward_mean  | -0.138      |
| test/info_shaping_reward_min   | -0.217      |
| test/Q                         | -18.743301  |
| test/Q_plus_P                  | -18.743301  |
| test/reward_per_eps            | -31.1       |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.164       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0438     |
| train/info_shaping_reward_mean | -0.146      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.5       |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 427        |
| stats_o/mean                   | 0.20337886 |
| stats_o/std                    | 0.10489137 |
| test/episodes                  | 4280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.06       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0337    |
| test/info_shaping_reward_mean  | -0.143     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -19.285332 |
| test/Q_plus_P                  | -19.285332 |
| test/reward_per_eps            | -37.6      |
| test/steps                     | 171200     |
| train/episodes                 | 17120      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0906     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0581    |
| train/info_shaping_reward_mean | -0.152     |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.4      |
| train/steps                    | 684800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 428        |
| stats_o/mean                   | 0.20336935 |
| stats_o/std                    | 0.10495349 |
| test/episodes                  | 4290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.142      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0149    |
| test/info_shaping_reward_mean  | -0.129     |
| test/info_shaping_reward_min   | -0.242     |
| test/Q                         | -17.217262 |
| test/Q_plus_P                  | -17.217262 |
| test/reward_per_eps            | -34.3      |
| test/steps                     | 171600     |
| train/episodes                 | 17160      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.152      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0454    |
| train/info_shaping_reward_mean | -0.145     |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.9      |
| train/steps                    | 686400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 429        |
| stats_o/mean                   | 0.20334113 |
| stats_o/std                    | 0.10498981 |
| test/episodes                  | 4300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0875     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0307    |
| test/info_shaping_reward_mean  | -0.138     |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -23.691422 |
| test/Q_plus_P                  | -23.691422 |
| test/reward_per_eps            | -36.5      |
| test/steps                     | 172000     |
| train/episodes                 | 17200      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.131      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.045     |
| train/info_shaping_reward_mean | -0.143     |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.8      |
| train/steps                    | 688000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 430        |
| stats_o/mean                   | 0.20330375 |
| stats_o/std                    | 0.1050622  |
| test/episodes                  | 4310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.287      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00692   |
| test/info_shaping_reward_mean  | -0.126     |
| test/info_shaping_reward_min   | -0.202     |
| test/Q                         | -18.968412 |
| test/Q_plus_P                  | -18.968412 |
| test/reward_per_eps            | -28.5      |
| test/steps                     | 172400     |
| train/episodes                 | 17240      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.129      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0514    |
| train/info_shaping_reward_mean | -0.149     |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.8      |
| train/steps                    | 689600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 431        |
| stats_o/mean                   | 0.20330788 |
| stats_o/std                    | 0.10513765 |
| test/episodes                  | 4320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.258      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0213    |
| test/info_shaping_reward_mean  | -0.116     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -16.374317 |
| test/Q_plus_P                  | -16.374317 |
| test/reward_per_eps            | -29.7      |
| test/steps                     | 172800     |
| train/episodes                 | 17280      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.133      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0326    |
| train/info_shaping_reward_mean | -0.154     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.7      |
| train/steps                    | 691200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 432        |
| stats_o/mean                   | 0.20330009 |
| stats_o/std                    | 0.10521842 |
| test/episodes                  | 4330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.378      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0117    |
| test/info_shaping_reward_mean  | -0.11      |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -13.931965 |
| test/Q_plus_P                  | -13.931965 |
| test/reward_per_eps            | -24.9      |
| test/steps                     | 173200     |
| train/episodes                 | 17320      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.07       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0709    |
| train/info_shaping_reward_mean | -0.164     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.2      |
| train/steps                    | 692800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 433        |
| stats_o/mean                   | 0.20329037 |
| stats_o/std                    | 0.10524818 |
| test/episodes                  | 4340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.273      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0177    |
| test/info_shaping_reward_mean  | -0.122     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -19.344782 |
| test/Q_plus_P                  | -19.344782 |
| test/reward_per_eps            | -29.1      |
| test/steps                     | 173600     |
| train/episodes                 | 17360      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0794     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0556    |
| train/info_shaping_reward_mean | -0.149     |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.8      |
| train/steps                    | 694400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 434        |
| stats_o/mean                   | 0.20328228 |
| stats_o/std                    | 0.10527087 |
| test/episodes                  | 4350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.195      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0133    |
| test/info_shaping_reward_mean  | -0.136     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -19.317415 |
| test/Q_plus_P                  | -19.317415 |
| test/reward_per_eps            | -32.2      |
| test/steps                     | 174000     |
| train/episodes                 | 17400      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.145      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0289    |
| train/info_shaping_reward_mean | -0.146     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.2      |
| train/steps                    | 696000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 435         |
| stats_o/mean                   | 0.2032609   |
| stats_o/std                    | 0.105337754 |
| test/episodes                  | 4360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.133       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0244     |
| test/info_shaping_reward_mean  | -0.146      |
| test/info_shaping_reward_min   | -0.193      |
| test/Q                         | -18.66927   |
| test/Q_plus_P                  | -18.66927   |
| test/reward_per_eps            | -34.7       |
| test/steps                     | 174400      |
| train/episodes                 | 17440       |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.147       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0437     |
| train/info_shaping_reward_mean | -0.151      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.1       |
| train/steps                    | 697600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 436        |
| stats_o/mean                   | 0.20325832 |
| stats_o/std                    | 0.10534647 |
| test/episodes                  | 4370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.21       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.01      |
| test/info_shaping_reward_mean  | -0.135     |
| test/info_shaping_reward_min   | -0.338     |
| test/Q                         | -20.042015 |
| test/Q_plus_P                  | -20.042015 |
| test/reward_per_eps            | -31.6      |
| test/steps                     | 174800     |
| train/episodes                 | 17480      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.09       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0393    |
| train/info_shaping_reward_mean | -0.145     |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.4      |
| train/steps                    | 699200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 437        |
| stats_o/mean                   | 0.20323299 |
| stats_o/std                    | 0.10537669 |
| test/episodes                  | 4380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.328      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00946   |
| test/info_shaping_reward_mean  | -0.112     |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -10.820103 |
| test/Q_plus_P                  | -10.820103 |
| test/reward_per_eps            | -26.9      |
| test/steps                     | 175200     |
| train/episodes                 | 17520      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.128      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0521    |
| train/info_shaping_reward_mean | -0.135     |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.9      |
| train/steps                    | 700800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 438        |
| stats_o/mean                   | 0.20321429 |
| stats_o/std                    | 0.10542216 |
| test/episodes                  | 4390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.312      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0163    |
| test/info_shaping_reward_mean  | -0.118     |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -14.511857 |
| test/Q_plus_P                  | -14.511857 |
| test/reward_per_eps            | -27.5      |
| test/steps                     | 175600     |
| train/episodes                 | 17560      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.103      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0446    |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.288     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.9      |
| train/steps                    | 702400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 439        |
| stats_o/mean                   | 0.20321119 |
| stats_o/std                    | 0.10547939 |
| test/episodes                  | 4400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.38       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00433   |
| test/info_shaping_reward_mean  | -0.113     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -15.291858 |
| test/Q_plus_P                  | -15.291858 |
| test/reward_per_eps            | -24.8      |
| test/steps                     | 176000     |
| train/episodes                 | 17600      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.158      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0408    |
| train/info_shaping_reward_mean | -0.15      |
| train/info_shaping_reward_min  | -0.283     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.7      |
| train/steps                    | 704000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.20319735  |
| stats_o/std                    | 0.105459236 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.14        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0214     |
| test/info_shaping_reward_mean  | -0.147      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -21.500195  |
| test/Q_plus_P                  | -21.500195  |
| test/reward_per_eps            | -34.4       |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.09        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0344     |
| train/info_shaping_reward_mean | -0.143      |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.4       |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.20319311  |
| stats_o/std                    | 0.105524905 |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.26        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.016      |
| test/info_shaping_reward_mean  | -0.125      |
| test/info_shaping_reward_min   | -0.195      |
| test/Q                         | -19.31896   |
| test/Q_plus_P                  | -19.31896   |
| test/reward_per_eps            | -29.6       |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.12        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0415     |
| train/info_shaping_reward_mean | -0.136      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.2       |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 442         |
| stats_o/mean                   | 0.20320211  |
| stats_o/std                    | 0.105608225 |
| test/episodes                  | 4430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.147       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.022      |
| test/info_shaping_reward_mean  | -0.143      |
| test/info_shaping_reward_min   | -0.197      |
| test/Q                         | -24.870806  |
| test/Q_plus_P                  | -24.870806  |
| test/reward_per_eps            | -34.1       |
| test/steps                     | 177200      |
| train/episodes                 | 17720       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.162       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0292     |
| train/info_shaping_reward_mean | -0.133      |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.5       |
| train/steps                    | 708800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.20320152  |
| stats_o/std                    | 0.105672866 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.36        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00854    |
| test/info_shaping_reward_mean  | -0.11       |
| test/info_shaping_reward_min   | -0.198      |
| test/Q                         | -13.92771   |
| test/Q_plus_P                  | -13.92771   |
| test/reward_per_eps            | -25.6       |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0275      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0873     |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.298      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.9       |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 444        |
| stats_o/mean                   | 0.20319818 |
| stats_o/std                    | 0.10568599 |
| test/episodes                  | 4450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.117      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00255   |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.607     |
| test/Q                         | -20.612637 |
| test/Q_plus_P                  | -20.612637 |
| test/reward_per_eps            | -35.3      |
| test/steps                     | 178000     |
| train/episodes                 | 17800      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.159      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0394    |
| train/info_shaping_reward_mean | -0.15      |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.6      |
| train/steps                    | 712000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.2031985   |
| stats_o/std                    | 0.105755605 |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.188       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0114     |
| test/info_shaping_reward_mean  | -0.139      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -20.590874  |
| test/Q_plus_P                  | -20.590874  |
| test/reward_per_eps            | -32.5       |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.163       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0261     |
| train/info_shaping_reward_mean | -0.131      |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.5       |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 446         |
| stats_o/mean                   | 0.20319696  |
| stats_o/std                    | 0.105786994 |
| test/episodes                  | 4470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.29        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00244    |
| test/info_shaping_reward_mean  | -0.109      |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -10.598424  |
| test/Q_plus_P                  | -10.598424  |
| test/reward_per_eps            | -28.4       |
| test/steps                     | 178800      |
| train/episodes                 | 17880       |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.132       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0429     |
| train/info_shaping_reward_mean | -0.148      |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.7       |
| train/steps                    | 715200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 447        |
| stats_o/mean                   | 0.20318522 |
| stats_o/std                    | 0.10582859 |
| test/episodes                  | 4480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.075      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0184    |
| test/info_shaping_reward_mean  | -0.142     |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -21.56414  |
| test/Q_plus_P                  | -21.56414  |
| test/reward_per_eps            | -37        |
| test/steps                     | 179200     |
| train/episodes                 | 17920      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.106      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0492    |
| train/info_shaping_reward_mean | -0.142     |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.8      |
| train/steps                    | 716800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 448        |
| stats_o/mean                   | 0.20317811 |
| stats_o/std                    | 0.10588081 |
| test/episodes                  | 4490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.185      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0239    |
| test/info_shaping_reward_mean  | -0.116     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -16.853514 |
| test/Q_plus_P                  | -16.853514 |
| test/reward_per_eps            | -32.6      |
| test/steps                     | 179600     |
| train/episodes                 | 17960      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.148      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0329    |
| train/info_shaping_reward_mean | -0.135     |
| train/info_shaping_reward_min  | -0.307     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.1      |
| train/steps                    | 718400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 449        |
| stats_o/mean                   | 0.20317675 |
| stats_o/std                    | 0.10593353 |
| test/episodes                  | 4500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.128      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00926   |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.203     |
| test/Q                         | -18.83301  |
| test/Q_plus_P                  | -18.83301  |
| test/reward_per_eps            | -34.9      |
| test/steps                     | 180000     |
| train/episodes                 | 18000      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.122      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0522    |
| train/info_shaping_reward_mean | -0.151     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.1      |
| train/steps                    | 720000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.20316102 |
| stats_o/std                    | 0.10595223 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0025     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0445    |
| test/info_shaping_reward_mean  | -0.177     |
| test/info_shaping_reward_min   | -0.584     |
| test/Q                         | -23.338692 |
| test/Q_plus_P                  | -23.338692 |
| test/reward_per_eps            | -39.9      |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.18       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0341    |
| train/info_shaping_reward_mean | -0.136     |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.8      |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 451        |
| stats_o/mean                   | 0.20314091 |
| stats_o/std                    | 0.1061193  |
| test/episodes                  | 4520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.158      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00512   |
| test/info_shaping_reward_mean  | -0.15      |
| test/info_shaping_reward_min   | -0.62      |
| test/Q                         | -18.833202 |
| test/Q_plus_P                  | -18.833202 |
| test/reward_per_eps            | -33.7      |
| test/steps                     | 180800     |
| train/episodes                 | 18080      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.186      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0336    |
| train/info_shaping_reward_mean | -0.147     |
| train/info_shaping_reward_min  | -0.321     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.6      |
| train/steps                    | 723200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 452        |
| stats_o/mean                   | 0.20312826 |
| stats_o/std                    | 0.1061913  |
| test/episodes                  | 4530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.265      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0106    |
| test/info_shaping_reward_mean  | -0.121     |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -12.015415 |
| test/Q_plus_P                  | -12.015415 |
| test/reward_per_eps            | -29.4      |
| test/steps                     | 181200     |
| train/episodes                 | 18120      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.216      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0266    |
| train/info_shaping_reward_mean | -0.12      |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.4      |
| train/steps                    | 724800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 453         |
| stats_o/mean                   | 0.20312902  |
| stats_o/std                    | 0.106245935 |
| test/episodes                  | 4540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.535       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00532    |
| test/info_shaping_reward_mean  | -0.0729     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -6.916698   |
| test/Q_plus_P                  | -6.916698   |
| test/reward_per_eps            | -18.6       |
| test/steps                     | 181600      |
| train/episodes                 | 18160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.213       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0231     |
| train/info_shaping_reward_mean | -0.132      |
| train/info_shaping_reward_min  | -0.291      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.5       |
| train/steps                    | 726400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.20314066  |
| stats_o/std                    | 0.106383435 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.365       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0203     |
| test/info_shaping_reward_mean  | -0.0993     |
| test/info_shaping_reward_min   | -0.191      |
| test/Q                         | -12.780972  |
| test/Q_plus_P                  | -12.780972  |
| test/reward_per_eps            | -25.4       |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.117       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0277     |
| train/info_shaping_reward_mean | -0.141      |
| train/info_shaping_reward_min  | -0.281      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.3       |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.2031406   |
| stats_o/std                    | 0.10644639  |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.295       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00646    |
| test/info_shaping_reward_mean  | -0.0984     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -11.1707535 |
| test/Q_plus_P                  | -11.1707535 |
| test/reward_per_eps            | -28.2       |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.189       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0323     |
| train/info_shaping_reward_mean | -0.139      |
| train/info_shaping_reward_min  | -0.297      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.4       |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 456         |
| stats_o/mean                   | 0.20314705  |
| stats_o/std                    | 0.106498115 |
| test/episodes                  | 4570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.152       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0132     |
| test/info_shaping_reward_mean  | -0.13       |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -16.764416  |
| test/Q_plus_P                  | -16.764416  |
| test/reward_per_eps            | -33.9       |
| test/steps                     | 182800      |
| train/episodes                 | 18280       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.111       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0338     |
| train/info_shaping_reward_mean | -0.146      |
| train/info_shaping_reward_min  | -0.305      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.5       |
| train/steps                    | 731200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 457        |
| stats_o/mean                   | 0.2031368  |
| stats_o/std                    | 0.10654765 |
| test/episodes                  | 4580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.405      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0112    |
| test/info_shaping_reward_mean  | -0.133     |
| test/info_shaping_reward_min   | -0.652     |
| test/Q                         | -11.797956 |
| test/Q_plus_P                  | -11.797956 |
| test/reward_per_eps            | -23.8      |
| test/steps                     | 183200     |
| train/episodes                 | 18320      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.155      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0309    |
| train/info_shaping_reward_mean | -0.131     |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.8      |
| train/steps                    | 732800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 458        |
| stats_o/mean                   | 0.20315188 |
| stats_o/std                    | 0.106654   |
| test/episodes                  | 4590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.193      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0061    |
| test/info_shaping_reward_mean  | -0.128     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -12.801148 |
| test/Q_plus_P                  | -12.801148 |
| test/reward_per_eps            | -32.3      |
| test/steps                     | 183600     |
| train/episodes                 | 18360      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.143      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0301    |
| train/info_shaping_reward_mean | -0.141     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.3      |
| train/steps                    | 734400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 459        |
| stats_o/mean                   | 0.20316094 |
| stats_o/std                    | 0.1067202  |
| test/episodes                  | 4600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.273      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0129    |
| test/info_shaping_reward_mean  | -0.118     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -14.99405  |
| test/Q_plus_P                  | -14.99405  |
| test/reward_per_eps            | -29.1      |
| test/steps                     | 184000     |
| train/episodes                 | 18400      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.186      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0402    |
| train/info_shaping_reward_mean | -0.137     |
| train/info_shaping_reward_min  | -0.285     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.6      |
| train/steps                    | 736000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.20315762  |
| stats_o/std                    | 0.106773466 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.095       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00487    |
| test/info_shaping_reward_mean  | -0.17       |
| test/info_shaping_reward_min   | -0.704      |
| test/Q                         | -17.586838  |
| test/Q_plus_P                  | -17.586838  |
| test/reward_per_eps            | -36.2       |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0988      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0421     |
| train/info_shaping_reward_mean | -0.149      |
| train/info_shaping_reward_min  | -0.313      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36         |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 461        |
| stats_o/mean                   | 0.20315067 |
| stats_o/std                    | 0.10680823 |
| test/episodes                  | 4620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0975     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0217    |
| test/info_shaping_reward_mean  | -0.136     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -16.398863 |
| test/Q_plus_P                  | -16.398863 |
| test/reward_per_eps            | -36.1      |
| test/steps                     | 184800     |
| train/episodes                 | 18480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.276      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0182    |
| train/info_shaping_reward_mean | -0.119     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.9      |
| train/steps                    | 739200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 462         |
| stats_o/mean                   | 0.20313966  |
| stats_o/std                    | 0.106856614 |
| test/episodes                  | 4630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.235       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00453    |
| test/info_shaping_reward_mean  | -0.11       |
| test/info_shaping_reward_min   | -0.197      |
| test/Q                         | -12.990156  |
| test/Q_plus_P                  | -12.990156  |
| test/reward_per_eps            | -30.6       |
| test/steps                     | 185200      |
| train/episodes                 | 18520       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.176       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.029      |
| train/info_shaping_reward_mean | -0.129      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33         |
| train/steps                    | 740800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 463        |
| stats_o/mean                   | 0.2031444  |
| stats_o/std                    | 0.10690176 |
| test/episodes                  | 4640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.328      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00772   |
| test/info_shaping_reward_mean  | -0.0986    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -10.496817 |
| test/Q_plus_P                  | -10.496817 |
| test/reward_per_eps            | -26.9      |
| test/steps                     | 185600     |
| train/episodes                 | 18560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.213      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0165    |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.5      |
| train/steps                    | 742400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 464        |
| stats_o/mean                   | 0.2031594  |
| stats_o/std                    | 0.1069298  |
| test/episodes                  | 4650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.425      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00841   |
| test/info_shaping_reward_mean  | -0.085     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -7.8565545 |
| test/Q_plus_P                  | -7.8565545 |
| test/reward_per_eps            | -23        |
| test/steps                     | 186000     |
| train/episodes                 | 18600      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.178      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0318    |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.9      |
| train/steps                    | 744000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 465        |
| stats_o/mean                   | 0.2031619  |
| stats_o/std                    | 0.1070462  |
| test/episodes                  | 4660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.282      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0148    |
| test/info_shaping_reward_mean  | -0.111     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -12.246225 |
| test/Q_plus_P                  | -12.246225 |
| test/reward_per_eps            | -28.7      |
| test/steps                     | 186400     |
| train/episodes                 | 18640      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.138      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0283    |
| train/info_shaping_reward_mean | -0.138     |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.5      |
| train/steps                    | 745600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 466        |
| stats_o/mean                   | 0.20315166 |
| stats_o/std                    | 0.10706155 |
| test/episodes                  | 4670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.362      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0196    |
| test/info_shaping_reward_mean  | -0.113     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -11.305458 |
| test/Q_plus_P                  | -11.305458 |
| test/reward_per_eps            | -25.5      |
| test/steps                     | 186800     |
| train/episodes                 | 18680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.241      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0227    |
| train/info_shaping_reward_mean | -0.114     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.4      |
| train/steps                    | 747200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 467         |
| stats_o/mean                   | 0.20315334  |
| stats_o/std                    | 0.107138164 |
| test/episodes                  | 4680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.273       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0105     |
| test/info_shaping_reward_mean  | -0.111      |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -13.258304  |
| test/Q_plus_P                  | -13.258304  |
| test/reward_per_eps            | -29.1       |
| test/steps                     | 187200      |
| train/episodes                 | 18720       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.154       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0302     |
| train/info_shaping_reward_mean | -0.142      |
| train/info_shaping_reward_min  | -0.284      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.8       |
| train/steps                    | 748800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 468        |
| stats_o/mean                   | 0.20315225 |
| stats_o/std                    | 0.10714212 |
| test/episodes                  | 4690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.065      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0453    |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -18.731728 |
| test/Q_plus_P                  | -18.731728 |
| test/reward_per_eps            | -37.4      |
| test/steps                     | 187600     |
| train/episodes                 | 18760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.235      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0243    |
| train/info_shaping_reward_mean | -0.13      |
| train/info_shaping_reward_min  | -0.287     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.6      |
| train/steps                    | 750400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 469         |
| stats_o/mean                   | 0.20314123  |
| stats_o/std                    | 0.107168466 |
| test/episodes                  | 4700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.253       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00756    |
| test/info_shaping_reward_mean  | -0.119      |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -13.159588  |
| test/Q_plus_P                  | -13.159588  |
| test/reward_per_eps            | -29.9       |
| test/steps                     | 188000      |
| train/episodes                 | 18800       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.214       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0253     |
| train/info_shaping_reward_mean | -0.136      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.4       |
| train/steps                    | 752000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.20314352 |
| stats_o/std                    | 0.1072417  |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.395      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0123    |
| test/info_shaping_reward_mean  | -0.091     |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -11.433538 |
| test/Q_plus_P                  | -11.433538 |
| test/reward_per_eps            | -24.2      |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.149      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0299    |
| train/info_shaping_reward_mean | -0.135     |
| train/info_shaping_reward_min  | -0.28      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34        |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.20316605 |
| stats_o/std                    | 0.10728355 |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.15       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0141    |
| test/info_shaping_reward_mean  | -0.136     |
| test/info_shaping_reward_min   | -0.342     |
| test/Q                         | -16.953218 |
| test/Q_plus_P                  | -16.953218 |
| test/reward_per_eps            | -34        |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.189      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0246    |
| train/info_shaping_reward_mean | -0.133     |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.4      |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.20318209 |
| stats_o/std                    | 0.1072992  |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.195      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0106    |
| test/info_shaping_reward_mean  | -0.129     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -14.610351 |
| test/Q_plus_P                  | -14.610351 |
| test/reward_per_eps            | -32.2      |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.189      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0186    |
| train/info_shaping_reward_mean | -0.139     |
| train/info_shaping_reward_min  | -0.292     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.5      |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 473        |
| stats_o/mean                   | 0.20319852 |
| stats_o/std                    | 0.10732522 |
| test/episodes                  | 4740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.427      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00893   |
| test/info_shaping_reward_mean  | -0.0951    |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -9.091369  |
| test/Q_plus_P                  | -9.091369  |
| test/reward_per_eps            | -22.9      |
| test/steps                     | 189600     |
| train/episodes                 | 18960      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.165      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0341    |
| train/info_shaping_reward_mean | -0.137     |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.4      |
| train/steps                    | 758400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 474         |
| stats_o/mean                   | 0.20320463  |
| stats_o/std                    | 0.107332304 |
| test/episodes                  | 4750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.287       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0101     |
| test/info_shaping_reward_mean  | -0.145      |
| test/info_shaping_reward_min   | -0.793      |
| test/Q                         | -15.045446  |
| test/Q_plus_P                  | -15.045446  |
| test/reward_per_eps            | -28.5       |
| test/steps                     | 190000      |
| train/episodes                 | 19000       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.299       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0189     |
| train/info_shaping_reward_mean | -0.124      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.1       |
| train/steps                    | 760000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 475        |
| stats_o/mean                   | 0.20320505 |
| stats_o/std                    | 0.10737449 |
| test/episodes                  | 4760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.205      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0137    |
| test/info_shaping_reward_mean  | -0.0994    |
| test/info_shaping_reward_min   | -0.216     |
| test/Q                         | -10.86522  |
| test/Q_plus_P                  | -10.86522  |
| test/reward_per_eps            | -31.8      |
| test/steps                     | 190400     |
| train/episodes                 | 19040      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.206      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0222    |
| train/info_shaping_reward_mean | -0.115     |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.8      |
| train/steps                    | 761600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.20320077  |
| stats_o/std                    | 0.10742805  |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.34        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0164     |
| test/info_shaping_reward_mean  | -0.108      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -12.0484705 |
| test/Q_plus_P                  | -12.0484705 |
| test/reward_per_eps            | -26.4       |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.116       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0234     |
| train/info_shaping_reward_mean | -0.123      |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.4       |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 477        |
| stats_o/mean                   | 0.20319505 |
| stats_o/std                    | 0.10749559 |
| test/episodes                  | 4780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.247      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00779   |
| test/info_shaping_reward_mean  | -0.0957    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -11.139492 |
| test/Q_plus_P                  | -11.139492 |
| test/reward_per_eps            | -30.1      |
| test/steps                     | 191200     |
| train/episodes                 | 19120      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.212      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0217    |
| train/info_shaping_reward_mean | -0.132     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.5      |
| train/steps                    | 764800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 478        |
| stats_o/mean                   | 0.20320806 |
| stats_o/std                    | 0.10747396 |
| test/episodes                  | 4790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.23       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00818   |
| test/info_shaping_reward_mean  | -0.108     |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -10.374839 |
| test/Q_plus_P                  | -10.374839 |
| test/reward_per_eps            | -30.8      |
| test/steps                     | 191600     |
| train/episodes                 | 19160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.209      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0221    |
| train/info_shaping_reward_mean | -0.13      |
| train/info_shaping_reward_min  | -0.292     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.6      |
| train/steps                    | 766400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.20322767  |
| stats_o/std                    | 0.107525006 |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.48        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00777    |
| test/info_shaping_reward_mean  | -0.0962     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -8.317996   |
| test/Q_plus_P                  | -8.317996   |
| test/reward_per_eps            | -20.8       |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.202       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0255     |
| train/info_shaping_reward_mean | -0.119      |
| train/info_shaping_reward_min  | -0.211      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.9       |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 480        |
| stats_o/mean                   | 0.20323186 |
| stats_o/std                    | 0.10757619 |
| test/episodes                  | 4810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.347      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0153    |
| test/info_shaping_reward_mean  | -0.0853    |
| test/info_shaping_reward_min   | -0.213     |
| test/Q                         | -6.60375   |
| test/Q_plus_P                  | -6.60375   |
| test/reward_per_eps            | -26.1      |
| test/steps                     | 192400     |
| train/episodes                 | 19240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.274      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0157    |
| train/info_shaping_reward_mean | -0.114     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.1      |
| train/steps                    | 769600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 481        |
| stats_o/mean                   | 0.2032197  |
| stats_o/std                    | 0.10763357 |
| test/episodes                  | 4820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.42       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0136    |
| test/info_shaping_reward_mean  | -0.0731    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -4.9834504 |
| test/Q_plus_P                  | -4.9834504 |
| test/reward_per_eps            | -23.2      |
| test/steps                     | 192800     |
| train/episodes                 | 19280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.261      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0145    |
| train/info_shaping_reward_mean | -0.121     |
| train/info_shaping_reward_min  | -0.291     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.6      |
| train/steps                    | 771200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 482        |
| stats_o/mean                   | 0.20321625 |
| stats_o/std                    | 0.10771734 |
| test/episodes                  | 4830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.492      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0131    |
| test/info_shaping_reward_mean  | -0.0841    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -7.541388  |
| test/Q_plus_P                  | -7.541388  |
| test/reward_per_eps            | -20.3      |
| test/steps                     | 193200     |
| train/episodes                 | 19320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.245      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0203    |
| train/info_shaping_reward_mean | -0.112     |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.2      |
| train/steps                    | 772800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 483         |
| stats_o/mean                   | 0.20323147  |
| stats_o/std                    | 0.107776076 |
| test/episodes                  | 4840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.51        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0109     |
| test/info_shaping_reward_mean  | -0.0684     |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -4.2316914  |
| test/Q_plus_P                  | -4.2316914  |
| test/reward_per_eps            | -19.6       |
| test/steps                     | 193600      |
| train/episodes                 | 19360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.312       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0201     |
| train/info_shaping_reward_mean | -0.115      |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.5       |
| train/steps                    | 774400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.2032368   |
| stats_o/std                    | 0.107855186 |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.568       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00548    |
| test/info_shaping_reward_mean  | -0.0967     |
| test/info_shaping_reward_min   | -0.568      |
| test/Q                         | -6.6651692  |
| test/Q_plus_P                  | -6.6651692  |
| test/reward_per_eps            | -17.3       |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.285       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0152     |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.198      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.6       |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 485        |
| stats_o/mean                   | 0.20323856 |
| stats_o/std                    | 0.10787292 |
| test/episodes                  | 4860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.525      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00804   |
| test/info_shaping_reward_mean  | -0.0767    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -5.726586  |
| test/Q_plus_P                  | -5.726586  |
| test/reward_per_eps            | -19        |
| test/steps                     | 194400     |
| train/episodes                 | 19440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.336      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0174    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.6      |
| train/steps                    | 777600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 486        |
| stats_o/mean                   | 0.2032358  |
| stats_o/std                    | 0.10790966 |
| test/episodes                  | 4870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.537      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00683   |
| test/info_shaping_reward_mean  | -0.0726    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -5.580113  |
| test/Q_plus_P                  | -5.580113  |
| test/reward_per_eps            | -18.5      |
| test/steps                     | 194800     |
| train/episodes                 | 19480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.356      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0163    |
| train/info_shaping_reward_mean | -0.118     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.8      |
| train/steps                    | 779200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 487         |
| stats_o/mean                   | 0.203241    |
| stats_o/std                    | 0.107939914 |
| test/episodes                  | 4880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.472       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00683    |
| test/info_shaping_reward_mean  | -0.0757     |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -5.9638047  |
| test/Q_plus_P                  | -5.9638047  |
| test/reward_per_eps            | -21.1       |
| test/steps                     | 195200      |
| train/episodes                 | 19520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.236       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0207     |
| train/info_shaping_reward_mean | -0.12       |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.6       |
| train/steps                    | 780800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 488        |
| stats_o/mean                   | 0.20322935 |
| stats_o/std                    | 0.10797721 |
| test/episodes                  | 4890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.47       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00488   |
| test/info_shaping_reward_mean  | -0.0847    |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -8.615016  |
| test/Q_plus_P                  | -8.615016  |
| test/reward_per_eps            | -21.2      |
| test/steps                     | 195600     |
| train/episodes                 | 19560      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.263      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0247    |
| train/info_shaping_reward_mean | -0.121     |
| train/info_shaping_reward_min  | -0.284     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.5      |
| train/steps                    | 782400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 489        |
| stats_o/mean                   | 0.2032345  |
| stats_o/std                    | 0.10803389 |
| test/episodes                  | 4900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.507      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00755   |
| test/info_shaping_reward_mean  | -0.0705    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -5.762555  |
| test/Q_plus_P                  | -5.762555  |
| test/reward_per_eps            | -19.7      |
| test/steps                     | 196000     |
| train/episodes                 | 19600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.4        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0118    |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24        |
| train/steps                    | 784000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 490        |
| stats_o/mean                   | 0.20324191 |
| stats_o/std                    | 0.10806966 |
| test/episodes                  | 4910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.372      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.012     |
| test/info_shaping_reward_mean  | -0.0797    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -6.277462  |
| test/Q_plus_P                  | -6.277462  |
| test/reward_per_eps            | -25.1      |
| test/steps                     | 196400     |
| train/episodes                 | 19640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.354      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0185    |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.9      |
| train/steps                    | 785600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 491        |
| stats_o/mean                   | 0.20324105 |
| stats_o/std                    | 0.10816387 |
| test/episodes                  | 4920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.325      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00411   |
| test/info_shaping_reward_mean  | -0.084     |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -6.6521583 |
| test/Q_plus_P                  | -6.6521583 |
| test/reward_per_eps            | -27        |
| test/steps                     | 196800     |
| train/episodes                 | 19680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.246      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0215    |
| train/info_shaping_reward_mean | -0.124     |
| train/info_shaping_reward_min  | -0.294     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.1      |
| train/steps                    | 787200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 492        |
| stats_o/mean                   | 0.20324978 |
| stats_o/std                    | 0.10822794 |
| test/episodes                  | 4930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.282      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0252    |
| test/info_shaping_reward_mean  | -0.107     |
| test/info_shaping_reward_min   | -0.292     |
| test/Q                         | -11.261046 |
| test/Q_plus_P                  | -11.261046 |
| test/reward_per_eps            | -28.7      |
| test/steps                     | 197200     |
| train/episodes                 | 19720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.309      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0171    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.6      |
| train/steps                    | 788800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.2032466  |
| stats_o/std                    | 0.1082733  |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.295      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00811   |
| test/info_shaping_reward_mean  | -0.108     |
| test/info_shaping_reward_min   | -0.638     |
| test/Q                         | -10.341528 |
| test/Q_plus_P                  | -10.341528 |
| test/reward_per_eps            | -28.2      |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.29       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0202    |
| train/info_shaping_reward_mean | -0.0985    |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.4      |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 494        |
| stats_o/mean                   | 0.20325556 |
| stats_o/std                    | 0.10827698 |
| test/episodes                  | 4950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.515      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0142    |
| test/info_shaping_reward_mean  | -0.074     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -4.8056993 |
| test/Q_plus_P                  | -4.8056993 |
| test/reward_per_eps            | -19.4      |
| test/steps                     | 198000     |
| train/episodes                 | 19800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.381      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0172    |
| train/info_shaping_reward_mean | -0.0991    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.8      |
| train/steps                    | 792000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 495        |
| stats_o/mean                   | 0.20326166 |
| stats_o/std                    | 0.10831139 |
| test/episodes                  | 4960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.642      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00507   |
| test/info_shaping_reward_mean  | -0.0691    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -5.458531  |
| test/Q_plus_P                  | -5.458531  |
| test/reward_per_eps            | -14.3      |
| test/steps                     | 198400     |
| train/episodes                 | 19840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.276      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0193    |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.9      |
| train/steps                    | 793600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 496        |
| stats_o/mean                   | 0.20328075 |
| stats_o/std                    | 0.1083784  |
| test/episodes                  | 4970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.42       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00665   |
| test/info_shaping_reward_mean  | -0.0956    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.017743  |
| test/Q_plus_P                  | -9.017743  |
| test/reward_per_eps            | -23.2      |
| test/steps                     | 198800     |
| train/episodes                 | 19880      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.316      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0213    |
| train/info_shaping_reward_mean | -0.111     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.4      |
| train/steps                    | 795200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 497        |
| stats_o/mean                   | 0.20330442 |
| stats_o/std                    | 0.10847525 |
| test/episodes                  | 4980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.55       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00827   |
| test/info_shaping_reward_mean  | -0.0786    |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -5.0210533 |
| test/Q_plus_P                  | -5.0210533 |
| test/reward_per_eps            | -18        |
| test/steps                     | 199200     |
| train/episodes                 | 19920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.265      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0213    |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.4      |
| train/steps                    | 796800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 498        |
| stats_o/mean                   | 0.20330356 |
| stats_o/std                    | 0.1085416  |
| test/episodes                  | 4990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.212      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0156    |
| test/info_shaping_reward_mean  | -0.0983    |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -9.47621   |
| test/Q_plus_P                  | -9.47621   |
| test/reward_per_eps            | -31.5      |
| test/steps                     | 199600     |
| train/episodes                 | 19960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.252      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0147    |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.9      |
| train/steps                    | 798400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 499        |
| stats_o/mean                   | 0.20331466 |
| stats_o/std                    | 0.10856461 |
| test/episodes                  | 5000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.512      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0121    |
| test/info_shaping_reward_mean  | -0.0797    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -5.535858  |
| test/Q_plus_P                  | -5.535858  |
| test/reward_per_eps            | -19.5      |
| test/steps                     | 200000     |
| train/episodes                 | 20000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.314      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.018     |
| train/info_shaping_reward_mean | -0.1       |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.4      |
| train/steps                    | 800000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 500        |
| stats_o/mean                   | 0.20332304 |
| stats_o/std                    | 0.10856641 |
| test/episodes                  | 5010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.198      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0166    |
| test/info_shaping_reward_mean  | -0.0947    |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -8.625915  |
| test/Q_plus_P                  | -8.625915  |
| test/reward_per_eps            | -32.1      |
| test/steps                     | 200400     |
| train/episodes                 | 20040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.339      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0183    |
| train/info_shaping_reward_mean | -0.1       |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.4      |
| train/steps                    | 801600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 501        |
| stats_o/mean                   | 0.20332724 |
| stats_o/std                    | 0.10855647 |
| test/episodes                  | 5020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.403      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0169    |
| test/info_shaping_reward_mean  | -0.0841    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -7.1295123 |
| test/Q_plus_P                  | -7.1295123 |
| test/reward_per_eps            | -23.9      |
| test/steps                     | 200800     |
| train/episodes                 | 20080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.374      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0185    |
| train/info_shaping_reward_mean | -0.0961    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.1      |
| train/steps                    | 803200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.203326    |
| stats_o/std                    | 0.108591825 |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.443       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00887    |
| test/info_shaping_reward_mean  | -0.0823     |
| test/info_shaping_reward_min   | -0.194      |
| test/Q                         | -6.635208   |
| test/Q_plus_P                  | -6.635208   |
| test/reward_per_eps            | -22.3       |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.239       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0186     |
| train/info_shaping_reward_mean | -0.11       |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.4       |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 503        |
| stats_o/mean                   | 0.20333853 |
| stats_o/std                    | 0.10863767 |
| test/episodes                  | 5040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.385      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00585   |
| test/info_shaping_reward_mean  | -0.119     |
| test/info_shaping_reward_min   | -0.594     |
| test/Q                         | -9.008324  |
| test/Q_plus_P                  | -9.008324  |
| test/reward_per_eps            | -24.6      |
| test/steps                     | 201600     |
| train/episodes                 | 20160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.308      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0118    |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.7      |
| train/steps                    | 806400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 504        |
| stats_o/mean                   | 0.20333828 |
| stats_o/std                    | 0.10866152 |
| test/episodes                  | 5050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.285      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0102    |
| test/info_shaping_reward_mean  | -0.106     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -7.639586  |
| test/Q_plus_P                  | -7.639586  |
| test/reward_per_eps            | -28.6      |
| test/steps                     | 202000     |
| train/episodes                 | 20200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.264      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0167    |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.4      |
| train/steps                    | 808000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 505         |
| stats_o/mean                   | 0.20334534  |
| stats_o/std                    | 0.108707316 |
| test/episodes                  | 5060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.19        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00684    |
| test/info_shaping_reward_mean  | -0.119      |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -10.490325  |
| test/Q_plus_P                  | -10.490325  |
| test/reward_per_eps            | -32.4       |
| test/steps                     | 202400      |
| train/episodes                 | 20240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.234       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0161     |
| train/info_shaping_reward_mean | -0.113      |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.6       |
| train/steps                    | 809600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 506        |
| stats_o/mean                   | 0.20335667 |
| stats_o/std                    | 0.10870114 |
| test/episodes                  | 5070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.355      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0029    |
| test/info_shaping_reward_mean  | -0.101     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -8.121064  |
| test/Q_plus_P                  | -8.121064  |
| test/reward_per_eps            | -25.8      |
| test/steps                     | 202800     |
| train/episodes                 | 20280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.241      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0221    |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.4      |
| train/steps                    | 811200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 507        |
| stats_o/mean                   | 0.20336582 |
| stats_o/std                    | 0.10872486 |
| test/episodes                  | 5080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.31       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0133    |
| test/info_shaping_reward_mean  | -0.101     |
| test/info_shaping_reward_min   | -0.346     |
| test/Q                         | -9.508128  |
| test/Q_plus_P                  | -9.508128  |
| test/reward_per_eps            | -27.6      |
| test/steps                     | 203200     |
| train/episodes                 | 20320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.278      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0127    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.9      |
| train/steps                    | 812800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 508        |
| stats_o/mean                   | 0.20338674 |
| stats_o/std                    | 0.10877027 |
| test/episodes                  | 5090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.312      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00298   |
| test/info_shaping_reward_mean  | -0.101     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -7.8410325 |
| test/Q_plus_P                  | -7.8410325 |
| test/reward_per_eps            | -27.5      |
| test/steps                     | 203600     |
| train/episodes                 | 20360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.236      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0195    |
| train/info_shaping_reward_mean | -0.118     |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.6      |
| train/steps                    | 814400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 509        |
| stats_o/mean                   | 0.20340537 |
| stats_o/std                    | 0.10884099 |
| test/episodes                  | 5100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.325      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00783   |
| test/info_shaping_reward_mean  | -0.111     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -8.875069  |
| test/Q_plus_P                  | -8.875069  |
| test/reward_per_eps            | -27        |
| test/steps                     | 204000     |
| train/episodes                 | 20400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.177      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0213    |
| train/info_shaping_reward_mean | -0.127     |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.9      |
| train/steps                    | 816000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 510        |
| stats_o/mean                   | 0.20342365 |
| stats_o/std                    | 0.10885662 |
| test/episodes                  | 5110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.432      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0109    |
| test/info_shaping_reward_mean  | -0.0824    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -8.477638  |
| test/Q_plus_P                  | -8.477638  |
| test/reward_per_eps            | -22.7      |
| test/steps                     | 204400     |
| train/episodes                 | 20440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.274      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0207    |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.1      |
| train/steps                    | 817600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 511        |
| stats_o/mean                   | 0.2034213  |
| stats_o/std                    | 0.10895115 |
| test/episodes                  | 5120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.43       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00798   |
| test/info_shaping_reward_mean  | -0.0824    |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -6.9459205 |
| test/Q_plus_P                  | -6.9459205 |
| test/reward_per_eps            | -22.8      |
| test/steps                     | 204800     |
| train/episodes                 | 20480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.236      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0224    |
| train/info_shaping_reward_mean | -0.124     |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.6      |
| train/steps                    | 819200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 512        |
| stats_o/mean                   | 0.20343357 |
| stats_o/std                    | 0.10896956 |
| test/episodes                  | 5130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.45       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00263   |
| test/info_shaping_reward_mean  | -0.0991    |
| test/info_shaping_reward_min   | -0.219     |
| test/Q                         | -7.3526154 |
| test/Q_plus_P                  | -7.3526154 |
| test/reward_per_eps            | -22        |
| test/steps                     | 205200     |
| train/episodes                 | 20520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.279      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0187    |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.9      |
| train/steps                    | 820800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 513        |
| stats_o/mean                   | 0.20345332 |
| stats_o/std                    | 0.10898869 |
| test/episodes                  | 5140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.35       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00574   |
| test/info_shaping_reward_mean  | -0.0911    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -9.147705  |
| test/Q_plus_P                  | -9.147705  |
| test/reward_per_eps            | -26        |
| test/steps                     | 205600     |
| train/episodes                 | 20560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.312      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0193    |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.5      |
| train/steps                    | 822400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 514        |
| stats_o/mean                   | 0.20346984 |
| stats_o/std                    | 0.10902281 |
| test/episodes                  | 5150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.562      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00471   |
| test/info_shaping_reward_mean  | -0.0719    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -6.511897  |
| test/Q_plus_P                  | -6.511897  |
| test/reward_per_eps            | -17.5      |
| test/steps                     | 206000     |
| train/episodes                 | 20600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.308      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0185    |
| train/info_shaping_reward_mean | -0.112     |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.7      |
| train/steps                    | 824000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 515         |
| stats_o/mean                   | 0.20345907  |
| stats_o/std                    | 0.109036885 |
| test/episodes                  | 5160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.575       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00752    |
| test/info_shaping_reward_mean  | -0.0639     |
| test/info_shaping_reward_min   | -0.197      |
| test/Q                         | -4.5935783  |
| test/Q_plus_P                  | -4.5935783  |
| test/reward_per_eps            | -17         |
| test/steps                     | 206400      |
| train/episodes                 | 20640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.238       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0143     |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.198      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.5       |
| train/steps                    | 825600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 516        |
| stats_o/mean                   | 0.20345424 |
| stats_o/std                    | 0.10908018 |
| test/episodes                  | 5170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.618      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00678   |
| test/info_shaping_reward_mean  | -0.062     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.8231115 |
| test/Q_plus_P                  | -3.8231115 |
| test/reward_per_eps            | -15.3      |
| test/steps                     | 206800     |
| train/episodes                 | 20680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.282      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0193    |
| train/info_shaping_reward_mean | -0.114     |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.7      |
| train/steps                    | 827200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 517        |
| stats_o/mean                   | 0.20345443 |
| stats_o/std                    | 0.10910715 |
| test/episodes                  | 5180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00999   |
| test/info_shaping_reward_mean  | -0.0652    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -4.7218213 |
| test/Q_plus_P                  | -4.7218213 |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 207200     |
| train/episodes                 | 20720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.338      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0112    |
| train/info_shaping_reward_mean | -0.112     |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.5      |
| train/steps                    | 828800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 518        |
| stats_o/mean                   | 0.20347041 |
| stats_o/std                    | 0.10911147 |
| test/episodes                  | 5190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.615      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00337   |
| test/info_shaping_reward_mean  | -0.0651    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -4.3911    |
| test/Q_plus_P                  | -4.3911    |
| test/reward_per_eps            | -15.4      |
| test/steps                     | 207600     |
| train/episodes                 | 20760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.369      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0181    |
| train/info_shaping_reward_mean | -0.096     |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.2      |
| train/steps                    | 830400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 519        |
| stats_o/mean                   | 0.20347811 |
| stats_o/std                    | 0.10909885 |
| test/episodes                  | 5200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.56       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0042    |
| test/info_shaping_reward_mean  | -0.0681    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -5.1615314 |
| test/Q_plus_P                  | -5.1615314 |
| test/reward_per_eps            | -17.6      |
| test/steps                     | 208000     |
| train/episodes                 | 20800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.416      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0124    |
| train/info_shaping_reward_mean | -0.0972    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.4      |
| train/steps                    | 832000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.20347281  |
| stats_o/std                    | 0.109088294 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.585       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0097     |
| test/info_shaping_reward_mean  | -0.0747     |
| test/info_shaping_reward_min   | -0.213      |
| test/Q                         | -5.4102077  |
| test/Q_plus_P                  | -5.4102077  |
| test/reward_per_eps            | -16.6       |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.395       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0129     |
| train/info_shaping_reward_mean | -0.0909     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.2       |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 521        |
| stats_o/mean                   | 0.20347194 |
| stats_o/std                    | 0.10904875 |
| test/episodes                  | 5220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.56       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00373   |
| test/info_shaping_reward_mean  | -0.0704    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -5.443096  |
| test/Q_plus_P                  | -5.443096  |
| test/reward_per_eps            | -17.6      |
| test/steps                     | 208800     |
| train/episodes                 | 20880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.449      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.018     |
| train/info_shaping_reward_mean | -0.0867    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.1      |
| train/steps                    | 835200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 522        |
| stats_o/mean                   | 0.20347652 |
| stats_o/std                    | 0.10905566 |
| test/episodes                  | 5230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.388      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0105    |
| test/info_shaping_reward_mean  | -0.0937    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -8.258208  |
| test/Q_plus_P                  | -8.258208  |
| test/reward_per_eps            | -24.5      |
| test/steps                     | 209200     |
| train/episodes                 | 20920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.318      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.014     |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.3      |
| train/steps                    | 836800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 523        |
| stats_o/mean                   | 0.20347023 |
| stats_o/std                    | 0.10908303 |
| test/episodes                  | 5240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0109    |
| test/info_shaping_reward_mean  | -0.0645    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -3.764203  |
| test/Q_plus_P                  | -3.764203  |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 209600     |
| train/episodes                 | 20960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.309      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0169    |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.6      |
| train/steps                    | 838400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 524         |
| stats_o/mean                   | 0.20346437  |
| stats_o/std                    | 0.109082125 |
| test/episodes                  | 5250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.492       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00114    |
| test/info_shaping_reward_mean  | -0.0819     |
| test/info_shaping_reward_min   | -0.196      |
| test/Q                         | -6.059171   |
| test/Q_plus_P                  | -6.059171   |
| test/reward_per_eps            | -20.3       |
| test/steps                     | 210000      |
| train/episodes                 | 21000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.319       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0156     |
| train/info_shaping_reward_mean | -0.103      |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.2       |
| train/steps                    | 840000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 525        |
| stats_o/mean                   | 0.20346476 |
| stats_o/std                    | 0.10906697 |
| test/episodes                  | 5260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.458      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00434   |
| test/info_shaping_reward_mean  | -0.0916    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -7.350272  |
| test/Q_plus_P                  | -7.350272  |
| test/reward_per_eps            | -21.7      |
| test/steps                     | 210400     |
| train/episodes                 | 21040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.354      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0141    |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.8      |
| train/steps                    | 841600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 526        |
| stats_o/mean                   | 0.20346326 |
| stats_o/std                    | 0.1091016  |
| test/episodes                  | 5270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.275      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00209   |
| test/info_shaping_reward_mean  | -0.092     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -8.241706  |
| test/Q_plus_P                  | -8.241706  |
| test/reward_per_eps            | -29        |
| test/steps                     | 210800     |
| train/episodes                 | 21080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.311      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0131    |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.6      |
| train/steps                    | 843200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 527        |
| stats_o/mean                   | 0.20347825 |
| stats_o/std                    | 0.10917425 |
| test/episodes                  | 5280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.367      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0109    |
| test/info_shaping_reward_mean  | -0.0949    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -9.627403  |
| test/Q_plus_P                  | -9.627403  |
| test/reward_per_eps            | -25.3      |
| test/steps                     | 211200     |
| train/episodes                 | 21120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.291      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0134    |
| train/info_shaping_reward_mean | -0.112     |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.4      |
| train/steps                    | 844800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 528         |
| stats_o/mean                   | 0.20348912  |
| stats_o/std                    | 0.109158784 |
| test/episodes                  | 5290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.57        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00746    |
| test/info_shaping_reward_mean  | -0.0646     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -4.216614   |
| test/Q_plus_P                  | -4.216614   |
| test/reward_per_eps            | -17.2       |
| test/steps                     | 211600      |
| train/episodes                 | 21160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.392       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0165     |
| train/info_shaping_reward_mean | -0.0938     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.3       |
| train/steps                    | 846400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.2035036   |
| stats_o/std                    | 0.109213114 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.37        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00764    |
| test/info_shaping_reward_mean  | -0.0883     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -7.1449122  |
| test/Q_plus_P                  | -7.1449122  |
| test/reward_per_eps            | -25.2       |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.253       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0156     |
| train/info_shaping_reward_mean | -0.113      |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.9       |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 530        |
| stats_o/mean                   | 0.20349747 |
| stats_o/std                    | 0.10925    |
| test/episodes                  | 5310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.595      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0082    |
| test/info_shaping_reward_mean  | -0.0773    |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -4.0363264 |
| test/Q_plus_P                  | -4.0363264 |
| test/reward_per_eps            | -16.2      |
| test/steps                     | 212400     |
| train/episodes                 | 21240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.294      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0132    |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.2      |
| train/steps                    | 849600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 531        |
| stats_o/mean                   | 0.20348297 |
| stats_o/std                    | 0.10924659 |
| test/episodes                  | 5320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.487      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00638   |
| test/info_shaping_reward_mean  | -0.0759    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -6.161238  |
| test/Q_plus_P                  | -6.161238  |
| test/reward_per_eps            | -20.5      |
| test/steps                     | 212800     |
| train/episodes                 | 21280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.294      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0153    |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.2      |
| train/steps                    | 851200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.20346664  |
| stats_o/std                    | 0.109265916 |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.565       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00242    |
| test/info_shaping_reward_mean  | -0.0693     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -4.6703467  |
| test/Q_plus_P                  | -4.6703467  |
| test/reward_per_eps            | -17.4       |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.289       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0166     |
| train/info_shaping_reward_mean | -0.113      |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.4       |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 533        |
| stats_o/mean                   | 0.20346648 |
| stats_o/std                    | 0.1092545  |
| test/episodes                  | 5340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.568      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0147    |
| test/info_shaping_reward_mean  | -0.0721    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -4.6897564 |
| test/Q_plus_P                  | -4.6897564 |
| test/reward_per_eps            | -17.3      |
| test/steps                     | 213600     |
| train/episodes                 | 21360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.389      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0135    |
| train/info_shaping_reward_mean | -0.0943    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.4      |
| train/steps                    | 854400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 534        |
| stats_o/mean                   | 0.20346077 |
| stats_o/std                    | 0.10927183 |
| test/episodes                  | 5350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.415      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0072    |
| test/info_shaping_reward_mean  | -0.0909    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -7.5147743 |
| test/Q_plus_P                  | -7.5147743 |
| test/reward_per_eps            | -23.4      |
| test/steps                     | 214000     |
| train/episodes                 | 21400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.378      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0162    |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.9      |
| train/steps                    | 856000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 535        |
| stats_o/mean                   | 0.2034662  |
| stats_o/std                    | 0.10929803 |
| test/episodes                  | 5360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.37       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00407   |
| test/info_shaping_reward_mean  | -0.0988    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -8.233993  |
| test/Q_plus_P                  | -8.233993  |
| test/reward_per_eps            | -25.2      |
| test/steps                     | 214400     |
| train/episodes                 | 21440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.256      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0159    |
| train/info_shaping_reward_mean | -0.122     |
| train/info_shaping_reward_min  | -0.292     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.8      |
| train/steps                    | 857600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 536         |
| stats_o/mean                   | 0.2034698   |
| stats_o/std                    | 0.109345034 |
| test/episodes                  | 5370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.637       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0133     |
| test/info_shaping_reward_mean  | -0.0639     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -3.4638085  |
| test/Q_plus_P                  | -3.4638085  |
| test/reward_per_eps            | -14.5       |
| test/steps                     | 214800      |
| train/episodes                 | 21480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.269       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0143     |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.2       |
| train/steps                    | 859200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 537        |
| stats_o/mean                   | 0.203447   |
| stats_o/std                    | 0.10936471 |
| test/episodes                  | 5380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.585      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0104    |
| test/info_shaping_reward_mean  | -0.0688    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -4.1680965 |
| test/Q_plus_P                  | -4.1680965 |
| test/reward_per_eps            | -16.6      |
| test/steps                     | 215200     |
| train/episodes                 | 21520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.374      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0126    |
| train/info_shaping_reward_mean | -0.0969    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.1      |
| train/steps                    | 860800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 538         |
| stats_o/mean                   | 0.20344292  |
| stats_o/std                    | 0.109369606 |
| test/episodes                  | 5390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.495       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00731    |
| test/info_shaping_reward_mean  | -0.0969     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -7.9197903  |
| test/Q_plus_P                  | -7.9197903  |
| test/reward_per_eps            | -20.2       |
| test/steps                     | 215600      |
| train/episodes                 | 21560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.361       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.015      |
| train/info_shaping_reward_mean | -0.0991     |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.6       |
| train/steps                    | 862400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 539         |
| stats_o/mean                   | 0.203437    |
| stats_o/std                    | 0.109389745 |
| test/episodes                  | 5400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.585       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00901    |
| test/info_shaping_reward_mean  | -0.0681     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -4.956792   |
| test/Q_plus_P                  | -4.956792   |
| test/reward_per_eps            | -16.6       |
| test/steps                     | 216000      |
| train/episodes                 | 21600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.371       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.017      |
| train/info_shaping_reward_mean | -0.102      |
| train/info_shaping_reward_min  | -0.213      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.2       |
| train/steps                    | 864000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 540        |
| stats_o/mean                   | 0.20343351 |
| stats_o/std                    | 0.10940794 |
| test/episodes                  | 5410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.393      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.011     |
| test/info_shaping_reward_mean  | -0.0753    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -6.304736  |
| test/Q_plus_P                  | -6.304736  |
| test/reward_per_eps            | -24.3      |
| test/steps                     | 216400     |
| train/episodes                 | 21640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.318      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0155    |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.305     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.3      |
| train/steps                    | 865600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.20343883  |
| stats_o/std                    | 0.109412216 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.645       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00524    |
| test/info_shaping_reward_mean  | -0.0631     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -4.133635   |
| test/Q_plus_P                  | -4.133635   |
| test/reward_per_eps            | -14.2       |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.441       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0107     |
| train/info_shaping_reward_mean | -0.0948     |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.4       |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 542         |
| stats_o/mean                   | 0.20343897  |
| stats_o/std                    | 0.109386854 |
| test/episodes                  | 5430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.625       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0021     |
| test/info_shaping_reward_mean  | -0.062      |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -4.0487814  |
| test/Q_plus_P                  | -4.0487814  |
| test/reward_per_eps            | -15         |
| test/steps                     | 217200      |
| train/episodes                 | 21720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.397       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0149     |
| train/info_shaping_reward_mean | -0.0975     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.1       |
| train/steps                    | 868800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 543        |
| stats_o/mean                   | 0.20342754 |
| stats_o/std                    | 0.10937634 |
| test/episodes                  | 5440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.568      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000738  |
| test/info_shaping_reward_mean  | -0.0762    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -3.6541016 |
| test/Q_plus_P                  | -3.6541016 |
| test/reward_per_eps            | -17.3      |
| test/steps                     | 217600     |
| train/episodes                 | 21760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.389      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0128    |
| train/info_shaping_reward_mean | -0.0973    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.4      |
| train/steps                    | 870400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 544        |
| stats_o/mean                   | 0.203426   |
| stats_o/std                    | 0.10933551 |
| test/episodes                  | 5450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.54       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0018    |
| test/info_shaping_reward_mean  | -0.0666    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -4.1065164 |
| test/Q_plus_P                  | -4.1065164 |
| test/reward_per_eps            | -18.4      |
| test/steps                     | 218000     |
| train/episodes                 | 21800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.469      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0136    |
| train/info_shaping_reward_mean | -0.0927    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 872000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 545        |
| stats_o/mean                   | 0.20342895 |
| stats_o/std                    | 0.10933212 |
| test/episodes                  | 5460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.657      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00519   |
| test/info_shaping_reward_mean  | -0.102     |
| test/info_shaping_reward_min   | -0.771     |
| test/Q                         | -4.7799416 |
| test/Q_plus_P                  | -4.7799416 |
| test/reward_per_eps            | -13.7      |
| test/steps                     | 218400     |
| train/episodes                 | 21840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.339      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0126    |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.4      |
| train/steps                    | 873600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 546        |
| stats_o/mean                   | 0.20342603 |
| stats_o/std                    | 0.10932145 |
| test/episodes                  | 5470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.585      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000608  |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.2472816 |
| test/Q_plus_P                  | -3.2472816 |
| test/reward_per_eps            | -16.6      |
| test/steps                     | 218800     |
| train/episodes                 | 21880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.436      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.013     |
| train/info_shaping_reward_mean | -0.0932    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.6      |
| train/steps                    | 875200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 547        |
| stats_o/mean                   | 0.20343384 |
| stats_o/std                    | 0.10930231 |
| test/episodes                  | 5480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.605      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000254  |
| test/info_shaping_reward_mean  | -0.0637    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.4646106 |
| test/Q_plus_P                  | -3.4646106 |
| test/reward_per_eps            | -15.8      |
| test/steps                     | 219200     |
| train/episodes                 | 21920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.473      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0128    |
| train/info_shaping_reward_mean | -0.0892    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 876800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 548        |
| stats_o/mean                   | 0.20343725 |
| stats_o/std                    | 0.10927029 |
| test/episodes                  | 5490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.627      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00339   |
| test/info_shaping_reward_mean  | -0.0654    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -3.883562  |
| test/Q_plus_P                  | -3.883562  |
| test/reward_per_eps            | -14.9      |
| test/steps                     | 219600     |
| train/episodes                 | 21960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.445      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0118    |
| train/info_shaping_reward_mean | -0.085     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.2      |
| train/steps                    | 878400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 549        |
| stats_o/mean                   | 0.20342956 |
| stats_o/std                    | 0.10924383 |
| test/episodes                  | 5500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.69       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00193   |
| test/info_shaping_reward_mean  | -0.0599    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -2.7983823 |
| test/Q_plus_P                  | -2.7983823 |
| test/reward_per_eps            | -12.4      |
| test/steps                     | 220000     |
| train/episodes                 | 22000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.406      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0118    |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.8      |
| train/steps                    | 880000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 550        |
| stats_o/mean                   | 0.20343877 |
| stats_o/std                    | 0.10923574 |
| test/episodes                  | 5510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.56       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00811   |
| test/info_shaping_reward_mean  | -0.0761    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -4.0455666 |
| test/Q_plus_P                  | -4.0455666 |
| test/reward_per_eps            | -17.6      |
| test/steps                     | 220400     |
| train/episodes                 | 22040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.408      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0156    |
| train/info_shaping_reward_mean | -0.0905    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.7      |
| train/steps                    | 881600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 551        |
| stats_o/mean                   | 0.20343857 |
| stats_o/std                    | 0.10922329 |
| test/episodes                  | 5520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.588      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.016     |
| test/info_shaping_reward_mean  | -0.0798    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -3.7464013 |
| test/Q_plus_P                  | -3.7464013 |
| test/reward_per_eps            | -16.5      |
| test/steps                     | 220800     |
| train/episodes                 | 22080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.468      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0162    |
| train/info_shaping_reward_mean | -0.0936    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.3      |
| train/steps                    | 883200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 552        |
| stats_o/mean                   | 0.20343256 |
| stats_o/std                    | 0.10922985 |
| test/episodes                  | 5530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.57       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00589   |
| test/info_shaping_reward_mean  | -0.0802    |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -4.879288  |
| test/Q_plus_P                  | -4.879288  |
| test/reward_per_eps            | -17.2      |
| test/steps                     | 221200     |
| train/episodes                 | 22120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.368      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0132    |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.3      |
| train/steps                    | 884800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 553         |
| stats_o/mean                   | 0.20342635  |
| stats_o/std                    | 0.109214775 |
| test/episodes                  | 5540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.593       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00571    |
| test/info_shaping_reward_mean  | -0.0721     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -4.118267   |
| test/Q_plus_P                  | -4.118267   |
| test/reward_per_eps            | -16.3       |
| test/steps                     | 221600      |
| train/episodes                 | 22160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.371       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0168     |
| train/info_shaping_reward_mean | -0.0954     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.2       |
| train/steps                    | 886400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 554         |
| stats_o/mean                   | 0.20342398  |
| stats_o/std                    | 0.109198645 |
| test/episodes                  | 5550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.545       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0139     |
| test/info_shaping_reward_mean  | -0.0842     |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -4.52233    |
| test/Q_plus_P                  | -4.52233    |
| test/reward_per_eps            | -18.2       |
| test/steps                     | 222000      |
| train/episodes                 | 22200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.431       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0142     |
| train/info_shaping_reward_mean | -0.0881     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.8       |
| train/steps                    | 888000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 555        |
| stats_o/mean                   | 0.2034094  |
| stats_o/std                    | 0.10918352 |
| test/episodes                  | 5560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.28       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00848   |
| test/info_shaping_reward_mean  | -0.111     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.063793  |
| test/Q_plus_P                  | -9.063793  |
| test/reward_per_eps            | -28.8      |
| test/steps                     | 222400     |
| train/episodes                 | 22240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.394      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0137    |
| train/info_shaping_reward_mean | -0.0952    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.2      |
| train/steps                    | 889600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 556         |
| stats_o/mean                   | 0.20340328  |
| stats_o/std                    | 0.109184474 |
| test/episodes                  | 5570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.6         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00835    |
| test/info_shaping_reward_mean  | -0.0637     |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -3.1417933  |
| test/Q_plus_P                  | -3.1417933  |
| test/reward_per_eps            | -16         |
| test/steps                     | 222800      |
| train/episodes                 | 22280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.428       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0167     |
| train/info_shaping_reward_mean | -0.103      |
| train/info_shaping_reward_min  | -0.281      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.9       |
| train/steps                    | 891200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 557        |
| stats_o/mean                   | 0.20339511 |
| stats_o/std                    | 0.10917649 |
| test/episodes                  | 5580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.555      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00513   |
| test/info_shaping_reward_mean  | -0.0744    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -4.008305  |
| test/Q_plus_P                  | -4.008305  |
| test/reward_per_eps            | -17.8      |
| test/steps                     | 223200     |
| train/episodes                 | 22320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.463      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0153    |
| train/info_shaping_reward_mean | -0.092     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.5      |
| train/steps                    | 892800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 558        |
| stats_o/mean                   | 0.20339698 |
| stats_o/std                    | 0.10914959 |
| test/episodes                  | 5590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.627      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.011     |
| test/info_shaping_reward_mean  | -0.0734    |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -3.7925665 |
| test/Q_plus_P                  | -3.7925665 |
| test/reward_per_eps            | -14.9      |
| test/steps                     | 223600     |
| train/episodes                 | 22360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.354      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0138    |
| train/info_shaping_reward_mean | -0.0977    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.9      |
| train/steps                    | 894400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 559         |
| stats_o/mean                   | 0.2033848   |
| stats_o/std                    | 0.109165475 |
| test/episodes                  | 5600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.613       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00663    |
| test/info_shaping_reward_mean  | -0.0655     |
| test/info_shaping_reward_min   | -0.201      |
| test/Q                         | -3.0584915  |
| test/Q_plus_P                  | -3.0584915  |
| test/reward_per_eps            | -15.5       |
| test/steps                     | 224000      |
| train/episodes                 | 22400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.419       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0128     |
| train/info_shaping_reward_mean | -0.0935     |
| train/info_shaping_reward_min  | -0.217      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.2       |
| train/steps                    | 896000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 560        |
| stats_o/mean                   | 0.20338957 |
| stats_o/std                    | 0.1091477  |
| test/episodes                  | 5610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000809  |
| test/info_shaping_reward_mean  | -0.0609    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -2.528843  |
| test/Q_plus_P                  | -2.528843  |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 224400     |
| train/episodes                 | 22440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.462      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0144    |
| train/info_shaping_reward_mean | -0.0978    |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.5      |
| train/steps                    | 897600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 561        |
| stats_o/mean                   | 0.20339753 |
| stats_o/std                    | 0.10911915 |
| test/episodes                  | 5620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00286   |
| test/info_shaping_reward_mean  | -0.06      |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.250591  |
| test/Q_plus_P                  | -2.250591  |
| test/reward_per_eps            | -12        |
| test/steps                     | 224800     |
| train/episodes                 | 22480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.449      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00989   |
| train/info_shaping_reward_mean | -0.0866    |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22        |
| train/steps                    | 899200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 562        |
| stats_o/mean                   | 0.20340388 |
| stats_o/std                    | 0.10907373 |
| test/episodes                  | 5630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00669   |
| test/info_shaping_reward_mean  | -0.0571    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -2.086164  |
| test/Q_plus_P                  | -2.086164  |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 225200     |
| train/episodes                 | 22520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.499      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00931   |
| train/info_shaping_reward_mean | -0.0823    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 900800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.20340309  |
| stats_o/std                    | 0.109074004 |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00848    |
| test/info_shaping_reward_mean  | -0.0605     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.7530355  |
| test/Q_plus_P                  | -2.7530355  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.451       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0111     |
| train/info_shaping_reward_mean | -0.0952     |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.9       |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 564        |
| stats_o/mean                   | 0.20339583 |
| stats_o/std                    | 0.10904118 |
| test/episodes                  | 5650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00857   |
| test/info_shaping_reward_mean  | -0.0584    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.3519156 |
| test/Q_plus_P                  | -2.3519156 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 226000     |
| train/episodes                 | 22600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0123    |
| train/info_shaping_reward_mean | -0.0838    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 904000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 565        |
| stats_o/mean                   | 0.20340021 |
| stats_o/std                    | 0.10901251 |
| test/episodes                  | 5660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00626   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.1098275 |
| test/Q_plus_P                  | -2.1098275 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 226400     |
| train/episodes                 | 22640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.484      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0091    |
| train/info_shaping_reward_mean | -0.087     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 905600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.20340095  |
| stats_o/std                    | 0.108990565 |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00433    |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -2.1093354  |
| test/Q_plus_P                  | -2.1093354  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.463       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0108     |
| train/info_shaping_reward_mean | -0.092      |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.5       |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 567        |
| stats_o/mean                   | 0.20339912 |
| stats_o/std                    | 0.10896778 |
| test/episodes                  | 5680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.61       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00752   |
| test/info_shaping_reward_mean  | -0.0633    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -3.32084   |
| test/Q_plus_P                  | -3.32084   |
| test/reward_per_eps            | -15.6      |
| test/steps                     | 227200     |
| train/episodes                 | 22720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.476      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0105    |
| train/info_shaping_reward_mean | -0.0838    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21        |
| train/steps                    | 908800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 568        |
| stats_o/mean                   | 0.20339684 |
| stats_o/std                    | 0.10894533 |
| test/episodes                  | 5690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.645      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00428   |
| test/info_shaping_reward_mean  | -0.0589    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -2.9344904 |
| test/Q_plus_P                  | -2.9344904 |
| test/reward_per_eps            | -14.2      |
| test/steps                     | 227600     |
| train/episodes                 | 22760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.434      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0125    |
| train/info_shaping_reward_mean | -0.0874    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.6      |
| train/steps                    | 910400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.20339228  |
| stats_o/std                    | 0.108910285 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0102     |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -2.0372438  |
| test/Q_plus_P                  | -2.0372438  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.468       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0128     |
| train/info_shaping_reward_mean | -0.0841     |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.3       |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 570        |
| stats_o/mean                   | 0.20338045 |
| stats_o/std                    | 0.10888784 |
| test/episodes                  | 5710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0155    |
| test/info_shaping_reward_mean  | -0.0606    |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -2.2327955 |
| test/Q_plus_P                  | -2.2327955 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 228400     |
| train/episodes                 | 22840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.526      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00988   |
| train/info_shaping_reward_mean | -0.0798    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 913600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 571         |
| stats_o/mean                   | 0.20338584  |
| stats_o/std                    | 0.108860366 |
| test/episodes                  | 5720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.63        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0104     |
| test/info_shaping_reward_mean  | -0.0698     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -2.3797522  |
| test/Q_plus_P                  | -2.3797522  |
| test/reward_per_eps            | -14.8       |
| test/steps                     | 228800      |
| train/episodes                 | 22880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.5         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.012      |
| train/info_shaping_reward_mean | -0.0819     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20         |
| train/steps                    | 915200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 572        |
| stats_o/mean                   | 0.2033917  |
| stats_o/std                    | 0.10883171 |
| test/episodes                  | 5730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.625      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0101    |
| test/info_shaping_reward_mean  | -0.0619    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -3.737383  |
| test/Q_plus_P                  | -3.737383  |
| test/reward_per_eps            | -15        |
| test/steps                     | 229200     |
| train/episodes                 | 22920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.513      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0116    |
| train/info_shaping_reward_mean | -0.0903    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.5      |
| train/steps                    | 916800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 573         |
| stats_o/mean                   | 0.20339195  |
| stats_o/std                    | 0.108789876 |
| test/episodes                  | 5740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00384    |
| test/info_shaping_reward_mean  | -0.056      |
| test/info_shaping_reward_min   | -0.195      |
| test/Q                         | -2.2149787  |
| test/Q_plus_P                  | -2.2149787  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 229600      |
| train/episodes                 | 22960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.476       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0102     |
| train/info_shaping_reward_mean | -0.0807     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.9       |
| train/steps                    | 918400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 574        |
| stats_o/mean                   | 0.20338711 |
| stats_o/std                    | 0.10875192 |
| test/episodes                  | 5750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0143    |
| test/info_shaping_reward_mean  | -0.0616    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.3946986 |
| test/Q_plus_P                  | -2.3946986 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 230000     |
| train/episodes                 | 23000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.463      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0109    |
| train/info_shaping_reward_mean | -0.0806    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.5      |
| train/steps                    | 920000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 575        |
| stats_o/mean                   | 0.20338383 |
| stats_o/std                    | 0.10872038 |
| test/episodes                  | 5760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.555      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0103    |
| test/info_shaping_reward_mean  | -0.0726    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -3.5998905 |
| test/Q_plus_P                  | -3.5998905 |
| test/reward_per_eps            | -17.8      |
| test/steps                     | 230400     |
| train/episodes                 | 23040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0122    |
| train/info_shaping_reward_mean | -0.0827    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 921600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.20337416  |
| stats_o/std                    | 0.108676806 |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00481    |
| test/info_shaping_reward_mean  | -0.0579     |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -2.0604827  |
| test/Q_plus_P                  | -2.0604827  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.499       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00898    |
| train/info_shaping_reward_mean | -0.0775     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 577        |
| stats_o/mean                   | 0.20335506 |
| stats_o/std                    | 0.10866186 |
| test/episodes                  | 5780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00474   |
| test/info_shaping_reward_mean  | -0.0567    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.7205265 |
| test/Q_plus_P                  | -1.7205265 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 231200     |
| train/episodes                 | 23120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.451      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0101    |
| train/info_shaping_reward_mean | -0.0944    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22        |
| train/steps                    | 924800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 578         |
| stats_o/mean                   | 0.20336008  |
| stats_o/std                    | 0.108640045 |
| test/episodes                  | 5790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.652       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00365    |
| test/info_shaping_reward_mean  | -0.0592     |
| test/info_shaping_reward_min   | -0.208      |
| test/Q                         | -2.3871126  |
| test/Q_plus_P                  | -2.3871126  |
| test/reward_per_eps            | -13.9       |
| test/steps                     | 231600      |
| train/episodes                 | 23160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.471       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00771    |
| train/info_shaping_reward_mean | -0.0949     |
| train/info_shaping_reward_min  | -0.272      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.1       |
| train/steps                    | 926400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 579        |
| stats_o/mean                   | 0.20336415 |
| stats_o/std                    | 0.10861254 |
| test/episodes                  | 5800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00681   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.91334   |
| test/Q_plus_P                  | -1.91334   |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 232000     |
| train/episodes                 | 23200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.518      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0102    |
| train/info_shaping_reward_mean | -0.084     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 928000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 580        |
| stats_o/mean                   | 0.20335418 |
| stats_o/std                    | 0.10859299 |
| test/episodes                  | 5810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00608   |
| test/info_shaping_reward_mean  | -0.0547    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.8802176 |
| test/Q_plus_P                  | -1.8802176 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 232400     |
| train/episodes                 | 23240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.514      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00978   |
| train/info_shaping_reward_mean | -0.0851    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 929600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 581        |
| stats_o/mean                   | 0.2033493  |
| stats_o/std                    | 0.10853526 |
| test/episodes                  | 5820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00785   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.1067476 |
| test/Q_plus_P                  | -2.1067476 |
| test/reward_per_eps            | -11        |
| test/steps                     | 232800     |
| train/episodes                 | 23280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0107    |
| train/info_shaping_reward_mean | -0.0795    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 931200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 582        |
| stats_o/mean                   | 0.20333739 |
| stats_o/std                    | 0.10853593 |
| test/episodes                  | 5830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00338   |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -2.139294  |
| test/Q_plus_P                  | -2.139294  |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 233200     |
| train/episodes                 | 23320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.463      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0157    |
| train/info_shaping_reward_mean | -0.0901    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.5      |
| train/steps                    | 932800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 583        |
| stats_o/mean                   | 0.20333073 |
| stats_o/std                    | 0.10850037 |
| test/episodes                  | 5840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00527   |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.8433652 |
| test/Q_plus_P                  | -1.8433652 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 233600     |
| train/episodes                 | 23360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00949   |
| train/info_shaping_reward_mean | -0.0855    |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 934400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 584        |
| stats_o/mean                   | 0.20332542 |
| stats_o/std                    | 0.10845204 |
| test/episodes                  | 5850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00273   |
| test/info_shaping_reward_mean  | -0.0563    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.8072575 |
| test/Q_plus_P                  | -1.8072575 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 234000     |
| train/episodes                 | 23400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.456      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00986   |
| train/info_shaping_reward_mean | -0.0841    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 936000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 585        |
| stats_o/mean                   | 0.20332944 |
| stats_o/std                    | 0.10842103 |
| test/episodes                  | 5860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000618  |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.8459065 |
| test/Q_plus_P                  | -1.8459065 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 234400     |
| train/episodes                 | 23440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00853   |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 937600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 586        |
| stats_o/mean                   | 0.20331734 |
| stats_o/std                    | 0.1084151  |
| test/episodes                  | 5870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00563   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.108755  |
| test/Q_plus_P                  | -2.108755  |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 234800     |
| train/episodes                 | 23480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00884   |
| train/info_shaping_reward_mean | -0.0781    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 939200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 587        |
| stats_o/mean                   | 0.20331328 |
| stats_o/std                    | 0.10840516 |
| test/episodes                  | 5880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00296   |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -1.9023767 |
| test/Q_plus_P                  | -1.9023767 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 235200     |
| train/episodes                 | 23520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.518      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0118    |
| train/info_shaping_reward_mean | -0.0827    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 940800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 588        |
| stats_o/mean                   | 0.20331421 |
| stats_o/std                    | 0.10839848 |
| test/episodes                  | 5890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00596   |
| test/info_shaping_reward_mean  | -0.0685    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -3.703394  |
| test/Q_plus_P                  | -3.703394  |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 235600     |
| train/episodes                 | 23560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.544      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0133    |
| train/info_shaping_reward_mean | -0.0911    |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 942400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 589        |
| stats_o/mean                   | 0.20331736 |
| stats_o/std                    | 0.10836001 |
| test/episodes                  | 5900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0045    |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.0712605 |
| test/Q_plus_P                  | -2.0712605 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 236000     |
| train/episodes                 | 23600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.538      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00823   |
| train/info_shaping_reward_mean | -0.0831    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 944000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 590        |
| stats_o/mean                   | 0.20331398 |
| stats_o/std                    | 0.10833485 |
| test/episodes                  | 5910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.595      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00384   |
| test/info_shaping_reward_mean  | -0.0641    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -3.4303222 |
| test/Q_plus_P                  | -3.4303222 |
| test/reward_per_eps            | -16.2      |
| test/steps                     | 236400     |
| train/episodes                 | 23640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.472      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0131    |
| train/info_shaping_reward_mean | -0.0845    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 945600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 591        |
| stats_o/mean                   | 0.20330313 |
| stats_o/std                    | 0.10834628 |
| test/episodes                  | 5920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00396   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -2.579312  |
| test/Q_plus_P                  | -2.579312  |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 236800     |
| train/episodes                 | 23680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.416      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0123    |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.267     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.4      |
| train/steps                    | 947200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 592        |
| stats_o/mean                   | 0.20330632 |
| stats_o/std                    | 0.10832055 |
| test/episodes                  | 5930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00452   |
| test/info_shaping_reward_mean  | -0.053     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.8932163 |
| test/Q_plus_P                  | -1.8932163 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 237200     |
| train/episodes                 | 23720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.525      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00915   |
| train/info_shaping_reward_mean | -0.078     |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 948800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.20330888  |
| stats_o/std                    | 0.108273126 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00619    |
| test/info_shaping_reward_mean  | -0.0497     |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -1.487787   |
| test/Q_plus_P                  | -1.487787   |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.451       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00966    |
| train/info_shaping_reward_mean | -0.0816     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22         |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 594        |
| stats_o/mean                   | 0.20331594 |
| stats_o/std                    | 0.10826629 |
| test/episodes                  | 5950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00916   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.577254  |
| test/Q_plus_P                  | -1.577254  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 238000     |
| train/episodes                 | 23800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.493      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0116    |
| train/info_shaping_reward_mean | -0.0919    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.3      |
| train/steps                    | 952000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 595        |
| stats_o/mean                   | 0.20331627 |
| stats_o/std                    | 0.10823774 |
| test/episodes                  | 5960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00285   |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -1.672155  |
| test/Q_plus_P                  | -1.672155  |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 238400     |
| train/episodes                 | 23840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.509      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0133    |
| train/info_shaping_reward_mean | -0.0831    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 953600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 596         |
| stats_o/mean                   | 0.20332779  |
| stats_o/std                    | 0.108194396 |
| test/episodes                  | 5970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0116     |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -2.091234   |
| test/Q_plus_P                  | -2.091234   |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 238800      |
| train/episodes                 | 23880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.504       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0137     |
| train/info_shaping_reward_mean | -0.0838     |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.8       |
| train/steps                    | 955200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 597        |
| stats_o/mean                   | 0.20333427 |
| stats_o/std                    | 0.10815671 |
| test/episodes                  | 5980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00503   |
| test/info_shaping_reward_mean  | -0.0598    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -2.1326113 |
| test/Q_plus_P                  | -2.1326113 |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 239200     |
| train/episodes                 | 23920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00756   |
| train/info_shaping_reward_mean | -0.0744    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 956800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.20332828  |
| stats_o/std                    | 0.108144216 |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0106     |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -1.7061217  |
| test/Q_plus_P                  | -1.7061217  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.476       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00861    |
| train/info_shaping_reward_mean | -0.0943     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21         |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.2033246  |
| stats_o/std                    | 0.10812152 |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00601   |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -1.682558  |
| test/Q_plus_P                  | -1.682558  |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.489      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00798   |
| train/info_shaping_reward_mean | -0.0877    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
