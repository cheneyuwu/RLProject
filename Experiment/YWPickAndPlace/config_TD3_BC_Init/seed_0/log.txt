Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPickAndPlace/config_TD3_BC_Init/seed_0
epoch: 19 policy initialization loss: 0.009072628803551197
epoch: 39 policy initialization loss: 0.0027932380326092243
epoch: 59 policy initialization loss: 0.0019539729692041874
epoch: 79 policy initialization loss: 0.003098755143582821
epoch: 99 policy initialization loss: 0.0015231689903885126
epoch: 119 policy initialization loss: 0.00022428555530495942
epoch: 139 policy initialization loss: 0.0002980149583891034
epoch: 159 policy initialization loss: 0.0009481763117946684
epoch: 179 policy initialization loss: 0.00020154309459030628
epoch: 199 policy initialization loss: 0.00022366567282006145
epoch: 219 policy initialization loss: 0.0003404010785743594
epoch: 239 policy initialization loss: 0.00020315460278652608
epoch: 259 policy initialization loss: 0.0003234128816984594
epoch: 279 policy initialization loss: 0.0004491339495871216
epoch: 299 policy initialization loss: 0.0013703317381441593
epoch: 319 policy initialization loss: 0.001447778893634677
epoch: 339 policy initialization loss: 8.077144593698904e-05
epoch: 359 policy initialization loss: 0.00035016145557165146
epoch: 379 policy initialization loss: 0.00036939667188562453
epoch: 399 policy initialization loss: 0.00011940201511606574
epoch: 419 policy initialization loss: 3.291864777565934e-05
epoch: 439 policy initialization loss: 0.00017966859741136432
epoch: 459 policy initialization loss: 9.962548210751265e-05
epoch: 479 policy initialization loss: 0.00018910941435024142
epoch: 499 policy initialization loss: 8.015611820155755e-05
epoch: 519 policy initialization loss: 0.0005100583657622337
epoch: 539 policy initialization loss: 0.0007411629194393754
epoch: 559 policy initialization loss: 2.7710735594155267e-05
epoch: 579 policy initialization loss: 0.00034100739867426455
epoch: 599 policy initialization loss: 3.789251786656678e-05
epoch: 619 policy initialization loss: 2.879256135202013e-05
epoch: 639 policy initialization loss: 0.00025846101925708354
epoch: 659 policy initialization loss: 8.239594899350777e-05
epoch: 679 policy initialization loss: 6.448788190027699e-05
epoch: 699 policy initialization loss: 0.00012407988833729178
epoch: 719 policy initialization loss: 0.0018931333906948566
epoch: 739 policy initialization loss: 7.821244071237743e-05
epoch: 759 policy initialization loss: 0.00019636916113086045
epoch: 779 policy initialization loss: 3.149216354358941e-05
epoch: 799 policy initialization loss: 3.3858941606013104e-05
epoch: 819 policy initialization loss: 0.00021902957814745605
epoch: 839 policy initialization loss: 2.4377339286729693e-05
epoch: 859 policy initialization loss: 0.0003225173568353057
epoch: 879 policy initialization loss: 8.065534348133951e-05
epoch: 899 policy initialization loss: 0.0013467362150549889
epoch: 919 policy initialization loss: 0.00024084726464934647
epoch: 939 policy initialization loss: 0.00022529363923240453
epoch: 959 policy initialization loss: 2.6581941710901447e-05
epoch: 979 policy initialization loss: 4.9980801122728735e-05
epoch: 999 policy initialization loss: 1.981871173484251e-05
epoch: 1019 policy initialization loss: 0.00010941054642898962
epoch: 1039 policy initialization loss: 2.9870119760744274e-05
epoch: 1059 policy initialization loss: 6.082196341594681e-05
epoch: 1079 policy initialization loss: 9.687604324426502e-05
epoch: 1099 policy initialization loss: 2.7371093892725185e-05
epoch: 1119 policy initialization loss: 0.00010379716695751995
epoch: 1139 policy initialization loss: 0.00018151698168367147
epoch: 1159 policy initialization loss: 1.478660215070704e-05
epoch: 1179 policy initialization loss: 4.822463233722374e-05
epoch: 1199 policy initialization loss: 3.19556929753162e-05
epoch: 1219 policy initialization loss: 1.858996256487444e-05
epoch: 1239 policy initialization loss: 1.5744557458674535e-05
epoch: 1259 policy initialization loss: 9.014285751618445e-05
epoch: 1279 policy initialization loss: 5.851912283105776e-05
epoch: 1299 policy initialization loss: 1.5701398297096603e-05
epoch: 1319 policy initialization loss: 2.5846764401649125e-05
epoch: 1339 policy initialization loss: 5.633124601445161e-05
epoch: 1359 policy initialization loss: 0.000410528271459043
epoch: 1379 policy initialization loss: 3.469720104476437e-05
epoch: 1399 policy initialization loss: 1.2345573850325309e-05
epoch: 1419 policy initialization loss: 0.0001412593701388687
epoch: 1439 policy initialization loss: 4.5310414861887693e-05
epoch: 1459 policy initialization loss: 9.480172593612224e-06
epoch: 1479 policy initialization loss: 2.2979940695222467e-05
epoch: 1499 policy initialization loss: 2.2583233658224344e-05
epoch: 1519 policy initialization loss: 3.637062036432326e-05
epoch: 1539 policy initialization loss: 2.0981329726055264e-05
epoch: 1559 policy initialization loss: 2.143693927791901e-05
epoch: 1579 policy initialization loss: 1.2294005500734784e-05
epoch: 1599 policy initialization loss: 0.00015953689580783248
epoch: 1619 policy initialization loss: 1.8121831089956686e-05
epoch: 1639 policy initialization loss: 8.416637865593657e-05
epoch: 1659 policy initialization loss: 7.37710070097819e-05
epoch: 1679 policy initialization loss: 0.00027222541393712163
epoch: 1699 policy initialization loss: 1.2566191799123771e-05
epoch: 1719 policy initialization loss: 1.3582071915152483e-05
epoch: 1739 policy initialization loss: 1.2600535228557419e-05
epoch: 1759 policy initialization loss: 0.00019514209998305887
epoch: 1779 policy initialization loss: 1.4833476598141715e-05
epoch: 1799 policy initialization loss: 8.544961929146666e-06
epoch: 1819 policy initialization loss: 1.5230649296427146e-05
epoch: 1839 policy initialization loss: 0.00012325534771662205
epoch: 1859 policy initialization loss: 0.00015929342771414667
epoch: 1879 policy initialization loss: 7.692344297538511e-06
epoch: 1899 policy initialization loss: 1.1342461220920086e-05
epoch: 1919 policy initialization loss: 0.0005806478438898921
epoch: 1939 policy initialization loss: 1.6987067283480428e-05
epoch: 1959 policy initialization loss: 8.300097761093639e-06
epoch: 1979 policy initialization loss: 2.0138852050877176e-05
epoch: 1999 policy initialization loss: 0.00017279325402341783
Saving initial policy.
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.2037596   |
| stats_o/std                    | 0.046941616 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3847709  |
| test/Q_plus_P                  | -1.3847709  |
| test/reward_per_eps            | -40         |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 1          |
| stats_o/mean                   | 0.20257205 |
| stats_o/std                    | 0.0471116  |
| test/episodes                  | 20         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.7393209 |
| test/Q_plus_P                  | -1.7393209 |
| test/reward_per_eps            | -40        |
| test/steps                     | 800        |
| train/episodes                 | 80         |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 3200       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 2           |
| stats_o/mean                   | 0.20351021  |
| stats_o/std                    | 0.046585217 |
| test/episodes                  | 30          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.1247241  |
| test/Q_plus_P                  | -2.1247241  |
| test/reward_per_eps            | -40         |
| test/steps                     | 1200        |
| train/episodes                 | 120         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 4800        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 3           |
| stats_o/mean                   | 0.20435612  |
| stats_o/std                    | 0.045614425 |
| test/episodes                  | 40          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.4595704  |
| test/Q_plus_P                  | -2.4595704  |
| test/reward_per_eps            | -40         |
| test/steps                     | 1600        |
| train/episodes                 | 160         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 6400        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 4          |
| stats_o/mean                   | 0.2052657  |
| stats_o/std                    | 0.04529078 |
| test/episodes                  | 50         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.8424811 |
| test/Q_plus_P                  | -2.8424811 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2000       |
| train/episodes                 | 200        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 8000       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 5           |
| stats_o/mean                   | 0.20496547  |
| stats_o/std                    | 0.044832584 |
| test/episodes                  | 60          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -3.2035127  |
| test/Q_plus_P                  | -3.2035127  |
| test/reward_per_eps            | -40         |
| test/steps                     | 2400        |
| train/episodes                 | 240         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 9600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 6           |
| stats_o/mean                   | 0.20502785  |
| stats_o/std                    | 0.044471465 |
| test/episodes                  | 70          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -3.574375   |
| test/Q_plus_P                  | -3.574375   |
| test/reward_per_eps            | -40         |
| test/steps                     | 2800        |
| train/episodes                 | 280         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 11200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 7          |
| stats_o/mean                   | 0.20552297 |
| stats_o/std                    | 0.04416895 |
| test/episodes                  | 80         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.9088495 |
| test/Q_plus_P                  | -3.9088495 |
| test/reward_per_eps            | -40        |
| test/steps                     | 3200       |
| train/episodes                 | 320        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 12800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 8           |
| stats_o/mean                   | 0.2052592   |
| stats_o/std                    | 0.043906614 |
| test/episodes                  | 90          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -4.3236957  |
| test/Q_plus_P                  | -4.3236957  |
| test/reward_per_eps            | -40         |
| test/steps                     | 3600        |
| train/episodes                 | 360         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 14400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 9          |
| stats_o/mean                   | 0.20492852 |
| stats_o/std                    | 0.04375347 |
| test/episodes                  | 100        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -4.7167253 |
| test/Q_plus_P                  | -4.7167253 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4000       |
| train/episodes                 | 400        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 16000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 10          |
| stats_o/mean                   | 0.20440692  |
| stats_o/std                    | 0.043587696 |
| test/episodes                  | 110         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -5.115985   |
| test/Q_plus_P                  | -5.115985   |
| test/reward_per_eps            | -40         |
| test/steps                     | 4400        |
| train/episodes                 | 440         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 17600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 11         |
| stats_o/mean                   | 0.20397787 |
| stats_o/std                    | 0.04343484 |
| test/episodes                  | 120        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -5.4945393 |
| test/Q_plus_P                  | -5.4945393 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4800       |
| train/episodes                 | 480        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 19200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.20405039  |
| stats_o/std                    | 0.043318044 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -5.797437   |
| test/Q_plus_P                  | -5.797437   |
| test/reward_per_eps            | -40         |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 13         |
| stats_o/mean                   | 0.20390944 |
| stats_o/std                    | 0.04324234 |
| test/episodes                  | 140        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -6.184519  |
| test/Q_plus_P                  | -6.184519  |
| test/reward_per_eps            | -40        |
| test/steps                     | 5600       |
| train/episodes                 | 560        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 22400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 14         |
| stats_o/mean                   | 0.20408256 |
| stats_o/std                    | 0.0431415  |
| test/episodes                  | 150        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -6.525848  |
| test/Q_plus_P                  | -6.525848  |
| test/reward_per_eps            | -40        |
| test/steps                     | 6000       |
| train/episodes                 | 600        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 24000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 15         |
| stats_o/mean                   | 0.20388252 |
| stats_o/std                    | 0.04298623 |
| test/episodes                  | 160        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -6.9515953 |
| test/Q_plus_P                  | -6.9515953 |
| test/reward_per_eps            | -40        |
| test/steps                     | 6400       |
| train/episodes                 | 640        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 25600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 16          |
| stats_o/mean                   | 0.2038247   |
| stats_o/std                    | 0.042890567 |
| test/episodes                  | 170         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -7.3002734  |
| test/Q_plus_P                  | -7.3002734  |
| test/reward_per_eps            | -40         |
| test/steps                     | 6800        |
| train/episodes                 | 680         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 27200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 17          |
| stats_o/mean                   | 0.20393662  |
| stats_o/std                    | 0.042790312 |
| test/episodes                  | 180         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -7.6523046  |
| test/Q_plus_P                  | -7.6523046  |
| test/reward_per_eps            | -40         |
| test/steps                     | 7200        |
| train/episodes                 | 720         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 28800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 18          |
| stats_o/mean                   | 0.2040179   |
| stats_o/std                    | 0.042767793 |
| test/episodes                  | 190         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -8.039619   |
| test/Q_plus_P                  | -8.039619   |
| test/reward_per_eps            | -40         |
| test/steps                     | 7600        |
| train/episodes                 | 760         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 30400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.20404926 |
| stats_o/std                    | 0.04275232 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -8.4411    |
| test/Q_plus_P                  | -8.4411    |
| test/reward_per_eps            | -40        |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 20          |
| stats_o/mean                   | 0.20399089  |
| stats_o/std                    | 0.042666312 |
| test/episodes                  | 210         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -8.819502   |
| test/Q_plus_P                  | -8.819502   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8400        |
| train/episodes                 | 840         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 33600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 21          |
| stats_o/mean                   | 0.20400693  |
| stats_o/std                    | 0.042573404 |
| test/episodes                  | 220         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -9.104886   |
| test/Q_plus_P                  | -9.104886   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8800        |
| train/episodes                 | 880         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 35200       |
------------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 22        |
| stats_o/mean                   | 0.2039932 |
| stats_o/std                    | 0.0425438 |
| test/episodes                  | 230       |
| test/info_is_success_max       | 0         |
| test/info_is_success_mean      | 0         |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.173    |
| test/info_shaping_reward_mean  | -0.173    |
| test/info_shaping_reward_min   | -0.173    |
| test/Q                         | -9.461178 |
| test/Q_plus_P                  | -9.461178 |
| test/reward_per_eps            | -40       |
| test/steps                     | 9200      |
| train/episodes                 | 920       |
| train/info_is_success_max      | 0         |
| train/info_is_success_mean     | 0         |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.173    |
| train/info_shaping_reward_mean | -0.173    |
| train/info_shaping_reward_min  | -0.173    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -40       |
| train/steps                    | 36800     |
----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 23          |
| stats_o/mean                   | 0.20388106  |
| stats_o/std                    | 0.042455573 |
| test/episodes                  | 240         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -9.801941   |
| test/Q_plus_P                  | -9.801941   |
| test/reward_per_eps            | -40         |
| test/steps                     | 9600        |
| train/episodes                 | 960         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 38400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 24         |
| stats_o/mean                   | 0.20387375 |
| stats_o/std                    | 0.04239336 |
| test/episodes                  | 250        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -10.188198 |
| test/Q_plus_P                  | -10.188198 |
| test/reward_per_eps            | -40        |
| test/steps                     | 10000      |
| train/episodes                 | 1000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 40000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 25         |
| stats_o/mean                   | 0.20377338 |
| stats_o/std                    | 0.04232788 |
| test/episodes                  | 260        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -10.565315 |
| test/Q_plus_P                  | -10.565315 |
| test/reward_per_eps            | -40        |
| test/steps                     | 10400      |
| train/episodes                 | 1040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 41600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 26         |
| stats_o/mean                   | 0.203791   |
| stats_o/std                    | 0.04227103 |
| test/episodes                  | 270        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -10.937333 |
| test/Q_plus_P                  | -10.937333 |
| test/reward_per_eps            | -40        |
| test/steps                     | 10800      |
| train/episodes                 | 1080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 43200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 27         |
| stats_o/mean                   | 0.203863   |
| stats_o/std                    | 0.04217811 |
| test/episodes                  | 280        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -11.186643 |
| test/Q_plus_P                  | -11.186643 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11200      |
| train/episodes                 | 1120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 44800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 28          |
| stats_o/mean                   | 0.20376007  |
| stats_o/std                    | 0.042129505 |
| test/episodes                  | 290         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -11.563325  |
| test/Q_plus_P                  | -11.563325  |
| test/reward_per_eps            | -40         |
| test/steps                     | 11600       |
| train/episodes                 | 1160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 46400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.20375341 |
| stats_o/std                    | 0.04208387 |
| test/episodes                  | 300        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -11.824161 |
| test/Q_plus_P                  | -11.824161 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 30          |
| stats_o/mean                   | 0.20384194  |
| stats_o/std                    | 0.041994534 |
| test/episodes                  | 310         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -12.186903  |
| test/Q_plus_P                  | -12.186903  |
| test/reward_per_eps            | -40         |
| test/steps                     | 12400       |
| train/episodes                 | 1240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 49600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 31          |
| stats_o/mean                   | 0.20392022  |
| stats_o/std                    | 0.041960746 |
| test/episodes                  | 320         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -12.5085745 |
| test/Q_plus_P                  | -12.5085745 |
| test/reward_per_eps            | -40         |
| test/steps                     | 12800       |
| train/episodes                 | 1280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 51200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 32         |
| stats_o/mean                   | 0.20394892 |
| stats_o/std                    | 0.04190589 |
| test/episodes                  | 330        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -12.853868 |
| test/Q_plus_P                  | -12.853868 |
| test/reward_per_eps            | -40        |
| test/steps                     | 13200      |
| train/episodes                 | 1320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 52800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 33          |
| stats_o/mean                   | 0.20396551  |
| stats_o/std                    | 0.041844152 |
| test/episodes                  | 340         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -13.207058  |
| test/Q_plus_P                  | -13.207058  |
| test/reward_per_eps            | -40         |
| test/steps                     | 13600       |
| train/episodes                 | 1360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 54400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 34          |
| stats_o/mean                   | 0.20381981  |
| stats_o/std                    | 0.041775446 |
| test/episodes                  | 350         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -13.496581  |
| test/Q_plus_P                  | -13.496581  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14000       |
| train/episodes                 | 1400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 56000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 35          |
| stats_o/mean                   | 0.2037764   |
| stats_o/std                    | 0.041715097 |
| test/episodes                  | 360         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -13.812942  |
| test/Q_plus_P                  | -13.812942  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14400       |
| train/episodes                 | 1440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 57600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 36          |
| stats_o/mean                   | 0.20379582  |
| stats_o/std                    | 0.041675862 |
| test/episodes                  | 370         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -14.147772  |
| test/Q_plus_P                  | -14.147772  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14800       |
| train/episodes                 | 1480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 59200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.20377235  |
| stats_o/std                    | 0.041663215 |
| test/episodes                  | 380         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -14.481431  |
| test/Q_plus_P                  | -14.481431  |
| test/reward_per_eps            | -40         |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 38         |
| stats_o/mean                   | 0.20374218 |
| stats_o/std                    | 0.04161852 |
| test/episodes                  | 390        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -14.77022  |
| test/Q_plus_P                  | -14.77022  |
| test/reward_per_eps            | -40        |
| test/steps                     | 15600      |
| train/episodes                 | 1560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 62400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 39         |
| stats_o/mean                   | 0.20381346 |
| stats_o/std                    | 0.04160908 |
| test/episodes                  | 400        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -15.005119 |
| test/Q_plus_P                  | -15.005119 |
| test/reward_per_eps            | -40        |
| test/steps                     | 16000      |
| train/episodes                 | 1600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 64000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 40         |
| stats_o/mean                   | 0.20376389 |
| stats_o/std                    | 0.04165346 |
| test/episodes                  | 410        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -15.418937 |
| test/Q_plus_P                  | -15.418937 |
| test/reward_per_eps            | -40        |
| test/steps                     | 16400      |
| train/episodes                 | 1640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 65600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.20374756  |
| stats_o/std                    | 0.042586576 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -15.625132  |
| test/Q_plus_P                  | -15.625132  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 42          |
| stats_o/mean                   | 0.203806    |
| stats_o/std                    | 0.044305176 |
| test/episodes                  | 430         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.191      |
| test/Q                         | -15.763627  |
| test/Q_plus_P                  | -15.763627  |
| test/reward_per_eps            | -40         |
| test/steps                     | 17200       |
| train/episodes                 | 1720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 68800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 43         |
| stats_o/mean                   | 0.20390424 |
| stats_o/std                    | 0.04666359 |
| test/episodes                  | 440        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -15.935648 |
| test/Q_plus_P                  | -15.935648 |
| test/reward_per_eps            | -40        |
| test/steps                     | 17600      |
| train/episodes                 | 1760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 70400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 44          |
| stats_o/mean                   | 0.2039558   |
| stats_o/std                    | 0.048169326 |
| test/episodes                  | 450         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -15.88218   |
| test/Q_plus_P                  | -15.88218   |
| test/reward_per_eps            | -40         |
| test/steps                     | 18000       |
| train/episodes                 | 1800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 72000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.20397228  |
| stats_o/std                    | 0.051047496 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.175      |
| test/info_shaping_reward_min   | -0.198      |
| test/Q                         | -15.72913   |
| test/Q_plus_P                  | -15.72913   |
| test/reward_per_eps            | -40         |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.192      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 46          |
| stats_o/mean                   | 0.20392948  |
| stats_o/std                    | 0.052997813 |
| test/episodes                  | 470         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -15.906558  |
| test/Q_plus_P                  | -15.906558  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18800       |
| train/episodes                 | 1880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 75200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 47         |
| stats_o/mean                   | 0.20396589 |
| stats_o/std                    | 0.05341458 |
| test/episodes                  | 480        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -15.046766 |
| test/Q_plus_P                  | -15.046766 |
| test/reward_per_eps            | -40        |
| test/steps                     | 19200      |
| train/episodes                 | 1920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 76800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 48          |
| stats_o/mean                   | 0.20401995  |
| stats_o/std                    | 0.053820696 |
| test/episodes                  | 490         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -15.679634  |
| test/Q_plus_P                  | -15.679634  |
| test/reward_per_eps            | -40         |
| test/steps                     | 19600       |
| train/episodes                 | 1960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 78400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 49         |
| stats_o/mean                   | 0.20401956 |
| stats_o/std                    | 0.0546187  |
| test/episodes                  | 500        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -15.375457 |
| test/Q_plus_P                  | -15.375457 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20000      |
| train/episodes                 | 2000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 80000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.20387515 |
| stats_o/std                    | 0.05608736 |
| test/episodes                  | 510        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -16.054768 |
| test/Q_plus_P                  | -16.054768 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.274     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 51          |
| stats_o/mean                   | 0.20387901  |
| stats_o/std                    | 0.056315504 |
| test/episodes                  | 520         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -15.920762  |
| test/Q_plus_P                  | -15.920762  |
| test/reward_per_eps            | -40         |
| test/steps                     | 20800       |
| train/episodes                 | 2080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 83200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.20381707 |
| stats_o/std                    | 0.05656554 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -15.713733 |
| test/Q_plus_P                  | -15.713733 |
| test/reward_per_eps            | -40        |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.20378257  |
| stats_o/std                    | 0.056988202 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -15.777588  |
| test/Q_plus_P                  | -15.777588  |
| test/reward_per_eps            | -40         |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 54         |
| stats_o/mean                   | 0.20377116 |
| stats_o/std                    | 0.05743544 |
| test/episodes                  | 550        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -15.605321 |
| test/Q_plus_P                  | -15.605321 |
| test/reward_per_eps            | -40        |
| test/steps                     | 22000      |
| train/episodes                 | 2200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 88000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 55          |
| stats_o/mean                   | 0.20376988  |
| stats_o/std                    | 0.057531115 |
| test/episodes                  | 560         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -15.329499  |
| test/Q_plus_P                  | -15.329499  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22400       |
| train/episodes                 | 2240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 89600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 56          |
| stats_o/mean                   | 0.20372826  |
| stats_o/std                    | 0.057631675 |
| test/episodes                  | 570         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -15.211584  |
| test/Q_plus_P                  | -15.211584  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22800       |
| train/episodes                 | 2280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 91200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 57         |
| stats_o/mean                   | 0.2037575  |
| stats_o/std                    | 0.05770927 |
| test/episodes                  | 580        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -16.808796 |
| test/Q_plus_P                  | -16.808796 |
| test/reward_per_eps            | -40        |
| test/steps                     | 23200      |
| train/episodes                 | 2320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 92800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 58         |
| stats_o/mean                   | 0.20365117 |
| stats_o/std                    | 0.05800983 |
| test/episodes                  | 590        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -16.15536  |
| test/Q_plus_P                  | -16.15536  |
| test/reward_per_eps            | -40        |
| test/steps                     | 23600      |
| train/episodes                 | 2360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 94400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 59          |
| stats_o/mean                   | 0.20364524  |
| stats_o/std                    | 0.058148213 |
| test/episodes                  | 600         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -17.039879  |
| test/Q_plus_P                  | -17.039879  |
| test/reward_per_eps            | -40         |
| test/steps                     | 24000       |
| train/episodes                 | 2400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 96000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 60         |
| stats_o/mean                   | 0.20367658 |
| stats_o/std                    | 0.058118   |
| test/episodes                  | 610        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -16.555382 |
| test/Q_plus_P                  | -16.555382 |
| test/reward_per_eps            | -40        |
| test/steps                     | 24400      |
| train/episodes                 | 2440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 97600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 61         |
| stats_o/mean                   | 0.20369829 |
| stats_o/std                    | 0.05830823 |
| test/episodes                  | 620        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -16.629211 |
| test/Q_plus_P                  | -16.629211 |
| test/reward_per_eps            | -40        |
| test/steps                     | 24800      |
| train/episodes                 | 2480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 99200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 62         |
| stats_o/mean                   | 0.20367607 |
| stats_o/std                    | 0.05835097 |
| test/episodes                  | 630        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -16.440712 |
| test/Q_plus_P                  | -16.440712 |
| test/reward_per_eps            | -40        |
| test/steps                     | 25200      |
| train/episodes                 | 2520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 100800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 63         |
| stats_o/mean                   | 0.20369595 |
| stats_o/std                    | 0.05837101 |
| test/episodes                  | 640        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -16.836784 |
| test/Q_plus_P                  | -16.836784 |
| test/reward_per_eps            | -40        |
| test/steps                     | 25600      |
| train/episodes                 | 2560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 102400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.20362495 |
| stats_o/std                    | 0.05861802 |
| test/episodes                  | 650        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -17.584654 |
| test/Q_plus_P                  | -17.584654 |
| test/reward_per_eps            | -40        |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 65          |
| stats_o/mean                   | 0.20358448  |
| stats_o/std                    | 0.058710184 |
| test/episodes                  | 660         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -17.47036   |
| test/Q_plus_P                  | -17.47036   |
| test/reward_per_eps            | -40         |
| test/steps                     | 26400       |
| train/episodes                 | 2640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 105600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 66         |
| stats_o/mean                   | 0.20355369 |
| stats_o/std                    | 0.05878879 |
| test/episodes                  | 670        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -17.001053 |
| test/Q_plus_P                  | -17.001053 |
| test/reward_per_eps            | -40        |
| test/steps                     | 26800      |
| train/episodes                 | 2680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 107200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 67          |
| stats_o/mean                   | 0.20357218  |
| stats_o/std                    | 0.059319515 |
| test/episodes                  | 680         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -17.88064   |
| test/Q_plus_P                  | -17.88064   |
| test/reward_per_eps            | -40         |
| test/steps                     | 27200       |
| train/episodes                 | 2720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 108800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 68         |
| stats_o/mean                   | 0.2035945  |
| stats_o/std                    | 0.05928589 |
| test/episodes                  | 690        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -17.611612 |
| test/Q_plus_P                  | -17.611612 |
| test/reward_per_eps            | -40        |
| test/steps                     | 27600      |
| train/episodes                 | 2760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 110400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 69          |
| stats_o/mean                   | 0.20362242  |
| stats_o/std                    | 0.059335634 |
| test/episodes                  | 700         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.177      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -18.386509  |
| test/Q_plus_P                  | -18.386509  |
| test/reward_per_eps            | -40         |
| test/steps                     | 28000       |
| train/episodes                 | 2800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 112000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 70          |
| stats_o/mean                   | 0.20359558  |
| stats_o/std                    | 0.059421528 |
| test/episodes                  | 710         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -17.8376    |
| test/Q_plus_P                  | -17.8376    |
| test/reward_per_eps            | -40         |
| test/steps                     | 28400       |
| train/episodes                 | 2840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 113600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 71         |
| stats_o/mean                   | 0.20355032 |
| stats_o/std                    | 0.05941309 |
| test/episodes                  | 720        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -18.867535 |
| test/Q_plus_P                  | -18.867535 |
| test/reward_per_eps            | -40        |
| test/steps                     | 28800      |
| train/episodes                 | 2880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 115200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 72          |
| stats_o/mean                   | 0.20354225  |
| stats_o/std                    | 0.059410647 |
| test/episodes                  | 730         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -18.390562  |
| test/Q_plus_P                  | -18.390562  |
| test/reward_per_eps            | -40         |
| test/steps                     | 29200       |
| train/episodes                 | 2920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 116800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.20349991 |
| stats_o/std                    | 0.06002104 |
| test/episodes                  | 740        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -18.529076 |
| test/Q_plus_P                  | -18.529076 |
| test/reward_per_eps            | -40        |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 74          |
| stats_o/mean                   | 0.20355436  |
| stats_o/std                    | 0.060565833 |
| test/episodes                  | 750         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.31225   |
| test/Q_plus_P                  | -19.31225   |
| test/reward_per_eps            | -40         |
| test/steps                     | 30000       |
| train/episodes                 | 3000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 120000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 75          |
| stats_o/mean                   | 0.20351425  |
| stats_o/std                    | 0.060687426 |
| test/episodes                  | 760         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.388117  |
| test/Q_plus_P                  | -19.388117  |
| test/reward_per_eps            | -40         |
| test/steps                     | 30400       |
| train/episodes                 | 3040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 121600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 76         |
| stats_o/mean                   | 0.2035406  |
| stats_o/std                    | 0.06064873 |
| test/episodes                  | 770        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -19.422539 |
| test/Q_plus_P                  | -19.422539 |
| test/reward_per_eps            | -40        |
| test/steps                     | 30800      |
| train/episodes                 | 3080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.175     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 123200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 77          |
| stats_o/mean                   | 0.20351739  |
| stats_o/std                    | 0.060820177 |
| test/episodes                  | 780         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.4912    |
| test/Q_plus_P                  | -19.4912    |
| test/reward_per_eps            | -40         |
| test/steps                     | 31200       |
| train/episodes                 | 3120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 124800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 78          |
| stats_o/mean                   | 0.2035509   |
| stats_o/std                    | 0.061014205 |
| test/episodes                  | 790         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.193      |
| test/Q                         | -20.33036   |
| test/Q_plus_P                  | -20.33036   |
| test/reward_per_eps            | -40         |
| test/steps                     | 31600       |
| train/episodes                 | 3160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 126400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 79          |
| stats_o/mean                   | 0.20356864  |
| stats_o/std                    | 0.061003666 |
| test/episodes                  | 800         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -20.327782  |
| test/Q_plus_P                  | -20.327782  |
| test/reward_per_eps            | -40         |
| test/steps                     | 32000       |
| train/episodes                 | 3200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 128000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.2035895   |
| stats_o/std                    | 0.060995955 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.181      |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -20.56141   |
| test/Q_plus_P                  | -20.56141   |
| test/reward_per_eps            | -40         |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 81          |
| stats_o/mean                   | 0.20361298  |
| stats_o/std                    | 0.061024953 |
| test/episodes                  | 820         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -20.727655  |
| test/Q_plus_P                  | -20.727655  |
| test/reward_per_eps            | -40         |
| test/steps                     | 32800       |
| train/episodes                 | 3280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 131200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.20361605  |
| stats_o/std                    | 0.061071005 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -20.96543   |
| test/Q_plus_P                  | -20.96543   |
| test/reward_per_eps            | -40         |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 83          |
| stats_o/mean                   | 0.20361124  |
| stats_o/std                    | 0.061430715 |
| test/episodes                  | 840         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -20.599926  |
| test/Q_plus_P                  | -20.599926  |
| test/reward_per_eps            | -40         |
| test/steps                     | 33600       |
| train/episodes                 | 3360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.18       |
| train/info_shaping_reward_min  | -0.282      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 134400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 84          |
| stats_o/mean                   | 0.20360096  |
| stats_o/std                    | 0.062052414 |
| test/episodes                  | 850         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.144      |
| test/info_shaping_reward_mean  | -0.191      |
| test/info_shaping_reward_min   | -0.509      |
| test/Q                         | -22.164223  |
| test/Q_plus_P                  | -22.164223  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34000       |
| train/episodes                 | 3400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.181      |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 136000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.2036543   |
| stats_o/std                    | 0.062138066 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -21.159971  |
| test/Q_plus_P                  | -21.159971  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.21       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 86         |
| stats_o/mean                   | 0.20369357 |
| stats_o/std                    | 0.06257967 |
| test/episodes                  | 870        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -21.595903 |
| test/Q_plus_P                  | -21.595903 |
| test/reward_per_eps            | -40        |
| test/steps                     | 34800      |
| train/episodes                 | 3480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 139200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 87         |
| stats_o/mean                   | 0.20372188 |
| stats_o/std                    | 0.06251047 |
| test/episodes                  | 880        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.192     |
| test/info_shaping_reward_min   | -0.574     |
| test/Q                         | -22.276085 |
| test/Q_plus_P                  | -22.276085 |
| test/reward_per_eps            | -40        |
| test/steps                     | 35200      |
| train/episodes                 | 3520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 140800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 88          |
| stats_o/mean                   | 0.2036871   |
| stats_o/std                    | 0.062437262 |
| test/episodes                  | 890         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -21.76812   |
| test/Q_plus_P                  | -21.76812   |
| test/reward_per_eps            | -40         |
| test/steps                     | 35600       |
| train/episodes                 | 3560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 142400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 89          |
| stats_o/mean                   | 0.20369856  |
| stats_o/std                    | 0.063253224 |
| test/episodes                  | 900         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -21.725063  |
| test/Q_plus_P                  | -21.725063  |
| test/reward_per_eps            | -40         |
| test/steps                     | 36000       |
| train/episodes                 | 3600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.192      |
| train/info_shaping_reward_min  | -0.392      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 144000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 90         |
| stats_o/mean                   | 0.20367955 |
| stats_o/std                    | 0.06381733 |
| test/episodes                  | 910        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -22.210772 |
| test/Q_plus_P                  | -22.210772 |
| test/reward_per_eps            | -40        |
| test/steps                     | 36400      |
| train/episodes                 | 3640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 145600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 91          |
| stats_o/mean                   | 0.20362005  |
| stats_o/std                    | 0.063737966 |
| test/episodes                  | 920         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -22.896282  |
| test/Q_plus_P                  | -22.896282  |
| test/reward_per_eps            | -40         |
| test/steps                     | 36800       |
| train/episodes                 | 3680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 147200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 92         |
| stats_o/mean                   | 0.2036113  |
| stats_o/std                    | 0.06382335 |
| test/episodes                  | 930        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -22.294924 |
| test/Q_plus_P                  | -22.294924 |
| test/reward_per_eps            | -40        |
| test/steps                     | 37200      |
| train/episodes                 | 3720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 148800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 93         |
| stats_o/mean                   | 0.20356594 |
| stats_o/std                    | 0.06382005 |
| test/episodes                  | 940        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -23.267895 |
| test/Q_plus_P                  | -23.267895 |
| test/reward_per_eps            | -40        |
| test/steps                     | 37600      |
| train/episodes                 | 3760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 150400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 94         |
| stats_o/mean                   | 0.2035784  |
| stats_o/std                    | 0.06391034 |
| test/episodes                  | 950        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -23.149155 |
| test/Q_plus_P                  | -23.149155 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38000      |
| train/episodes                 | 3800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 152000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 95         |
| stats_o/mean                   | 0.2036318  |
| stats_o/std                    | 0.06411287 |
| test/episodes                  | 960        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -23.061905 |
| test/Q_plus_P                  | -23.061905 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38400      |
| train/episodes                 | 3840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.28      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 153600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 96         |
| stats_o/mean                   | 0.20361485 |
| stats_o/std                    | 0.06410865 |
| test/episodes                  | 970        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -23.742771 |
| test/Q_plus_P                  | -23.742771 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38800      |
| train/episodes                 | 3880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 155200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 97         |
| stats_o/mean                   | 0.20361975 |
| stats_o/std                    | 0.06412259 |
| test/episodes                  | 980        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -24.00864  |
| test/Q_plus_P                  | -24.00864  |
| test/reward_per_eps            | -40        |
| test/steps                     | 39200      |
| train/episodes                 | 3920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 156800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 98         |
| stats_o/mean                   | 0.20357153 |
| stats_o/std                    | 0.06419931 |
| test/episodes                  | 990        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -23.644087 |
| test/Q_plus_P                  | -23.644087 |
| test/reward_per_eps            | -40        |
| test/steps                     | 39600      |
| train/episodes                 | 3960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 158400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.20355307 |
| stats_o/std                    | 0.06432563 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -23.975044 |
| test/Q_plus_P                  | -23.975044 |
| test/reward_per_eps            | -40        |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.28      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 100        |
| stats_o/mean                   | 0.20355345 |
| stats_o/std                    | 0.06455935 |
| test/episodes                  | 1010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -24.238066 |
| test/Q_plus_P                  | -24.238066 |
| test/reward_per_eps            | -40        |
| test/steps                     | 40400      |
| train/episodes                 | 4040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 161600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 101        |
| stats_o/mean                   | 0.20354527 |
| stats_o/std                    | 0.06463702 |
| test/episodes                  | 1020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.108     |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -24.150251 |
| test/Q_plus_P                  | -24.150251 |
| test/reward_per_eps            | -40        |
| test/steps                     | 40800      |
| train/episodes                 | 4080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 163200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 102        |
| stats_o/mean                   | 0.20355348 |
| stats_o/std                    | 0.06480095 |
| test/episodes                  | 1030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -24.589006 |
| test/Q_plus_P                  | -24.589006 |
| test/reward_per_eps            | -40        |
| test/steps                     | 41200      |
| train/episodes                 | 4120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 164800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 103         |
| stats_o/mean                   | 0.20356968  |
| stats_o/std                    | 0.064952284 |
| test/episodes                  | 1040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -24.780214  |
| test/Q_plus_P                  | -24.780214  |
| test/reward_per_eps            | -40         |
| test/steps                     | 41600       |
| train/episodes                 | 4160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.18       |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 166400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 104        |
| stats_o/mean                   | 0.20356806 |
| stats_o/std                    | 0.06520694 |
| test/episodes                  | 1050       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.119     |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -24.722746 |
| test/Q_plus_P                  | -24.722746 |
| test/reward_per_eps            | -40        |
| test/steps                     | 42000      |
| train/episodes                 | 4200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 168000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 105        |
| stats_o/mean                   | 0.20360704 |
| stats_o/std                    | 0.06614714 |
| test/episodes                  | 1060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.125     |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -24.746992 |
| test/Q_plus_P                  | -24.746992 |
| test/reward_per_eps            | -40        |
| test/steps                     | 42400      |
| train/episodes                 | 4240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.295     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 169600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 106        |
| stats_o/mean                   | 0.20359944 |
| stats_o/std                    | 0.06626394 |
| test/episodes                  | 1070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -24.928894 |
| test/Q_plus_P                  | -24.928894 |
| test/reward_per_eps            | -40        |
| test/steps                     | 42800      |
| train/episodes                 | 4280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 171200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 107        |
| stats_o/mean                   | 0.20364556 |
| stats_o/std                    | 0.06639343 |
| test/episodes                  | 1080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.025      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0319    |
| test/info_shaping_reward_mean  | -0.177     |
| test/info_shaping_reward_min   | -0.563     |
| test/Q                         | -25.254839 |
| test/Q_plus_P                  | -25.254839 |
| test/reward_per_eps            | -39        |
| test/steps                     | 43200      |
| train/episodes                 | 4320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 172800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 108        |
| stats_o/mean                   | 0.20365423 |
| stats_o/std                    | 0.06671043 |
| test/episodes                  | 1090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.035      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0327    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -24.97691  |
| test/Q_plus_P                  | -24.97691  |
| test/reward_per_eps            | -38.6      |
| test/steps                     | 43600      |
| train/episodes                 | 4360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 174400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 109        |
| stats_o/mean                   | 0.20364244 |
| stats_o/std                    | 0.06696106 |
| test/episodes                  | 1100       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.125     |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -25.41694  |
| test/Q_plus_P                  | -25.41694  |
| test/reward_per_eps            | -40        |
| test/steps                     | 44000      |
| train/episodes                 | 4400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 176000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 110        |
| stats_o/mean                   | 0.20360751 |
| stats_o/std                    | 0.06731739 |
| test/episodes                  | 1110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.122     |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -25.98018  |
| test/Q_plus_P                  | -25.98018  |
| test/reward_per_eps            | -40        |
| test/steps                     | 44400      |
| train/episodes                 | 4440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 177600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 111         |
| stats_o/mean                   | 0.20363268  |
| stats_o/std                    | 0.067650996 |
| test/episodes                  | 1120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0679     |
| test/info_shaping_reward_mean  | -0.168      |
| test/info_shaping_reward_min   | -0.196      |
| test/Q                         | -25.69443   |
| test/Q_plus_P                  | -25.69443   |
| test/reward_per_eps            | -40         |
| test/steps                     | 44800       |
| train/episodes                 | 4480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.185      |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 179200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 112        |
| stats_o/mean                   | 0.20363146 |
| stats_o/std                    | 0.06773318 |
| test/episodes                  | 1130       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0676    |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -26.009045 |
| test/Q_plus_P                  | -26.009045 |
| test/reward_per_eps            | -40        |
| test/steps                     | 45200      |
| train/episodes                 | 4520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 180800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 113        |
| stats_o/mean                   | 0.20367132 |
| stats_o/std                    | 0.06812711 |
| test/episodes                  | 1140       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -26.219822 |
| test/Q_plus_P                  | -26.219822 |
| test/reward_per_eps            | -40        |
| test/steps                     | 45600      |
| train/episodes                 | 4560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 182400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 114        |
| stats_o/mean                   | 0.20369647 |
| stats_o/std                    | 0.0682859  |
| test/episodes                  | 1150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -0.5       |
| test/Q                         | -26.324745 |
| test/Q_plus_P                  | -26.324745 |
| test/reward_per_eps            | -40        |
| test/steps                     | 46000      |
| train/episodes                 | 4600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 184000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 115         |
| stats_o/mean                   | 0.2037431   |
| stats_o/std                    | 0.068508714 |
| test/episodes                  | 1160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -26.30543   |
| test/Q_plus_P                  | -26.30543   |
| test/reward_per_eps            | -40         |
| test/steps                     | 46400       |
| train/episodes                 | 4640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.183      |
| train/info_shaping_reward_min  | -0.294      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 185600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 116        |
| stats_o/mean                   | 0.20374146 |
| stats_o/std                    | 0.0686833  |
| test/episodes                  | 1170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0847    |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -26.282488 |
| test/Q_plus_P                  | -26.282488 |
| test/reward_per_eps            | -40        |
| test/steps                     | 46800      |
| train/episodes                 | 4680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 187200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 117         |
| stats_o/mean                   | 0.20374416  |
| stats_o/std                    | 0.069034815 |
| test/episodes                  | 1180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.118      |
| test/info_shaping_reward_mean  | -0.199      |
| test/info_shaping_reward_min   | -0.691      |
| test/Q                         | -27.094505  |
| test/Q_plus_P                  | -27.094505  |
| test/reward_per_eps            | -40         |
| test/steps                     | 47200       |
| train/episodes                 | 4720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.169      |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 188800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.20377845  |
| stats_o/std                    | 0.069106326 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.117      |
| test/info_shaping_reward_mean  | -0.17       |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -26.809942  |
| test/Q_plus_P                  | -26.809942  |
| test/reward_per_eps            | -40         |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 119         |
| stats_o/mean                   | 0.2038186   |
| stats_o/std                    | 0.069537625 |
| test/episodes                  | 1200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0632     |
| test/info_shaping_reward_mean  | -0.169      |
| test/info_shaping_reward_min   | -0.353      |
| test/Q                         | -26.919512  |
| test/Q_plus_P                  | -26.919512  |
| test/reward_per_eps            | -40         |
| test/steps                     | 48000       |
| train/episodes                 | 4800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.186      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 192000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 120        |
| stats_o/mean                   | 0.20384304 |
| stats_o/std                    | 0.06971038 |
| test/episodes                  | 1210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0808    |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -27.78961  |
| test/Q_plus_P                  | -27.78961  |
| test/reward_per_eps            | -40        |
| test/steps                     | 48400      |
| train/episodes                 | 4840       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0175     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 193600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.2038665  |
| stats_o/std                    | 0.07013651 |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.111     |
| test/info_shaping_reward_mean  | -0.188     |
| test/info_shaping_reward_min   | -0.691     |
| test/Q                         | -27.215065 |
| test/Q_plus_P                  | -27.215065 |
| test/reward_per_eps            | -40        |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.265     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 122        |
| stats_o/mean                   | 0.2038022  |
| stats_o/std                    | 0.07039023 |
| test/episodes                  | 1230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -27.413254 |
| test/Q_plus_P                  | -27.413254 |
| test/reward_per_eps            | -40        |
| test/steps                     | 49200      |
| train/episodes                 | 4920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 196800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 123         |
| stats_o/mean                   | 0.20377828  |
| stats_o/std                    | 0.070995055 |
| test/episodes                  | 1240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.112      |
| test/info_shaping_reward_mean  | -0.172      |
| test/info_shaping_reward_min   | -0.198      |
| test/Q                         | -27.47311   |
| test/Q_plus_P                  | -27.47311   |
| test/reward_per_eps            | -40         |
| test/steps                     | 49600       |
| train/episodes                 | 4960        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.01        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.187      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 198400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 124        |
| stats_o/mean                   | 0.20378174 |
| stats_o/std                    | 0.07134767 |
| test/episodes                  | 1250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0759    |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.222     |
| test/Q                         | -27.461391 |
| test/Q_plus_P                  | -27.461391 |
| test/reward_per_eps            | -40        |
| test/steps                     | 50000      |
| train/episodes                 | 5000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 200000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 125         |
| stats_o/mean                   | 0.20376909  |
| stats_o/std                    | 0.071584985 |
| test/episodes                  | 1260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -28.13983   |
| test/Q_plus_P                  | -28.13983   |
| test/reward_per_eps            | -40         |
| test/steps                     | 50400       |
| train/episodes                 | 5040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.181      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 201600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 126        |
| stats_o/mean                   | 0.20373912 |
| stats_o/std                    | 0.0717612  |
| test/episodes                  | 1270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0125     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0117    |
| test/info_shaping_reward_mean  | -0.178     |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -27.91184  |
| test/Q_plus_P                  | -27.91184  |
| test/reward_per_eps            | -39.5      |
| test/steps                     | 50800      |
| train/episodes                 | 5080       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.288     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 203200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 127        |
| stats_o/mean                   | 0.20376047 |
| stats_o/std                    | 0.07200482 |
| test/episodes                  | 1280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -27.967968 |
| test/Q_plus_P                  | -27.967968 |
| test/reward_per_eps            | -40        |
| test/steps                     | 51200      |
| train/episodes                 | 5120       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0256     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 204800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 128         |
| stats_o/mean                   | 0.20381567  |
| stats_o/std                    | 0.072665274 |
| test/episodes                  | 1290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.065      |
| test/info_shaping_reward_mean  | -0.163      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -27.731403  |
| test/Q_plus_P                  | -27.731403  |
| test/reward_per_eps            | -40         |
| test/steps                     | 51600       |
| train/episodes                 | 5160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.191      |
| train/info_shaping_reward_min  | -0.403      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 206400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 129        |
| stats_o/mean                   | 0.20379059 |
| stats_o/std                    | 0.07305578 |
| test/episodes                  | 1300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.177     |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -28.076078 |
| test/Q_plus_P                  | -28.076078 |
| test/reward_per_eps            | -40        |
| test/steps                     | 52000      |
| train/episodes                 | 5200       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0163     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.311     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 208000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 130        |
| stats_o/mean                   | 0.20376873 |
| stats_o/std                    | 0.07338812 |
| test/episodes                  | 1310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.136     |
| test/info_shaping_reward_mean  | -0.181     |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -28.552423 |
| test/Q_plus_P                  | -28.552423 |
| test/reward_per_eps            | -40        |
| test/steps                     | 52400      |
| train/episodes                 | 5240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.4       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 209600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 131         |
| stats_o/mean                   | 0.20382635  |
| stats_o/std                    | 0.073805116 |
| test/episodes                  | 1320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0475      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0174     |
| test/info_shaping_reward_mean  | -0.168      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -28.025108  |
| test/Q_plus_P                  | -28.025108  |
| test/reward_per_eps            | -38.1       |
| test/steps                     | 52800       |
| train/episodes                 | 5280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 211200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 132        |
| stats_o/mean                   | 0.2038124  |
| stats_o/std                    | 0.07382905 |
| test/episodes                  | 1330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.149     |
| test/info_shaping_reward_mean  | -0.206     |
| test/info_shaping_reward_min   | -0.788     |
| test/Q                         | -28.74307  |
| test/Q_plus_P                  | -28.74307  |
| test/reward_per_eps            | -40        |
| test/steps                     | 53200      |
| train/episodes                 | 5320       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0219     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 212800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 133        |
| stats_o/mean                   | 0.20381798 |
| stats_o/std                    | 0.07389469 |
| test/episodes                  | 1340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -28.624268 |
| test/Q_plus_P                  | -28.624268 |
| test/reward_per_eps            | -40        |
| test/steps                     | 53600      |
| train/episodes                 | 5360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 214400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 134        |
| stats_o/mean                   | 0.20385808 |
| stats_o/std                    | 0.07395369 |
| test/episodes                  | 1350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.03       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0236    |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -28.672157 |
| test/Q_plus_P                  | -28.672157 |
| test/reward_per_eps            | -38.8      |
| test/steps                     | 54000      |
| train/episodes                 | 5400       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0144     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 216000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 135         |
| stats_o/mean                   | 0.20388924  |
| stats_o/std                    | 0.074295044 |
| test/episodes                  | 1360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.171      |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -28.597929  |
| test/Q_plus_P                  | -28.597929  |
| test/reward_per_eps            | -40         |
| test/steps                     | 54400       |
| train/episodes                 | 5440        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.000625    |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.299      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 217600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 136         |
| stats_o/mean                   | 0.20389895  |
| stats_o/std                    | 0.074303396 |
| test/episodes                  | 1370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0826     |
| test/info_shaping_reward_mean  | -0.167      |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -28.513172  |
| test/Q_plus_P                  | -28.513172  |
| test/reward_per_eps            | -40         |
| test/steps                     | 54800       |
| train/episodes                 | 5480        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00937     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 219200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 137        |
| stats_o/mean                   | 0.20384237 |
| stats_o/std                    | 0.07473423 |
| test/episodes                  | 1380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0861    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -28.644985 |
| test/Q_plus_P                  | -28.644985 |
| test/reward_per_eps            | -40        |
| test/steps                     | 55200      |
| train/episodes                 | 5520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.299     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 220800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.20383036 |
| stats_o/std                    | 0.07496535 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0554    |
| test/info_shaping_reward_mean  | -0.193     |
| test/info_shaping_reward_min   | -0.645     |
| test/Q                         | -29.28377  |
| test/Q_plus_P                  | -29.28377  |
| test/reward_per_eps            | -40        |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.117     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.20385183 |
| stats_o/std                    | 0.07532767 |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.187     |
| test/info_shaping_reward_min   | -0.507     |
| test/Q                         | -29.068325 |
| test/Q_plus_P                  | -29.068325 |
| test/reward_per_eps            | -40        |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 140         |
| stats_o/mean                   | 0.20384957  |
| stats_o/std                    | 0.075357005 |
| test/episodes                  | 1410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0713     |
| test/info_shaping_reward_mean  | -0.163      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -28.612167  |
| test/Q_plus_P                  | -28.612167  |
| test/reward_per_eps            | -40         |
| test/steps                     | 56400       |
| train/episodes                 | 5640        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0138      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 225600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.2038051  |
| stats_o/std                    | 0.07551008 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.095     |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -28.794731 |
| test/Q_plus_P                  | -28.794731 |
| test/reward_per_eps            | -40        |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.12      |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 142        |
| stats_o/mean                   | 0.20382628 |
| stats_o/std                    | 0.07581381 |
| test/episodes                  | 1430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0922    |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -29.087744 |
| test/Q_plus_P                  | -29.087744 |
| test/reward_per_eps            | -40        |
| test/steps                     | 57200      |
| train/episodes                 | 5720       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.01       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 228800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 143        |
| stats_o/mean                   | 0.20378174 |
| stats_o/std                    | 0.07599246 |
| test/episodes                  | 1440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.136     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -29.31606  |
| test/Q_plus_P                  | -29.31606  |
| test/reward_per_eps            | -40        |
| test/steps                     | 57600      |
| train/episodes                 | 5760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.267     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 230400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 144        |
| stats_o/mean                   | 0.2038197  |
| stats_o/std                    | 0.07642436 |
| test/episodes                  | 1450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.218     |
| test/Q                         | -29.915499 |
| test/Q_plus_P                  | -29.915499 |
| test/reward_per_eps            | -40        |
| test/steps                     | 58000      |
| train/episodes                 | 5800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 232000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 145         |
| stats_o/mean                   | 0.20385027  |
| stats_o/std                    | 0.076728985 |
| test/episodes                  | 1460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.168      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -29.599567  |
| test/Q_plus_P                  | -29.599567  |
| test/reward_per_eps            | -40         |
| test/steps                     | 58400       |
| train/episodes                 | 5840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.297      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 233600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 146         |
| stats_o/mean                   | 0.2038534   |
| stats_o/std                    | 0.076857105 |
| test/episodes                  | 1470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.065       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0396     |
| test/info_shaping_reward_mean  | -0.159      |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -29.27031   |
| test/Q_plus_P                  | -29.27031   |
| test/reward_per_eps            | -37.4       |
| test/steps                     | 58800       |
| train/episodes                 | 5880        |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.03        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.11       |
| train/info_shaping_reward_mean | -0.169      |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.8       |
| train/steps                    | 235200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 147        |
| stats_o/mean                   | 0.20383623 |
| stats_o/std                    | 0.07711468 |
| test/episodes                  | 1480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.03       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0316    |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -29.128935 |
| test/Q_plus_P                  | -29.128935 |
| test/reward_per_eps            | -38.8      |
| test/steps                     | 59200      |
| train/episodes                 | 5920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 236800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 148         |
| stats_o/mean                   | 0.20382753  |
| stats_o/std                    | 0.077262215 |
| test/episodes                  | 1490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.122      |
| test/info_shaping_reward_mean  | -0.169      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -29.887802  |
| test/Q_plus_P                  | -29.887802  |
| test/reward_per_eps            | -40         |
| test/steps                     | 59600       |
| train/episodes                 | 5960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 238400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 149         |
| stats_o/mean                   | 0.20386948  |
| stats_o/std                    | 0.077524394 |
| test/episodes                  | 1500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.14       |
| test/info_shaping_reward_mean  | -0.171      |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -30.236872  |
| test/Q_plus_P                  | -30.236872  |
| test/reward_per_eps            | -40         |
| test/steps                     | 60000       |
| train/episodes                 | 6000        |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0231      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.106      |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.211      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.1       |
| train/steps                    | 240000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 150        |
| stats_o/mean                   | 0.20386492 |
| stats_o/std                    | 0.07754173 |
| test/episodes                  | 1510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.134     |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -30.373606 |
| test/Q_plus_P                  | -30.373606 |
| test/reward_per_eps            | -40        |
| test/steps                     | 60400      |
| train/episodes                 | 6040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 241600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 151        |
| stats_o/mean                   | 0.20386001 |
| stats_o/std                    | 0.07768232 |
| test/episodes                  | 1520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.144     |
| test/info_shaping_reward_mean  | -0.188     |
| test/info_shaping_reward_min   | -0.5       |
| test/Q                         | -30.127779 |
| test/Q_plus_P                  | -30.127779 |
| test/reward_per_eps            | -40        |
| test/steps                     | 60800      |
| train/episodes                 | 6080       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0106     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 243200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 152        |
| stats_o/mean                   | 0.20389515 |
| stats_o/std                    | 0.07793269 |
| test/episodes                  | 1530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0581    |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -30.0192   |
| test/Q_plus_P                  | -30.0192   |
| test/reward_per_eps            | -40        |
| test/steps                     | 61200      |
| train/episodes                 | 6120       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0244     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 244800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 153        |
| stats_o/mean                   | 0.20388009 |
| stats_o/std                    | 0.07803272 |
| test/episodes                  | 1540       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.182     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -30.9239   |
| test/Q_plus_P                  | -30.9239   |
| test/reward_per_eps            | -40        |
| test/steps                     | 61600      |
| train/episodes                 | 6160       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00375    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 246400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 154         |
| stats_o/mean                   | 0.20391293  |
| stats_o/std                    | 0.078212254 |
| test/episodes                  | 1550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.135      |
| test/info_shaping_reward_mean  | -0.17       |
| test/info_shaping_reward_min   | -0.193      |
| test/Q                         | -30.40125   |
| test/Q_plus_P                  | -30.40125   |
| test/reward_per_eps            | -40         |
| test/steps                     | 62000       |
| train/episodes                 | 6200        |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.00812     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.115      |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.22       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 248000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.20390476 |
| stats_o/std                    | 0.07844658 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0616    |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -29.994722 |
| test/Q_plus_P                  | -29.994722 |
| test/reward_per_eps            | -40        |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00187    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 156        |
| stats_o/mean                   | 0.20389271 |
| stats_o/std                    | 0.07856269 |
| test/episodes                  | 1570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.209     |
| test/info_shaping_reward_min   | -0.745     |
| test/Q                         | -30.679844 |
| test/Q_plus_P                  | -30.679844 |
| test/reward_per_eps            | -40        |
| test/steps                     | 62800      |
| train/episodes                 | 6280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.265     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 251200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 157        |
| stats_o/mean                   | 0.20390503 |
| stats_o/std                    | 0.07889658 |
| test/episodes                  | 1580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.065      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0147    |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.224     |
| test/Q                         | -29.937872 |
| test/Q_plus_P                  | -29.937872 |
| test/reward_per_eps            | -37.4      |
| test/steps                     | 63200      |
| train/episodes                 | 6320       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00313    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.305     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 252800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 158        |
| stats_o/mean                   | 0.20387855 |
| stats_o/std                    | 0.07905174 |
| test/episodes                  | 1590       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0999    |
| test/info_shaping_reward_mean  | -0.177     |
| test/info_shaping_reward_min   | -0.363     |
| test/Q                         | -30.544985 |
| test/Q_plus_P                  | -30.544985 |
| test/reward_per_eps            | -40        |
| test/steps                     | 63600      |
| train/episodes                 | 6360       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0375     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.116     |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 254400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.203812    |
| stats_o/std                    | 0.079269186 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0325      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00553    |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.212      |
| test/Q                         | -30.469799  |
| test/Q_plus_P                  | -30.469799  |
| test/reward_per_eps            | -38.7       |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0144      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.169      |
| train/info_shaping_reward_min  | -0.201      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 160        |
| stats_o/mean                   | 0.2038027  |
| stats_o/std                    | 0.07923984 |
| test/episodes                  | 1610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0075     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0285    |
| test/info_shaping_reward_mean  | -0.154     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -30.5091   |
| test/Q_plus_P                  | -30.5091   |
| test/reward_per_eps            | -39.7      |
| test/steps                     | 64400      |
| train/episodes                 | 6440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.287     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 257600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 161        |
| stats_o/mean                   | 0.20379686 |
| stats_o/std                    | 0.07921323 |
| test/episodes                  | 1620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.183     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -31.661728 |
| test/Q_plus_P                  | -31.661728 |
| test/reward_per_eps            | -40        |
| test/steps                     | 64800      |
| train/episodes                 | 6480       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00313    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 259200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 162        |
| stats_o/mean                   | 0.20377678 |
| stats_o/std                    | 0.07929167 |
| test/episodes                  | 1630       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -31.546665 |
| test/Q_plus_P                  | -31.546665 |
| test/reward_per_eps            | -40        |
| test/steps                     | 65200      |
| train/episodes                 | 6520       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0144     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.302     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 260800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 163         |
| stats_o/mean                   | 0.20376328  |
| stats_o/std                    | 0.079290554 |
| test/episodes                  | 1640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -31.269054  |
| test/Q_plus_P                  | -31.269054  |
| test/reward_per_eps            | -40         |
| test/steps                     | 65600       |
| train/episodes                 | 6560        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00187     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 262400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 164        |
| stats_o/mean                   | 0.20377111 |
| stats_o/std                    | 0.07940438 |
| test/episodes                  | 1650       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.178     |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -31.808355 |
| test/Q_plus_P                  | -31.808355 |
| test/reward_per_eps            | -40        |
| test/steps                     | 66000      |
| train/episodes                 | 6600       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.386     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 264000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 165        |
| stats_o/mean                   | 0.20377828 |
| stats_o/std                    | 0.07949171 |
| test/episodes                  | 1660       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0942    |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -31.046412 |
| test/Q_plus_P                  | -31.046412 |
| test/reward_per_eps            | -40        |
| test/steps                     | 66400      |
| train/episodes                 | 6640       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00313    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 265600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 166        |
| stats_o/mean                   | 0.20379835 |
| stats_o/std                    | 0.07950611 |
| test/episodes                  | 1670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.206     |
| test/Q                         | -31.972881 |
| test/Q_plus_P                  | -31.972881 |
| test/reward_per_eps            | -40        |
| test/steps                     | 66800      |
| train/episodes                 | 6680       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0144     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 267200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.20382199 |
| stats_o/std                    | 0.07991374 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.005      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0417    |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.206     |
| test/Q                         | -31.217138 |
| test/Q_plus_P                  | -31.217138 |
| test/reward_per_eps            | -39.8      |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0131     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.122     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 168        |
| stats_o/mean                   | 0.20384072 |
| stats_o/std                    | 0.0800201  |
| test/episodes                  | 1690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0075     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0169    |
| test/info_shaping_reward_mean  | -0.158     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -30.382446 |
| test/Q_plus_P                  | -30.382446 |
| test/reward_per_eps            | -39.7      |
| test/steps                     | 67600      |
| train/episodes                 | 6760       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0238     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.107     |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 270400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 169         |
| stats_o/mean                   | 0.20380986  |
| stats_o/std                    | 0.080278166 |
| test/episodes                  | 1700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.045       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0279     |
| test/info_shaping_reward_mean  | -0.161      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -30.373589  |
| test/Q_plus_P                  | -30.373589  |
| test/reward_per_eps            | -38.2       |
| test/steps                     | 68000       |
| train/episodes                 | 6800        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0131      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.124      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.281      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 272000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 170        |
| stats_o/mean                   | 0.20381635 |
| stats_o/std                    | 0.08048699 |
| test/episodes                  | 1710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.177      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00581   |
| test/info_shaping_reward_mean  | -0.133     |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -29.195396 |
| test/Q_plus_P                  | -29.195396 |
| test/reward_per_eps            | -32.9      |
| test/steps                     | 68400      |
| train/episodes                 | 6840       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0075     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 273600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 171         |
| stats_o/mean                   | 0.2038207   |
| stats_o/std                    | 0.080562994 |
| test/episodes                  | 1720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.175      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -32.390186  |
| test/Q_plus_P                  | -32.390186  |
| test/reward_per_eps            | -40         |
| test/steps                     | 68800       |
| train/episodes                 | 6880        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0425      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0853     |
| train/info_shaping_reward_mean | -0.166      |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.3       |
| train/steps                    | 275200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 172        |
| stats_o/mean                   | 0.20375368 |
| stats_o/std                    | 0.0807452  |
| test/episodes                  | 1730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.065      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0252    |
| test/info_shaping_reward_mean  | -0.157     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -30.897436 |
| test/Q_plus_P                  | -30.897436 |
| test/reward_per_eps            | -37.4      |
| test/steps                     | 69200      |
| train/episodes                 | 6920       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 276800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 173        |
| stats_o/mean                   | 0.20377937 |
| stats_o/std                    | 0.08112156 |
| test/episodes                  | 1740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0075     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0479    |
| test/info_shaping_reward_mean  | -0.155     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -30.64988  |
| test/Q_plus_P                  | -30.64988  |
| test/reward_per_eps            | -39.7      |
| test/steps                     | 69600      |
| train/episodes                 | 6960       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0619     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0573    |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 278400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 174        |
| stats_o/mean                   | 0.203859   |
| stats_o/std                    | 0.08131662 |
| test/episodes                  | 1750       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0942    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.316     |
| test/Q                         | -31.588276 |
| test/Q_plus_P                  | -31.588276 |
| test/reward_per_eps            | -40        |
| test/steps                     | 70000      |
| train/episodes                 | 7000       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.082     |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 280000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 175        |
| stats_o/mean                   | 0.20381555 |
| stats_o/std                    | 0.08146095 |
| test/episodes                  | 1760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0025     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0456    |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -31.021633 |
| test/Q_plus_P                  | -31.021633 |
| test/reward_per_eps            | -39.9      |
| test/steps                     | 70400      |
| train/episodes                 | 7040       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0075     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 281600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 176         |
| stats_o/mean                   | 0.20381646  |
| stats_o/std                    | 0.081602305 |
| test/episodes                  | 1770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.113      |
| test/info_shaping_reward_mean  | -0.178      |
| test/info_shaping_reward_min   | -0.371      |
| test/Q                         | -31.077183  |
| test/Q_plus_P                  | -31.077183  |
| test/reward_per_eps            | -40         |
| test/steps                     | 70800       |
| train/episodes                 | 7080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 283200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 177        |
| stats_o/mean                   | 0.20381889 |
| stats_o/std                    | 0.08180653 |
| test/episodes                  | 1780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.141     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.214     |
| test/Q                         | -32.115444 |
| test/Q_plus_P                  | -32.115444 |
| test/reward_per_eps            | -40        |
| test/steps                     | 71200      |
| train/episodes                 | 7120       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0269     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0855    |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 284800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.20382248 |
| stats_o/std                    | 0.0818841  |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0868    |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.226     |
| test/Q                         | -31.888113 |
| test/Q_plus_P                  | -31.888113 |
| test/reward_per_eps            | -40        |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0106     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 179        |
| stats_o/mean                   | 0.2038359  |
| stats_o/std                    | 0.08226542 |
| test/episodes                  | 1800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.055      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0492    |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.376     |
| test/Q                         | -31.232536 |
| test/Q_plus_P                  | -31.232536 |
| test/reward_per_eps            | -37.8      |
| test/steps                     | 72000      |
| train/episodes                 | 7200       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0025     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.101     |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.386     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 288000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 180        |
| stats_o/mean                   | 0.20384018 |
| stats_o/std                    | 0.08259636 |
| test/episodes                  | 1810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.07       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.017     |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.216     |
| test/Q                         | -31.5951   |
| test/Q_plus_P                  | -31.5951   |
| test/reward_per_eps            | -37.2      |
| test/steps                     | 72400      |
| train/episodes                 | 7240       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0431     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0763    |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.3      |
| train/steps                    | 289600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 181        |
| stats_o/mean                   | 0.20382506 |
| stats_o/std                    | 0.0829216  |
| test/episodes                  | 1820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.152      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0175    |
| test/info_shaping_reward_mean  | -0.152     |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -30.402956 |
| test/Q_plus_P                  | -30.402956 |
| test/reward_per_eps            | -33.9      |
| test/steps                     | 72800      |
| train/episodes                 | 7280       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0519     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0672    |
| train/info_shaping_reward_mean | -0.16      |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.9      |
| train/steps                    | 291200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 182        |
| stats_o/mean                   | 0.20382072 |
| stats_o/std                    | 0.08301142 |
| test/episodes                  | 1830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.203      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0215    |
| test/info_shaping_reward_mean  | -0.127     |
| test/info_shaping_reward_min   | -0.222     |
| test/Q                         | -28.723534 |
| test/Q_plus_P                  | -28.723534 |
| test/reward_per_eps            | -31.9      |
| test/steps                     | 73200      |
| train/episodes                 | 7320       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0488     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.059     |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38        |
| train/steps                    | 292800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 183        |
| stats_o/mean                   | 0.20382869 |
| stats_o/std                    | 0.08310915 |
| test/episodes                  | 1840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0525    |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -31.67557  |
| test/Q_plus_P                  | -31.67557  |
| test/reward_per_eps            | -40        |
| test/steps                     | 73600      |
| train/episodes                 | 7360       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0206     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0549    |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 294400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 184         |
| stats_o/mean                   | 0.20382276  |
| stats_o/std                    | 0.083241396 |
| test/episodes                  | 1850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.102       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00368    |
| test/info_shaping_reward_mean  | -0.145      |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -30.30645   |
| test/Q_plus_P                  | -30.30645   |
| test/reward_per_eps            | -35.9       |
| test/steps                     | 74000       |
| train/episodes                 | 7400        |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0338      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0742     |
| train/info_shaping_reward_mean | -0.163      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.6       |
| train/steps                    | 296000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 185        |
| stats_o/mean                   | 0.20381805 |
| stats_o/std                    | 0.08341342 |
| test/episodes                  | 1860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.203      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.013     |
| test/info_shaping_reward_mean  | -0.115     |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -27.882452 |
| test/Q_plus_P                  | -27.882452 |
| test/reward_per_eps            | -31.9      |
| test/steps                     | 74400      |
| train/episodes                 | 7440       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0631     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0518    |
| train/info_shaping_reward_mean | -0.154     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 297600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 186         |
| stats_o/mean                   | 0.20381975  |
| stats_o/std                    | 0.083617836 |
| test/episodes                  | 1870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.19        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0175     |
| test/info_shaping_reward_mean  | -0.132      |
| test/info_shaping_reward_min   | -0.202      |
| test/Q                         | -28.716293  |
| test/Q_plus_P                  | -28.716293  |
| test/reward_per_eps            | -32.4       |
| test/steps                     | 74800       |
| train/episodes                 | 7480        |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0419      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0814     |
| train/info_shaping_reward_mean | -0.163      |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.3       |
| train/steps                    | 299200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.20384769 |
| stats_o/std                    | 0.08399127 |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0875     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0194    |
| test/info_shaping_reward_mean  | -0.141     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -29.546816 |
| test/Q_plus_P                  | -29.546816 |
| test/reward_per_eps            | -36.5      |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0531     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0535    |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.282     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.9      |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 188        |
| stats_o/mean                   | 0.20383161 |
| stats_o/std                    | 0.08399483 |
| test/episodes                  | 1890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.08       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0295    |
| test/info_shaping_reward_mean  | -0.145     |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -29.818552 |
| test/Q_plus_P                  | -29.818552 |
| test/reward_per_eps            | -36.8      |
| test/steps                     | 75600      |
| train/episodes                 | 7560       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0112     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0972    |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.285     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 302400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 189        |
| stats_o/mean                   | 0.2038128  |
| stats_o/std                    | 0.08430531 |
| test/episodes                  | 1900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.07       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0349    |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.22      |
| test/Q                         | -30.850014 |
| test/Q_plus_P                  | -30.850014 |
| test/reward_per_eps            | -37.2      |
| test/steps                     | 76000      |
| train/episodes                 | 7600       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0625     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0904    |
| train/info_shaping_reward_mean | -0.156     |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 304000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 190        |
| stats_o/mean                   | 0.20383388 |
| stats_o/std                    | 0.08446499 |
| test/episodes                  | 1910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.237      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0097    |
| test/info_shaping_reward_mean  | -0.13      |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -29.08047  |
| test/Q_plus_P                  | -29.08047  |
| test/reward_per_eps            | -30.5      |
| test/steps                     | 76400      |
| train/episodes                 | 7640       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0225     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0797    |
| train/info_shaping_reward_mean | -0.16      |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 305600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 191        |
| stats_o/mean                   | 0.20387429 |
| stats_o/std                    | 0.08481609 |
| test/episodes                  | 1920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.185      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0229    |
| test/info_shaping_reward_mean  | -0.125     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -28.724775 |
| test/Q_plus_P                  | -28.724775 |
| test/reward_per_eps            | -32.6      |
| test/steps                     | 76800      |
| train/episodes                 | 7680       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0706     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0603    |
| train/info_shaping_reward_mean | -0.155     |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.2      |
| train/steps                    | 307200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 192        |
| stats_o/mean                   | 0.20385225 |
| stats_o/std                    | 0.08494572 |
| test/episodes                  | 1930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0775     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0333    |
| test/info_shaping_reward_mean  | -0.142     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -30.18061  |
| test/Q_plus_P                  | -30.18061  |
| test/reward_per_eps            | -36.9      |
| test/steps                     | 77200      |
| train/episodes                 | 7720       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.104     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 308800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 193        |
| stats_o/mean                   | 0.2038412  |
| stats_o/std                    | 0.08511433 |
| test/episodes                  | 1940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0675     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0324    |
| test/info_shaping_reward_mean  | -0.141     |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -29.788345 |
| test/Q_plus_P                  | -29.788345 |
| test/reward_per_eps            | -37.3      |
| test/steps                     | 77600      |
| train/episodes                 | 7760       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0744     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0673    |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37        |
| train/steps                    | 310400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 194         |
| stats_o/mean                   | 0.20382649  |
| stats_o/std                    | 0.085151665 |
| test/episodes                  | 1950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.02        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0178     |
| test/info_shaping_reward_mean  | -0.161      |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -31.253935  |
| test/Q_plus_P                  | -31.253935  |
| test/reward_per_eps            | -39.2       |
| test/steps                     | 78000       |
| train/episodes                 | 7800        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0544      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0796     |
| train/info_shaping_reward_mean | -0.154      |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.8       |
| train/steps                    | 312000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 195        |
| stats_o/mean                   | 0.20380172 |
| stats_o/std                    | 0.08533489 |
| test/episodes                  | 1960       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0544    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -32.261986 |
| test/Q_plus_P                  | -32.261986 |
| test/reward_per_eps            | -40        |
| test/steps                     | 78400      |
| train/episodes                 | 7840       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0438     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0727    |
| train/info_shaping_reward_mean | -0.157     |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.2      |
| train/steps                    | 313600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 196        |
| stats_o/mean                   | 0.2038077  |
| stats_o/std                    | 0.08557106 |
| test/episodes                  | 1970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0581    |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -31.833136 |
| test/Q_plus_P                  | -31.833136 |
| test/reward_per_eps            | -40        |
| test/steps                     | 78800      |
| train/episodes                 | 7880       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0213     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0833    |
| train/info_shaping_reward_mean | -0.159     |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 315200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 197        |
| stats_o/mean                   | 0.20381914 |
| stats_o/std                    | 0.08575531 |
| test/episodes                  | 1980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.08       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0188    |
| test/info_shaping_reward_mean  | -0.147     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -30.801044 |
| test/Q_plus_P                  | -30.801044 |
| test/reward_per_eps            | -36.8      |
| test/steps                     | 79200      |
| train/episodes                 | 7920       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0813     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0777    |
| train/info_shaping_reward_mean | -0.158     |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.8      |
| train/steps                    | 316800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.20379633  |
| stats_o/std                    | 0.085962035 |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0275      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0379     |
| test/info_shaping_reward_mean  | -0.16       |
| test/info_shaping_reward_min   | -0.212      |
| test/Q                         | -31.63253   |
| test/Q_plus_P                  | -31.63253   |
| test/reward_per_eps            | -38.9       |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0713      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0527     |
| train/info_shaping_reward_mean | -0.155      |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.1       |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.20377745 |
| stats_o/std                    | 0.08613517 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0725     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0249    |
| test/info_shaping_reward_mean  | -0.155     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -30.541986 |
| test/Q_plus_P                  | -30.541986 |
| test/reward_per_eps            | -37.1      |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0381     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0783    |
| train/info_shaping_reward_mean | -0.157     |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 200         |
| stats_o/mean                   | 0.20377952  |
| stats_o/std                    | 0.086266845 |
| test/episodes                  | 2010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.135       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0119     |
| test/info_shaping_reward_mean  | -0.146      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -28.478256  |
| test/Q_plus_P                  | -28.478256  |
| test/reward_per_eps            | -34.6       |
| test/steps                     | 80400       |
| train/episodes                 | 8040        |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0244      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0732     |
| train/info_shaping_reward_mean | -0.157      |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39         |
| train/steps                    | 321600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 201        |
| stats_o/mean                   | 0.20374352 |
| stats_o/std                    | 0.08631713 |
| test/episodes                  | 2020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.172      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0137    |
| test/info_shaping_reward_mean  | -0.143     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -29.707335 |
| test/Q_plus_P                  | -29.707335 |
| test/reward_per_eps            | -33.1      |
| test/steps                     | 80800      |
| train/episodes                 | 8080       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0669     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0929    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.3      |
| train/steps                    | 323200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 202        |
| stats_o/mean                   | 0.2037461  |
| stats_o/std                    | 0.08642872 |
| test/episodes                  | 2030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -31.998606 |
| test/Q_plus_P                  | -31.998606 |
| test/reward_per_eps            | -40        |
| test/steps                     | 81200      |
| train/episodes                 | 8120       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.03       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.105     |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 324800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 203        |
| stats_o/mean                   | 0.20374054 |
| stats_o/std                    | 0.08654668 |
| test/episodes                  | 2040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0518    |
| test/info_shaping_reward_mean  | -0.157     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -29.513212 |
| test/Q_plus_P                  | -29.513212 |
| test/reward_per_eps            | -40        |
| test/steps                     | 81600      |
| train/episodes                 | 8160       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0444     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.075     |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.2      |
| train/steps                    | 326400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 204         |
| stats_o/mean                   | 0.20372812  |
| stats_o/std                    | 0.086632945 |
| test/episodes                  | 2050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.055       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0358     |
| test/info_shaping_reward_mean  | -0.159      |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -29.821095  |
| test/Q_plus_P                  | -29.821095  |
| test/reward_per_eps            | -37.8       |
| test/steps                     | 82000       |
| train/episodes                 | 8200        |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0331      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.112      |
| train/info_shaping_reward_mean | -0.167      |
| train/info_shaping_reward_min  | -0.201      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.7       |
| train/steps                    | 328000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 205         |
| stats_o/mean                   | 0.20370293  |
| stats_o/std                    | 0.086836725 |
| test/episodes                  | 2060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.105       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0147     |
| test/info_shaping_reward_mean  | -0.158      |
| test/info_shaping_reward_min   | -0.2        |
| test/Q                         | -30.024086  |
| test/Q_plus_P                  | -30.024086  |
| test/reward_per_eps            | -35.8       |
| test/steps                     | 82400       |
| train/episodes                 | 8240        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.045       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0761     |
| train/info_shaping_reward_mean | -0.153      |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.2       |
| train/steps                    | 329600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 206        |
| stats_o/mean                   | 0.20370905 |
| stats_o/std                    | 0.08687773 |
| test/episodes                  | 2070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0503    |
| test/info_shaping_reward_mean  | -0.146     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -28.882025 |
| test/Q_plus_P                  | -28.882025 |
| test/reward_per_eps            | -40        |
| test/steps                     | 82800      |
| train/episodes                 | 8280       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.134      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0499    |
| train/info_shaping_reward_mean | -0.147     |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.6      |
| train/steps                    | 331200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.20370765 |
| stats_o/std                    | 0.08694788 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.065      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0168    |
| test/info_shaping_reward_mean  | -0.158     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -29.96006  |
| test/Q_plus_P                  | -29.96006  |
| test/reward_per_eps            | -37.4      |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0156     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.129     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 208        |
| stats_o/mean                   | 0.20367818 |
| stats_o/std                    | 0.08708001 |
| test/episodes                  | 2090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.133      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.012     |
| test/info_shaping_reward_mean  | -0.15      |
| test/info_shaping_reward_min   | -0.331     |
| test/Q                         | -29.03336  |
| test/Q_plus_P                  | -29.03336  |
| test/reward_per_eps            | -34.7      |
| test/steps                     | 83600      |
| train/episodes                 | 8360       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0488     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0939    |
| train/info_shaping_reward_mean | -0.164     |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38        |
| train/steps                    | 334400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 209        |
| stats_o/mean                   | 0.20367281 |
| stats_o/std                    | 0.08719658 |
| test/episodes                  | 2100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.07       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0096    |
| test/info_shaping_reward_mean  | -0.157     |
| test/info_shaping_reward_min   | -0.203     |
| test/Q                         | -29.645195 |
| test/Q_plus_P                  | -29.645195 |
| test/reward_per_eps            | -37.2      |
| test/steps                     | 84000      |
| train/episodes                 | 8400       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0544     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0828    |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 336000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 210        |
| stats_o/mean                   | 0.20367761 |
| stats_o/std                    | 0.08734349 |
| test/episodes                  | 2110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.06       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0188    |
| test/info_shaping_reward_mean  | -0.148     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -28.675117 |
| test/Q_plus_P                  | -28.675117 |
| test/reward_per_eps            | -37.6      |
| test/steps                     | 84400      |
| train/episodes                 | 8440       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0475     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0821    |
| train/info_shaping_reward_mean | -0.16      |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.1      |
| train/steps                    | 337600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 211        |
| stats_o/mean                   | 0.20367509 |
| stats_o/std                    | 0.08735296 |
| test/episodes                  | 2120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.188      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0145    |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -26.980148 |
| test/Q_plus_P                  | -26.980148 |
| test/reward_per_eps            | -32.5      |
| test/steps                     | 84800      |
| train/episodes                 | 8480       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0325     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0858    |
| train/info_shaping_reward_mean | -0.162     |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 339200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 212        |
| stats_o/mean                   | 0.2037072  |
| stats_o/std                    | 0.08747595 |
| test/episodes                  | 2130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0025     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0493    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -30.100908 |
| test/Q_plus_P                  | -30.100908 |
| test/reward_per_eps            | -39.9      |
| test/steps                     | 85200      |
| train/episodes                 | 8520       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0625     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0668    |
| train/info_shaping_reward_mean | -0.159     |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 340800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 213         |
| stats_o/mean                   | 0.20368946  |
| stats_o/std                    | 0.087553084 |
| test/episodes                  | 2140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.2         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0205     |
| test/info_shaping_reward_mean  | -0.134      |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -27.305952  |
| test/Q_plus_P                  | -27.305952  |
| test/reward_per_eps            | -32         |
| test/steps                     | 85600       |
| train/episodes                 | 8560        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0606      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0884     |
| train/info_shaping_reward_mean | -0.159      |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.6       |
| train/steps                    | 342400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 214        |
| stats_o/mean                   | 0.20367588 |
| stats_o/std                    | 0.08763888 |
| test/episodes                  | 2150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.14       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00322   |
| test/info_shaping_reward_mean  | -0.145     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -28.158653 |
| test/Q_plus_P                  | -28.158653 |
| test/reward_per_eps            | -34.4      |
| test/steps                     | 86000      |
| train/episodes                 | 8600       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.102      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0683    |
| train/info_shaping_reward_mean | -0.154     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.9      |
| train/steps                    | 344000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 215        |
| stats_o/mean                   | 0.20366143 |
| stats_o/std                    | 0.08779988 |
| test/episodes                  | 2160       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0681    |
| test/info_shaping_reward_mean  | -0.161     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -30.524595 |
| test/Q_plus_P                  | -30.524595 |
| test/reward_per_eps            | -40        |
| test/steps                     | 86400      |
| train/episodes                 | 8640       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0312     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.103     |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 345600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 216        |
| stats_o/mean                   | 0.20368008 |
| stats_o/std                    | 0.08804897 |
| test/episodes                  | 2170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.017     |
| test/info_shaping_reward_mean  | -0.156     |
| test/info_shaping_reward_min   | -0.21      |
| test/Q                         | -28.73886  |
| test/Q_plus_P                  | -28.73886  |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 86800      |
| train/episodes                 | 8680       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.01       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0819    |
| train/info_shaping_reward_mean | -0.167     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 347200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 217        |
| stats_o/mean                   | 0.20367722 |
| stats_o/std                    | 0.08828741 |
| test/episodes                  | 2180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0775     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0024    |
| test/info_shaping_reward_mean  | -0.138     |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -28.338634 |
| test/Q_plus_P                  | -28.338634 |
| test/reward_per_eps            | -36.9      |
| test/steps                     | 87200      |
| train/episodes                 | 8720       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.105      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.046     |
| train/info_shaping_reward_mean | -0.146     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.8      |
| train/steps                    | 348800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 218         |
| stats_o/mean                   | 0.2036694   |
| stats_o/std                    | 0.088427804 |
| test/episodes                  | 2190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.14        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0235     |
| test/info_shaping_reward_mean  | -0.142      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -28.169256  |
| test/Q_plus_P                  | -28.169256  |
| test/reward_per_eps            | -34.4       |
| test/steps                     | 87600       |
| train/episodes                 | 8760        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.05        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0724     |
| train/info_shaping_reward_mean | -0.156      |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38         |
| train/steps                    | 350400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.20366825  |
| stats_o/std                    | 0.088545345 |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.085       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00989    |
| test/info_shaping_reward_mean  | -0.139      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -27.618233  |
| test/Q_plus_P                  | -27.618233  |
| test/reward_per_eps            | -36.6       |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.0912      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.051      |
| train/info_shaping_reward_mean | -0.15       |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.4       |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 220        |
| stats_o/mean                   | 0.20366636 |
| stats_o/std                    | 0.08860925 |
| test/episodes                  | 2210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.107     |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -31.018661 |
| test/Q_plus_P                  | -31.018661 |
| test/reward_per_eps            | -40        |
| test/steps                     | 88400      |
| train/episodes                 | 8840       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.102      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0381    |
| train/info_shaping_reward_mean | -0.147     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.9      |
| train/steps                    | 353600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 221        |
| stats_o/mean                   | 0.20365517 |
| stats_o/std                    | 0.08868357 |
| test/episodes                  | 2220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0295    |
| test/info_shaping_reward_mean  | -0.149     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -28.93043  |
| test/Q_plus_P                  | -28.93043  |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 88800      |
| train/episodes                 | 8880       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.106      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0575    |
| train/info_shaping_reward_mean | -0.155     |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.8      |
| train/steps                    | 355200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.203648   |
| stats_o/std                    | 0.0887866  |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.133      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0128    |
| test/info_shaping_reward_mean  | -0.143     |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -28.630772 |
| test/Q_plus_P                  | -28.630772 |
| test/reward_per_eps            | -34.7      |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0381     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0841    |
| train/info_shaping_reward_mean | -0.16      |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 223         |
| stats_o/mean                   | 0.20365652  |
| stats_o/std                    | 0.088921525 |
| test/episodes                  | 2240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.147       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00899    |
| test/info_shaping_reward_mean  | -0.127      |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -25.785732  |
| test/Q_plus_P                  | -25.785732  |
| test/reward_per_eps            | -34.1       |
| test/steps                     | 89600       |
| train/episodes                 | 8960        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.0906      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0346     |
| train/info_shaping_reward_mean | -0.142      |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.4       |
| train/steps                    | 358400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 224         |
| stats_o/mean                   | 0.20364298  |
| stats_o/std                    | 0.089051634 |
| test/episodes                  | 2250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.155       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0277     |
| test/info_shaping_reward_mean  | -0.109      |
| test/info_shaping_reward_min   | -0.201      |
| test/Q                         | -23.818638  |
| test/Q_plus_P                  | -23.818638  |
| test/reward_per_eps            | -33.8       |
| test/steps                     | 90000       |
| train/episodes                 | 9000        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.135       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0535     |
| train/info_shaping_reward_mean | -0.149      |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.6       |
| train/steps                    | 360000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 225        |
| stats_o/mean                   | 0.20365761 |
| stats_o/std                    | 0.0891783  |
| test/episodes                  | 2260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0675     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0293    |
| test/info_shaping_reward_mean  | -0.182     |
| test/info_shaping_reward_min   | -0.58      |
| test/Q                         | -28.87351  |
| test/Q_plus_P                  | -28.87351  |
| test/reward_per_eps            | -37.3      |
| test/steps                     | 90400      |
| train/episodes                 | 9040       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0656     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0829    |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.4      |
| train/steps                    | 361600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 226        |
| stats_o/mean                   | 0.20364787 |
| stats_o/std                    | 0.0893434  |
| test/episodes                  | 2270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.35       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.015     |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.676     |
| test/Q                         | -24.763174 |
| test/Q_plus_P                  | -24.763174 |
| test/reward_per_eps            | -26        |
| test/steps                     | 90800      |
| train/episodes                 | 9080       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0925     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0429    |
| train/info_shaping_reward_mean | -0.158     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.3      |
| train/steps                    | 363200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 227         |
| stats_o/mean                   | 0.20367296  |
| stats_o/std                    | 0.089535564 |
| test/episodes                  | 2280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.28        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0203     |
| test/info_shaping_reward_mean  | -0.123      |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -23.753792  |
| test/Q_plus_P                  | -23.753792  |
| test/reward_per_eps            | -28.8       |
| test/steps                     | 91200       |
| train/episodes                 | 9120        |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0712      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0953     |
| train/info_shaping_reward_mean | -0.17       |
| train/info_shaping_reward_min  | -0.292      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.1       |
| train/steps                    | 364800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 228        |
| stats_o/mean                   | 0.20368503 |
| stats_o/std                    | 0.08965517 |
| test/episodes                  | 2290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.075      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0187    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -27.966526 |
| test/Q_plus_P                  | -27.966526 |
| test/reward_per_eps            | -37        |
| test/steps                     | 91600      |
| train/episodes                 | 9160       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0581     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0723    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.7      |
| train/steps                    | 366400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 229        |
| stats_o/mean                   | 0.20374283 |
| stats_o/std                    | 0.09016401 |
| test/episodes                  | 2300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.203      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0101    |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.691     |
| test/Q                         | -25.07134  |
| test/Q_plus_P                  | -25.07134  |
| test/reward_per_eps            | -31.9      |
| test/steps                     | 92000      |
| train/episodes                 | 9200       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0206     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0613    |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.381     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 368000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 230        |
| stats_o/mean                   | 0.2037207  |
| stats_o/std                    | 0.09021744 |
| test/episodes                  | 2310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.242      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0185    |
| test/info_shaping_reward_mean  | -0.117     |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -23.063112 |
| test/Q_plus_P                  | -23.063112 |
| test/reward_per_eps            | -30.3      |
| test/steps                     | 92400      |
| train/episodes                 | 9240       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0762     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0751    |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.298     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37        |
| train/steps                    | 369600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 231        |
| stats_o/mean                   | 0.20375471 |
| stats_o/std                    | 0.09051592 |
| test/episodes                  | 2320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.228      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0117    |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -23.57383  |
| test/Q_plus_P                  | -23.57383  |
| test/reward_per_eps            | -30.9      |
| test/steps                     | 92800      |
| train/episodes                 | 9280       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0819     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0483    |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.393     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.7      |
| train/steps                    | 371200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 232        |
| stats_o/mean                   | 0.2037599  |
| stats_o/std                    | 0.09065584 |
| test/episodes                  | 2330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.697     |
| test/Q                         | -30.183144 |
| test/Q_plus_P                  | -30.183144 |
| test/reward_per_eps            | -40        |
| test/steps                     | 93200      |
| train/episodes                 | 9320       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0944     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0571    |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.2      |
| train/steps                    | 372800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 233        |
| stats_o/mean                   | 0.20376962 |
| stats_o/std                    | 0.09083726 |
| test/episodes                  | 2340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.198      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0141    |
| test/info_shaping_reward_mean  | -0.143     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -25.22649  |
| test/Q_plus_P                  | -25.22649  |
| test/reward_per_eps            | -32.1      |
| test/steps                     | 93600      |
| train/episodes                 | 9360       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0813     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.066     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.8      |
| train/steps                    | 374400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.20378429 |
| stats_o/std                    | 0.09112902 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.075      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00379   |
| test/info_shaping_reward_mean  | -0.193     |
| test/info_shaping_reward_min   | -0.537     |
| test/Q                         | -26.922047 |
| test/Q_plus_P                  | -26.922047 |
| test/reward_per_eps            | -37        |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0906     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0836    |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.4      |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 235        |
| stats_o/mean                   | 0.2037777  |
| stats_o/std                    | 0.09124355 |
| test/episodes                  | 2360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.35       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00864   |
| test/info_shaping_reward_mean  | -0.116     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -21.14657  |
| test/Q_plus_P                  | -21.14657  |
| test/reward_per_eps            | -26        |
| test/steps                     | 94400      |
| train/episodes                 | 9440       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0981     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.057     |
| train/info_shaping_reward_mean | -0.167     |
| train/info_shaping_reward_min  | -0.277     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.1      |
| train/steps                    | 377600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 236        |
| stats_o/mean                   | 0.20376149 |
| stats_o/std                    | 0.09129775 |
| test/episodes                  | 2370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.125      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0205    |
| test/info_shaping_reward_mean  | -0.152     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -25.794611 |
| test/Q_plus_P                  | -25.794611 |
| test/reward_per_eps            | -35        |
| test/steps                     | 94800      |
| train/episodes                 | 9480       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0537     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0901    |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.9      |
| train/steps                    | 379200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 237         |
| stats_o/mean                   | 0.20378737  |
| stats_o/std                    | 0.091414906 |
| test/episodes                  | 2380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.133       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.023      |
| test/info_shaping_reward_mean  | -0.143      |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -24.845598  |
| test/Q_plus_P                  | -24.845598  |
| test/reward_per_eps            | -34.7       |
| test/steps                     | 95200       |
| train/episodes                 | 9520        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.149       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0463     |
| train/info_shaping_reward_mean | -0.167      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34         |
| train/steps                    | 380800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 238         |
| stats_o/mean                   | 0.20377825  |
| stats_o/std                    | 0.091486126 |
| test/episodes                  | 2390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.27        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00162    |
| test/info_shaping_reward_mean  | -0.131      |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -25.310114  |
| test/Q_plus_P                  | -25.310114  |
| test/reward_per_eps            | -29.2       |
| test/steps                     | 95600       |
| train/episodes                 | 9560        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.0906      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0543     |
| train/info_shaping_reward_mean | -0.167      |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.4       |
| train/steps                    | 382400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 239        |
| stats_o/mean                   | 0.20377548 |
| stats_o/std                    | 0.09155133 |
| test/episodes                  | 2400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.14       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0284    |
| test/info_shaping_reward_mean  | -0.177     |
| test/info_shaping_reward_min   | -0.533     |
| test/Q                         | -26.954283 |
| test/Q_plus_P                  | -26.954283 |
| test/reward_per_eps            | -34.4      |
| test/steps                     | 96000      |
| train/episodes                 | 9600       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.085      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0738    |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.6      |
| train/steps                    | 384000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 240         |
| stats_o/mean                   | 0.20377527  |
| stats_o/std                    | 0.091655016 |
| test/episodes                  | 2410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.25        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00952    |
| test/info_shaping_reward_mean  | -0.13       |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -21.26211   |
| test/Q_plus_P                  | -21.26211   |
| test/reward_per_eps            | -30         |
| test/steps                     | 96400       |
| train/episodes                 | 9640        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0994      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0658     |
| train/info_shaping_reward_mean | -0.151      |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36         |
| train/steps                    | 385600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 241        |
| stats_o/mean                   | 0.20377533 |
| stats_o/std                    | 0.09166697 |
| test/episodes                  | 2420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.15       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00308   |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.628     |
| test/Q                         | -25.431782 |
| test/Q_plus_P                  | -25.431782 |
| test/reward_per_eps            | -34        |
| test/steps                     | 96800      |
| train/episodes                 | 9680       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.143      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0297    |
| train/info_shaping_reward_mean | -0.157     |
| train/info_shaping_reward_min  | -0.275     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.3      |
| train/steps                    | 387200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 242        |
| stats_o/mean                   | 0.20378006 |
| stats_o/std                    | 0.0918683  |
| test/episodes                  | 2430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.198      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0261    |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.669     |
| test/Q                         | -24.373608 |
| test/Q_plus_P                  | -24.373608 |
| test/reward_per_eps            | -32.1      |
| test/steps                     | 97200      |
| train/episodes                 | 9720       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.131      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0742    |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.8      |
| train/steps                    | 388800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 243        |
| stats_o/mean                   | 0.2037713  |
| stats_o/std                    | 0.09189417 |
| test/episodes                  | 2440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.142      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0195    |
| test/info_shaping_reward_mean  | -0.137     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -24.128983 |
| test/Q_plus_P                  | -24.128983 |
| test/reward_per_eps            | -34.3      |
| test/steps                     | 97600      |
| train/episodes                 | 9760       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0981     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.066     |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.1      |
| train/steps                    | 390400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.20375645 |
| stats_o/std                    | 0.09197607 |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.168      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0196    |
| test/info_shaping_reward_mean  | -0.14      |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -23.560904 |
| test/Q_plus_P                  | -23.560904 |
| test/reward_per_eps            | -33.3      |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.075      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0557    |
| train/info_shaping_reward_mean | -0.16      |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37        |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 245         |
| stats_o/mean                   | 0.20374514  |
| stats_o/std                    | 0.092082225 |
| test/episodes                  | 2460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.25        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00587    |
| test/info_shaping_reward_mean  | -0.119      |
| test/info_shaping_reward_min   | -0.212      |
| test/Q                         | -20.683193  |
| test/Q_plus_P                  | -20.683193  |
| test/reward_per_eps            | -30         |
| test/steps                     | 98400       |
| train/episodes                 | 9840        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0931      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0519     |
| train/info_shaping_reward_mean | -0.156      |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.3       |
| train/steps                    | 393600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 246        |
| stats_o/mean                   | 0.20369597 |
| stats_o/std                    | 0.09215011 |
| test/episodes                  | 2470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.133      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0131    |
| test/info_shaping_reward_mean  | -0.199     |
| test/info_shaping_reward_min   | -0.594     |
| test/Q                         | -26.952618 |
| test/Q_plus_P                  | -26.952618 |
| test/reward_per_eps            | -34.7      |
| test/steps                     | 98800      |
| train/episodes                 | 9880       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.104      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0615    |
| train/info_shaping_reward_mean | -0.151     |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.8      |
| train/steps                    | 395200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 247         |
| stats_o/mean                   | 0.20368706  |
| stats_o/std                    | 0.092403404 |
| test/episodes                  | 2480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.147       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0343     |
| test/info_shaping_reward_mean  | -0.147      |
| test/info_shaping_reward_min   | -0.21       |
| test/Q                         | -26.730198  |
| test/Q_plus_P                  | -26.730198  |
| test/reward_per_eps            | -34.1       |
| test/steps                     | 99200       |
| train/episodes                 | 9920        |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.102       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0551     |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.9       |
| train/steps                    | 396800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.20369738 |
| stats_o/std                    | 0.09256864 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.25       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0173    |
| test/info_shaping_reward_mean  | -0.158     |
| test/info_shaping_reward_min   | -0.546     |
| test/Q                         | -21.56023  |
| test/Q_plus_P                  | -21.56023  |
| test/reward_per_eps            | -30        |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.085      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0569    |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.6      |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 249        |
| stats_o/mean                   | 0.20370406 |
| stats_o/std                    | 0.09276435 |
| test/episodes                  | 2500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.135      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0163    |
| test/info_shaping_reward_mean  | -0.148     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -25.532705 |
| test/Q_plus_P                  | -25.532705 |
| test/reward_per_eps            | -34.6      |
| test/steps                     | 100000     |
| train/episodes                 | 10000      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.152      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0332    |
| train/info_shaping_reward_mean | -0.141     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.9      |
| train/steps                    | 400000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 250        |
| stats_o/mean                   | 0.20369627 |
| stats_o/std                    | 0.0927516  |
| test/episodes                  | 2510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.122      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.022     |
| test/info_shaping_reward_mean  | -0.145     |
| test/info_shaping_reward_min   | -0.371     |
| test/Q                         | -25.013926 |
| test/Q_plus_P                  | -25.013926 |
| test/reward_per_eps            | -35.1      |
| test/steps                     | 100400     |
| train/episodes                 | 10040      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.165      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0329    |
| train/info_shaping_reward_mean | -0.153     |
| train/info_shaping_reward_min  | -0.278     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.4      |
| train/steps                    | 401600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 251         |
| stats_o/mean                   | 0.20371333  |
| stats_o/std                    | 0.092751116 |
| test/episodes                  | 2520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.075       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0212     |
| test/info_shaping_reward_mean  | -0.152      |
| test/info_shaping_reward_min   | -0.199      |
| test/Q                         | -27.242998  |
| test/Q_plus_P                  | -27.242998  |
| test/reward_per_eps            | -37         |
| test/steps                     | 100800      |
| train/episodes                 | 10080       |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.118       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.049      |
| train/info_shaping_reward_mean | -0.141      |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.3       |
| train/steps                    | 403200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 252        |
| stats_o/mean                   | 0.20369273 |
| stats_o/std                    | 0.0927781  |
| test/episodes                  | 2530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.22       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0157    |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.806     |
| test/Q                         | -24.586143 |
| test/Q_plus_P                  | -24.586143 |
| test/reward_per_eps            | -31.2      |
| test/steps                     | 101200     |
| train/episodes                 | 10120      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.192      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0261    |
| train/info_shaping_reward_mean | -0.137     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.3      |
| train/steps                    | 404800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.20369354  |
| stats_o/std                    | 0.092901036 |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.312       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0171     |
| test/info_shaping_reward_mean  | -0.109      |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -18.855919  |
| test/Q_plus_P                  | -18.855919  |
| test/reward_per_eps            | -27.5       |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.18        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0199     |
| train/info_shaping_reward_mean | -0.163      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.8       |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 254        |
| stats_o/mean                   | 0.20368344 |
| stats_o/std                    | 0.09297823 |
| test/episodes                  | 2550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.285      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0204    |
| test/info_shaping_reward_mean  | -0.125     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -21.838472 |
| test/Q_plus_P                  | -21.838472 |
| test/reward_per_eps            | -28.6      |
| test/steps                     | 102000     |
| train/episodes                 | 10200      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.116      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0387    |
| train/info_shaping_reward_mean | -0.156     |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.4      |
| train/steps                    | 408000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 255        |
| stats_o/mean                   | 0.20368092 |
| stats_o/std                    | 0.09296403 |
| test/episodes                  | 2560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.323      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00969   |
| test/info_shaping_reward_mean  | -0.113     |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -18.836882 |
| test/Q_plus_P                  | -18.836882 |
| test/reward_per_eps            | -27.1      |
| test/steps                     | 102400     |
| train/episodes                 | 10240      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.164      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0366    |
| train/info_shaping_reward_mean | -0.147     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.4      |
| train/steps                    | 409600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 256        |
| stats_o/mean                   | 0.20368394 |
| stats_o/std                    | 0.09296975 |
| test/episodes                  | 2570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.28       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00794   |
| test/info_shaping_reward_mean  | -0.125     |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -21.83511  |
| test/Q_plus_P                  | -21.83511  |
| test/reward_per_eps            | -28.8      |
| test/steps                     | 102800     |
| train/episodes                 | 10280      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.153      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0456    |
| train/info_shaping_reward_mean | -0.152     |
| train/info_shaping_reward_min  | -0.276     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.9      |
| train/steps                    | 411200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 257         |
| stats_o/mean                   | 0.20368485  |
| stats_o/std                    | 0.093055956 |
| test/episodes                  | 2580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.31        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0116     |
| test/info_shaping_reward_mean  | -0.123      |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -20.324234  |
| test/Q_plus_P                  | -20.324234  |
| test/reward_per_eps            | -27.6       |
| test/steps                     | 103200      |
| train/episodes                 | 10320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.194       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0178     |
| train/info_shaping_reward_mean | -0.167      |
| train/info_shaping_reward_min  | -0.411      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.2       |
| train/steps                    | 412800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 258        |
| stats_o/mean                   | 0.20368403 |
| stats_o/std                    | 0.09318889 |
| test/episodes                  | 2590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0675     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0134    |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -27.695707 |
| test/Q_plus_P                  | -27.695707 |
| test/reward_per_eps            | -37.3      |
| test/steps                     | 103600     |
| train/episodes                 | 10360      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.105      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0552    |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.317     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.8      |
| train/steps                    | 414400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 259        |
| stats_o/mean                   | 0.20369051 |
| stats_o/std                    | 0.09334129 |
| test/episodes                  | 2600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0675     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0121    |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -28.174965 |
| test/Q_plus_P                  | -28.174965 |
| test/reward_per_eps            | -37.3      |
| test/steps                     | 104000     |
| train/episodes                 | 10400      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.202      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0341    |
| train/info_shaping_reward_mean | -0.155     |
| train/info_shaping_reward_min  | -0.288     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.9      |
| train/steps                    | 416000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 260        |
| stats_o/mean                   | 0.20364767 |
| stats_o/std                    | 0.09337246 |
| test/episodes                  | 2610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.207      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0134    |
| test/info_shaping_reward_mean  | -0.137     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -24.265224 |
| test/Q_plus_P                  | -24.265224 |
| test/reward_per_eps            | -31.7      |
| test/steps                     | 104400     |
| train/episodes                 | 10440      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.117      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0385    |
| train/info_shaping_reward_mean | -0.148     |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.3      |
| train/steps                    | 417600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 261         |
| stats_o/mean                   | 0.20364271  |
| stats_o/std                    | 0.093382396 |
| test/episodes                  | 2620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.29        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0132     |
| test/info_shaping_reward_mean  | -0.127      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -22.238798  |
| test/Q_plus_P                  | -22.238798  |
| test/reward_per_eps            | -28.4       |
| test/steps                     | 104800      |
| train/episodes                 | 10480       |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.111       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0622     |
| train/info_shaping_reward_mean | -0.154      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.6       |
| train/steps                    | 419200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.20362411  |
| stats_o/std                    | 0.093303815 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.362       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00743    |
| test/info_shaping_reward_mean  | -0.115      |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -20.273901  |
| test/Q_plus_P                  | -20.273901  |
| test/reward_per_eps            | -25.5       |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.116       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0584     |
| train/info_shaping_reward_mean | -0.149      |
| train/info_shaping_reward_min  | -0.202      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.4       |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.20360442  |
| stats_o/std                    | 0.093247615 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.205       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00897    |
| test/info_shaping_reward_mean  | -0.144      |
| test/info_shaping_reward_min   | -0.433      |
| test/Q                         | -23.567696  |
| test/Q_plus_P                  | -23.567696  |
| test/reward_per_eps            | -31.8       |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.184       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0467     |
| train/info_shaping_reward_mean | -0.144      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.6       |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 264         |
| stats_o/mean                   | 0.20358151  |
| stats_o/std                    | 0.093348265 |
| test/episodes                  | 2650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.282       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0143     |
| test/info_shaping_reward_mean  | -0.122      |
| test/info_shaping_reward_min   | -0.2        |
| test/Q                         | -20.686934  |
| test/Q_plus_P                  | -20.686934  |
| test/reward_per_eps            | -28.7       |
| test/steps                     | 106000      |
| train/episodes                 | 10600       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.202       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0231     |
| train/info_shaping_reward_mean | -0.158      |
| train/info_shaping_reward_min  | -0.37       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.9       |
| train/steps                    | 424000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 265        |
| stats_o/mean                   | 0.20356667 |
| stats_o/std                    | 0.09333656 |
| test/episodes                  | 2660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.27       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0155    |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -22.290283 |
| test/Q_plus_P                  | -22.290283 |
| test/reward_per_eps            | -29.2      |
| test/steps                     | 106400     |
| train/episodes                 | 10640      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.12       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0448    |
| train/info_shaping_reward_mean | -0.141     |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.2      |
| train/steps                    | 425600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.2035651   |
| stats_o/std                    | 0.093265735 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.41        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00513    |
| test/info_shaping_reward_mean  | -0.0966     |
| test/info_shaping_reward_min   | -0.193      |
| test/Q                         | -15.433158  |
| test/Q_plus_P                  | -15.433158  |
| test/reward_per_eps            | -23.6       |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.175       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0533     |
| train/info_shaping_reward_mean | -0.14       |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33         |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 267         |
| stats_o/mean                   | 0.20355672  |
| stats_o/std                    | 0.093301445 |
| test/episodes                  | 2680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.255       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00885    |
| test/info_shaping_reward_mean  | -0.132      |
| test/info_shaping_reward_min   | -0.202      |
| test/Q                         | -21.556835  |
| test/Q_plus_P                  | -21.556835  |
| test/reward_per_eps            | -29.8       |
| test/steps                     | 107200      |
| train/episodes                 | 10720       |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.129       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0465     |
| train/info_shaping_reward_mean | -0.153      |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.9       |
| train/steps                    | 428800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 268        |
| stats_o/mean                   | 0.20355414 |
| stats_o/std                    | 0.09324486 |
| test/episodes                  | 2690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.328      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0122    |
| test/info_shaping_reward_mean  | -0.0998    |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -17.150263 |
| test/Q_plus_P                  | -17.150263 |
| test/reward_per_eps            | -26.9      |
| test/steps                     | 107600     |
| train/episodes                 | 10760      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.157      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0409    |
| train/info_shaping_reward_mean | -0.144     |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.7      |
| train/steps                    | 430400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 269        |
| stats_o/mean                   | 0.20355225 |
| stats_o/std                    | 0.09322574 |
| test/episodes                  | 2700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.268      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0177    |
| test/info_shaping_reward_mean  | -0.125     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -21.680483 |
| test/Q_plus_P                  | -21.680483 |
| test/reward_per_eps            | -29.3      |
| test/steps                     | 108000     |
| train/episodes                 | 10800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.246      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.015     |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.2      |
| train/steps                    | 432000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 270        |
| stats_o/mean                   | 0.20356013 |
| stats_o/std                    | 0.09320878 |
| test/episodes                  | 2710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.205      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00826   |
| test/info_shaping_reward_mean  | -0.134     |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -22.4023   |
| test/Q_plus_P                  | -22.4023   |
| test/reward_per_eps            | -31.8      |
| test/steps                     | 108400     |
| train/episodes                 | 10840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.261      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0169    |
| train/info_shaping_reward_mean | -0.137     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.6      |
| train/steps                    | 433600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 271        |
| stats_o/mean                   | 0.20356451 |
| stats_o/std                    | 0.09321499 |
| test/episodes                  | 2720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.133      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0302    |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.42      |
| test/Q                         | -27.124395 |
| test/Q_plus_P                  | -27.124395 |
| test/reward_per_eps            | -34.7      |
| test/steps                     | 108800     |
| train/episodes                 | 10880      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.137      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0288    |
| train/info_shaping_reward_mean | -0.143     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.5      |
| train/steps                    | 435200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 272        |
| stats_o/mean                   | 0.20356663 |
| stats_o/std                    | 0.09328495 |
| test/episodes                  | 2730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.255      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00454   |
| test/info_shaping_reward_mean  | -0.114     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -18.014555 |
| test/Q_plus_P                  | -18.014555 |
| test/reward_per_eps            | -29.8      |
| test/steps                     | 109200     |
| train/episodes                 | 10920      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.212      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0377    |
| train/info_shaping_reward_mean | -0.141     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.5      |
| train/steps                    | 436800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 273        |
| stats_o/mean                   | 0.20355673 |
| stats_o/std                    | 0.09331458 |
| test/episodes                  | 2740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.235      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0164    |
| test/info_shaping_reward_mean  | -0.113     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -20.570795 |
| test/Q_plus_P                  | -20.570795 |
| test/reward_per_eps            | -30.6      |
| test/steps                     | 109600     |
| train/episodes                 | 10960      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.144      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0447    |
| train/info_shaping_reward_mean | -0.153     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.2      |
| train/steps                    | 438400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 274        |
| stats_o/mean                   | 0.2035477  |
| stats_o/std                    | 0.09328825 |
| test/episodes                  | 2750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.32       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00284   |
| test/info_shaping_reward_mean  | -0.115     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -19.20322  |
| test/Q_plus_P                  | -19.20322  |
| test/reward_per_eps            | -27.2      |
| test/steps                     | 110000     |
| train/episodes                 | 11000      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.17       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0413    |
| train/info_shaping_reward_mean | -0.146     |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.2      |
| train/steps                    | 440000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 275        |
| stats_o/mean                   | 0.20357287 |
| stats_o/std                    | 0.09340623 |
| test/episodes                  | 2760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.215      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0123    |
| test/info_shaping_reward_mean  | -0.131     |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -15.402508 |
| test/Q_plus_P                  | -15.402508 |
| test/reward_per_eps            | -31.4      |
| test/steps                     | 110400     |
| train/episodes                 | 11040      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.115      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0385    |
| train/info_shaping_reward_mean | -0.146     |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.4      |
| train/steps                    | 441600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 276         |
| stats_o/mean                   | 0.20357288  |
| stats_o/std                    | 0.093364805 |
| test/episodes                  | 2770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.147       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00433    |
| test/info_shaping_reward_mean  | -0.144      |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -25.740473  |
| test/Q_plus_P                  | -25.740473  |
| test/reward_per_eps            | -34.1       |
| test/steps                     | 110800      |
| train/episodes                 | 11080       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.234       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0243     |
| train/info_shaping_reward_mean | -0.13       |
| train/info_shaping_reward_min  | -0.211      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.6       |
| train/steps                    | 443200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 277         |
| stats_o/mean                   | 0.20357576  |
| stats_o/std                    | 0.093454234 |
| test/episodes                  | 2780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.338       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0058     |
| test/info_shaping_reward_mean  | -0.113      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -18.717457  |
| test/Q_plus_P                  | -18.717457  |
| test/reward_per_eps            | -26.5       |
| test/steps                     | 111200      |
| train/episodes                 | 11120       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.216       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0227     |
| train/info_shaping_reward_mean | -0.148      |
| train/info_shaping_reward_min  | -0.297      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.4       |
| train/steps                    | 444800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 278        |
| stats_o/mean                   | 0.20357956 |
| stats_o/std                    | 0.09346536 |
| test/episodes                  | 2790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.432      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00901   |
| test/info_shaping_reward_mean  | -0.105     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -16.604347 |
| test/Q_plus_P                  | -16.604347 |
| test/reward_per_eps            | -22.7      |
| test/steps                     | 111600     |
| train/episodes                 | 11160      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.134      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0682    |
| train/info_shaping_reward_mean | -0.153     |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.6      |
| train/steps                    | 446400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 279        |
| stats_o/mean                   | 0.20357308 |
| stats_o/std                    | 0.093516   |
| test/episodes                  | 2800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.435      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0161    |
| test/info_shaping_reward_mean  | -0.105     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -14.374519 |
| test/Q_plus_P                  | -14.374519 |
| test/reward_per_eps            | -22.6      |
| test/steps                     | 112000     |
| train/episodes                 | 11200      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.217      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0177    |
| train/info_shaping_reward_mean | -0.13      |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.3      |
| train/steps                    | 448000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 280        |
| stats_o/mean                   | 0.20359504 |
| stats_o/std                    | 0.09370694 |
| test/episodes                  | 2810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.355      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0129    |
| test/info_shaping_reward_mean  | -0.103     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -17.006737 |
| test/Q_plus_P                  | -17.006737 |
| test/reward_per_eps            | -25.8      |
| test/steps                     | 112400     |
| train/episodes                 | 11240      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.162      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0488    |
| train/info_shaping_reward_mean | -0.149     |
| train/info_shaping_reward_min  | -0.29      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.5      |
| train/steps                    | 449600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.20357077 |
| stats_o/std                    | 0.0936943  |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.223      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0233    |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -21.563221 |
| test/Q_plus_P                  | -21.563221 |
| test/reward_per_eps            | -31.1      |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.206      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0119    |
| train/info_shaping_reward_mean | -0.141     |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.8      |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.20353453 |
| stats_o/std                    | 0.09369206 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.352      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0091    |
| test/info_shaping_reward_mean  | -0.105     |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -16.246515 |
| test/Q_plus_P                  | -16.246515 |
| test/reward_per_eps            | -25.9      |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.215      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0205    |
| train/info_shaping_reward_mean | -0.131     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.4      |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 283         |
| stats_o/mean                   | 0.20353712  |
| stats_o/std                    | 0.093766205 |
| test/episodes                  | 2840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.212       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0061     |
| test/info_shaping_reward_mean  | -0.134      |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -22.921724  |
| test/Q_plus_P                  | -22.921724  |
| test/reward_per_eps            | -31.5       |
| test/steps                     | 113600      |
| train/episodes                 | 11360       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.166       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0474     |
| train/info_shaping_reward_mean | -0.152      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.4       |
| train/steps                    | 454400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 284        |
| stats_o/mean                   | 0.20353056 |
| stats_o/std                    | 0.09394502 |
| test/episodes                  | 2850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.215      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0182    |
| test/info_shaping_reward_mean  | -0.133     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -22.149769 |
| test/Q_plus_P                  | -22.149769 |
| test/reward_per_eps            | -31.4      |
| test/steps                     | 114000     |
| train/episodes                 | 11400      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.175      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0338    |
| train/info_shaping_reward_mean | -0.143     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33        |
| train/steps                    | 456000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 285        |
| stats_o/mean                   | 0.203537   |
| stats_o/std                    | 0.09401202 |
| test/episodes                  | 2860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.28       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0269    |
| test/info_shaping_reward_mean  | -0.134     |
| test/info_shaping_reward_min   | -0.208     |
| test/Q                         | -19.925186 |
| test/Q_plus_P                  | -19.925186 |
| test/reward_per_eps            | -28.8      |
| test/steps                     | 114400     |
| train/episodes                 | 11440      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.155      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0302    |
| train/info_shaping_reward_mean | -0.151     |
| train/info_shaping_reward_min  | -0.29      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.8      |
| train/steps                    | 457600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 286        |
| stats_o/mean                   | 0.20354871 |
| stats_o/std                    | 0.09399929 |
| test/episodes                  | 2870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.362      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00897   |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.774     |
| test/Q                         | -17.096031 |
| test/Q_plus_P                  | -17.096031 |
| test/reward_per_eps            | -25.5      |
| test/steps                     | 114800     |
| train/episodes                 | 11480      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.171      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0425    |
| train/info_shaping_reward_mean | -0.135     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.2      |
| train/steps                    | 459200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.20354092 |
| stats_o/std                    | 0.09399035 |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.475      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0106    |
| test/info_shaping_reward_mean  | -0.0952    |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -13.294225 |
| test/Q_plus_P                  | -13.294225 |
| test/reward_per_eps            | -21        |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.248      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0208    |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.1      |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 288        |
| stats_o/mean                   | 0.20352587 |
| stats_o/std                    | 0.09401059 |
| test/episodes                  | 2890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.287      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00692   |
| test/info_shaping_reward_mean  | -0.118     |
| test/info_shaping_reward_min   | -0.223     |
| test/Q                         | -19.304089 |
| test/Q_plus_P                  | -19.304089 |
| test/reward_per_eps            | -28.5      |
| test/steps                     | 115600     |
| train/episodes                 | 11560      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.24       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0165    |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.4      |
| train/steps                    | 462400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 289        |
| stats_o/mean                   | 0.20350607 |
| stats_o/std                    | 0.09399869 |
| test/episodes                  | 2900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.287      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.014     |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.522     |
| test/Q                         | -20.723042 |
| test/Q_plus_P                  | -20.723042 |
| test/reward_per_eps            | -28.5      |
| test/steps                     | 116000     |
| train/episodes                 | 11600      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.284      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0169    |
| train/info_shaping_reward_mean | -0.117     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.6      |
| train/steps                    | 464000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 290        |
| stats_o/mean                   | 0.203516   |
| stats_o/std                    | 0.0940562  |
| test/episodes                  | 2910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.417      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00977   |
| test/info_shaping_reward_mean  | -0.105     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -14.829991 |
| test/Q_plus_P                  | -14.829991 |
| test/reward_per_eps            | -23.3      |
| test/steps                     | 116400     |
| train/episodes                 | 11640      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.216      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0306    |
| train/info_shaping_reward_mean | -0.148     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.4      |
| train/steps                    | 465600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 291        |
| stats_o/mean                   | 0.20351587 |
| stats_o/std                    | 0.09404531 |
| test/episodes                  | 2920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.233      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00605   |
| test/info_shaping_reward_mean  | -0.134     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -19.725203 |
| test/Q_plus_P                  | -19.725203 |
| test/reward_per_eps            | -30.7      |
| test/steps                     | 116800     |
| train/episodes                 | 11680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.212      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.016     |
| train/info_shaping_reward_mean | -0.134     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.5      |
| train/steps                    | 467200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 292        |
| stats_o/mean                   | 0.2034988  |
| stats_o/std                    | 0.09411375 |
| test/episodes                  | 2930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.28       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0196    |
| test/info_shaping_reward_mean  | -0.131     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -19.616007 |
| test/Q_plus_P                  | -19.616007 |
| test/reward_per_eps            | -28.8      |
| test/steps                     | 117200     |
| train/episodes                 | 11720      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.145      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0289    |
| train/info_shaping_reward_mean | -0.155     |
| train/info_shaping_reward_min  | -0.302     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.2      |
| train/steps                    | 468800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 293        |
| stats_o/mean                   | 0.20349827 |
| stats_o/std                    | 0.09420378 |
| test/episodes                  | 2940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.292      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0062    |
| test/info_shaping_reward_mean  | -0.121     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -17.203716 |
| test/Q_plus_P                  | -17.203716 |
| test/reward_per_eps            | -28.3      |
| test/steps                     | 117600     |
| train/episodes                 | 11760      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.172      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.033     |
| train/info_shaping_reward_mean | -0.158     |
| train/info_shaping_reward_min  | -0.297     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.1      |
| train/steps                    | 470400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 294        |
| stats_o/mean                   | 0.20349094 |
| stats_o/std                    | 0.09419992 |
| test/episodes                  | 2950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.275      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0179    |
| test/info_shaping_reward_mean  | -0.122     |
| test/info_shaping_reward_min   | -0.209     |
| test/Q                         | -17.769785 |
| test/Q_plus_P                  | -17.769785 |
| test/reward_per_eps            | -29        |
| test/steps                     | 118000     |
| train/episodes                 | 11800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.259      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0127    |
| train/info_shaping_reward_mean | -0.124     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.6      |
| train/steps                    | 472000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 295        |
| stats_o/mean                   | 0.20348455 |
| stats_o/std                    | 0.0941909  |
| test/episodes                  | 2960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.583      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00894   |
| test/info_shaping_reward_mean  | -0.0829    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -8.736657  |
| test/Q_plus_P                  | -8.736657  |
| test/reward_per_eps            | -16.7      |
| test/steps                     | 118400     |
| train/episodes                 | 11840      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.225      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0332    |
| train/info_shaping_reward_mean | -0.136     |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31        |
| train/steps                    | 473600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 296        |
| stats_o/mean                   | 0.20349242 |
| stats_o/std                    | 0.09421342 |
| test/episodes                  | 2970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.412      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0149    |
| test/info_shaping_reward_mean  | -0.118     |
| test/info_shaping_reward_min   | -0.506     |
| test/Q                         | -13.347738 |
| test/Q_plus_P                  | -13.347738 |
| test/reward_per_eps            | -23.5      |
| test/steps                     | 118800     |
| train/episodes                 | 11880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.277      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0137    |
| train/info_shaping_reward_mean | -0.139     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.9      |
| train/steps                    | 475200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 297         |
| stats_o/mean                   | 0.20349087  |
| stats_o/std                    | 0.094167374 |
| test/episodes                  | 2980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.448       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00597    |
| test/info_shaping_reward_mean  | -0.0905     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -11.593198  |
| test/Q_plus_P                  | -11.593198  |
| test/reward_per_eps            | -22.1       |
| test/steps                     | 119200      |
| train/episodes                 | 11920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.289       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0126     |
| train/info_shaping_reward_mean | -0.118      |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.4       |
| train/steps                    | 476800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 298        |
| stats_o/mean                   | 0.203466   |
| stats_o/std                    | 0.09416635 |
| test/episodes                  | 2990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.517      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0133    |
| test/info_shaping_reward_mean  | -0.0899    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -12.604013 |
| test/Q_plus_P                  | -12.604013 |
| test/reward_per_eps            | -19.3      |
| test/steps                     | 119600     |
| train/episodes                 | 11960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.268      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.016     |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.3      |
| train/steps                    | 478400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 299        |
| stats_o/mean                   | 0.20346506 |
| stats_o/std                    | 0.09415531 |
| test/episodes                  | 3000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.142      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0108    |
| test/info_shaping_reward_mean  | -0.152     |
| test/info_shaping_reward_min   | -0.22      |
| test/Q                         | -25.261509 |
| test/Q_plus_P                  | -25.261509 |
| test/reward_per_eps            | -34.3      |
| test/steps                     | 120000     |
| train/episodes                 | 12000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.263      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0133    |
| train/info_shaping_reward_mean | -0.121     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.5      |
| train/steps                    | 480000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 300        |
| stats_o/mean                   | 0.20345761 |
| stats_o/std                    | 0.09416852 |
| test/episodes                  | 3010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.352      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00917   |
| test/info_shaping_reward_mean  | -0.116     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -18.385777 |
| test/Q_plus_P                  | -18.385777 |
| test/reward_per_eps            | -25.9      |
| test/steps                     | 120400     |
| train/episodes                 | 12040      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.28       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.023     |
| train/info_shaping_reward_mean | -0.133     |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.8      |
| train/steps                    | 481600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.20347297 |
| stats_o/std                    | 0.09417009 |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.453      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0109    |
| test/info_shaping_reward_mean  | -0.0966    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -14.48137  |
| test/Q_plus_P                  | -14.48137  |
| test/reward_per_eps            | -21.9      |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.258      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0263    |
| train/info_shaping_reward_mean | -0.13      |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.7      |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 302        |
| stats_o/mean                   | 0.20347255 |
| stats_o/std                    | 0.09411094 |
| test/episodes                  | 3030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.297      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0136    |
| test/info_shaping_reward_mean  | -0.108     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -17.831598 |
| test/Q_plus_P                  | -17.831598 |
| test/reward_per_eps            | -28.1      |
| test/steps                     | 121200     |
| train/episodes                 | 12120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.274      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0108    |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.1      |
| train/steps                    | 484800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 303         |
| stats_o/mean                   | 0.2034774   |
| stats_o/std                    | 0.094089545 |
| test/episodes                  | 3040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.458       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00744    |
| test/info_shaping_reward_mean  | -0.095      |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -12.190976  |
| test/Q_plus_P                  | -12.190976  |
| test/reward_per_eps            | -21.7       |
| test/steps                     | 121600      |
| train/episodes                 | 12160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.268       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0167     |
| train/info_shaping_reward_mean | -0.129      |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.3       |
| train/steps                    | 486400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.20346649  |
| stats_o/std                    | 0.094098195 |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.36        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00753    |
| test/info_shaping_reward_mean  | -0.115      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -15.229197  |
| test/Q_plus_P                  | -15.229197  |
| test/reward_per_eps            | -25.6       |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.226       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0218     |
| train/info_shaping_reward_mean | -0.127      |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31         |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 305         |
| stats_o/mean                   | 0.20349303  |
| stats_o/std                    | 0.094058484 |
| test/episodes                  | 3060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.287       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.027      |
| test/info_shaping_reward_mean  | -0.13       |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -20.21445   |
| test/Q_plus_P                  | -20.21445   |
| test/reward_per_eps            | -28.5       |
| test/steps                     | 122400      |
| train/episodes                 | 12240       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.324       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0238     |
| train/info_shaping_reward_mean | -0.113      |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.1       |
| train/steps                    | 489600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 306         |
| stats_o/mean                   | 0.20350416  |
| stats_o/std                    | 0.094064206 |
| test/episodes                  | 3070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.6         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00789    |
| test/info_shaping_reward_mean  | -0.0778     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -9.245358   |
| test/Q_plus_P                  | -9.245358   |
| test/reward_per_eps            | -16         |
| test/steps                     | 122800      |
| train/episodes                 | 12280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.341       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0185     |
| train/info_shaping_reward_mean | -0.11       |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.4       |
| train/steps                    | 491200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.20350824 |
| stats_o/std                    | 0.09405728 |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.215      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000154  |
| test/info_shaping_reward_mean  | -0.133     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -22.559248 |
| test/Q_plus_P                  | -22.559248 |
| test/reward_per_eps            | -31.4      |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.232      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0176    |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.7      |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 308        |
| stats_o/mean                   | 0.20350713 |
| stats_o/std                    | 0.09404722 |
| test/episodes                  | 3090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.51       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0103    |
| test/info_shaping_reward_mean  | -0.0868    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -11.189827 |
| test/Q_plus_P                  | -11.189827 |
| test/reward_per_eps            | -19.6      |
| test/steps                     | 123600     |
| train/episodes                 | 12360      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.266      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0241    |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.4      |
| train/steps                    | 494400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 309        |
| stats_o/mean                   | 0.20350002 |
| stats_o/std                    | 0.09405306 |
| test/episodes                  | 3100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.195      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00499   |
| test/info_shaping_reward_mean  | -0.138     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -23.165485 |
| test/Q_plus_P                  | -23.165485 |
| test/reward_per_eps            | -32.2      |
| test/steps                     | 124000     |
| train/episodes                 | 12400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.258      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0157    |
| train/info_shaping_reward_mean | -0.13      |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.7      |
| train/steps                    | 496000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 310        |
| stats_o/mean                   | 0.2034978  |
| stats_o/std                    | 0.09405016 |
| test/episodes                  | 3110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.445      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00219   |
| test/info_shaping_reward_mean  | -0.0971    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -14.28492  |
| test/Q_plus_P                  | -14.28492  |
| test/reward_per_eps            | -22.2      |
| test/steps                     | 124400     |
| train/episodes                 | 12440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.233      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0122    |
| train/info_shaping_reward_mean | -0.131     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.7      |
| train/steps                    | 497600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.2034929  |
| stats_o/std                    | 0.09403832 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.58       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00606   |
| test/info_shaping_reward_mean  | -0.0804    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.947961  |
| test/Q_plus_P                  | -9.947961  |
| test/reward_per_eps            | -16.8      |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.217      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0235    |
| train/info_shaping_reward_mean | -0.134     |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.3      |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 312        |
| stats_o/mean                   | 0.20348439 |
| stats_o/std                    | 0.09403821 |
| test/episodes                  | 3130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.432      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00502   |
| test/info_shaping_reward_mean  | -0.104     |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -14.468542 |
| test/Q_plus_P                  | -14.468542 |
| test/reward_per_eps            | -22.7      |
| test/steps                     | 125200     |
| train/episodes                 | 12520      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.195      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0234    |
| train/info_shaping_reward_mean | -0.132     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.2      |
| train/steps                    | 500800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.2034915   |
| stats_o/std                    | 0.094002664 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.38        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0173     |
| test/info_shaping_reward_mean  | -0.108      |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -15.888039  |
| test/Q_plus_P                  | -15.888039  |
| test/reward_per_eps            | -24.8       |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.284       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0243     |
| train/info_shaping_reward_mean | -0.123      |
| train/info_shaping_reward_min  | -0.197      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.6       |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 314        |
| stats_o/mean                   | 0.20349117 |
| stats_o/std                    | 0.09395411 |
| test/episodes                  | 3150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.26       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0122    |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.206     |
| test/Q                         | -21.031649 |
| test/Q_plus_P                  | -21.031649 |
| test/reward_per_eps            | -29.6      |
| test/steps                     | 126000     |
| train/episodes                 | 12600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.224      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0166    |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.1      |
| train/steps                    | 504000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 315        |
| stats_o/mean                   | 0.20350042 |
| stats_o/std                    | 0.09393096 |
| test/episodes                  | 3160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.427      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00441   |
| test/info_shaping_reward_mean  | -0.0992    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -15.421862 |
| test/Q_plus_P                  | -15.421862 |
| test/reward_per_eps            | -22.9      |
| test/steps                     | 126400     |
| train/episodes                 | 12640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.304      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0127    |
| train/info_shaping_reward_mean | -0.115     |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.9      |
| train/steps                    | 505600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.20350532  |
| stats_o/std                    | 0.093903124 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.28        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00866    |
| test/info_shaping_reward_mean  | -0.11       |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -16.951574  |
| test/Q_plus_P                  | -16.951574  |
| test/reward_per_eps            | -28.8       |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.296       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0162     |
| train/info_shaping_reward_mean | -0.123      |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.2       |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 317        |
| stats_o/mean                   | 0.2034992  |
| stats_o/std                    | 0.09393212 |
| test/episodes                  | 3180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.287      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000661  |
| test/info_shaping_reward_mean  | -0.131     |
| test/info_shaping_reward_min   | -0.283     |
| test/Q                         | -19.592548 |
| test/Q_plus_P                  | -19.592548 |
| test/reward_per_eps            | -28.5      |
| test/steps                     | 127200     |
| train/episodes                 | 12720      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.302      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0276    |
| train/info_shaping_reward_mean | -0.118     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.9      |
| train/steps                    | 508800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 318        |
| stats_o/mean                   | 0.20349735 |
| stats_o/std                    | 0.09397994 |
| test/episodes                  | 3190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.405      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00495   |
| test/info_shaping_reward_mean  | -0.125     |
| test/info_shaping_reward_min   | -0.663     |
| test/Q                         | -15.068467 |
| test/Q_plus_P                  | -15.068467 |
| test/reward_per_eps            | -23.8      |
| test/steps                     | 127600     |
| train/episodes                 | 12760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.314      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00993   |
| train/info_shaping_reward_mean | -0.119     |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.4      |
| train/steps                    | 510400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 319        |
| stats_o/mean                   | 0.20349583 |
| stats_o/std                    | 0.0939224  |
| test/episodes                  | 3200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00389   |
| test/info_shaping_reward_mean  | -0.073     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -8.641035  |
| test/Q_plus_P                  | -8.641035  |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 128000     |
| train/episodes                 | 12800      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.297      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0169    |
| train/info_shaping_reward_mean | -0.111     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.1      |
| train/steps                    | 512000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 320        |
| stats_o/mean                   | 0.2034863  |
| stats_o/std                    | 0.0938967  |
| test/episodes                  | 3210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.41       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0172    |
| test/info_shaping_reward_mean  | -0.104     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -12.691495 |
| test/Q_plus_P                  | -12.691495 |
| test/reward_per_eps            | -23.6      |
| test/steps                     | 128400     |
| train/episodes                 | 12840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.329      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0128    |
| train/info_shaping_reward_mean | -0.112     |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.8      |
| train/steps                    | 513600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 321        |
| stats_o/mean                   | 0.20347577 |
| stats_o/std                    | 0.09388191 |
| test/episodes                  | 3220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.357      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00885   |
| test/info_shaping_reward_mean  | -0.119     |
| test/info_shaping_reward_min   | -0.572     |
| test/Q                         | -17.636255 |
| test/Q_plus_P                  | -17.636255 |
| test/reward_per_eps            | -25.7      |
| test/steps                     | 128800     |
| train/episodes                 | 12880      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.244      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0193    |
| train/info_shaping_reward_mean | -0.122     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.2      |
| train/steps                    | 515200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 322         |
| stats_o/mean                   | 0.20346636  |
| stats_o/std                    | 0.093837604 |
| test/episodes                  | 3230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.345       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0125     |
| test/info_shaping_reward_mean  | -0.0923     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -13.175462  |
| test/Q_plus_P                  | -13.175462  |
| test/reward_per_eps            | -26.2       |
| test/steps                     | 129200      |
| train/episodes                 | 12920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.289       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0129     |
| train/info_shaping_reward_mean | -0.115      |
| train/info_shaping_reward_min  | -0.203      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.4       |
| train/steps                    | 516800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 323        |
| stats_o/mean                   | 0.20346302 |
| stats_o/std                    | 0.09387761 |
| test/episodes                  | 3240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.455      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00603   |
| test/info_shaping_reward_mean  | -0.0894    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -9.689155  |
| test/Q_plus_P                  | -9.689155  |
| test/reward_per_eps            | -21.8      |
| test/steps                     | 129600     |
| train/episodes                 | 12960      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.303      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0153    |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.9      |
| train/steps                    | 518400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 324        |
| stats_o/mean                   | 0.20346327 |
| stats_o/std                    | 0.09386549 |
| test/episodes                  | 3250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.502      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0093    |
| test/info_shaping_reward_mean  | -0.0888    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -11.400188 |
| test/Q_plus_P                  | -11.400188 |
| test/reward_per_eps            | -19.9      |
| test/steps                     | 130000     |
| train/episodes                 | 13000      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.16       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0195    |
| train/info_shaping_reward_mean | -0.133     |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.6      |
| train/steps                    | 520000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 325         |
| stats_o/mean                   | 0.20343809  |
| stats_o/std                    | 0.093886025 |
| test/episodes                  | 3260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.36        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0043     |
| test/info_shaping_reward_mean  | -0.1        |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -14.907205  |
| test/Q_plus_P                  | -14.907205  |
| test/reward_per_eps            | -25.6       |
| test/steps                     | 130400      |
| train/episodes                 | 13040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.267       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0137     |
| train/info_shaping_reward_mean | -0.124      |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.3       |
| train/steps                    | 521600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 326        |
| stats_o/mean                   | 0.20344476 |
| stats_o/std                    | 0.09387289 |
| test/episodes                  | 3270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.573      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00391   |
| test/info_shaping_reward_mean  | -0.0784    |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -9.297574  |
| test/Q_plus_P                  | -9.297574  |
| test/reward_per_eps            | -17.1      |
| test/steps                     | 130800     |
| train/episodes                 | 13080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.285      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0156    |
| train/info_shaping_reward_mean | -0.12      |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.6      |
| train/steps                    | 523200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 327        |
| stats_o/mean                   | 0.20341294 |
| stats_o/std                    | 0.09386249 |
| test/episodes                  | 3280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.585      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00344   |
| test/info_shaping_reward_mean  | -0.0823    |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -8.383707  |
| test/Q_plus_P                  | -8.383707  |
| test/reward_per_eps            | -16.6      |
| test/steps                     | 131200     |
| train/episodes                 | 13120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.369      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0147    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.2      |
| train/steps                    | 524800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 328         |
| stats_o/mean                   | 0.20339862  |
| stats_o/std                    | 0.093898475 |
| test/episodes                  | 3290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.495       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00538    |
| test/info_shaping_reward_mean  | -0.0912     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -11.596461  |
| test/Q_plus_P                  | -11.596461  |
| test/reward_per_eps            | -20.2       |
| test/steps                     | 131600      |
| train/episodes                 | 13160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.351       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0105     |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26         |
| train/steps                    | 526400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 329         |
| stats_o/mean                   | 0.20337342  |
| stats_o/std                    | 0.093919404 |
| test/episodes                  | 3300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.253       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00555    |
| test/info_shaping_reward_mean  | -0.122      |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -19.395187  |
| test/Q_plus_P                  | -19.395187  |
| test/reward_per_eps            | -29.9       |
| test/steps                     | 132000      |
| train/episodes                 | 13200       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.209       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0176     |
| train/info_shaping_reward_mean | -0.125      |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.6       |
| train/steps                    | 528000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 330        |
| stats_o/mean                   | 0.20336932 |
| stats_o/std                    | 0.09393116 |
| test/episodes                  | 3310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.312      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.011     |
| test/info_shaping_reward_mean  | -0.113     |
| test/info_shaping_reward_min   | -0.212     |
| test/Q                         | -16.33931  |
| test/Q_plus_P                  | -16.33931  |
| test/reward_per_eps            | -27.5      |
| test/steps                     | 132400     |
| train/episodes                 | 13240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.386      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0119    |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.6      |
| train/steps                    | 529600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 331        |
| stats_o/mean                   | 0.20336711 |
| stats_o/std                    | 0.09392722 |
| test/episodes                  | 3320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.608      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00314   |
| test/info_shaping_reward_mean  | -0.0749    |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -7.279414  |
| test/Q_plus_P                  | -7.279414  |
| test/reward_per_eps            | -15.7      |
| test/steps                     | 132800     |
| train/episodes                 | 13280      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.284      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.015     |
| train/info_shaping_reward_mean | -0.113     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.6      |
| train/steps                    | 531200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 332        |
| stats_o/mean                   | 0.20338285 |
| stats_o/std                    | 0.09390703 |
| test/episodes                  | 3330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.393      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00508   |
| test/info_shaping_reward_mean  | -0.103     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -12.825105 |
| test/Q_plus_P                  | -12.825105 |
| test/reward_per_eps            | -24.3      |
| test/steps                     | 133200     |
| train/episodes                 | 13320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.351      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.01      |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.9      |
| train/steps                    | 532800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 333         |
| stats_o/mean                   | 0.20338574  |
| stats_o/std                    | 0.093968794 |
| test/episodes                  | 3340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.362       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00595    |
| test/info_shaping_reward_mean  | -0.1        |
| test/info_shaping_reward_min   | -0.213      |
| test/Q                         | -13.267195  |
| test/Q_plus_P                  | -13.267195  |
| test/reward_per_eps            | -25.5       |
| test/steps                     | 133600      |
| train/episodes                 | 13360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.227       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0166     |
| train/info_shaping_reward_mean | -0.132      |
| train/info_shaping_reward_min  | -0.296      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.9       |
| train/steps                    | 534400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 334         |
| stats_o/mean                   | 0.20338362  |
| stats_o/std                    | 0.093937285 |
| test/episodes                  | 3350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.347       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00189    |
| test/info_shaping_reward_mean  | -0.112      |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -12.866499  |
| test/Q_plus_P                  | -12.866499  |
| test/reward_per_eps            | -26.1       |
| test/steps                     | 134000      |
| train/episodes                 | 13400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.319       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00685    |
| train/info_shaping_reward_mean | -0.102      |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.2       |
| train/steps                    | 536000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.2033742  |
| stats_o/std                    | 0.09398954 |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.49       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00534   |
| test/info_shaping_reward_mean  | -0.0833    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -11.254382 |
| test/Q_plus_P                  | -11.254382 |
| test/reward_per_eps            | -20.4      |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.302      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0101    |
| train/info_shaping_reward_mean | -0.12      |
| train/info_shaping_reward_min  | -0.277     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.9      |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 336        |
| stats_o/mean                   | 0.20334528 |
| stats_o/std                    | 0.09401486 |
| test/episodes                  | 3370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.522      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00233   |
| test/info_shaping_reward_mean  | -0.0782    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -6.5183716 |
| test/Q_plus_P                  | -6.5183716 |
| test/reward_per_eps            | -19.1      |
| test/steps                     | 134800     |
| train/episodes                 | 13480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.29       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0119    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.4      |
| train/steps                    | 539200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.2033383   |
| stats_o/std                    | 0.094054244 |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.487       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00277    |
| test/info_shaping_reward_mean  | -0.0867     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -11.295515  |
| test/Q_plus_P                  | -11.295515  |
| test/reward_per_eps            | -20.5       |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.328       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00584    |
| train/info_shaping_reward_mean | -0.115      |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.9       |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 338         |
| stats_o/mean                   | 0.20336051  |
| stats_o/std                    | 0.094140604 |
| test/episodes                  | 3390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.367       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00291    |
| test/info_shaping_reward_mean  | -0.1        |
| test/info_shaping_reward_min   | -0.324      |
| test/Q                         | -13.059074  |
| test/Q_plus_P                  | -13.059074  |
| test/reward_per_eps            | -25.3       |
| test/steps                     | 135600      |
| train/episodes                 | 13560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.346       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0113     |
| train/info_shaping_reward_mean | -0.107      |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.2       |
| train/steps                    | 542400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.20337616  |
| stats_o/std                    | 0.094106026 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.505       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00711    |
| test/info_shaping_reward_mean  | -0.087      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -10.355884  |
| test/Q_plus_P                  | -10.355884  |
| test/reward_per_eps            | -19.8       |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.409       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0884     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.6       |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 340        |
| stats_o/mean                   | 0.20337214 |
| stats_o/std                    | 0.09415074 |
| test/episodes                  | 3410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.595      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0635    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -5.2602444 |
| test/Q_plus_P                  | -5.2602444 |
| test/reward_per_eps            | -16.2      |
| test/steps                     | 136400     |
| train/episodes                 | 13640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.384      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00829   |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.6      |
| train/steps                    | 545600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 341        |
| stats_o/mean                   | 0.20336303 |
| stats_o/std                    | 0.09419322 |
| test/episodes                  | 3420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.608      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0028    |
| test/info_shaping_reward_mean  | -0.0851    |
| test/info_shaping_reward_min   | -0.789     |
| test/Q                         | -7.1654015 |
| test/Q_plus_P                  | -7.1654015 |
| test/reward_per_eps            | -15.7      |
| test/steps                     | 136800     |
| train/episodes                 | 13680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.342      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00827   |
| train/info_shaping_reward_mean | -0.106     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.3      |
| train/steps                    | 547200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 342        |
| stats_o/mean                   | 0.20336576 |
| stats_o/std                    | 0.09424059 |
| test/episodes                  | 3430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.65       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00149   |
| test/info_shaping_reward_mean  | -0.0668    |
| test/info_shaping_reward_min   | -0.209     |
| test/Q                         | -4.760636  |
| test/Q_plus_P                  | -4.760636  |
| test/reward_per_eps            | -14        |
| test/steps                     | 137200     |
| train/episodes                 | 13720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.358      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00545   |
| train/info_shaping_reward_mean | -0.0932    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.7      |
| train/steps                    | 548800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 343        |
| stats_o/mean                   | 0.20334494 |
| stats_o/std                    | 0.0942582  |
| test/episodes                  | 3440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.605      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.003     |
| test/info_shaping_reward_mean  | -0.065     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -6.230705  |
| test/Q_plus_P                  | -6.230705  |
| test/reward_per_eps            | -15.8      |
| test/steps                     | 137600     |
| train/episodes                 | 13760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.352      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00748   |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.296     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.9      |
| train/steps                    | 550400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 344        |
| stats_o/mean                   | 0.20333257 |
| stats_o/std                    | 0.09438109 |
| test/episodes                  | 3450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.41       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00187   |
| test/info_shaping_reward_mean  | -0.0824    |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -10.064664 |
| test/Q_plus_P                  | -10.064664 |
| test/reward_per_eps            | -23.6      |
| test/steps                     | 138000     |
| train/episodes                 | 13800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.352      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00669   |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.9      |
| train/steps                    | 552000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 345        |
| stats_o/mean                   | 0.20332356 |
| stats_o/std                    | 0.09447429 |
| test/episodes                  | 3460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.6        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00228   |
| test/info_shaping_reward_mean  | -0.0669    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -6.453871  |
| test/Q_plus_P                  | -6.453871  |
| test/reward_per_eps            | -16        |
| test/steps                     | 138400     |
| train/episodes                 | 13840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.301      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00489   |
| train/info_shaping_reward_mean | -0.127     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28        |
| train/steps                    | 553600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 346        |
| stats_o/mean                   | 0.20330131 |
| stats_o/std                    | 0.09449966 |
| test/episodes                  | 3470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.585      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0028    |
| test/info_shaping_reward_mean  | -0.0657    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -5.4073296 |
| test/Q_plus_P                  | -5.4073296 |
| test/reward_per_eps            | -16.6      |
| test/steps                     | 138800     |
| train/episodes                 | 13880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.304      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0102    |
| train/info_shaping_reward_mean | -0.122     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.8      |
| train/steps                    | 555200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 347        |
| stats_o/mean                   | 0.20329045 |
| stats_o/std                    | 0.09452134 |
| test/episodes                  | 3480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.438      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00525   |
| test/info_shaping_reward_mean  | -0.123     |
| test/info_shaping_reward_min   | -0.98      |
| test/Q                         | -11.249307 |
| test/Q_plus_P                  | -11.249307 |
| test/reward_per_eps            | -22.5      |
| test/steps                     | 139200     |
| train/episodes                 | 13920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.356      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00925   |
| train/info_shaping_reward_mean | -0.119     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.8      |
| train/steps                    | 556800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.20329271  |
| stats_o/std                    | 0.094608575 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.52        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00595    |
| test/info_shaping_reward_mean  | -0.0868     |
| test/info_shaping_reward_min   | -0.196      |
| test/Q                         | -7.796444   |
| test/Q_plus_P                  | -7.796444   |
| test/reward_per_eps            | -19.2       |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.341       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00549    |
| train/info_shaping_reward_mean | -0.123      |
| train/info_shaping_reward_min  | -0.376      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.4       |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 349        |
| stats_o/mean                   | 0.2032833  |
| stats_o/std                    | 0.09465047 |
| test/episodes                  | 3500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00292   |
| test/info_shaping_reward_mean  | -0.0613    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -4.8140373 |
| test/Q_plus_P                  | -4.8140373 |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 140000     |
| train/episodes                 | 14000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.389      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00643   |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.4      |
| train/steps                    | 560000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 350        |
| stats_o/mean                   | 0.2032879  |
| stats_o/std                    | 0.0946956  |
| test/episodes                  | 3510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.542      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00407   |
| test/info_shaping_reward_mean  | -0.0679    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -5.0070014 |
| test/Q_plus_P                  | -5.0070014 |
| test/reward_per_eps            | -18.3      |
| test/steps                     | 140400     |
| train/episodes                 | 14040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.371      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00861   |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.2      |
| train/steps                    | 561600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 351        |
| stats_o/mean                   | 0.20328002 |
| stats_o/std                    | 0.09474514 |
| test/episodes                  | 3520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.557      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00593   |
| test/info_shaping_reward_mean  | -0.0883    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -7.8217835 |
| test/Q_plus_P                  | -7.8217835 |
| test/reward_per_eps            | -17.7      |
| test/steps                     | 140800     |
| train/episodes                 | 14080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.358      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00983   |
| train/info_shaping_reward_mean | -0.124     |
| train/info_shaping_reward_min  | -0.354     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.7      |
| train/steps                    | 563200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 352        |
| stats_o/mean                   | 0.20329285 |
| stats_o/std                    | 0.09479067 |
| test/episodes                  | 3530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.637      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00245   |
| test/info_shaping_reward_mean  | -0.071     |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -5.4907837 |
| test/Q_plus_P                  | -5.4907837 |
| test/reward_per_eps            | -14.5      |
| test/steps                     | 141200     |
| train/episodes                 | 14120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.323      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00763   |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.1      |
| train/steps                    | 564800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 353        |
| stats_o/mean                   | 0.20328997 |
| stats_o/std                    | 0.09480953 |
| test/episodes                  | 3540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.455      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00269   |
| test/info_shaping_reward_mean  | -0.11      |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -7.9504533 |
| test/Q_plus_P                  | -7.9504533 |
| test/reward_per_eps            | -21.8      |
| test/steps                     | 141600     |
| train/episodes                 | 14160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.298      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00919   |
| train/info_shaping_reward_mean | -0.106     |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.1      |
| train/steps                    | 566400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 354        |
| stats_o/mean                   | 0.20327754 |
| stats_o/std                    | 0.0948331  |
| test/episodes                  | 3550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00303   |
| test/info_shaping_reward_mean  | -0.065     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -4.595526  |
| test/Q_plus_P                  | -4.595526  |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 142000     |
| train/episodes                 | 14200      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.319      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0122    |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.2      |
| train/steps                    | 568000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 355        |
| stats_o/mean                   | 0.20326912 |
| stats_o/std                    | 0.09484581 |
| test/episodes                  | 3560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00968   |
| test/info_shaping_reward_mean  | -0.0593    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.4794917 |
| test/Q_plus_P                  | -2.4794917 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 142400     |
| train/episodes                 | 14240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.364      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00539   |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.4      |
| train/steps                    | 569600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 356        |
| stats_o/mean                   | 0.20328534 |
| stats_o/std                    | 0.0948379  |
| test/episodes                  | 3570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000763  |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.5127125 |
| test/Q_plus_P                  | -2.5127125 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 142800     |
| train/episodes                 | 14280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.384      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00709   |
| train/info_shaping_reward_mean | -0.0917    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.6      |
| train/steps                    | 571200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 357        |
| stats_o/mean                   | 0.20327958 |
| stats_o/std                    | 0.09480327 |
| test/episodes                  | 3580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.583      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00651   |
| test/info_shaping_reward_mean  | -0.0684    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.6159878 |
| test/Q_plus_P                  | -3.6159878 |
| test/reward_per_eps            | -16.7      |
| test/steps                     | 143200     |
| train/episodes                 | 14320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.382      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0107    |
| train/info_shaping_reward_mean | -0.0937    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.7      |
| train/steps                    | 572800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 358        |
| stats_o/mean                   | 0.20327376 |
| stats_o/std                    | 0.09476574 |
| test/episodes                  | 3590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.537      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0109    |
| test/info_shaping_reward_mean  | -0.0747    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.6523936 |
| test/Q_plus_P                  | -3.6523936 |
| test/reward_per_eps            | -18.5      |
| test/steps                     | 143600     |
| train/episodes                 | 14360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.384      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00723   |
| train/info_shaping_reward_mean | -0.0919    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.6      |
| train/steps                    | 574400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.20329818  |
| stats_o/std                    | 0.094762735 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.645       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00273    |
| test/info_shaping_reward_mean  | -0.0628     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -3.546427   |
| test/Q_plus_P                  | -3.546427   |
| test/reward_per_eps            | -14.2       |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.376       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00661    |
| train/info_shaping_reward_mean | -0.0958     |
| train/info_shaping_reward_min  | -0.202      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.9       |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 360        |
| stats_o/mean                   | 0.20330355 |
| stats_o/std                    | 0.09470609 |
| test/episodes                  | 3610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.637      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0042    |
| test/info_shaping_reward_mean  | -0.0713    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -3.3529835 |
| test/Q_plus_P                  | -3.3529835 |
| test/reward_per_eps            | -14.5      |
| test/steps                     | 144400     |
| train/episodes                 | 14440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.421      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00779   |
| train/info_shaping_reward_mean | -0.0976    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.1      |
| train/steps                    | 577600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 361        |
| stats_o/mean                   | 0.20330544 |
| stats_o/std                    | 0.09473204 |
| test/episodes                  | 3620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.642      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00446   |
| test/info_shaping_reward_mean  | -0.0773    |
| test/info_shaping_reward_min   | -0.209     |
| test/Q                         | -4.361788  |
| test/Q_plus_P                  | -4.361788  |
| test/reward_per_eps            | -14.3      |
| test/steps                     | 144800     |
| train/episodes                 | 14480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.4        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00886   |
| train/info_shaping_reward_mean | -0.1       |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24        |
| train/steps                    | 579200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 362        |
| stats_o/mean                   | 0.2033119  |
| stats_o/std                    | 0.09467308 |
| test/episodes                  | 3630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0024    |
| test/info_shaping_reward_mean  | -0.0595    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.1949894 |
| test/Q_plus_P                  | -2.1949894 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 145200     |
| train/episodes                 | 14520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.442      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00829   |
| train/info_shaping_reward_mean | -0.0853    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.3      |
| train/steps                    | 580800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 363        |
| stats_o/mean                   | 0.20331943 |
| stats_o/std                    | 0.09464731 |
| test/episodes                  | 3640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.637      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0048    |
| test/info_shaping_reward_mean  | -0.0666    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.1652546 |
| test/Q_plus_P                  | -3.1652546 |
| test/reward_per_eps            | -14.5      |
| test/steps                     | 145600     |
| train/episodes                 | 14560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.495      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0081    |
| train/info_shaping_reward_mean | -0.0857    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 582400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 364        |
| stats_o/mean                   | 0.20331374 |
| stats_o/std                    | 0.0946168  |
| test/episodes                  | 3650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.472      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0025    |
| test/info_shaping_reward_mean  | -0.0964    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -7.1847625 |
| test/Q_plus_P                  | -7.1847625 |
| test/reward_per_eps            | -21.1      |
| test/steps                     | 146000     |
| train/episodes                 | 14600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.422      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00659   |
| train/info_shaping_reward_mean | -0.0842    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.1      |
| train/steps                    | 584000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 365        |
| stats_o/mean                   | 0.203314   |
| stats_o/std                    | 0.09457428 |
| test/episodes                  | 3660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00113   |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.2674227 |
| test/Q_plus_P                  | -2.2674227 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 146400     |
| train/episodes                 | 14640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.466      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00415   |
| train/info_shaping_reward_mean | -0.0811    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 585600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 366        |
| stats_o/mean                   | 0.20329951 |
| stats_o/std                    | 0.09457834 |
| test/episodes                  | 3670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0033    |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -2.1115656 |
| test/Q_plus_P                  | -2.1115656 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 146800     |
| train/episodes                 | 14680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.394      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00682   |
| train/info_shaping_reward_mean | -0.0944    |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.2      |
| train/steps                    | 587200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 367        |
| stats_o/mean                   | 0.20330048 |
| stats_o/std                    | 0.0945355  |
| test/episodes                  | 3680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00194   |
| test/info_shaping_reward_mean  | -0.0602    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -4.5763574 |
| test/Q_plus_P                  | -4.5763574 |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 147200     |
| train/episodes                 | 14720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.426      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00604   |
| train/info_shaping_reward_mean | -0.0843    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23        |
| train/steps                    | 588800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 368        |
| stats_o/mean                   | 0.20328206 |
| stats_o/std                    | 0.09451504 |
| test/episodes                  | 3690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.652      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.0581    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.5913846 |
| test/Q_plus_P                  | -2.5913846 |
| test/reward_per_eps            | -13.9      |
| test/steps                     | 147600     |
| train/episodes                 | 14760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.426      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0066    |
| train/info_shaping_reward_mean | -0.0902    |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.9      |
| train/steps                    | 590400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 369        |
| stats_o/mean                   | 0.20329374 |
| stats_o/std                    | 0.0944487  |
| test/episodes                  | 3700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.537      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0018    |
| test/info_shaping_reward_mean  | -0.069     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -3.3185098 |
| test/Q_plus_P                  | -3.3185098 |
| test/reward_per_eps            | -18.5      |
| test/steps                     | 148000     |
| train/episodes                 | 14800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.491      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00407   |
| train/info_shaping_reward_mean | -0.0802    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 592000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 370         |
| stats_o/mean                   | 0.20328808  |
| stats_o/std                    | 0.094390504 |
| test/episodes                  | 3710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.665       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00562    |
| test/info_shaping_reward_mean  | -0.0643     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -2.3895228  |
| test/Q_plus_P                  | -2.3895228  |
| test/reward_per_eps            | -13.4       |
| test/steps                     | 148400      |
| train/episodes                 | 14840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.374       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00747    |
| train/info_shaping_reward_mean | -0.0904     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25         |
| train/steps                    | 593600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 371        |
| stats_o/mean                   | 0.20328298 |
| stats_o/std                    | 0.0943397  |
| test/episodes                  | 3720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00211   |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.9547    |
| test/Q_plus_P                  | -1.9547    |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 148800     |
| train/episodes                 | 14880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.427      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00741   |
| train/info_shaping_reward_mean | -0.0926    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.9      |
| train/steps                    | 595200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 372        |
| stats_o/mean                   | 0.20328453 |
| stats_o/std                    | 0.09436622 |
| test/episodes                  | 3730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.645      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0018    |
| test/info_shaping_reward_mean  | -0.0674    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.883389  |
| test/Q_plus_P                  | -2.883389  |
| test/reward_per_eps            | -14.2      |
| test/steps                     | 149200     |
| train/episodes                 | 14920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.365      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00833   |
| train/info_shaping_reward_mean | -0.0979    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.4      |
| train/steps                    | 596800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 373        |
| stats_o/mean                   | 0.20328148 |
| stats_o/std                    | 0.09431203 |
| test/episodes                  | 3740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.61       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00238   |
| test/info_shaping_reward_mean  | -0.0618    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.736312  |
| test/Q_plus_P                  | -2.736312  |
| test/reward_per_eps            | -15.6      |
| test/steps                     | 149600     |
| train/episodes                 | 14960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.412      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.0849    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.5      |
| train/steps                    | 598400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.20327935 |
| stats_o/std                    | 0.09425404 |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.557      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00294   |
| test/info_shaping_reward_mean  | -0.0717    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -4.807663  |
| test/Q_plus_P                  | -4.807663  |
| test/reward_per_eps            | -17.7      |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.479      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00533   |
| train/info_shaping_reward_mean | -0.0796    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 375        |
| stats_o/mean                   | 0.20325647 |
| stats_o/std                    | 0.09427156 |
| test/episodes                  | 3760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.527      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00342   |
| test/info_shaping_reward_mean  | -0.0764    |
| test/info_shaping_reward_min   | -0.203     |
| test/Q                         | -5.5538464 |
| test/Q_plus_P                  | -5.5538464 |
| test/reward_per_eps            | -18.9      |
| test/steps                     | 150400     |
| train/episodes                 | 15040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.399      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00515   |
| train/info_shaping_reward_mean | -0.0959    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.1      |
| train/steps                    | 601600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.20325366 |
| stats_o/std                    | 0.0942247  |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00127   |
| test/info_shaping_reward_mean  | -0.0594    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.1718159 |
| test/Q_plus_P                  | -2.1718159 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.436      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.092     |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.6      |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 377        |
| stats_o/mean                   | 0.2032504  |
| stats_o/std                    | 0.09414762 |
| test/episodes                  | 3780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00587   |
| test/info_shaping_reward_mean  | -0.0588    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.4035485 |
| test/Q_plus_P                  | -2.4035485 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 151200     |
| train/episodes                 | 15120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.464      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00379   |
| train/info_shaping_reward_mean | -0.0822    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 604800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 378         |
| stats_o/mean                   | 0.20324782  |
| stats_o/std                    | 0.094088346 |
| test/episodes                  | 3790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00263    |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.8329276  |
| test/Q_plus_P                  | -1.8329276  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 151600      |
| train/episodes                 | 15160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.458       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00509    |
| train/info_shaping_reward_mean | -0.0777     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.7       |
| train/steps                    | 606400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 379        |
| stats_o/mean                   | 0.20323078 |
| stats_o/std                    | 0.09411345 |
| test/episodes                  | 3800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.585      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000845  |
| test/info_shaping_reward_mean  | -0.0611    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.9722245 |
| test/Q_plus_P                  | -2.9722245 |
| test/reward_per_eps            | -16.6      |
| test/steps                     | 152000     |
| train/episodes                 | 15200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.416      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00537   |
| train/info_shaping_reward_mean | -0.0901    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.4      |
| train/steps                    | 608000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.20322706  |
| stats_o/std                    | 0.094053134 |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.662       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00253    |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.7163837  |
| test/Q_plus_P                  | -2.7163837  |
| test/reward_per_eps            | -13.5       |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.446       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00552    |
| train/info_shaping_reward_mean | -0.0837     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.1       |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 381         |
| stats_o/mean                   | 0.20322868  |
| stats_o/std                    | 0.094082974 |
| test/episodes                  | 3820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.66        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0022     |
| test/info_shaping_reward_mean  | -0.0601     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -2.7166464  |
| test/Q_plus_P                  | -2.7166464  |
| test/reward_per_eps            | -13.6       |
| test/steps                     | 152800      |
| train/episodes                 | 15280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.454       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00567    |
| train/info_shaping_reward_mean | -0.0906     |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.9       |
| train/steps                    | 611200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.20322165 |
| stats_o/std                    | 0.09401944 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.632      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00192   |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.6233792 |
| test/Q_plus_P                  | -2.6233792 |
| test/reward_per_eps            | -14.7      |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.489      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00427   |
| train/info_shaping_reward_mean | -0.0787    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.20320264  |
| stats_o/std                    | 0.094000824 |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00221    |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.7522849  |
| test/Q_plus_P                  | -1.7522849  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.477       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00457    |
| train/info_shaping_reward_mean | -0.0828     |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.9       |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 384         |
| stats_o/mean                   | 0.20319776  |
| stats_o/std                    | 0.093960226 |
| test/episodes                  | 3850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00177    |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5550013  |
| test/Q_plus_P                  | -1.5550013  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 154000      |
| train/episodes                 | 15400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.477       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00456    |
| train/info_shaping_reward_mean | -0.0782     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.9       |
| train/steps                    | 616000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 385        |
| stats_o/mean                   | 0.20320965 |
| stats_o/std                    | 0.0939075  |
| test/episodes                  | 3860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.625      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00169   |
| test/info_shaping_reward_mean  | -0.0681    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.7353745 |
| test/Q_plus_P                  | -2.7353745 |
| test/reward_per_eps            | -15        |
| test/steps                     | 154400     |
| train/episodes                 | 15440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.454      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0064    |
| train/info_shaping_reward_mean | -0.0871    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.9      |
| train/steps                    | 617600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 386        |
| stats_o/mean                   | 0.20321241 |
| stats_o/std                    | 0.0938587  |
| test/episodes                  | 3870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00126   |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.351736  |
| test/Q_plus_P                  | -2.351736  |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 154800     |
| train/episodes                 | 15480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.49       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00423   |
| train/info_shaping_reward_mean | -0.0947    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 619200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.20320986  |
| stats_o/std                    | 0.093842305 |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.578       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00277    |
| test/info_shaping_reward_mean  | -0.0697     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -2.7917624  |
| test/Q_plus_P                  | -2.7917624  |
| test/reward_per_eps            | -16.9       |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.512       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00508    |
| train/info_shaping_reward_mean | -0.0783     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.5       |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 388        |
| stats_o/mean                   | 0.20320475 |
| stats_o/std                    | 0.09376101 |
| test/episodes                  | 3890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0038    |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.7460742 |
| test/Q_plus_P                  | -1.7460742 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 155600     |
| train/episodes                 | 15560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.501      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00596   |
| train/info_shaping_reward_mean | -0.0837    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20        |
| train/steps                    | 622400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 389        |
| stats_o/mean                   | 0.20319809 |
| stats_o/std                    | 0.09369323 |
| test/episodes                  | 3900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00159   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.876501  |
| test/Q_plus_P                  | -1.876501  |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 156000     |
| train/episodes                 | 15600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00432   |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 624000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 390        |
| stats_o/mean                   | 0.20318168 |
| stats_o/std                    | 0.0936436  |
| test/episodes                  | 3910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.627      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00424   |
| test/info_shaping_reward_mean  | -0.0659    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.5562658 |
| test/Q_plus_P                  | -2.5562658 |
| test/reward_per_eps            | -14.9      |
| test/steps                     | 156400     |
| train/episodes                 | 15640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.472      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00585   |
| train/info_shaping_reward_mean | -0.0853    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 625600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.20318353  |
| stats_o/std                    | 0.093583606 |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0109     |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.4612472  |
| test/Q_plus_P                  | -1.4612472  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.506       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00441    |
| train/info_shaping_reward_mean | -0.0788     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.8       |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 392        |
| stats_o/mean                   | 0.203193   |
| stats_o/std                    | 0.09362525 |
| test/episodes                  | 3930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.627      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00193   |
| test/info_shaping_reward_mean  | -0.0714    |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -3.7170348 |
| test/Q_plus_P                  | -3.7170348 |
| test/reward_per_eps            | -14.9      |
| test/steps                     | 157200     |
| train/episodes                 | 15720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.533      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00474   |
| train/info_shaping_reward_mean | -0.082     |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 628800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 393        |
| stats_o/mean                   | 0.2031921  |
| stats_o/std                    | 0.09356843 |
| test/episodes                  | 3940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000759  |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.4443921 |
| test/Q_plus_P                  | -1.4443921 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 157600     |
| train/episodes                 | 15760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00466   |
| train/info_shaping_reward_mean | -0.0805    |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 630400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 394        |
| stats_o/mean                   | 0.20319264 |
| stats_o/std                    | 0.09352107 |
| test/episodes                  | 3950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00382   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.678671  |
| test/Q_plus_P                  | -1.678671  |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 158000     |
| train/episodes                 | 15800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.42       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00735   |
| train/info_shaping_reward_mean | -0.0881    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.2      |
| train/steps                    | 632000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 395        |
| stats_o/mean                   | 0.20318991 |
| stats_o/std                    | 0.09347317 |
| test/episodes                  | 3960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00215   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.003906  |
| test/Q_plus_P                  | -2.003906  |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 158400     |
| train/episodes                 | 15840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.544      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0052    |
| train/info_shaping_reward_mean | -0.0785    |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 633600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 396        |
| stats_o/mean                   | 0.20319492 |
| stats_o/std                    | 0.09344582 |
| test/episodes                  | 3970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00563   |
| test/info_shaping_reward_mean  | -0.0606    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.194177  |
| test/Q_plus_P                  | -3.194177  |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 158800     |
| train/episodes                 | 15880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.491      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00374   |
| train/info_shaping_reward_mean | -0.0871    |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 635200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 397        |
| stats_o/mean                   | 0.20319591 |
| stats_o/std                    | 0.09337724 |
| test/episodes                  | 3980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0149    |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.8402419 |
| test/Q_plus_P                  | -1.8402419 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 159200     |
| train/episodes                 | 15920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.595      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00524   |
| train/info_shaping_reward_mean | -0.0689    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 636800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 398         |
| stats_o/mean                   | 0.20318426  |
| stats_o/std                    | 0.093342856 |
| test/episodes                  | 3990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00295    |
| test/info_shaping_reward_mean  | -0.0618     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.1821167  |
| test/Q_plus_P                  | -2.1821167  |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 159600      |
| train/episodes                 | 15960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.502       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00585    |
| train/info_shaping_reward_mean | -0.0874     |
| train/info_shaping_reward_min  | -0.202      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.9       |
| train/steps                    | 638400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 399        |
| stats_o/mean                   | 0.20318677 |
| stats_o/std                    | 0.09330975 |
| test/episodes                  | 4000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00667   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.8648403 |
| test/Q_plus_P                  | -1.8648403 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 160000     |
| train/episodes                 | 16000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.515      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00437   |
| train/info_shaping_reward_mean | -0.0788    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 640000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 400         |
| stats_o/mean                   | 0.20317963  |
| stats_o/std                    | 0.093249254 |
| test/episodes                  | 4010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00435    |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -1.8025607  |
| test/Q_plus_P                  | -1.8025607  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 160400      |
| train/episodes                 | 16040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.544       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00379    |
| train/info_shaping_reward_mean | -0.0724     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 641600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 401        |
| stats_o/mean                   | 0.20318392 |
| stats_o/std                    | 0.09318209 |
| test/episodes                  | 4020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.59       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00292   |
| test/info_shaping_reward_mean  | -0.0735    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.73151   |
| test/Q_plus_P                  | -3.73151   |
| test/reward_per_eps            | -16.4      |
| test/steps                     | 160800     |
| train/episodes                 | 16080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.528      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0824    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 643200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.20318554 |
| stats_o/std                    | 0.09314271 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00295   |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.7649498 |
| test/Q_plus_P                  | -1.7649498 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.454      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00555   |
| train/info_shaping_reward_mean | -0.0813    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 403        |
| stats_o/mean                   | 0.20318276 |
| stats_o/std                    | 0.09309803 |
| test/episodes                  | 4040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00267   |
| test/info_shaping_reward_mean  | -0.0648    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -3.7559578 |
| test/Q_plus_P                  | -3.7559578 |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 161600     |
| train/episodes                 | 16160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.527      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00442   |
| train/info_shaping_reward_mean | -0.0794    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 646400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 404        |
| stats_o/mean                   | 0.2031821  |
| stats_o/std                    | 0.09307764 |
| test/episodes                  | 4050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0082    |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.6102527 |
| test/Q_plus_P                  | -1.6102527 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 162000     |
| train/episodes                 | 16200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.469      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00606   |
| train/info_shaping_reward_mean | -0.0873    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 648000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.20317188  |
| stats_o/std                    | 0.093071826 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0104     |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.7368933  |
| test/Q_plus_P                  | -1.7368933  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.497       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00466    |
| train/info_shaping_reward_mean | -0.0954     |
| train/info_shaping_reward_min  | -0.273      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 406        |
| stats_o/mean                   | 0.2031777  |
| stats_o/std                    | 0.09303693 |
| test/episodes                  | 4070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00928   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.6744744 |
| test/Q_plus_P                  | -1.6744744 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 162800     |
| train/episodes                 | 16280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.547      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0061    |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 651200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 407        |
| stats_o/mean                   | 0.20318712 |
| stats_o/std                    | 0.09299742 |
| test/episodes                  | 4080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.014     |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6739845 |
| test/Q_plus_P                  | -1.6739845 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 163200     |
| train/episodes                 | 16320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.504      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00607   |
| train/info_shaping_reward_mean | -0.0784    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 652800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 408         |
| stats_o/mean                   | 0.20318636  |
| stats_o/std                    | 0.093005516 |
| test/episodes                  | 4090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00762    |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.3871667  |
| test/Q_plus_P                  | -1.3871667  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 163600      |
| train/episodes                 | 16360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.469       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00643    |
| train/info_shaping_reward_mean | -0.0965     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.2       |
| train/steps                    | 654400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 409        |
| stats_o/mean                   | 0.20318237 |
| stats_o/std                    | 0.09295808 |
| test/episodes                  | 4100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.005     |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.716634  |
| test/Q_plus_P                  | -1.716634  |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 164000     |
| train/episodes                 | 16400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00429   |
| train/info_shaping_reward_mean | -0.0802    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 656000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 410        |
| stats_o/mean                   | 0.20318447 |
| stats_o/std                    | 0.09293888 |
| test/episodes                  | 4110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.019     |
| test/info_shaping_reward_mean  | -0.0615    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.8451761 |
| test/Q_plus_P                  | -1.8451761 |
| test/reward_per_eps            | -11        |
| test/steps                     | 164400     |
| train/episodes                 | 16440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.505      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0055    |
| train/info_shaping_reward_mean | -0.0879    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 657600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 411        |
| stats_o/mean                   | 0.20318365 |
| stats_o/std                    | 0.09286223 |
| test/episodes                  | 4120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0111    |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5004214 |
| test/Q_plus_P                  | -1.5004214 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 164800     |
| train/episodes                 | 16480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0045    |
| train/info_shaping_reward_mean | -0.0762    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 659200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 412        |
| stats_o/mean                   | 0.20319253 |
| stats_o/std                    | 0.09283578 |
| test/episodes                  | 4130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.657      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00501   |
| test/info_shaping_reward_mean  | -0.0659    |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -3.4508648 |
| test/Q_plus_P                  | -3.4508648 |
| test/reward_per_eps            | -13.7      |
| test/steps                     | 165200     |
| train/episodes                 | 16520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.52       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00603   |
| train/info_shaping_reward_mean | -0.0788    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 660800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 413        |
| stats_o/mean                   | 0.20318711 |
| stats_o/std                    | 0.09278385 |
| test/episodes                  | 4140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00352   |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4684643 |
| test/Q_plus_P                  | -1.4684643 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 165600     |
| train/episodes                 | 16560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00641   |
| train/info_shaping_reward_mean | -0.0726    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 662400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 414        |
| stats_o/mean                   | 0.20317607 |
| stats_o/std                    | 0.09278    |
| test/episodes                  | 4150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00332   |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1692321 |
| test/Q_plus_P                  | -1.1692321 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 166000     |
| train/episodes                 | 16600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.458      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00652   |
| train/info_shaping_reward_mean | -0.1       |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.7      |
| train/steps                    | 664000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 415        |
| stats_o/mean                   | 0.20317186 |
| stats_o/std                    | 0.09276893 |
| test/episodes                  | 4160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00965   |
| test/info_shaping_reward_mean  | -0.0612    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5090822 |
| test/Q_plus_P                  | -1.5090822 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 166400     |
| train/episodes                 | 16640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00712   |
| train/info_shaping_reward_mean | -0.0815    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 665600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 416        |
| stats_o/mean                   | 0.20316689 |
| stats_o/std                    | 0.09273019 |
| test/episodes                  | 4170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00245   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5211557 |
| test/Q_plus_P                  | -1.5211557 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 166800     |
| train/episodes                 | 16680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.435      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00752   |
| train/info_shaping_reward_mean | -0.0964    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.6      |
| train/steps                    | 667200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 417        |
| stats_o/mean                   | 0.20317182 |
| stats_o/std                    | 0.09270996 |
| test/episodes                  | 4180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0149    |
| test/info_shaping_reward_mean  | -0.0567    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4692142 |
| test/Q_plus_P                  | -1.4692142 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 167200     |
| train/episodes                 | 16720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00891   |
| train/info_shaping_reward_mean | -0.0912    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 668800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 418        |
| stats_o/mean                   | 0.20317334 |
| stats_o/std                    | 0.09267779 |
| test/episodes                  | 4190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00907   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2719872 |
| test/Q_plus_P                  | -1.2719872 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 167600     |
| train/episodes                 | 16760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00595   |
| train/info_shaping_reward_mean | -0.0758    |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 670400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 419        |
| stats_o/mean                   | 0.20316212 |
| stats_o/std                    | 0.09262923 |
| test/episodes                  | 4200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00363   |
| test/info_shaping_reward_mean  | -0.0658    |
| test/info_shaping_reward_min   | -0.203     |
| test/Q                         | -2.7053454 |
| test/Q_plus_P                  | -2.7053454 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 168000     |
| train/episodes                 | 16800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.506      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00703   |
| train/info_shaping_reward_mean | -0.0819    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 672000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 420        |
| stats_o/mean                   | 0.20315252 |
| stats_o/std                    | 0.09259864 |
| test/episodes                  | 4210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00433   |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4253922 |
| test/Q_plus_P                  | -1.4253922 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 168400     |
| train/episodes                 | 16840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.503      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00883   |
| train/info_shaping_reward_mean | -0.0848    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 673600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 421        |
| stats_o/mean                   | 0.20315273 |
| stats_o/std                    | 0.09251907 |
| test/episodes                  | 4220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5110862 |
| test/Q_plus_P                  | -1.5110862 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 168800     |
| train/episodes                 | 16880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00694   |
| train/info_shaping_reward_mean | -0.0743    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 675200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 422        |
| stats_o/mean                   | 0.20315976 |
| stats_o/std                    | 0.09244921 |
| test/episodes                  | 4230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00825   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3638282 |
| test/Q_plus_P                  | -1.3638282 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 169200     |
| train/episodes                 | 16920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00648   |
| train/info_shaping_reward_mean | -0.0835    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 676800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 423        |
| stats_o/mean                   | 0.20315655 |
| stats_o/std                    | 0.09239155 |
| test/episodes                  | 4240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00729   |
| test/info_shaping_reward_mean  | -0.0602    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.5059154 |
| test/Q_plus_P                  | -2.5059154 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 169600     |
| train/episodes                 | 16960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.548      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.005     |
| train/info_shaping_reward_mean | -0.0805    |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 678400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 424        |
| stats_o/mean                   | 0.20315717 |
| stats_o/std                    | 0.09232168 |
| test/episodes                  | 4250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00248   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.563373  |
| test/Q_plus_P                  | -1.563373  |
| test/reward_per_eps            | -10        |
| test/steps                     | 170000     |
| train/episodes                 | 17000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00517   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 680000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.20315607  |
| stats_o/std                    | 0.092274114 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00909    |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5947069  |
| test/Q_plus_P                  | -1.5947069  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.566       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00624    |
| train/info_shaping_reward_mean | -0.071      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.20316091  |
| stats_o/std                    | 0.092232436 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00789    |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3123552  |
| test/Q_plus_P                  | -1.3123552  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.573       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00765    |
| train/info_shaping_reward_mean | -0.0813     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 427        |
| stats_o/mean                   | 0.2031692  |
| stats_o/std                    | 0.09218924 |
| test/episodes                  | 4280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00954   |
| test/info_shaping_reward_mean  | -0.0561    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.3981607 |
| test/Q_plus_P                  | -1.3981607 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 171200     |
| train/episodes                 | 17120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.584      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00645   |
| train/info_shaping_reward_mean | -0.0781    |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 684800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.20317754  |
| stats_o/std                    | 0.092168264 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000751   |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -1.6890922  |
| test/Q_plus_P                  | -1.6890922  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.565       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00567    |
| train/info_shaping_reward_mean | -0.0808     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 429        |
| stats_o/mean                   | 0.20318298 |
| stats_o/std                    | 0.09211328 |
| test/episodes                  | 4300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00447   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3464158 |
| test/Q_plus_P                  | -1.3464158 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 172000     |
| train/episodes                 | 17200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00614   |
| train/info_shaping_reward_mean | -0.0771    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 688000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 430        |
| stats_o/mean                   | 0.20318474 |
| stats_o/std                    | 0.09206531 |
| test/episodes                  | 4310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00776   |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3903466 |
| test/Q_plus_P                  | -1.3903466 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 172400     |
| train/episodes                 | 17240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0044    |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 689600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 431        |
| stats_o/mean                   | 0.20318787 |
| stats_o/std                    | 0.09200597 |
| test/episodes                  | 4320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0157    |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2855471 |
| test/Q_plus_P                  | -1.2855471 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 172800     |
| train/episodes                 | 17280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00642   |
| train/info_shaping_reward_mean | -0.0778    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 691200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 432        |
| stats_o/mean                   | 0.20320171 |
| stats_o/std                    | 0.0919485  |
| test/episodes                  | 4330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00553   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3183165 |
| test/Q_plus_P                  | -1.3183165 |
| test/reward_per_eps            | -9         |
| test/steps                     | 173200     |
| train/episodes                 | 17320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.567      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0064    |
| train/info_shaping_reward_mean | -0.0754    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 692800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 433        |
| stats_o/mean                   | 0.203198   |
| stats_o/std                    | 0.09188454 |
| test/episodes                  | 4340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00314   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4230108 |
| test/Q_plus_P                  | -1.4230108 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 173600     |
| train/episodes                 | 17360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.516      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00539   |
| train/info_shaping_reward_mean | -0.0761    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 694400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 434        |
| stats_o/mean                   | 0.20320049 |
| stats_o/std                    | 0.09184156 |
| test/episodes                  | 4350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00646   |
| test/info_shaping_reward_mean  | -0.0549    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4564095 |
| test/Q_plus_P                  | -1.4564095 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 174000     |
| train/episodes                 | 17400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00711   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 696000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 435         |
| stats_o/mean                   | 0.20319162  |
| stats_o/std                    | 0.091795966 |
| test/episodes                  | 4360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00659    |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.5124769  |
| test/Q_plus_P                  | -1.5124769  |
| test/reward_per_eps            | -10         |
| test/steps                     | 174400      |
| train/episodes                 | 17440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.54        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.0795     |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 697600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 436        |
| stats_o/mean                   | 0.20320012 |
| stats_o/std                    | 0.09175868 |
| test/episodes                  | 4370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000701  |
| test/info_shaping_reward_mean  | -0.046     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.2632636 |
| test/Q_plus_P                  | -1.2632636 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 174800     |
| train/episodes                 | 17480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00615   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 699200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 437        |
| stats_o/mean                   | 0.20320326 |
| stats_o/std                    | 0.09173378 |
| test/episodes                  | 4380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00778   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2864537 |
| test/Q_plus_P                  | -1.2864537 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 175200     |
| train/episodes                 | 17520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.52       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00676   |
| train/info_shaping_reward_mean | -0.0839    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 700800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 438        |
| stats_o/mean                   | 0.20320162 |
| stats_o/std                    | 0.09167696 |
| test/episodes                  | 4390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.012     |
| test/info_shaping_reward_mean  | -0.0713    |
| test/info_shaping_reward_min   | -0.214     |
| test/Q                         | -2.767585  |
| test/Q_plus_P                  | -2.767585  |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 175600     |
| train/episodes                 | 17560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00707   |
| train/info_shaping_reward_mean | -0.0734    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 702400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 439         |
| stats_o/mean                   | 0.20320325  |
| stats_o/std                    | 0.091668576 |
| test/episodes                  | 4400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0132     |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.3585187  |
| test/Q_plus_P                  | -1.3585187  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 176000      |
| train/episodes                 | 17600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.523       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00619    |
| train/info_shaping_reward_mean | -0.0928     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 704000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 440        |
| stats_o/mean                   | 0.20322134 |
| stats_o/std                    | 0.0916263  |
| test/episodes                  | 4410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.013     |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3268605 |
| test/Q_plus_P                  | -1.3268605 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 176400     |
| train/episodes                 | 17640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.558      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00492   |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 705600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 441        |
| stats_o/mean                   | 0.20321701 |
| stats_o/std                    | 0.09161114 |
| test/episodes                  | 4420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00215   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5867482 |
| test/Q_plus_P                  | -1.5867482 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 176800     |
| train/episodes                 | 17680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00515   |
| train/info_shaping_reward_mean | -0.0782    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 707200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 442        |
| stats_o/mean                   | 0.20321718 |
| stats_o/std                    | 0.09156388 |
| test/episodes                  | 4430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0021    |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6073016 |
| test/Q_plus_P                  | -1.6073016 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 177200     |
| train/episodes                 | 17720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00617   |
| train/info_shaping_reward_mean | -0.0747    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 708800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 443        |
| stats_o/mean                   | 0.20321707 |
| stats_o/std                    | 0.09149274 |
| test/episodes                  | 4440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0119    |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.109375  |
| test/Q_plus_P                  | -1.109375  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 177600     |
| train/episodes                 | 17760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00836   |
| train/info_shaping_reward_mean | -0.0667    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 710400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 444         |
| stats_o/mean                   | 0.20322692  |
| stats_o/std                    | 0.091460735 |
| test/episodes                  | 4450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00831    |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.2959971  |
| test/Q_plus_P                  | -1.2959971  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 178000      |
| train/episodes                 | 17800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00667    |
| train/info_shaping_reward_mean | -0.0747     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 712000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 445        |
| stats_o/mean                   | 0.20322889 |
| stats_o/std                    | 0.09142328 |
| test/episodes                  | 4460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00247   |
| test/info_shaping_reward_mean  | -0.0467    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3755385 |
| test/Q_plus_P                  | -1.3755385 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 178400     |
| train/episodes                 | 17840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.58       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00571   |
| train/info_shaping_reward_mean | -0.0705    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 713600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 446         |
| stats_o/mean                   | 0.20322767  |
| stats_o/std                    | 0.091359474 |
| test/episodes                  | 4470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000966   |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.424735   |
| test/Q_plus_P                  | -1.424735   |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 178800      |
| train/episodes                 | 17880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00533    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 715200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 447         |
| stats_o/mean                   | 0.20323943  |
| stats_o/std                    | 0.091308095 |
| test/episodes                  | 4480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00956    |
| test/info_shaping_reward_mean  | -0.0518     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2714697  |
| test/Q_plus_P                  | -1.2714697  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 179200      |
| train/episodes                 | 17920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00611    |
| train/info_shaping_reward_mean | -0.0683     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 716800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 448        |
| stats_o/mean                   | 0.20324352 |
| stats_o/std                    | 0.09125658 |
| test/episodes                  | 4490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00656   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3014885 |
| test/Q_plus_P                  | -1.3014885 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 179600     |
| train/episodes                 | 17960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00521   |
| train/info_shaping_reward_mean | -0.0726    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 718400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 449         |
| stats_o/mean                   | 0.20324488  |
| stats_o/std                    | 0.091206476 |
| test/episodes                  | 4500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00725    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3098711  |
| test/Q_plus_P                  | -1.3098711  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 180000      |
| train/episodes                 | 18000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.56        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00708    |
| train/info_shaping_reward_mean | -0.0804     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 720000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.20325445 |
| stats_o/std                    | 0.09115584 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00853   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3415039 |
| test/Q_plus_P                  | -1.3415039 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0053    |
| train/info_shaping_reward_mean | -0.0762    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 451         |
| stats_o/mean                   | 0.20325474  |
| stats_o/std                    | 0.091083154 |
| test/episodes                  | 4520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0042     |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1275048  |
| test/Q_plus_P                  | -1.1275048  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 180800      |
| train/episodes                 | 18080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00628    |
| train/info_shaping_reward_mean | -0.0695     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 723200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 452        |
| stats_o/mean                   | 0.20325212 |
| stats_o/std                    | 0.09102417 |
| test/episodes                  | 4530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0147    |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2264934 |
| test/Q_plus_P                  | -1.2264934 |
| test/reward_per_eps            | -9         |
| test/steps                     | 181200     |
| train/episodes                 | 18120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00786   |
| train/info_shaping_reward_mean | -0.0755    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 724800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 453        |
| stats_o/mean                   | 0.20325556 |
| stats_o/std                    | 0.09095598 |
| test/episodes                  | 4540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0147    |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2061669 |
| test/Q_plus_P                  | -1.2061669 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 181600     |
| train/episodes                 | 18160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00328   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 726400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 454        |
| stats_o/mean                   | 0.20325966 |
| stats_o/std                    | 0.09092437 |
| test/episodes                  | 4550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.5295084 |
| test/Q_plus_P                  | -1.5295084 |
| test/reward_per_eps            | -11        |
| test/steps                     | 182000     |
| train/episodes                 | 18200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00771   |
| train/info_shaping_reward_mean | -0.0686    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 728000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.20327091  |
| stats_o/std                    | 0.090884596 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000492   |
| test/info_shaping_reward_mean  | -0.0475     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.3104898  |
| test/Q_plus_P                  | -1.3104898  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.573       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00505    |
| train/info_shaping_reward_mean | -0.0752     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 456        |
| stats_o/mean                   | 0.20326783 |
| stats_o/std                    | 0.09083302 |
| test/episodes                  | 4570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00314   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3625761 |
| test/Q_plus_P                  | -1.3625761 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 182800     |
| train/episodes                 | 18280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00694   |
| train/info_shaping_reward_mean | -0.0801    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 731200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 457        |
| stats_o/mean                   | 0.20327191 |
| stats_o/std                    | 0.09075944 |
| test/episodes                  | 4580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00145   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.3656601 |
| test/Q_plus_P                  | -1.3656601 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 183200     |
| train/episodes                 | 18320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00523   |
| train/info_shaping_reward_mean | -0.0683    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 732800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 458        |
| stats_o/mean                   | 0.20326279 |
| stats_o/std                    | 0.09071491 |
| test/episodes                  | 4590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000509  |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.276094  |
| test/Q_plus_P                  | -1.276094  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 183600     |
| train/episodes                 | 18360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.595      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00633   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 734400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 459         |
| stats_o/mean                   | 0.20326568  |
| stats_o/std                    | 0.090699606 |
| test/episodes                  | 4600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00232    |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5012479  |
| test/Q_plus_P                  | -1.5012479  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 184000      |
| train/episodes                 | 18400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.552       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00729    |
| train/info_shaping_reward_mean | -0.0806     |
| train/info_shaping_reward_min  | -0.211      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 736000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 460        |
| stats_o/mean                   | 0.20326883 |
| stats_o/std                    | 0.09064926 |
| test/episodes                  | 4610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00235   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2085431 |
| test/Q_plus_P                  | -1.2085431 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 184400     |
| train/episodes                 | 18440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00541   |
| train/info_shaping_reward_mean | -0.0743    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 737600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 461        |
| stats_o/mean                   | 0.2032684  |
| stats_o/std                    | 0.09060517 |
| test/episodes                  | 4620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00216   |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3939316 |
| test/Q_plus_P                  | -1.3939316 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 184800     |
| train/episodes                 | 18480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00474   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 739200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 462        |
| stats_o/mean                   | 0.2032746  |
| stats_o/std                    | 0.0905427  |
| test/episodes                  | 4630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00399   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2970991 |
| test/Q_plus_P                  | -1.2970991 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 185200     |
| train/episodes                 | 18520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00585   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 740800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 463         |
| stats_o/mean                   | 0.20328009  |
| stats_o/std                    | 0.090476625 |
| test/episodes                  | 4640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.6134027  |
| test/Q_plus_P                  | -1.6134027  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 185600      |
| train/episodes                 | 18560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.615       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00449    |
| train/info_shaping_reward_mean | -0.0694     |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 742400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 464        |
| stats_o/mean                   | 0.20328043 |
| stats_o/std                    | 0.0904184  |
| test/episodes                  | 4650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00382   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2457616 |
| test/Q_plus_P                  | -1.2457616 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 186000     |
| train/episodes                 | 18600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00538   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 744000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 465        |
| stats_o/mean                   | 0.20328483 |
| stats_o/std                    | 0.09037204 |
| test/episodes                  | 4660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00372   |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1806748 |
| test/Q_plus_P                  | -1.1806748 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 186400     |
| train/episodes                 | 18640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.566      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00799   |
| train/info_shaping_reward_mean | -0.0751    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 745600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.20329148  |
| stats_o/std                    | 0.090307824 |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00278    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.2555678  |
| test/Q_plus_P                  | -1.2555678  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00465    |
| train/info_shaping_reward_mean | -0.0661     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 467        |
| stats_o/mean                   | 0.20329252 |
| stats_o/std                    | 0.09025256 |
| test/episodes                  | 4680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00651   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.213624  |
| test/Q_plus_P                  | -1.213624  |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 187200     |
| train/episodes                 | 18720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00828   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 748800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 468        |
| stats_o/mean                   | 0.20329626 |
| stats_o/std                    | 0.09018737 |
| test/episodes                  | 4690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00218   |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1779656 |
| test/Q_plus_P                  | -1.1779656 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 187600     |
| train/episodes                 | 18760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00825   |
| train/info_shaping_reward_mean | -0.0727    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 750400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 469        |
| stats_o/mean                   | 0.20329559 |
| stats_o/std                    | 0.0901287  |
| test/episodes                  | 4700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00278   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.4166261 |
| test/Q_plus_P                  | -1.4166261 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 188000     |
| train/episodes                 | 18800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00617   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 752000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.20329304 |
| stats_o/std                    | 0.09009667 |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00198   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2775905 |
| test/Q_plus_P                  | -1.2775905 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.548      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00798   |
| train/info_shaping_reward_mean | -0.0817    |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.2032964  |
| stats_o/std                    | 0.09003916 |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00218   |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3674965 |
| test/Q_plus_P                  | -1.3674965 |
| test/reward_per_eps            | -9         |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00666   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.20330329 |
| stats_o/std                    | 0.0899831  |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0027    |
| test/info_shaping_reward_mean  | -0.05      |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1052115 |
| test/Q_plus_P                  | -1.1052115 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00441   |
| train/info_shaping_reward_mean | -0.0732    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 473        |
| stats_o/mean                   | 0.20330878 |
| stats_o/std                    | 0.08992185 |
| test/episodes                  | 4740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00373   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3187004 |
| test/Q_plus_P                  | -1.3187004 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 189600     |
| train/episodes                 | 18960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.6        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00693   |
| train/info_shaping_reward_mean | -0.0684    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 758400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 474        |
| stats_o/mean                   | 0.20330915 |
| stats_o/std                    | 0.08985697 |
| test/episodes                  | 4750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0103    |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2062477 |
| test/Q_plus_P                  | -1.2062477 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 190000     |
| train/episodes                 | 19000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.625      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00479   |
| train/info_shaping_reward_mean | -0.0691    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 760000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 475        |
| stats_o/mean                   | 0.20330288 |
| stats_o/std                    | 0.08980747 |
| test/episodes                  | 4760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00782   |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2463694 |
| test/Q_plus_P                  | -1.2463694 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 190400     |
| train/episodes                 | 19040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00729   |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 761600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 476        |
| stats_o/mean                   | 0.20331235 |
| stats_o/std                    | 0.08974936 |
| test/episodes                  | 4770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.0531    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.4094998 |
| test/Q_plus_P                  | -1.4094998 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 190800     |
| train/episodes                 | 19080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0091    |
| train/info_shaping_reward_mean | -0.0714    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 763200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 477        |
| stats_o/mean                   | 0.20331396 |
| stats_o/std                    | 0.08971827 |
| test/episodes                  | 4780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00886   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1712668 |
| test/Q_plus_P                  | -1.1712668 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 191200     |
| train/episodes                 | 19120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.0784    |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 764800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 478         |
| stats_o/mean                   | 0.20331149  |
| stats_o/std                    | 0.089677446 |
| test/episodes                  | 4790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00281    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.3572931  |
| test/Q_plus_P                  | -1.3572931  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 191600      |
| train/episodes                 | 19160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00576    |
| train/info_shaping_reward_mean | -0.0678     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 766400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 479        |
| stats_o/mean                   | 0.20331565 |
| stats_o/std                    | 0.08961551 |
| test/episodes                  | 4800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00263   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.1825775 |
| test/Q_plus_P                  | -1.1825775 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 192000     |
| train/episodes                 | 19200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00637   |
| train/info_shaping_reward_mean | -0.0697    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 768000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 480        |
| stats_o/mean                   | 0.20330995 |
| stats_o/std                    | 0.08957781 |
| test/episodes                  | 4810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00624   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.3084987 |
| test/Q_plus_P                  | -1.3084987 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 192400     |
| train/episodes                 | 19240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00706   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 769600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 481        |
| stats_o/mean                   | 0.20330599 |
| stats_o/std                    | 0.08955309 |
| test/episodes                  | 4820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000554  |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.2761341 |
| test/Q_plus_P                  | -1.2761341 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 192800     |
| train/episodes                 | 19280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00787   |
| train/info_shaping_reward_mean | -0.0715    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 771200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 482        |
| stats_o/mean                   | 0.2033069  |
| stats_o/std                    | 0.08949683 |
| test/episodes                  | 4830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00435   |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.382722  |
| test/Q_plus_P                  | -1.382722  |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 193200     |
| train/episodes                 | 19320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00709   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 772800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 483        |
| stats_o/mean                   | 0.20330538 |
| stats_o/std                    | 0.0894632  |
| test/episodes                  | 4840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00527   |
| test/info_shaping_reward_mean  | -0.047     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0833175 |
| test/Q_plus_P                  | -1.0833175 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 193600     |
| train/episodes                 | 19360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0058    |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 774400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 484        |
| stats_o/mean                   | 0.20330396 |
| stats_o/std                    | 0.08941612 |
| test/episodes                  | 4850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00263   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2276452 |
| test/Q_plus_P                  | -1.2276452 |
| test/reward_per_eps            | -9         |
| test/steps                     | 194000     |
| train/episodes                 | 19400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00719   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 776000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.20330907  |
| stats_o/std                    | 0.089394614 |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00192    |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.0718069  |
| test/Q_plus_P                  | -1.0718069  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.578       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0066     |
| train/info_shaping_reward_mean | -0.072      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 486        |
| stats_o/mean                   | 0.20331119 |
| stats_o/std                    | 0.0893506  |
| test/episodes                  | 4870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000997  |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.2231932 |
| test/Q_plus_P                  | -1.2231932 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 194800     |
| train/episodes                 | 19480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00632   |
| train/info_shaping_reward_mean | -0.0722    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 779200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 487        |
| stats_o/mean                   | 0.20331778 |
| stats_o/std                    | 0.08929885 |
| test/episodes                  | 4880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1371516 |
| test/Q_plus_P                  | -1.1371516 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 195200     |
| train/episodes                 | 19520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00571   |
| train/info_shaping_reward_mean | -0.0702    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 780800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 488        |
| stats_o/mean                   | 0.2033228  |
| stats_o/std                    | 0.08925388 |
| test/episodes                  | 4890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0127    |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2641224 |
| test/Q_plus_P                  | -1.2641224 |
| test/reward_per_eps            | -9         |
| test/steps                     | 195600     |
| train/episodes                 | 19560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00611   |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 782400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 489        |
| stats_o/mean                   | 0.20332812 |
| stats_o/std                    | 0.08921168 |
| test/episodes                  | 4900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00871   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2620503 |
| test/Q_plus_P                  | -1.2620503 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 196000     |
| train/episodes                 | 19600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.628      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00485   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 784000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 490        |
| stats_o/mean                   | 0.20333149 |
| stats_o/std                    | 0.08915924 |
| test/episodes                  | 4910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00686   |
| test/info_shaping_reward_mean  | -0.048     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.0952382 |
| test/Q_plus_P                  | -1.0952382 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 196400     |
| train/episodes                 | 19640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00637   |
| train/info_shaping_reward_mean | -0.066     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 785600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 491        |
| stats_o/mean                   | 0.20334013 |
| stats_o/std                    | 0.0891268  |
| test/episodes                  | 4920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00364   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.051968  |
| test/Q_plus_P                  | -1.051968  |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 196800     |
| train/episodes                 | 19680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.658      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00584   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 787200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 492         |
| stats_o/mean                   | 0.20333931  |
| stats_o/std                    | 0.089072324 |
| test/episodes                  | 4930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0025     |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2157956  |
| test/Q_plus_P                  | -1.2157956  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 197200      |
| train/episodes                 | 19720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0065     |
| train/info_shaping_reward_mean | -0.0686     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 788800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.20333678 |
| stats_o/std                    | 0.08902333 |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00104   |
| test/info_shaping_reward_mean  | -0.0441    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.125475  |
| test/Q_plus_P                  | -1.125475  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00664   |
| train/info_shaping_reward_mean | -0.0738    |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.20333602  |
| stats_o/std                    | 0.089010105 |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00378    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.1921576  |
| test/Q_plus_P                  | -1.1921576  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00751    |
| train/info_shaping_reward_mean | -0.0729     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 495        |
| stats_o/mean                   | 0.2033359  |
| stats_o/std                    | 0.08896422 |
| test/episodes                  | 4960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00373   |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0922681 |
| test/Q_plus_P                  | -1.0922681 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 198400     |
| train/episodes                 | 19840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00609   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 793600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 496        |
| stats_o/mean                   | 0.20333618 |
| stats_o/std                    | 0.08892689 |
| test/episodes                  | 4970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00361   |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0535533 |
| test/Q_plus_P                  | -1.0535533 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 198800     |
| train/episodes                 | 19880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00518   |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 795200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 497        |
| stats_o/mean                   | 0.2033384  |
| stats_o/std                    | 0.08891245 |
| test/episodes                  | 4980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00304   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1310989 |
| test/Q_plus_P                  | -1.1310989 |
| test/reward_per_eps            | -9         |
| test/steps                     | 199200     |
| train/episodes                 | 19920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00795   |
| train/info_shaping_reward_mean | -0.0753    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 796800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 498        |
| stats_o/mean                   | 0.20334595 |
| stats_o/std                    | 0.08889654 |
| test/episodes                  | 4990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000728  |
| test/info_shaping_reward_mean  | -0.0448    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1131186 |
| test/Q_plus_P                  | -1.1131186 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 199600     |
| train/episodes                 | 19960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.563      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00755   |
| train/info_shaping_reward_mean | -0.076     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 798400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.20334692  |
| stats_o/std                    | 0.088854276 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00582    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.06664    |
| test/Q_plus_P                  | -1.06664    |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00697    |
| train/info_shaping_reward_mean | -0.0682     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 500        |
| stats_o/mean                   | 0.20334049 |
| stats_o/std                    | 0.08880706 |
| test/episodes                  | 5010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00393   |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.11667   |
| test/Q_plus_P                  | -1.11667   |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 200400     |
| train/episodes                 | 20040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00682   |
| train/info_shaping_reward_mean | -0.0655    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 801600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 501        |
| stats_o/mean                   | 0.20334598 |
| stats_o/std                    | 0.08876176 |
| test/episodes                  | 5020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00523   |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.226572  |
| test/Q_plus_P                  | -1.226572  |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 200800     |
| train/episodes                 | 20080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00613   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 803200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 502        |
| stats_o/mean                   | 0.203343   |
| stats_o/std                    | 0.08873221 |
| test/episodes                  | 5030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00158   |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0425462 |
| test/Q_plus_P                  | -1.0425462 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 201200     |
| train/episodes                 | 20120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0063    |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 804800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 503        |
| stats_o/mean                   | 0.20335193 |
| stats_o/std                    | 0.08872945 |
| test/episodes                  | 5040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00109   |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.044537  |
| test/Q_plus_P                  | -1.044537  |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 201600     |
| train/episodes                 | 20160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00598   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 806400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 504        |
| stats_o/mean                   | 0.20335577 |
| stats_o/std                    | 0.08868128 |
| test/episodes                  | 5050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.2185302 |
| test/Q_plus_P                  | -1.2185302 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 202000     |
| train/episodes                 | 20200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00739   |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 808000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 505        |
| stats_o/mean                   | 0.20335908 |
| stats_o/std                    | 0.08865977 |
| test/episodes                  | 5060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00245   |
| test/info_shaping_reward_mean  | -0.0472    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.038647  |
| test/Q_plus_P                  | -1.038647  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 202400     |
| train/episodes                 | 20240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00473   |
| train/info_shaping_reward_mean | -0.0764    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 809600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 506        |
| stats_o/mean                   | 0.2033628  |
| stats_o/std                    | 0.08862227 |
| test/episodes                  | 5070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0037    |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1854012 |
| test/Q_plus_P                  | -1.1854012 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 202800     |
| train/episodes                 | 20280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00519   |
| train/info_shaping_reward_mean | -0.069     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 811200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 507         |
| stats_o/mean                   | 0.2033759   |
| stats_o/std                    | 0.088600256 |
| test/episodes                  | 5080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00636    |
| test/info_shaping_reward_mean  | -0.0538     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.123959   |
| test/Q_plus_P                  | -1.123959   |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 203200      |
| train/episodes                 | 20320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00622    |
| train/info_shaping_reward_mean | -0.0682     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 812800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 508         |
| stats_o/mean                   | 0.20337734  |
| stats_o/std                    | 0.088562556 |
| test/episodes                  | 5090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00585    |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.9836155  |
| test/Q_plus_P                  | -0.9836155  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 203600      |
| train/episodes                 | 20360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00809    |
| train/info_shaping_reward_mean | -0.067      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 814400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 509         |
| stats_o/mean                   | 0.20338032  |
| stats_o/std                    | 0.088500425 |
| test/episodes                  | 5100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.016      |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.2886506  |
| test/Q_plus_P                  | -1.2886506  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 204000      |
| train/episodes                 | 20400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00541    |
| train/info_shaping_reward_mean | -0.0674     |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 816000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 510        |
| stats_o/mean                   | 0.20339337 |
| stats_o/std                    | 0.0884603  |
| test/episodes                  | 5110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00398   |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.2077197 |
| test/Q_plus_P                  | -1.2077197 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 204400     |
| train/episodes                 | 20440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00484   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 817600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 511        |
| stats_o/mean                   | 0.20338827 |
| stats_o/std                    | 0.08841157 |
| test/episodes                  | 5120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1114508 |
| test/Q_plus_P                  | -1.1114508 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 204800     |
| train/episodes                 | 20480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00561   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 819200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 512        |
| stats_o/mean                   | 0.2033846  |
| stats_o/std                    | 0.08837612 |
| test/episodes                  | 5130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00603   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1397889 |
| test/Q_plus_P                  | -1.1397889 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 205200     |
| train/episodes                 | 20520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00741   |
| train/info_shaping_reward_mean | -0.0762    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 820800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 513        |
| stats_o/mean                   | 0.20338905 |
| stats_o/std                    | 0.08835344 |
| test/episodes                  | 5140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00214   |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0345871 |
| test/Q_plus_P                  | -1.0345871 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 205600     |
| train/episodes                 | 20560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00531   |
| train/info_shaping_reward_mean | -0.0784    |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 822400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 514         |
| stats_o/mean                   | 0.20338905  |
| stats_o/std                    | 0.088323765 |
| test/episodes                  | 5150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00198    |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0677284  |
| test/Q_plus_P                  | -1.0677284  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 206000      |
| train/episodes                 | 20600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00481    |
| train/info_shaping_reward_mean | -0.0805     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 824000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 515         |
| stats_o/mean                   | 0.20339671  |
| stats_o/std                    | 0.088273875 |
| test/episodes                  | 5160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00132    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -1.0950981  |
| test/Q_plus_P                  | -1.0950981  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 206400      |
| train/episodes                 | 20640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00439    |
| train/info_shaping_reward_mean | -0.0689     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 825600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 516        |
| stats_o/mean                   | 0.20340198 |
| stats_o/std                    | 0.08822733 |
| test/episodes                  | 5170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00482   |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.0571557 |
| test/Q_plus_P                  | -1.0571557 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 206800     |
| train/episodes                 | 20680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.607      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00648   |
| train/info_shaping_reward_mean | -0.0722    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 827200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 517        |
| stats_o/mean                   | 0.20339613 |
| stats_o/std                    | 0.08822725 |
| test/episodes                  | 5180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00412   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.1173915 |
| test/Q_plus_P                  | -1.1173915 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 207200     |
| train/episodes                 | 20720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00745   |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 828800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 518        |
| stats_o/mean                   | 0.20340593 |
| stats_o/std                    | 0.08818951 |
| test/episodes                  | 5190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0126    |
| test/info_shaping_reward_mean  | -0.0617    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.2107322 |
| test/Q_plus_P                  | -2.2107322 |
| test/reward_per_eps            | -12        |
| test/steps                     | 207600     |
| train/episodes                 | 20760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00706   |
| train/info_shaping_reward_mean | -0.0663    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 830400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 519        |
| stats_o/mean                   | 0.20341049 |
| stats_o/std                    | 0.08816733 |
| test/episodes                  | 5200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0103    |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1537194 |
| test/Q_plus_P                  | -1.1537194 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 208000     |
| train/episodes                 | 20800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00742   |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 832000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 520        |
| stats_o/mean                   | 0.20341344 |
| stats_o/std                    | 0.08815217 |
| test/episodes                  | 5210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00107   |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1325418 |
| test/Q_plus_P                  | -1.1325418 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 208400     |
| train/episodes                 | 20840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00604   |
| train/info_shaping_reward_mean | -0.0771    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 833600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 521        |
| stats_o/mean                   | 0.2034083  |
| stats_o/std                    | 0.08813752 |
| test/episodes                  | 5220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0162    |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1259304 |
| test/Q_plus_P                  | -1.1259304 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 208800     |
| train/episodes                 | 20880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00514   |
| train/info_shaping_reward_mean | -0.07      |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 835200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 522         |
| stats_o/mean                   | 0.20340906  |
| stats_o/std                    | 0.088102214 |
| test/episodes                  | 5230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00217    |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.1524827  |
| test/Q_plus_P                  | -1.1524827  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 209200      |
| train/episodes                 | 20920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.602       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0052     |
| train/info_shaping_reward_mean | -0.0691     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 836800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 523         |
| stats_o/mean                   | 0.20340449  |
| stats_o/std                    | 0.088090375 |
| test/episodes                  | 5240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00199    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.0660048  |
| test/Q_plus_P                  | -1.0660048  |
| test/reward_per_eps            | -9          |
| test/steps                     | 209600      |
| train/episodes                 | 20960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0042     |
| train/info_shaping_reward_mean | -0.081      |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 838400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 524        |
| stats_o/mean                   | 0.20340402 |
| stats_o/std                    | 0.08803651 |
| test/episodes                  | 5250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00201   |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1000019 |
| test/Q_plus_P                  | -1.1000019 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 210000     |
| train/episodes                 | 21000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00599   |
| train/info_shaping_reward_mean | -0.0716    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 840000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 525        |
| stats_o/mean                   | 0.20340806 |
| stats_o/std                    | 0.08802389 |
| test/episodes                  | 5260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0103    |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.1502384 |
| test/Q_plus_P                  | -1.1502384 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 210400     |
| train/episodes                 | 21040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00585   |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 841600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.20341058  |
| stats_o/std                    | 0.087989606 |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00457    |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.2913382  |
| test/Q_plus_P                  | -1.2913382  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.576       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00501    |
| train/info_shaping_reward_mean | -0.0791     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 527        |
| stats_o/mean                   | 0.20341289 |
| stats_o/std                    | 0.08796903 |
| test/episodes                  | 5280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000953  |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.062407  |
| test/Q_plus_P                  | -1.062407  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 211200     |
| train/episodes                 | 21120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.587      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00627   |
| train/info_shaping_reward_mean | -0.0711    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 844800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 528        |
| stats_o/mean                   | 0.203405   |
| stats_o/std                    | 0.08795099 |
| test/episodes                  | 5290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000995  |
| test/info_shaping_reward_mean  | -0.0413    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.039381  |
| test/Q_plus_P                  | -1.039381  |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 211600     |
| train/episodes                 | 21160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00659   |
| train/info_shaping_reward_mean | -0.0734    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 846400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.20340559  |
| stats_o/std                    | 0.087921344 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0014     |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.1287701  |
| test/Q_plus_P                  | -1.1287701  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00567    |
| train/info_shaping_reward_mean | -0.0643     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 530        |
| stats_o/mean                   | 0.20341034 |
| stats_o/std                    | 0.08790143 |
| test/episodes                  | 5310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0031    |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1294765 |
| test/Q_plus_P                  | -1.1294765 |
| test/reward_per_eps            | -8         |
| test/steps                     | 212400     |
| train/episodes                 | 21240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00465   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 849600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 531        |
| stats_o/mean                   | 0.20341228 |
| stats_o/std                    | 0.08786631 |
| test/episodes                  | 5320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00301   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.2192202 |
| test/Q_plus_P                  | -1.2192202 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 212800     |
| train/episodes                 | 21280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0055    |
| train/info_shaping_reward_mean | -0.0676    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 851200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 532        |
| stats_o/mean                   | 0.20341732 |
| stats_o/std                    | 0.0878374  |
| test/episodes                  | 5330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00342   |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0248663 |
| test/Q_plus_P                  | -1.0248663 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 213200     |
| train/episodes                 | 21320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00698   |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 852800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 533        |
| stats_o/mean                   | 0.2034156  |
| stats_o/std                    | 0.08778957 |
| test/episodes                  | 5340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00322   |
| test/info_shaping_reward_mean  | -0.0423    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.049911  |
| test/Q_plus_P                  | -1.049911  |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 213600     |
| train/episodes                 | 21360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0053    |
| train/info_shaping_reward_mean | -0.0673    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 854400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 534        |
| stats_o/mean                   | 0.20342202 |
| stats_o/std                    | 0.08775135 |
| test/episodes                  | 5350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00436   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.2618915 |
| test/Q_plus_P                  | -1.2618915 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 214000     |
| train/episodes                 | 21400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00579   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 856000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 535         |
| stats_o/mean                   | 0.20341839  |
| stats_o/std                    | 0.087715924 |
| test/episodes                  | 5360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00419    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.1263001  |
| test/Q_plus_P                  | -1.1263001  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 214400      |
| train/episodes                 | 21440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00449    |
| train/info_shaping_reward_mean | -0.0702     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 857600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 536        |
| stats_o/mean                   | 0.20341988 |
| stats_o/std                    | 0.0876783  |
| test/episodes                  | 5370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00838   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1344913 |
| test/Q_plus_P                  | -1.1344913 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 214800     |
| train/episodes                 | 21480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.64       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00543   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 859200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 537        |
| stats_o/mean                   | 0.20342259 |
| stats_o/std                    | 0.08765268 |
| test/episodes                  | 5380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0014    |
| test/info_shaping_reward_mean  | -0.0444    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.089094  |
| test/Q_plus_P                  | -1.089094  |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 215200     |
| train/episodes                 | 21520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.587      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00345   |
| train/info_shaping_reward_mean | -0.0789    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 860800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 538        |
| stats_o/mean                   | 0.2034309  |
| stats_o/std                    | 0.08761679 |
| test/episodes                  | 5390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00475   |
| test/info_shaping_reward_mean  | -0.0463    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1473387 |
| test/Q_plus_P                  | -1.1473387 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 215600     |
| train/episodes                 | 21560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0053    |
| train/info_shaping_reward_mean | -0.0667    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 862400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 539         |
| stats_o/mean                   | 0.20342638  |
| stats_o/std                    | 0.087573946 |
| test/episodes                  | 5400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0039     |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0251994  |
| test/Q_plus_P                  | -1.0251994  |
| test/reward_per_eps            | -8          |
| test/steps                     | 216000      |
| train/episodes                 | 21600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0669     |
| train/info_shaping_reward_min  | -0.175      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 864000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 540        |
| stats_o/mean                   | 0.2034257  |
| stats_o/std                    | 0.0875334  |
| test/episodes                  | 5410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00309   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -0.9860347 |
| test/Q_plus_P                  | -0.9860347 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 216400     |
| train/episodes                 | 21640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00627   |
| train/info_shaping_reward_mean | -0.0685    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 865600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 541        |
| stats_o/mean                   | 0.20343286 |
| stats_o/std                    | 0.0875202  |
| test/episodes                  | 5420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000718  |
| test/info_shaping_reward_mean  | -0.0472    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.3946347 |
| test/Q_plus_P                  | -1.3946347 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 216800     |
| train/episodes                 | 21680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00454   |
| train/info_shaping_reward_mean | -0.0691    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 867200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 542        |
| stats_o/mean                   | 0.20343618 |
| stats_o/std                    | 0.08748148 |
| test/episodes                  | 5430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00398   |
| test/info_shaping_reward_mean  | -0.0487    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.0576423 |
| test/Q_plus_P                  | -1.0576423 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 217200     |
| train/episodes                 | 21720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00573   |
| train/info_shaping_reward_mean | -0.0709    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 868800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 543         |
| stats_o/mean                   | 0.20343564  |
| stats_o/std                    | 0.08749148  |
| test/episodes                  | 5440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00324    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.95270365 |
| test/Q_plus_P                  | -0.95270365 |
| test/reward_per_eps            | -8          |
| test/steps                     | 217600      |
| train/episodes                 | 21760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00489    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.215      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 870400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 544        |
| stats_o/mean                   | 0.20343742 |
| stats_o/std                    | 0.08745913 |
| test/episodes                  | 5450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00183   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.152158  |
| test/Q_plus_P                  | -1.152158  |
| test/reward_per_eps            | -9         |
| test/steps                     | 218000     |
| train/episodes                 | 21800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00805   |
| train/info_shaping_reward_mean | -0.0759    |
| train/info_shaping_reward_min  | -0.218     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 872000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 545        |
| stats_o/mean                   | 0.2034393  |
| stats_o/std                    | 0.08741177 |
| test/episodes                  | 5460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00178   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1671768 |
| test/Q_plus_P                  | -1.1671768 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 218400     |
| train/episodes                 | 21840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00515   |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 873600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.20344107  |
| stats_o/std                    | 0.087373994 |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00144    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.057456   |
| test/Q_plus_P                  | -1.057456   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00492    |
| train/info_shaping_reward_mean | -0.0683     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 547        |
| stats_o/mean                   | 0.20343994 |
| stats_o/std                    | 0.08733468 |
| test/episodes                  | 5480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00292   |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0534157 |
| test/Q_plus_P                  | -1.0534157 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 219200     |
| train/episodes                 | 21920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00616   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 876800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 548        |
| stats_o/mean                   | 0.20344521 |
| stats_o/std                    | 0.08733897 |
| test/episodes                  | 5490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00153   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1287074 |
| test/Q_plus_P                  | -1.1287074 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 219600     |
| train/episodes                 | 21960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00423   |
| train/info_shaping_reward_mean | -0.0736    |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 878400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 549        |
| stats_o/mean                   | 0.20344757 |
| stats_o/std                    | 0.08731356 |
| test/episodes                  | 5500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00948   |
| test/info_shaping_reward_mean  | -0.0557    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.206531  |
| test/Q_plus_P                  | -1.206531  |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 220000     |
| train/episodes                 | 22000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00709   |
| train/info_shaping_reward_mean | -0.0725    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 880000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 550        |
| stats_o/mean                   | 0.2034487  |
| stats_o/std                    | 0.0872671  |
| test/episodes                  | 5510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00412   |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0315242 |
| test/Q_plus_P                  | -1.0315242 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 220400     |
| train/episodes                 | 22040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00505   |
| train/info_shaping_reward_mean | -0.0654    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 881600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 551         |
| stats_o/mean                   | 0.20344707  |
| stats_o/std                    | 0.087221555 |
| test/episodes                  | 5520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0061     |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -1.154841   |
| test/Q_plus_P                  | -1.154841   |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 220800      |
| train/episodes                 | 22080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00551    |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 883200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 552        |
| stats_o/mean                   | 0.20344959 |
| stats_o/std                    | 0.08717827 |
| test/episodes                  | 5530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1672157 |
| test/Q_plus_P                  | -1.1672157 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 221200     |
| train/episodes                 | 22120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0063    |
| train/info_shaping_reward_mean | -0.0676    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 884800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 553        |
| stats_o/mean                   | 0.20344529 |
| stats_o/std                    | 0.08715932 |
| test/episodes                  | 5540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00218   |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.1993467 |
| test/Q_plus_P                  | -1.1993467 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 221600     |
| train/episodes                 | 22160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00712   |
| train/info_shaping_reward_mean | -0.0686    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 886400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 554        |
| stats_o/mean                   | 0.20344235 |
| stats_o/std                    | 0.08712922 |
| test/episodes                  | 5550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00433   |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -0.9924457 |
| test/Q_plus_P                  | -0.9924457 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 222000     |
| train/episodes                 | 22200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.612      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00732   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 888000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 555        |
| stats_o/mean                   | 0.20344219 |
| stats_o/std                    | 0.08709612 |
| test/episodes                  | 5560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00703   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.0846134 |
| test/Q_plus_P                  | -1.0846134 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 222400     |
| train/episodes                 | 22240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00527   |
| train/info_shaping_reward_mean | -0.0793    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 889600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 556        |
| stats_o/mean                   | 0.20344858 |
| stats_o/std                    | 0.08704663 |
| test/episodes                  | 5570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00207   |
| test/info_shaping_reward_mean  | -0.0508    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0803894 |
| test/Q_plus_P                  | -1.0803894 |
| test/reward_per_eps            | -9         |
| test/steps                     | 222800     |
| train/episodes                 | 22280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00592   |
| train/info_shaping_reward_mean | -0.0634    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 891200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 557        |
| stats_o/mean                   | 0.20345762 |
| stats_o/std                    | 0.08701918 |
| test/episodes                  | 5580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0048    |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0552526 |
| test/Q_plus_P                  | -1.0552526 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 223200     |
| train/episodes                 | 22320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00517   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 892800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 558         |
| stats_o/mean                   | 0.20345347  |
| stats_o/std                    | 0.086989224 |
| test/episodes                  | 5590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00696    |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.0722756  |
| test/Q_plus_P                  | -1.0722756  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 223600      |
| train/episodes                 | 22360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00706    |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 894400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 559        |
| stats_o/mean                   | 0.20345284 |
| stats_o/std                    | 0.08697761 |
| test/episodes                  | 5600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0057    |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.110903  |
| test/Q_plus_P                  | -1.110903  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 224000     |
| train/episodes                 | 22400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00533   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 896000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 560        |
| stats_o/mean                   | 0.20345311 |
| stats_o/std                    | 0.08697178 |
| test/episodes                  | 5610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000191  |
| test/info_shaping_reward_mean  | -0.0441    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0206927 |
| test/Q_plus_P                  | -1.0206927 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 224400     |
| train/episodes                 | 22440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00687   |
| train/info_shaping_reward_mean | -0.0719    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 897600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 561        |
| stats_o/mean                   | 0.20345135 |
| stats_o/std                    | 0.08694638 |
| test/episodes                  | 5620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00657   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0159898 |
| test/Q_plus_P                  | -1.0159898 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 224800     |
| train/episodes                 | 22480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.561      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00608   |
| train/info_shaping_reward_mean | -0.0847    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 899200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.2034532   |
| stats_o/std                    | 0.086911105 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00225    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0967067  |
| test/Q_plus_P                  | -1.0967067  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00734    |
| train/info_shaping_reward_mean | -0.0696     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 563        |
| stats_o/mean                   | 0.20346232 |
| stats_o/std                    | 0.08687191 |
| test/episodes                  | 5640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00177   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -1.095609  |
| test/Q_plus_P                  | -1.095609  |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 225600     |
| train/episodes                 | 22560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00532   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 902400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 564         |
| stats_o/mean                   | 0.20346186  |
| stats_o/std                    | 0.08685947  |
| test/episodes                  | 5650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00981    |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.99826235 |
| test/Q_plus_P                  | -0.99826235 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 226000      |
| train/episodes                 | 22600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.543       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00577    |
| train/info_shaping_reward_mean | -0.0806     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.3       |
| train/steps                    | 904000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 565        |
| stats_o/mean                   | 0.20347764 |
| stats_o/std                    | 0.08686061 |
| test/episodes                  | 5660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0228224 |
| test/Q_plus_P                  | -1.0228224 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 226400     |
| train/episodes                 | 22640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.57       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00479   |
| train/info_shaping_reward_mean | -0.0731    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 905600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 566        |
| stats_o/mean                   | 0.20347816 |
| stats_o/std                    | 0.08682807 |
| test/episodes                  | 5670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00123   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -1.0168818 |
| test/Q_plus_P                  | -1.0168818 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 226800     |
| train/episodes                 | 22680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00429   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 907200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 567        |
| stats_o/mean                   | 0.20348622 |
| stats_o/std                    | 0.08679122 |
| test/episodes                  | 5680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00934   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.134579  |
| test/Q_plus_P                  | -1.134579  |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 227200     |
| train/episodes                 | 22720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00624   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 908800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.20348991  |
| stats_o/std                    | 0.086767405 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00217    |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -1.0976368  |
| test/Q_plus_P                  | -1.0976368  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.61        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0046     |
| train/info_shaping_reward_mean | -0.0697     |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 569        |
| stats_o/mean                   | 0.2034916  |
| stats_o/std                    | 0.08673756 |
| test/episodes                  | 5700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00153   |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.1324275 |
| test/Q_plus_P                  | -1.1324275 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 228000     |
| train/episodes                 | 22800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00571   |
| train/info_shaping_reward_mean | -0.0668    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 912000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.20348705  |
| stats_o/std                    | 0.08670166  |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00457    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -0.96373713 |
| test/Q_plus_P                  | -0.96373713 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00449    |
| train/info_shaping_reward_mean | -0.0668     |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 571        |
| stats_o/mean                   | 0.2034891  |
| stats_o/std                    | 0.08667552 |
| test/episodes                  | 5720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00386   |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0984147 |
| test/Q_plus_P                  | -1.0984147 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 228800     |
| train/episodes                 | 22880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00521   |
| train/info_shaping_reward_mean | -0.0724    |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 915200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.20348564  |
| stats_o/std                    | 0.086635225 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00251    |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.0287904  |
| test/Q_plus_P                  | -1.0287904  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0048     |
| train/info_shaping_reward_mean | -0.0717     |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 573         |
| stats_o/mean                   | 0.20348936  |
| stats_o/std                    | 0.086622946 |
| test/episodes                  | 5740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00291    |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.1266853  |
| test/Q_plus_P                  | -1.1266853  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 229600      |
| train/episodes                 | 22960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00703    |
| train/info_shaping_reward_mean | -0.0742     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 918400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.20349377  |
| stats_o/std                    | 0.086600095 |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0044     |
| test/info_shaping_reward_mean  | -0.0472     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -0.9984018  |
| test/Q_plus_P                  | -0.9984018  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00502    |
| train/info_shaping_reward_mean | -0.0688     |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.20349312  |
| stats_o/std                    | 0.086583875 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00772    |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -0.9833166  |
| test/Q_plus_P                  | -0.9833166  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00665    |
| train/info_shaping_reward_mean | -0.0694     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.20349605  |
| stats_o/std                    | 0.086560495 |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0024     |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.0764402  |
| test/Q_plus_P                  | -1.0764402  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00501    |
| train/info_shaping_reward_mean | -0.0755     |
| train/info_shaping_reward_min  | -0.219      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 577        |
| stats_o/mean                   | 0.20349449 |
| stats_o/std                    | 0.08651903 |
| test/episodes                  | 5780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00238   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -0.9723908 |
| test/Q_plus_P                  | -0.9723908 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 231200     |
| train/episodes                 | 23120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00433   |
| train/info_shaping_reward_mean | -0.0643    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 924800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.20349997 |
| stats_o/std                    | 0.08648837 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00215   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.0223693 |
| test/Q_plus_P                  | -1.0223693 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0072    |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 579        |
| stats_o/mean                   | 0.20349768 |
| stats_o/std                    | 0.08644035 |
| test/episodes                  | 5800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00351   |
| test/info_shaping_reward_mean  | -0.0486    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0356038 |
| test/Q_plus_P                  | -1.0356038 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 232000     |
| train/episodes                 | 23200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.647      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00673   |
| train/info_shaping_reward_mean | -0.0644    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 928000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 580        |
| stats_o/mean                   | 0.2035024  |
| stats_o/std                    | 0.08640957 |
| test/episodes                  | 5810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000911  |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.0084822 |
| test/Q_plus_P                  | -1.0084822 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 232400     |
| train/episodes                 | 23240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.635      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00566   |
| train/info_shaping_reward_mean | -0.0652    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 929600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 581        |
| stats_o/mean                   | 0.20350336 |
| stats_o/std                    | 0.08637645 |
| test/episodes                  | 5820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00269   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0846428 |
| test/Q_plus_P                  | -1.0846428 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 232800     |
| train/episodes                 | 23280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00686   |
| train/info_shaping_reward_mean | -0.0665    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 931200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 582        |
| stats_o/mean                   | 0.20350838 |
| stats_o/std                    | 0.08633646 |
| test/episodes                  | 5830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000901  |
| test/info_shaping_reward_mean  | -0.0428    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.1764972 |
| test/Q_plus_P                  | -1.1764972 |
| test/reward_per_eps            | -9         |
| test/steps                     | 233200     |
| train/episodes                 | 23320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0062    |
| train/info_shaping_reward_mean | -0.0654    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 932800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 583         |
| stats_o/mean                   | 0.20351152  |
| stats_o/std                    | 0.086324655 |
| test/episodes                  | 5840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00254    |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0766629  |
| test/Q_plus_P                  | -1.0766629  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 233600      |
| train/episodes                 | 23360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00497    |
| train/info_shaping_reward_mean | -0.0704     |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 934400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 584         |
| stats_o/mean                   | 0.20350891  |
| stats_o/std                    | 0.086281806 |
| test/episodes                  | 5850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00249    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.0903025  |
| test/Q_plus_P                  | -1.0903025  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 234000      |
| train/episodes                 | 23400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00517    |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 936000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 585        |
| stats_o/mean                   | 0.20351103 |
| stats_o/std                    | 0.08626699 |
| test/episodes                  | 5860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00411   |
| test/info_shaping_reward_mean  | -0.043     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0083216 |
| test/Q_plus_P                  | -1.0083216 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 234400     |
| train/episodes                 | 23440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00552   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 937600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 586        |
| stats_o/mean                   | 0.20351838 |
| stats_o/std                    | 0.08624602 |
| test/episodes                  | 5870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00681   |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.1074498 |
| test/Q_plus_P                  | -1.1074498 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 234800     |
| train/episodes                 | 23480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00414   |
| train/info_shaping_reward_mean | -0.0638    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 939200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 587        |
| stats_o/mean                   | 0.20351347 |
| stats_o/std                    | 0.0862191  |
| test/episodes                  | 5880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00255   |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.098567  |
| test/Q_plus_P                  | -1.098567  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 235200     |
| train/episodes                 | 23520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00434   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 940800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 588        |
| stats_o/mean                   | 0.2035182  |
| stats_o/std                    | 0.08616567 |
| test/episodes                  | 5890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00198   |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -0.9489122 |
| test/Q_plus_P                  | -0.9489122 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 235600     |
| train/episodes                 | 23560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.668      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00635   |
| train/info_shaping_reward_mean | -0.0628    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 942400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 589         |
| stats_o/mean                   | 0.20352603  |
| stats_o/std                    | 0.08618823  |
| test/episodes                  | 5900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00201    |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -0.95395935 |
| test/Q_plus_P                  | -0.95395935 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 236000      |
| train/episodes                 | 23600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.0885     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 944000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.2035254   |
| stats_o/std                    | 0.086157195 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00277    |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -1.0121181  |
| test/Q_plus_P                  | -1.0121181  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00525    |
| train/info_shaping_reward_mean | -0.0637     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 591        |
| stats_o/mean                   | 0.20352787 |
| stats_o/std                    | 0.08612459 |
| test/episodes                  | 5920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00448   |
| test/info_shaping_reward_mean  | -0.0472    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.021423  |
| test/Q_plus_P                  | -1.021423  |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 236800     |
| train/episodes                 | 23680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.647      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00536   |
| train/info_shaping_reward_mean | -0.0651    |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 947200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 592        |
| stats_o/mean                   | 0.2035342  |
| stats_o/std                    | 0.0861112  |
| test/episodes                  | 5930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00194   |
| test/info_shaping_reward_mean  | -0.0427    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0080099 |
| test/Q_plus_P                  | -1.0080099 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 237200     |
| train/episodes                 | 23720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00712   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 948800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 593        |
| stats_o/mean                   | 0.2035304  |
| stats_o/std                    | 0.08608618 |
| test/episodes                  | 5940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0113    |
| test/info_shaping_reward_mean  | -0.0506    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0092001 |
| test/Q_plus_P                  | -1.0092001 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 237600     |
| train/episodes                 | 23760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00501   |
| train/info_shaping_reward_mean | -0.0745    |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 950400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 594        |
| stats_o/mean                   | 0.20353106 |
| stats_o/std                    | 0.08603722 |
| test/episodes                  | 5950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00112   |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0100019 |
| test/Q_plus_P                  | -1.0100019 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 238000     |
| train/episodes                 | 23800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00559   |
| train/info_shaping_reward_mean | -0.0664    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 952000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 595        |
| stats_o/mean                   | 0.20352745 |
| stats_o/std                    | 0.08601287 |
| test/episodes                  | 5960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00246   |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.0418885 |
| test/Q_plus_P                  | -1.0418885 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 238400     |
| train/episodes                 | 23840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00473   |
| train/info_shaping_reward_mean | -0.0675    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 953600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 596        |
| stats_o/mean                   | 0.20353122 |
| stats_o/std                    | 0.08599574 |
| test/episodes                  | 5970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0024    |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -1.0142436 |
| test/Q_plus_P                  | -1.0142436 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 238800     |
| train/episodes                 | 23880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00518   |
| train/info_shaping_reward_mean | -0.0653    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 955200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 597        |
| stats_o/mean                   | 0.2035384  |
| stats_o/std                    | 0.08597063 |
| test/episodes                  | 5980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00571   |
| test/info_shaping_reward_mean  | -0.0431    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -1.0679436 |
| test/Q_plus_P                  | -1.0679436 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 239200     |
| train/episodes                 | 23920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00551   |
| train/info_shaping_reward_mean | -0.0747    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 956800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.20354162  |
| stats_o/std                    | 0.085961625 |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00277    |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.0127205  |
| test/Q_plus_P                  | -1.0127205  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00467    |
| train/info_shaping_reward_mean | -0.0672     |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 599         |
| stats_o/mean                   | 0.20354617  |
| stats_o/std                    | 0.085924394 |
| test/episodes                  | 6000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00322    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.186358   |
| test/Q_plus_P                  | -1.186358   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 240000      |
| train/episodes                 | 24000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00518    |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 960000      |
------------------------------------------------
Saving latest policy.
