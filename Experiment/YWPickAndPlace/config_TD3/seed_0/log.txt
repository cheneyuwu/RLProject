Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPickAndPlace/config_TD3/seed_0
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.20377178  |
| stats_o/std                    | 0.046019047 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.4206537  |
| test/Q_plus_P                  | -1.4206537  |
| test/reward_per_eps            | -40         |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.20437226  |
| stats_o/std                    | 0.042390853 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.8097365  |
| test/Q_plus_P                  | -1.8097365  |
| test/reward_per_eps            | -40         |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 2          |
| stats_o/mean                   | 0.20433666 |
| stats_o/std                    | 0.04144886 |
| test/episodes                  | 30         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.1967983 |
| test/Q_plus_P                  | -2.1967983 |
| test/reward_per_eps            | -40        |
| test/steps                     | 1200       |
| train/episodes                 | 120        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 4800       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 3           |
| stats_o/mean                   | 0.20439176  |
| stats_o/std                    | 0.041187406 |
| test/episodes                  | 40          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.537424   |
| test/Q_plus_P                  | -2.537424   |
| test/reward_per_eps            | -40         |
| test/steps                     | 1600        |
| train/episodes                 | 160         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 6400        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 4           |
| stats_o/mean                   | 0.20425226  |
| stats_o/std                    | 0.040972605 |
| test/episodes                  | 50          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.9434214  |
| test/Q_plus_P                  | -2.9434214  |
| test/reward_per_eps            | -40         |
| test/steps                     | 2000        |
| train/episodes                 | 200         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 8000        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 5           |
| stats_o/mean                   | 0.20356661  |
| stats_o/std                    | 0.040672183 |
| test/episodes                  | 60          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -3.3496435  |
| test/Q_plus_P                  | -3.3496435  |
| test/reward_per_eps            | -40         |
| test/steps                     | 2400        |
| train/episodes                 | 240         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 9600        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 6          |
| stats_o/mean                   | 0.20372924 |
| stats_o/std                    | 0.04048414 |
| test/episodes                  | 70         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.733562  |
| test/Q_plus_P                  | -3.733562  |
| test/reward_per_eps            | -40        |
| test/steps                     | 2800       |
| train/episodes                 | 280        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 11200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 7           |
| stats_o/mean                   | 0.2035646   |
| stats_o/std                    | 0.040283754 |
| test/episodes                  | 80          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -4.1169777  |
| test/Q_plus_P                  | -4.1169777  |
| test/reward_per_eps            | -40         |
| test/steps                     | 3200        |
| train/episodes                 | 320         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 12800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 8           |
| stats_o/mean                   | 0.20319296  |
| stats_o/std                    | 0.040411044 |
| test/episodes                  | 90          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -4.493163   |
| test/Q_plus_P                  | -4.493163   |
| test/reward_per_eps            | -40         |
| test/steps                     | 3600        |
| train/episodes                 | 360         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 14400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 9           |
| stats_o/mean                   | 0.20312214  |
| stats_o/std                    | 0.040361136 |
| test/episodes                  | 100         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -4.8337216  |
| test/Q_plus_P                  | -4.8337216  |
| test/reward_per_eps            | -40         |
| test/steps                     | 4000        |
| train/episodes                 | 400         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 16000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 10          |
| stats_o/mean                   | 0.203034    |
| stats_o/std                    | 0.040424928 |
| test/episodes                  | 110         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -5.196513   |
| test/Q_plus_P                  | -5.196513   |
| test/reward_per_eps            | -40         |
| test/steps                     | 4400        |
| train/episodes                 | 440         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 17600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 11          |
| stats_o/mean                   | 0.20303604  |
| stats_o/std                    | 0.040502492 |
| test/episodes                  | 120         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -5.5287895  |
| test/Q_plus_P                  | -5.5287895  |
| test/reward_per_eps            | -40         |
| test/steps                     | 4800        |
| train/episodes                 | 480         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 19200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.20295633  |
| stats_o/std                    | 0.040553167 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -6.0018325  |
| test/Q_plus_P                  | -6.0018325  |
| test/reward_per_eps            | -40         |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 13          |
| stats_o/mean                   | 0.20301905  |
| stats_o/std                    | 0.040709473 |
| test/episodes                  | 140         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -6.305216   |
| test/Q_plus_P                  | -6.305216   |
| test/reward_per_eps            | -40         |
| test/steps                     | 5600        |
| train/episodes                 | 560         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 22400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 14         |
| stats_o/mean                   | 0.20299774 |
| stats_o/std                    | 0.04077791 |
| test/episodes                  | 150        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -6.703146  |
| test/Q_plus_P                  | -6.703146  |
| test/reward_per_eps            | -40        |
| test/steps                     | 6000       |
| train/episodes                 | 600        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 24000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 15          |
| stats_o/mean                   | 0.20317546  |
| stats_o/std                    | 0.040884465 |
| test/episodes                  | 160         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -7.008993   |
| test/Q_plus_P                  | -7.008993   |
| test/reward_per_eps            | -40         |
| test/steps                     | 6400        |
| train/episodes                 | 640         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 25600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 16          |
| stats_o/mean                   | 0.20315778  |
| stats_o/std                    | 0.040933646 |
| test/episodes                  | 170         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -7.371741   |
| test/Q_plus_P                  | -7.371741   |
| test/reward_per_eps            | -40         |
| test/steps                     | 6800        |
| train/episodes                 | 680         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 27200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 17         |
| stats_o/mean                   | 0.20335084 |
| stats_o/std                    | 0.04107996 |
| test/episodes                  | 180        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -7.7985644 |
| test/Q_plus_P                  | -7.7985644 |
| test/reward_per_eps            | -40        |
| test/steps                     | 7200       |
| train/episodes                 | 720        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 28800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 18          |
| stats_o/mean                   | 0.20336962  |
| stats_o/std                    | 0.041183017 |
| test/episodes                  | 190         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -8.084084   |
| test/Q_plus_P                  | -8.084084   |
| test/reward_per_eps            | -40         |
| test/steps                     | 7600        |
| train/episodes                 | 760         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 30400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 19          |
| stats_o/mean                   | 0.20352775  |
| stats_o/std                    | 0.041266505 |
| test/episodes                  | 200         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -8.544714   |
| test/Q_plus_P                  | -8.544714   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8000        |
| train/episodes                 | 800         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 32000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 20          |
| stats_o/mean                   | 0.2036092   |
| stats_o/std                    | 0.041349545 |
| test/episodes                  | 210         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -8.884729   |
| test/Q_plus_P                  | -8.884729   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8400        |
| train/episodes                 | 840         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 33600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 21          |
| stats_o/mean                   | 0.20344658  |
| stats_o/std                    | 0.041339293 |
| test/episodes                  | 220         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -9.162119   |
| test/Q_plus_P                  | -9.162119   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8800        |
| train/episodes                 | 880         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 35200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 22          |
| stats_o/mean                   | 0.20341791  |
| stats_o/std                    | 0.041434098 |
| test/episodes                  | 230         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -9.504826   |
| test/Q_plus_P                  | -9.504826   |
| test/reward_per_eps            | -40         |
| test/steps                     | 9200        |
| train/episodes                 | 920         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 36800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 23         |
| stats_o/mean                   | 0.20355359 |
| stats_o/std                    | 0.04146508 |
| test/episodes                  | 240        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.846579  |
| test/Q_plus_P                  | -9.846579  |
| test/reward_per_eps            | -40        |
| test/steps                     | 9600       |
| train/episodes                 | 960        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 38400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.20349666  |
| stats_o/std                    | 0.041509878 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -10.27032   |
| test/Q_plus_P                  | -10.27032   |
| test/reward_per_eps            | -40         |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 25          |
| stats_o/mean                   | 0.20349483  |
| stats_o/std                    | 0.041575298 |
| test/episodes                  | 260         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -10.634265  |
| test/Q_plus_P                  | -10.634265  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10400       |
| train/episodes                 | 1040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 41600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 26          |
| stats_o/mean                   | 0.20354514  |
| stats_o/std                    | 0.041558113 |
| test/episodes                  | 270         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -10.996329  |
| test/Q_plus_P                  | -10.996329  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10800       |
| train/episodes                 | 1080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 43200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 27          |
| stats_o/mean                   | 0.2035323   |
| stats_o/std                    | 0.041581966 |
| test/episodes                  | 280         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -11.252926  |
| test/Q_plus_P                  | -11.252926  |
| test/reward_per_eps            | -40         |
| test/steps                     | 11200       |
| train/episodes                 | 1120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 44800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.20355342 |
| stats_o/std                    | 0.04162155 |
| test/episodes                  | 290        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -11.572987 |
| test/Q_plus_P                  | -11.572987 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 29          |
| stats_o/mean                   | 0.20353962  |
| stats_o/std                    | 0.041664828 |
| test/episodes                  | 300         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -11.937425  |
| test/Q_plus_P                  | -11.937425  |
| test/reward_per_eps            | -40         |
| test/steps                     | 12000       |
| train/episodes                 | 1200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 48000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 30          |
| stats_o/mean                   | 0.203577    |
| stats_o/std                    | 0.041697167 |
| test/episodes                  | 310         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -12.238562  |
| test/Q_plus_P                  | -12.238562  |
| test/reward_per_eps            | -40         |
| test/steps                     | 12400       |
| train/episodes                 | 1240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 49600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 31          |
| stats_o/mean                   | 0.20355782  |
| stats_o/std                    | 0.041684557 |
| test/episodes                  | 320         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -12.546804  |
| test/Q_plus_P                  | -12.546804  |
| test/reward_per_eps            | -40         |
| test/steps                     | 12800       |
| train/episodes                 | 1280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 51200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 32         |
| stats_o/mean                   | 0.20362052 |
| stats_o/std                    | 0.04167138 |
| test/episodes                  | 330        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -12.970962 |
| test/Q_plus_P                  | -12.970962 |
| test/reward_per_eps            | -40        |
| test/steps                     | 13200      |
| train/episodes                 | 1320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 52800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 33         |
| stats_o/mean                   | 0.20372446 |
| stats_o/std                    | 0.04164453 |
| test/episodes                  | 340        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -13.275914 |
| test/Q_plus_P                  | -13.275914 |
| test/reward_per_eps            | -40        |
| test/steps                     | 13600      |
| train/episodes                 | 1360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 54400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 34          |
| stats_o/mean                   | 0.20370217  |
| stats_o/std                    | 0.042728305 |
| test/episodes                  | 350         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -13.595299  |
| test/Q_plus_P                  | -13.595299  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14000       |
| train/episodes                 | 1400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 56000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 35          |
| stats_o/mean                   | 0.20381184  |
| stats_o/std                    | 0.042697646 |
| test/episodes                  | 360         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -14.59215   |
| test/Q_plus_P                  | -14.59215   |
| test/reward_per_eps            | -40         |
| test/steps                     | 14400       |
| train/episodes                 | 1440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 57600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 36          |
| stats_o/mean                   | 0.2038553   |
| stats_o/std                    | 0.042631503 |
| test/episodes                  | 370         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -14.147129  |
| test/Q_plus_P                  | -14.147129  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14800       |
| train/episodes                 | 1480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 59200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 37         |
| stats_o/mean                   | 0.20388582 |
| stats_o/std                    | 0.04295588 |
| test/episodes                  | 380        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -14.48621  |
| test/Q_plus_P                  | -14.48621  |
| test/reward_per_eps            | -40        |
| test/steps                     | 15200      |
| train/episodes                 | 1520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 60800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 38          |
| stats_o/mean                   | 0.20393868  |
| stats_o/std                    | 0.042911034 |
| test/episodes                  | 390         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -14.739364  |
| test/Q_plus_P                  | -14.739364  |
| test/reward_per_eps            | -40         |
| test/steps                     | 15600       |
| train/episodes                 | 1560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 62400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 39          |
| stats_o/mean                   | 0.20394021  |
| stats_o/std                    | 0.042896148 |
| test/episodes                  | 400         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -15.137673  |
| test/Q_plus_P                  | -15.137673  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16000       |
| train/episodes                 | 1600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 64000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.20387883  |
| stats_o/std                    | 0.042898092 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -15.158843  |
| test/Q_plus_P                  | -15.158843  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.20388237  |
| stats_o/std                    | 0.042910844 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -15.674699  |
| test/Q_plus_P                  | -15.674699  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 42          |
| stats_o/mean                   | 0.20384401  |
| stats_o/std                    | 0.042974167 |
| test/episodes                  | 430         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -15.939235  |
| test/Q_plus_P                  | -15.939235  |
| test/reward_per_eps            | -40         |
| test/steps                     | 17200       |
| train/episodes                 | 1720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 68800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 43          |
| stats_o/mean                   | 0.20391153  |
| stats_o/std                    | 0.042960018 |
| test/episodes                  | 440         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -16.289705  |
| test/Q_plus_P                  | -16.289705  |
| test/reward_per_eps            | -40         |
| test/steps                     | 17600       |
| train/episodes                 | 1760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 70400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 44         |
| stats_o/mean                   | 0.20397589 |
| stats_o/std                    | 0.04343441 |
| test/episodes                  | 450        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -16.366936 |
| test/Q_plus_P                  | -16.366936 |
| test/reward_per_eps            | -40        |
| test/steps                     | 18000      |
| train/episodes                 | 1800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 72000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.20402761  |
| stats_o/std                    | 0.043989845 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -16.67646   |
| test/Q_plus_P                  | -16.67646   |
| test/reward_per_eps            | -40         |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.183      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 46          |
| stats_o/mean                   | 0.2040641   |
| stats_o/std                    | 0.043982558 |
| test/episodes                  | 470         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -17.112364  |
| test/Q_plus_P                  | -17.112364  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18800       |
| train/episodes                 | 1880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 75200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 47         |
| stats_o/mean                   | 0.20400295 |
| stats_o/std                    | 0.04434865 |
| test/episodes                  | 480        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -17.337543 |
| test/Q_plus_P                  | -17.337543 |
| test/reward_per_eps            | -40        |
| test/steps                     | 19200      |
| train/episodes                 | 1920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 76800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 48         |
| stats_o/mean                   | 0.20397753 |
| stats_o/std                    | 0.04432051 |
| test/episodes                  | 490        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -17.58751  |
| test/Q_plus_P                  | -17.58751  |
| test/reward_per_eps            | -40        |
| test/steps                     | 19600      |
| train/episodes                 | 1960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 78400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 49          |
| stats_o/mean                   | 0.20400277  |
| stats_o/std                    | 0.045900032 |
| test/episodes                  | 500         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -17.713564  |
| test/Q_plus_P                  | -17.713564  |
| test/reward_per_eps            | -40         |
| test/steps                     | 20000       |
| train/episodes                 | 2000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 80000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 50          |
| stats_o/mean                   | 0.20402735  |
| stats_o/std                    | 0.045843042 |
| test/episodes                  | 510         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -18.0568    |
| test/Q_plus_P                  | -18.0568    |
| test/reward_per_eps            | -40         |
| test/steps                     | 20400       |
| train/episodes                 | 2040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 81600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.20398918 |
| stats_o/std                    | 0.04577174 |
| test/episodes                  | 520        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -18.31542  |
| test/Q_plus_P                  | -18.31542  |
| test/reward_per_eps            | -40        |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.20404753 |
| stats_o/std                    | 0.04573281 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -18.537397 |
| test/Q_plus_P                  | -18.537397 |
| test/reward_per_eps            | -40        |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.20410168  |
| stats_o/std                    | 0.045678157 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -19.067425  |
| test/Q_plus_P                  | -19.067425  |
| test/reward_per_eps            | -40         |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 54          |
| stats_o/mean                   | 0.2040686   |
| stats_o/std                    | 0.046309136 |
| test/episodes                  | 550         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.025297  |
| test/Q_plus_P                  | -19.025297  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22000       |
| train/episodes                 | 2200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 88000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 55          |
| stats_o/mean                   | 0.2040794   |
| stats_o/std                    | 0.046246346 |
| test/episodes                  | 560         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.377243  |
| test/Q_plus_P                  | -19.377243  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22400       |
| train/episodes                 | 2240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 89600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 56          |
| stats_o/mean                   | 0.20412084  |
| stats_o/std                    | 0.047230773 |
| test/episodes                  | 570         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.115      |
| test/info_shaping_reward_mean  | -0.17       |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.678988  |
| test/Q_plus_P                  | -19.678988  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22800       |
| train/episodes                 | 2280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 91200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 57          |
| stats_o/mean                   | 0.20416532  |
| stats_o/std                    | 0.047865964 |
| test/episodes                  | 580         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -20.041454  |
| test/Q_plus_P                  | -20.041454  |
| test/reward_per_eps            | -40         |
| test/steps                     | 23200       |
| train/episodes                 | 2320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.181      |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 92800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.20418137  |
| stats_o/std                    | 0.048984602 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -20.28083   |
| test/Q_plus_P                  | -20.28083   |
| test/reward_per_eps            | -40         |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 59          |
| stats_o/mean                   | 0.20424752  |
| stats_o/std                    | 0.048925497 |
| test/episodes                  | 600         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -20.320398  |
| test/Q_plus_P                  | -20.320398  |
| test/reward_per_eps            | -40         |
| test/steps                     | 24000       |
| train/episodes                 | 2400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.177      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 96000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 60         |
| stats_o/mean                   | 0.20424852 |
| stats_o/std                    | 0.04940748 |
| test/episodes                  | 610        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.561193 |
| test/Q_plus_P                  | -20.561193 |
| test/reward_per_eps            | -40        |
| test/steps                     | 24400      |
| train/episodes                 | 2440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 97600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 61          |
| stats_o/mean                   | 0.2042321   |
| stats_o/std                    | 0.049361505 |
| test/episodes                  | 620         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -20.651339  |
| test/Q_plus_P                  | -20.651339  |
| test/reward_per_eps            | -40         |
| test/steps                     | 24800       |
| train/episodes                 | 2480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 99200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 62         |
| stats_o/mean                   | 0.20422737 |
| stats_o/std                    | 0.04945323 |
| test/episodes                  | 630        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -20.628155 |
| test/Q_plus_P                  | -20.628155 |
| test/reward_per_eps            | -40        |
| test/steps                     | 25200      |
| train/episodes                 | 2520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 100800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 63         |
| stats_o/mean                   | 0.20420748 |
| stats_o/std                    | 0.04981322 |
| test/episodes                  | 640        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.208     |
| test/Q                         | -20.729242 |
| test/Q_plus_P                  | -20.729242 |
| test/reward_per_eps            | -40        |
| test/steps                     | 25600      |
| train/episodes                 | 2560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 102400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 64          |
| stats_o/mean                   | 0.20419319  |
| stats_o/std                    | 0.050509688 |
| test/episodes                  | 650         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.177      |
| test/info_shaping_reward_min   | -0.501      |
| test/Q                         | -20.207893  |
| test/Q_plus_P                  | -20.207893  |
| test/reward_per_eps            | -40         |
| test/steps                     | 26000       |
| train/episodes                 | 2600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 104000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 65          |
| stats_o/mean                   | 0.20415553  |
| stats_o/std                    | 0.050975762 |
| test/episodes                  | 660         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -20.06027   |
| test/Q_plus_P                  | -20.06027   |
| test/reward_per_eps            | -40         |
| test/steps                     | 26400       |
| train/episodes                 | 2640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.206      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 105600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 66         |
| stats_o/mean                   | 0.20425628 |
| stats_o/std                    | 0.05174422 |
| test/episodes                  | 670        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.671894 |
| test/Q_plus_P                  | -20.671894 |
| test/reward_per_eps            | -40        |
| test/steps                     | 26800      |
| train/episodes                 | 2680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 107200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 67         |
| stats_o/mean                   | 0.20426539 |
| stats_o/std                    | 0.05215684 |
| test/episodes                  | 680        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -19.935152 |
| test/Q_plus_P                  | -19.935152 |
| test/reward_per_eps            | -40        |
| test/steps                     | 27200      |
| train/episodes                 | 2720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 108800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 68         |
| stats_o/mean                   | 0.20429508 |
| stats_o/std                    | 0.05283109 |
| test/episodes                  | 690        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.222     |
| test/Q                         | -19.147253 |
| test/Q_plus_P                  | -19.147253 |
| test/reward_per_eps            | -40        |
| test/steps                     | 27600      |
| train/episodes                 | 2760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 110400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 69          |
| stats_o/mean                   | 0.2042904   |
| stats_o/std                    | 0.053187303 |
| test/episodes                  | 700         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -19.600725  |
| test/Q_plus_P                  | -19.600725  |
| test/reward_per_eps            | -40         |
| test/steps                     | 28000       |
| train/episodes                 | 2800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 112000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.2042807  |
| stats_o/std                    | 0.05356003 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.18      |
| test/info_shaping_reward_min   | -0.499     |
| test/Q                         | -20.22373  |
| test/Q_plus_P                  | -20.22373  |
| test/reward_per_eps            | -40        |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 71          |
| stats_o/mean                   | 0.20426248  |
| stats_o/std                    | 0.053818002 |
| test/episodes                  | 720         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.206      |
| test/Q                         | -20.784378  |
| test/Q_plus_P                  | -20.784378  |
| test/reward_per_eps            | -40         |
| test/steps                     | 28800       |
| train/episodes                 | 2880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.203      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 115200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 72          |
| stats_o/mean                   | 0.20427835  |
| stats_o/std                    | 0.055088606 |
| test/episodes                  | 730         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.109      |
| test/info_shaping_reward_mean  | -0.17       |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -20.135359  |
| test/Q_plus_P                  | -20.135359  |
| test/reward_per_eps            | -40         |
| test/steps                     | 29200       |
| train/episodes                 | 2920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.183      |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 116800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.20427759 |
| stats_o/std                    | 0.05537961 |
| test/episodes                  | 740        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.206     |
| test/Q                         | -20.26241  |
| test/Q_plus_P                  | -20.26241  |
| test/reward_per_eps            | -40        |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 74         |
| stats_o/mean                   | 0.20428528 |
| stats_o/std                    | 0.05766491 |
| test/episodes                  | 750        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -19.249907 |
| test/Q_plus_P                  | -19.249907 |
| test/reward_per_eps            | -40        |
| test/steps                     | 30000      |
| train/episodes                 | 3000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.203     |
| train/info_shaping_reward_min  | -0.399     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 120000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 75          |
| stats_o/mean                   | 0.20431891  |
| stats_o/std                    | 0.058514796 |
| test/episodes                  | 760         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.193      |
| test/info_shaping_reward_min   | -0.81       |
| test/Q                         | -20.858574  |
| test/Q_plus_P                  | -20.858574  |
| test/reward_per_eps            | -40         |
| test/steps                     | 30400       |
| train/episodes                 | 3040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.189      |
| train/info_shaping_reward_min  | -0.276      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 121600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 76          |
| stats_o/mean                   | 0.2042938   |
| stats_o/std                    | 0.059175305 |
| test/episodes                  | 770         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -20.50658   |
| test/Q_plus_P                  | -20.50658   |
| test/reward_per_eps            | -40         |
| test/steps                     | 30800       |
| train/episodes                 | 3080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.184      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 123200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 77          |
| stats_o/mean                   | 0.20435041  |
| stats_o/std                    | 0.059691712 |
| test/episodes                  | 780         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.016554  |
| test/Q_plus_P                  | -19.016554  |
| test/reward_per_eps            | -40         |
| test/steps                     | 31200       |
| train/episodes                 | 3120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 124800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 78          |
| stats_o/mean                   | 0.20435546  |
| stats_o/std                    | 0.059759583 |
| test/episodes                  | 790         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -20.33554   |
| test/Q_plus_P                  | -20.33554   |
| test/reward_per_eps            | -40         |
| test/steps                     | 31600       |
| train/episodes                 | 3160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 126400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 79          |
| stats_o/mean                   | 0.20439088  |
| stats_o/std                    | 0.059872918 |
| test/episodes                  | 800         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.55503   |
| test/Q_plus_P                  | -19.55503   |
| test/reward_per_eps            | -40         |
| test/steps                     | 32000       |
| train/episodes                 | 3200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 128000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.20439933  |
| stats_o/std                    | 0.059907362 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.790565  |
| test/Q_plus_P                  | -19.790565  |
| test/reward_per_eps            | -40         |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 81          |
| stats_o/mean                   | 0.2043851   |
| stats_o/std                    | 0.060335506 |
| test/episodes                  | 820         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -20.862339  |
| test/Q_plus_P                  | -20.862339  |
| test/reward_per_eps            | -40         |
| test/steps                     | 32800       |
| train/episodes                 | 3280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 131200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 82         |
| stats_o/mean                   | 0.20434864 |
| stats_o/std                    | 0.06041212 |
| test/episodes                  | 830        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.287788 |
| test/Q_plus_P                  | -20.287788 |
| test/reward_per_eps            | -40        |
| test/steps                     | 33200      |
| train/episodes                 | 3320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 132800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 83         |
| stats_o/mean                   | 0.20432533 |
| stats_o/std                    | 0.06043318 |
| test/episodes                  | 840        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.358515 |
| test/Q_plus_P                  | -20.358515 |
| test/reward_per_eps            | -40        |
| test/steps                     | 33600      |
| train/episodes                 | 3360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 134400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 84          |
| stats_o/mean                   | 0.20432015  |
| stats_o/std                    | 0.060436934 |
| test/episodes                  | 850         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -20.062721  |
| test/Q_plus_P                  | -20.062721  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34000       |
| train/episodes                 | 3400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 136000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.20432839  |
| stats_o/std                    | 0.060503397 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -19.94465   |
| test/Q_plus_P                  | -19.94465   |
| test/reward_per_eps            | -40         |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 86         |
| stats_o/mean                   | 0.20429212 |
| stats_o/std                    | 0.06057296 |
| test/episodes                  | 870        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.422737 |
| test/Q_plus_P                  | -20.422737 |
| test/reward_per_eps            | -40        |
| test/steps                     | 34800      |
| train/episodes                 | 3480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 139200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 87         |
| stats_o/mean                   | 0.20425305 |
| stats_o/std                    | 0.06061969 |
| test/episodes                  | 880        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -21.506023 |
| test/Q_plus_P                  | -21.506023 |
| test/reward_per_eps            | -40        |
| test/steps                     | 35200      |
| train/episodes                 | 3520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 140800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 88         |
| stats_o/mean                   | 0.20423646 |
| stats_o/std                    | 0.06061622 |
| test/episodes                  | 890        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -21.018343 |
| test/Q_plus_P                  | -21.018343 |
| test/reward_per_eps            | -40        |
| test/steps                     | 35600      |
| train/episodes                 | 3560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 142400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 89          |
| stats_o/mean                   | 0.20423967  |
| stats_o/std                    | 0.060622703 |
| test/episodes                  | 900         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -20.777708  |
| test/Q_plus_P                  | -20.777708  |
| test/reward_per_eps            | -40         |
| test/steps                     | 36000       |
| train/episodes                 | 3600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 144000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 90         |
| stats_o/mean                   | 0.20426913 |
| stats_o/std                    | 0.06058291 |
| test/episodes                  | 910        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.732525 |
| test/Q_plus_P                  | -20.732525 |
| test/reward_per_eps            | -40        |
| test/steps                     | 36400      |
| train/episodes                 | 3640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 145600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 91         |
| stats_o/mean                   | 0.20425032 |
| stats_o/std                    | 0.06064136 |
| test/episodes                  | 920        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.936592 |
| test/Q_plus_P                  | -20.936592 |
| test/reward_per_eps            | -40        |
| test/steps                     | 36800      |
| train/episodes                 | 3680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 147200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 92         |
| stats_o/mean                   | 0.20428596 |
| stats_o/std                    | 0.06065324 |
| test/episodes                  | 930        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -22.74237  |
| test/Q_plus_P                  | -22.74237  |
| test/reward_per_eps            | -40        |
| test/steps                     | 37200      |
| train/episodes                 | 3720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 148800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 93         |
| stats_o/mean                   | 0.20429215 |
| stats_o/std                    | 0.06061384 |
| test/episodes                  | 940        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -21.482006 |
| test/Q_plus_P                  | -21.482006 |
| test/reward_per_eps            | -40        |
| test/steps                     | 37600      |
| train/episodes                 | 3760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 150400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 94          |
| stats_o/mean                   | 0.20426682  |
| stats_o/std                    | 0.060619593 |
| test/episodes                  | 950         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -22.450628  |
| test/Q_plus_P                  | -22.450628  |
| test/reward_per_eps            | -40         |
| test/steps                     | 38000       |
| train/episodes                 | 3800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 152000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 95          |
| stats_o/mean                   | 0.2042743   |
| stats_o/std                    | 0.060585793 |
| test/episodes                  | 960         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -22.66343   |
| test/Q_plus_P                  | -22.66343   |
| test/reward_per_eps            | -40         |
| test/steps                     | 38400       |
| train/episodes                 | 3840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 153600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 96         |
| stats_o/mean                   | 0.20428683 |
| stats_o/std                    | 0.06057823 |
| test/episodes                  | 970        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -22.679453 |
| test/Q_plus_P                  | -22.679453 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38800      |
| train/episodes                 | 3880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 155200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 97         |
| stats_o/mean                   | 0.20427477 |
| stats_o/std                    | 0.06055207 |
| test/episodes                  | 980        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -23.0756   |
| test/Q_plus_P                  | -23.0756   |
| test/reward_per_eps            | -40        |
| test/steps                     | 39200      |
| train/episodes                 | 3920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 156800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 98         |
| stats_o/mean                   | 0.20426361 |
| stats_o/std                    | 0.06055405 |
| test/episodes                  | 990        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -22.779844 |
| test/Q_plus_P                  | -22.779844 |
| test/reward_per_eps            | -40        |
| test/steps                     | 39600      |
| train/episodes                 | 3960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 158400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 99          |
| stats_o/mean                   | 0.20425437  |
| stats_o/std                    | 0.060548794 |
| test/episodes                  | 1000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -23.276121  |
| test/Q_plus_P                  | -23.276121  |
| test/reward_per_eps            | -40         |
| test/steps                     | 40000       |
| train/episodes                 | 4000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 160000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 100        |
| stats_o/mean                   | 0.20425604 |
| stats_o/std                    | 0.06051857 |
| test/episodes                  | 1010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -22.548033 |
| test/Q_plus_P                  | -22.548033 |
| test/reward_per_eps            | -40        |
| test/steps                     | 40400      |
| train/episodes                 | 4040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 161600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.20424552  |
| stats_o/std                    | 0.060502853 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -23.677908  |
| test/Q_plus_P                  | -23.677908  |
| test/reward_per_eps            | -40         |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 102        |
| stats_o/mean                   | 0.20427996 |
| stats_o/std                    | 0.06045396 |
| test/episodes                  | 1030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -23.256973 |
| test/Q_plus_P                  | -23.256973 |
| test/reward_per_eps            | -40        |
| test/steps                     | 41200      |
| train/episodes                 | 4120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.173     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 164800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 103         |
| stats_o/mean                   | 0.204261    |
| stats_o/std                    | 0.060430355 |
| test/episodes                  | 1040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -22.995815  |
| test/Q_plus_P                  | -22.995815  |
| test/reward_per_eps            | -40         |
| test/steps                     | 41600       |
| train/episodes                 | 4160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 166400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 104        |
| stats_o/mean                   | 0.20425944 |
| stats_o/std                    | 0.0605443  |
| test/episodes                  | 1050       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -23.33875  |
| test/Q_plus_P                  | -23.33875  |
| test/reward_per_eps            | -40        |
| test/steps                     | 42000      |
| train/episodes                 | 4200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 168000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 105         |
| stats_o/mean                   | 0.20423332  |
| stats_o/std                    | 0.060549773 |
| test/episodes                  | 1060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0965     |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -24.652037  |
| test/Q_plus_P                  | -24.652037  |
| test/reward_per_eps            | -40         |
| test/steps                     | 42400       |
| train/episodes                 | 4240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 169600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 106         |
| stats_o/mean                   | 0.20424679  |
| stats_o/std                    | 0.060470354 |
| test/episodes                  | 1070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -23.702133  |
| test/Q_plus_P                  | -23.702133  |
| test/reward_per_eps            | -40         |
| test/steps                     | 42800       |
| train/episodes                 | 4280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 171200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 107         |
| stats_o/mean                   | 0.20422018  |
| stats_o/std                    | 0.060441714 |
| test/episodes                  | 1080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -23.950771  |
| test/Q_plus_P                  | -23.950771  |
| test/reward_per_eps            | -40         |
| test/steps                     | 43200       |
| train/episodes                 | 4320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.173      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 172800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 108         |
| stats_o/mean                   | 0.20423816  |
| stats_o/std                    | 0.060437802 |
| test/episodes                  | 1090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -24.073381  |
| test/Q_plus_P                  | -24.073381  |
| test/reward_per_eps            | -40         |
| test/steps                     | 43600       |
| train/episodes                 | 4360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.174      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 174400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 109         |
| stats_o/mean                   | 0.20425606  |
| stats_o/std                    | 0.060431004 |
| test/episodes                  | 1100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -24.277786  |
| test/Q_plus_P                  | -24.277786  |
| test/reward_per_eps            | -40         |
| test/steps                     | 44000       |
| train/episodes                 | 4400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.206      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 176000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 110        |
| stats_o/mean                   | 0.20426285 |
| stats_o/std                    | 0.06046565 |
| test/episodes                  | 1110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -24.584654 |
| test/Q_plus_P                  | -24.584654 |
| test/reward_per_eps            | -40        |
| test/steps                     | 44400      |
| train/episodes                 | 4440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.174     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 177600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 111        |
| stats_o/mean                   | 0.20424426 |
| stats_o/std                    | 0.0604856  |
| test/episodes                  | 1120       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -24.401924 |
| test/Q_plus_P                  | -24.401924 |
| test/reward_per_eps            | -40        |
| test/steps                     | 44800      |
| train/episodes                 | 4480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 179200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 112        |
| stats_o/mean                   | 0.2042049  |
| stats_o/std                    | 0.06056297 |
| test/episodes                  | 1130       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -25.063044 |
| test/Q_plus_P                  | -25.063044 |
| test/reward_per_eps            | -40        |
| test/steps                     | 45200      |
| train/episodes                 | 4520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 180800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 113        |
| stats_o/mean                   | 0.2042379  |
| stats_o/std                    | 0.0610505  |
| test/episodes                  | 1140       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -25.133066 |
| test/Q_plus_P                  | -25.133066 |
| test/reward_per_eps            | -40        |
| test/steps                     | 45600      |
| train/episodes                 | 4560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.267     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 182400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 114         |
| stats_o/mean                   | 0.20419408  |
| stats_o/std                    | 0.061479725 |
| test/episodes                  | 1150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -24.67209   |
| test/Q_plus_P                  | -24.67209   |
| test/reward_per_eps            | -40         |
| test/steps                     | 46000       |
| train/episodes                 | 4600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.181      |
| train/info_shaping_reward_min  | -0.295      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 184000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 115        |
| stats_o/mean                   | 0.20414524 |
| stats_o/std                    | 0.0617618  |
| test/episodes                  | 1160       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -25.865791 |
| test/Q_plus_P                  | -25.865791 |
| test/reward_per_eps            | -40        |
| test/steps                     | 46400      |
| train/episodes                 | 4640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 185600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 116         |
| stats_o/mean                   | 0.20412117  |
| stats_o/std                    | 0.061985794 |
| test/episodes                  | 1170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -25.846136  |
| test/Q_plus_P                  | -25.846136  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46800       |
| train/episodes                 | 4680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.187      |
| train/info_shaping_reward_min  | -0.278      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 187200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 117         |
| stats_o/mean                   | 0.20407404  |
| stats_o/std                    | 0.062339336 |
| test/episodes                  | 1180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.195      |
| test/Q                         | -25.806091  |
| test/Q_plus_P                  | -25.806091  |
| test/reward_per_eps            | -40         |
| test/steps                     | 47200       |
| train/episodes                 | 4720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.18       |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 188800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 118        |
| stats_o/mean                   | 0.20406121 |
| stats_o/std                    | 0.06296119 |
| test/episodes                  | 1190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.129     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -25.649883 |
| test/Q_plus_P                  | -25.649883 |
| test/reward_per_eps            | -40        |
| test/steps                     | 47600      |
| train/episodes                 | 4760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.302     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 190400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 119        |
| stats_o/mean                   | 0.2040762  |
| stats_o/std                    | 0.06380153 |
| test/episodes                  | 1200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.11      |
| test/info_shaping_reward_mean  | -0.195     |
| test/info_shaping_reward_min   | -0.865     |
| test/Q                         | -25.552454 |
| test/Q_plus_P                  | -25.552454 |
| test/reward_per_eps            | -40        |
| test/steps                     | 48000      |
| train/episodes                 | 4800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 192000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 120        |
| stats_o/mean                   | 0.20401749 |
| stats_o/std                    | 0.06397963 |
| test/episodes                  | 1210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.128     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -26.18851  |
| test/Q_plus_P                  | -26.18851  |
| test/reward_per_eps            | -40        |
| test/steps                     | 48400      |
| train/episodes                 | 4840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 193600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.20401998 |
| stats_o/std                    | 0.06391921 |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -26.053787 |
| test/Q_plus_P                  | -26.053787 |
| test/reward_per_eps            | -40        |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 122         |
| stats_o/mean                   | 0.20400183  |
| stats_o/std                    | 0.063909896 |
| test/episodes                  | 1230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -26.150494  |
| test/Q_plus_P                  | -26.150494  |
| test/reward_per_eps            | -40         |
| test/steps                     | 49200       |
| train/episodes                 | 4920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.194      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 196800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 123        |
| stats_o/mean                   | 0.20396155 |
| stats_o/std                    | 0.06419598 |
| test/episodes                  | 1240       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.183     |
| test/info_shaping_reward_min   | -0.372     |
| test/Q                         | -27.129845 |
| test/Q_plus_P                  | -27.129845 |
| test/reward_per_eps            | -40        |
| test/steps                     | 49600      |
| train/episodes                 | 4960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 198400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 124        |
| stats_o/mean                   | 0.2039699  |
| stats_o/std                    | 0.06442319 |
| test/episodes                  | 1250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.122     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -26.616186 |
| test/Q_plus_P                  | -26.616186 |
| test/reward_per_eps            | -40        |
| test/steps                     | 50000      |
| train/episodes                 | 5000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 200000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 125         |
| stats_o/mean                   | 0.20398153  |
| stats_o/std                    | 0.064816155 |
| test/episodes                  | 1260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.195      |
| test/info_shaping_reward_min   | -0.504      |
| test/Q                         | -26.686216  |
| test/Q_plus_P                  | -26.686216  |
| test/reward_per_eps            | -40         |
| test/steps                     | 50400       |
| train/episodes                 | 5040        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00125     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.186      |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 201600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 126         |
| stats_o/mean                   | 0.20398119  |
| stats_o/std                    | 0.064733125 |
| test/episodes                  | 1270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.138      |
| test/info_shaping_reward_mean  | -0.171      |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -26.734045  |
| test/Q_plus_P                  | -26.734045  |
| test/reward_per_eps            | -40         |
| test/steps                     | 50800       |
| train/episodes                 | 5080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 203200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 127        |
| stats_o/mean                   | 0.20395672 |
| stats_o/std                    | 0.06505899 |
| test/episodes                  | 1280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -26.54101  |
| test/Q_plus_P                  | -26.54101  |
| test/reward_per_eps            | -40        |
| test/steps                     | 51200      |
| train/episodes                 | 5120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.191     |
| train/info_shaping_reward_min  | -0.298     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 204800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.20395789 |
| stats_o/std                    | 0.06531588 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.198     |
| test/info_shaping_reward_min   | -0.508     |
| test/Q                         | -27.649635 |
| test/Q_plus_P                  | -27.649635 |
| test/reward_per_eps            | -40        |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 129        |
| stats_o/mean                   | 0.20392682 |
| stats_o/std                    | 0.06557939 |
| test/episodes                  | 1300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0702    |
| test/info_shaping_reward_mean  | -0.186     |
| test/info_shaping_reward_min   | -0.579     |
| test/Q                         | -27.504694 |
| test/Q_plus_P                  | -27.504694 |
| test/reward_per_eps            | -40        |
| test/steps                     | 52000      |
| train/episodes                 | 5200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 208000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 130        |
| stats_o/mean                   | 0.20391735 |
| stats_o/std                    | 0.06553599 |
| test/episodes                  | 1310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -27.424719 |
| test/Q_plus_P                  | -27.424719 |
| test/reward_per_eps            | -40        |
| test/steps                     | 52400      |
| train/episodes                 | 5240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 209600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 131        |
| stats_o/mean                   | 0.20386893 |
| stats_o/std                    | 0.06609173 |
| test/episodes                  | 1320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -27.444893 |
| test/Q_plus_P                  | -27.444893 |
| test/reward_per_eps            | -40        |
| test/steps                     | 52800      |
| train/episodes                 | 5280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 211200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 132        |
| stats_o/mean                   | 0.20389171 |
| stats_o/std                    | 0.06614171 |
| test/episodes                  | 1330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -27.749836 |
| test/Q_plus_P                  | -27.749836 |
| test/reward_per_eps            | -40        |
| test/steps                     | 53200      |
| train/episodes                 | 5320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 212800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 133        |
| stats_o/mean                   | 0.20389694 |
| stats_o/std                    | 0.06628871 |
| test/episodes                  | 1340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.179     |
| test/info_shaping_reward_min   | -0.332     |
| test/Q                         | -28.560453 |
| test/Q_plus_P                  | -28.560453 |
| test/reward_per_eps            | -40        |
| test/steps                     | 53600      |
| train/episodes                 | 5360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 214400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 134        |
| stats_o/mean                   | 0.20389406 |
| stats_o/std                    | 0.06649826 |
| test/episodes                  | 1350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -27.961147 |
| test/Q_plus_P                  | -27.961147 |
| test/reward_per_eps            | -40        |
| test/steps                     | 54000      |
| train/episodes                 | 5400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 216000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 135         |
| stats_o/mean                   | 0.20388111  |
| stats_o/std                    | 0.066581815 |
| test/episodes                  | 1360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0848     |
| test/info_shaping_reward_mean  | -0.172      |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -27.940025  |
| test/Q_plus_P                  | -27.940025  |
| test/reward_per_eps            | -40         |
| test/steps                     | 54400       |
| train/episodes                 | 5440        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0144      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 217600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 136         |
| stats_o/mean                   | 0.20385683  |
| stats_o/std                    | 0.066807106 |
| test/episodes                  | 1370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0505     |
| test/info_shaping_reward_mean  | -0.165      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -27.725645  |
| test/Q_plus_P                  | -27.725645  |
| test/reward_per_eps            | -40         |
| test/steps                     | 54800       |
| train/episodes                 | 5480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.186      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 219200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 137         |
| stats_o/mean                   | 0.20386602  |
| stats_o/std                    | 0.067085326 |
| test/episodes                  | 1380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.02        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.025      |
| test/info_shaping_reward_mean  | -0.165      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -27.880028  |
| test/Q_plus_P                  | -27.880028  |
| test/reward_per_eps            | -39.2       |
| test/steps                     | 55200       |
| train/episodes                 | 5520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.192      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 220800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.2038536  |
| stats_o/std                    | 0.06707508 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.103     |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -28.352217 |
| test/Q_plus_P                  | -28.352217 |
| test/reward_per_eps            | -40        |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.20387495 |
| stats_o/std                    | 0.06714972 |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.116     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.536     |
| test/Q                         | -28.472633 |
| test/Q_plus_P                  | -28.472633 |
| test/reward_per_eps            | -40        |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 140         |
| stats_o/mean                   | 0.20384882  |
| stats_o/std                    | 0.067341305 |
| test/episodes                  | 1410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -28.486404  |
| test/Q_plus_P                  | -28.486404  |
| test/reward_per_eps            | -40         |
| test/steps                     | 56400       |
| train/episodes                 | 5640        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00125     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 225600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.2038497  |
| stats_o/std                    | 0.06745049 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.189     |
| test/info_shaping_reward_min   | -0.505     |
| test/Q                         | -29.3596   |
| test/Q_plus_P                  | -29.3596   |
| test/reward_per_eps            | -40        |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.285     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 142        |
| stats_o/mean                   | 0.20385177 |
| stats_o/std                    | 0.06773594 |
| test/episodes                  | 1430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.129     |
| test/info_shaping_reward_mean  | -0.187     |
| test/info_shaping_reward_min   | -0.526     |
| test/Q                         | -29.232397 |
| test/Q_plus_P                  | -29.232397 |
| test/reward_per_eps            | -40        |
| test/steps                     | 57200      |
| train/episodes                 | 5720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 228800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 143        |
| stats_o/mean                   | 0.20381308 |
| stats_o/std                    | 0.06777241 |
| test/episodes                  | 1440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0843    |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -28.437475 |
| test/Q_plus_P                  | -28.437475 |
| test/reward_per_eps            | -40        |
| test/steps                     | 57600      |
| train/episodes                 | 5760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 230400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 144        |
| stats_o/mean                   | 0.20382625 |
| stats_o/std                    | 0.0680124  |
| test/episodes                  | 1450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -28.979542 |
| test/Q_plus_P                  | -28.979542 |
| test/reward_per_eps            | -40        |
| test/steps                     | 58000      |
| train/episodes                 | 5800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 232000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 145        |
| stats_o/mean                   | 0.20382188 |
| stats_o/std                    | 0.06805161 |
| test/episodes                  | 1460       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.11      |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -29.26518  |
| test/Q_plus_P                  | -29.26518  |
| test/reward_per_eps            | -40        |
| test/steps                     | 58400      |
| train/episodes                 | 5840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 233600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 146         |
| stats_o/mean                   | 0.20382093  |
| stats_o/std                    | 0.068336844 |
| test/episodes                  | 1470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -29.384361  |
| test/Q_plus_P                  | -29.384361  |
| test/reward_per_eps            | -40         |
| test/steps                     | 58800       |
| train/episodes                 | 5880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.19       |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 235200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 147        |
| stats_o/mean                   | 0.20382914 |
| stats_o/std                    | 0.068515   |
| test/episodes                  | 1480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0846    |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -28.926659 |
| test/Q_plus_P                  | -28.926659 |
| test/reward_per_eps            | -40        |
| test/steps                     | 59200      |
| train/episodes                 | 5920       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 236800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 148        |
| stats_o/mean                   | 0.20384681 |
| stats_o/std                    | 0.0688125  |
| test/episodes                  | 1490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0751    |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -29.365206 |
| test/Q_plus_P                  | -29.365206 |
| test/reward_per_eps            | -40        |
| test/steps                     | 59600      |
| train/episodes                 | 5960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 238400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 149        |
| stats_o/mean                   | 0.20383732 |
| stats_o/std                    | 0.06886757 |
| test/episodes                  | 1500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.147     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -29.537285 |
| test/Q_plus_P                  | -29.537285 |
| test/reward_per_eps            | -40        |
| test/steps                     | 60000      |
| train/episodes                 | 6000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 240000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 150        |
| stats_o/mean                   | 0.20381857 |
| stats_o/std                    | 0.06889685 |
| test/episodes                  | 1510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -29.590801 |
| test/Q_plus_P                  | -29.590801 |
| test/reward_per_eps            | -40        |
| test/steps                     | 60400      |
| train/episodes                 | 6040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 241600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 151        |
| stats_o/mean                   | 0.20381138 |
| stats_o/std                    | 0.06884574 |
| test/episodes                  | 1520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -29.834936 |
| test/Q_plus_P                  | -29.834936 |
| test/reward_per_eps            | -40        |
| test/steps                     | 60800      |
| train/episodes                 | 6080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 243200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 152        |
| stats_o/mean                   | 0.20378186 |
| stats_o/std                    | 0.06898389 |
| test/episodes                  | 1530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.214     |
| test/Q                         | -29.775995 |
| test/Q_plus_P                  | -29.775995 |
| test/reward_per_eps            | -40        |
| test/steps                     | 61200      |
| train/episodes                 | 6120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 244800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 153        |
| stats_o/mean                   | 0.20380273 |
| stats_o/std                    | 0.06902531 |
| test/episodes                  | 1540       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -30.037004 |
| test/Q_plus_P                  | -30.037004 |
| test/reward_per_eps            | -40        |
| test/steps                     | 61600      |
| train/episodes                 | 6160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 246400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.20385042 |
| stats_o/std                    | 0.06921512 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -30.188896 |
| test/Q_plus_P                  | -30.188896 |
| test/reward_per_eps            | -40        |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.20387772 |
| stats_o/std                    | 0.06946247 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0075     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0269    |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.349     |
| test/Q                         | -30.263262 |
| test/Q_plus_P                  | -30.263262 |
| test/reward_per_eps            | -39.7      |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 156        |
| stats_o/mean                   | 0.20384304 |
| stats_o/std                    | 0.06955536 |
| test/episodes                  | 1570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -30.221235 |
| test/Q_plus_P                  | -30.221235 |
| test/reward_per_eps            | -40        |
| test/steps                     | 62800      |
| train/episodes                 | 6280       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00813    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 251200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 157        |
| stats_o/mean                   | 0.20386232 |
| stats_o/std                    | 0.06974699 |
| test/episodes                  | 1580       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -30.352837 |
| test/Q_plus_P                  | -30.352837 |
| test/reward_per_eps            | -40        |
| test/steps                     | 63200      |
| train/episodes                 | 6320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 252800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.20389423  |
| stats_o/std                    | 0.069917515 |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.174      |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -30.741915  |
| test/Q_plus_P                  | -30.741915  |
| test/reward_per_eps            | -40         |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0106      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.115      |
| train/info_shaping_reward_mean | -0.17       |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 159        |
| stats_o/mean                   | 0.20391211 |
| stats_o/std                    | 0.07004318 |
| test/episodes                  | 1600       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -30.791515 |
| test/Q_plus_P                  | -30.791515 |
| test/reward_per_eps            | -40        |
| test/steps                     | 64000      |
| train/episodes                 | 6400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 256000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 160        |
| stats_o/mean                   | 0.20394242 |
| stats_o/std                    | 0.07103056 |
| test/episodes                  | 1610       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.141     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -30.807749 |
| test/Q_plus_P                  | -30.807749 |
| test/reward_per_eps            | -40        |
| test/steps                     | 64400      |
| train/episodes                 | 6440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.216     |
| train/info_shaping_reward_min  | -0.488     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 257600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 161        |
| stats_o/mean                   | 0.20392007 |
| stats_o/std                    | 0.07117867 |
| test/episodes                  | 1620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.138     |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -30.952692 |
| test/Q_plus_P                  | -30.952692 |
| test/reward_per_eps            | -40        |
| test/steps                     | 64800      |
| train/episodes                 | 6480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 259200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 162        |
| stats_o/mean                   | 0.20394787 |
| stats_o/std                    | 0.07179808 |
| test/episodes                  | 1630       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.093     |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -30.32579  |
| test/Q_plus_P                  | -30.32579  |
| test/reward_per_eps            | -40        |
| test/steps                     | 65200      |
| train/episodes                 | 6520       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00375    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 260800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 163        |
| stats_o/mean                   | 0.20390515 |
| stats_o/std                    | 0.07197151 |
| test/episodes                  | 1640       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -31.271015 |
| test/Q_plus_P                  | -31.271015 |
| test/reward_per_eps            | -40        |
| test/steps                     | 65600      |
| train/episodes                 | 6560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 262400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 164        |
| stats_o/mean                   | 0.20391569 |
| stats_o/std                    | 0.07225761 |
| test/episodes                  | 1650       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.2       |
| test/info_shaping_reward_min   | -0.505     |
| test/Q                         | -31.039024 |
| test/Q_plus_P                  | -31.039024 |
| test/reward_per_eps            | -40        |
| test/steps                     | 66000      |
| train/episodes                 | 6600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 264000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 165        |
| stats_o/mean                   | 0.2039029  |
| stats_o/std                    | 0.07262876 |
| test/episodes                  | 1660       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0563    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -30.892406 |
| test/Q_plus_P                  | -30.892406 |
| test/reward_per_eps            | -40        |
| test/steps                     | 66400      |
| train/episodes                 | 6640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 265600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 166        |
| stats_o/mean                   | 0.2038664  |
| stats_o/std                    | 0.07277956 |
| test/episodes                  | 1670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.107     |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -30.749897 |
| test/Q_plus_P                  | -30.749897 |
| test/reward_per_eps            | -40        |
| test/steps                     | 66800      |
| train/episodes                 | 6680       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00625    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 267200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.20388214 |
| stats_o/std                    | 0.07306073 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.117     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.614     |
| test/Q                         | -31.131191 |
| test/Q_plus_P                  | -31.131191 |
| test/reward_per_eps            | -40        |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 168        |
| stats_o/mean                   | 0.2038638  |
| stats_o/std                    | 0.07381692 |
| test/episodes                  | 1690       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0692    |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.387     |
| test/Q                         | -30.855015 |
| test/Q_plus_P                  | -30.855015 |
| test/reward_per_eps            | -40        |
| test/steps                     | 67600      |
| train/episodes                 | 6760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.295     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 270400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.20390067 |
| stats_o/std                    | 0.07438239 |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.076     |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -30.674505 |
| test/Q_plus_P                  | -30.674505 |
| test/reward_per_eps            | -40        |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.295     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 170        |
| stats_o/mean                   | 0.20391208 |
| stats_o/std                    | 0.07466956 |
| test/episodes                  | 1710       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0913    |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -31.037937 |
| test/Q_plus_P                  | -31.037937 |
| test/reward_per_eps            | -40        |
| test/steps                     | 68400      |
| train/episodes                 | 6840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.285     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 273600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 171        |
| stats_o/mean                   | 0.20392928 |
| stats_o/std                    | 0.07515923 |
| test/episodes                  | 1720       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0675    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -31.322771 |
| test/Q_plus_P                  | -31.322771 |
| test/reward_per_eps            | -40        |
| test/steps                     | 68800      |
| train/episodes                 | 6880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.114     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 275200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 172        |
| stats_o/mean                   | 0.20393741 |
| stats_o/std                    | 0.07563654 |
| test/episodes                  | 1730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0749    |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -31.251537 |
| test/Q_plus_P                  | -31.251537 |
| test/reward_per_eps            | -40        |
| test/steps                     | 69200      |
| train/episodes                 | 6920       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0156     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.102     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.305     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 276800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 173        |
| stats_o/mean                   | 0.20395768 |
| stats_o/std                    | 0.075752   |
| test/episodes                  | 1740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0736    |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -31.314499 |
| test/Q_plus_P                  | -31.314499 |
| test/reward_per_eps            | -40        |
| test/steps                     | 69600      |
| train/episodes                 | 6960       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0206     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.113     |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 278400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 174         |
| stats_o/mean                   | 0.20393488  |
| stats_o/std                    | 0.075994484 |
| test/episodes                  | 1750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -31.825048  |
| test/Q_plus_P                  | -31.825048  |
| test/reward_per_eps            | -40         |
| test/steps                     | 70000       |
| train/episodes                 | 7000        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.000625    |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.121      |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 280000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 175        |
| stats_o/mean                   | 0.203965   |
| stats_o/std                    | 0.07635518 |
| test/episodes                  | 1760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0664    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -31.353596 |
| test/Q_plus_P                  | -31.353596 |
| test/reward_per_eps            | -40        |
| test/steps                     | 70400      |
| train/episodes                 | 7040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.223     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 281600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 176        |
| stats_o/mean                   | 0.20398435 |
| stats_o/std                    | 0.07662954 |
| test/episodes                  | 1770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0812    |
| test/info_shaping_reward_mean  | -0.178     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -31.516563 |
| test/Q_plus_P                  | -31.516563 |
| test/reward_per_eps            | -40        |
| test/steps                     | 70800      |
| train/episodes                 | 7080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.121     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 283200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 177        |
| stats_o/mean                   | 0.20398764 |
| stats_o/std                    | 0.07675374 |
| test/episodes                  | 1780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -32.02393  |
| test/Q_plus_P                  | -32.02393  |
| test/reward_per_eps            | -40        |
| test/steps                     | 71200      |
| train/episodes                 | 7120       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00125    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.118     |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 284800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.20399106 |
| stats_o/std                    | 0.07710562 |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0768    |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -30.921276 |
| test/Q_plus_P                  | -30.921276 |
| test/reward_per_eps            | -40        |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.289     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 179        |
| stats_o/mean                   | 0.20401452 |
| stats_o/std                    | 0.07756084 |
| test/episodes                  | 1800       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0936    |
| test/info_shaping_reward_mean  | -0.18      |
| test/info_shaping_reward_min   | -0.497     |
| test/Q                         | -32.208073 |
| test/Q_plus_P                  | -32.208073 |
| test/reward_per_eps            | -40        |
| test/steps                     | 72000      |
| train/episodes                 | 7200       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0119     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.112     |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.31      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 288000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 180         |
| stats_o/mean                   | 0.20405436  |
| stats_o/std                    | 0.077808894 |
| test/episodes                  | 1810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.126      |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -32.381596  |
| test/Q_plus_P                  | -32.381596  |
| test/reward_per_eps            | -40         |
| test/steps                     | 72400       |
| train/episodes                 | 7240        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0194      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.187      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 289600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 181         |
| stats_o/mean                   | 0.20404881  |
| stats_o/std                    | 0.077784546 |
| test/episodes                  | 1820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0973     |
| test/info_shaping_reward_mean  | -0.169      |
| test/info_shaping_reward_min   | -0.191      |
| test/Q                         | -31.458157  |
| test/Q_plus_P                  | -31.458157  |
| test/reward_per_eps            | -40         |
| test/steps                     | 72800       |
| train/episodes                 | 7280        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00313     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.285      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 291200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 182        |
| stats_o/mean                   | 0.20409706 |
| stats_o/std                    | 0.07801935 |
| test/episodes                  | 1830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0531    |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.22      |
| test/Q                         | -31.84027  |
| test/Q_plus_P                  | -31.84027  |
| test/reward_per_eps            | -40        |
| test/steps                     | 73200      |
| train/episodes                 | 7320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 292800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 183        |
| stats_o/mean                   | 0.2041135  |
| stats_o/std                    | 0.07810362 |
| test/episodes                  | 1840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0698    |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -31.802118 |
| test/Q_plus_P                  | -31.802118 |
| test/reward_per_eps            | -40        |
| test/steps                     | 73600      |
| train/episodes                 | 7360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 294400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 184        |
| stats_o/mean                   | 0.20410937 |
| stats_o/std                    | 0.07814522 |
| test/episodes                  | 1850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.181     |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -32.0707   |
| test/Q_plus_P                  | -32.0707   |
| test/reward_per_eps            | -40        |
| test/steps                     | 74000      |
| train/episodes                 | 7400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 296000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 185        |
| stats_o/mean                   | 0.2041255  |
| stats_o/std                    | 0.07832539 |
| test/episodes                  | 1860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0025     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.05      |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.236     |
| test/Q                         | -32.217674 |
| test/Q_plus_P                  | -32.217674 |
| test/reward_per_eps            | -39.9      |
| test/steps                     | 74400      |
| train/episodes                 | 7440       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0106     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.122     |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.305     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 297600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 186        |
| stats_o/mean                   | 0.20413353 |
| stats_o/std                    | 0.0784546  |
| test/episodes                  | 1870       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -31.741821 |
| test/Q_plus_P                  | -31.741821 |
| test/reward_per_eps            | -40        |
| test/steps                     | 74800      |
| train/episodes                 | 7480       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 299200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.20416279 |
| stats_o/std                    | 0.07887308 |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -32.468452 |
| test/Q_plus_P                  | -32.468452 |
| test/reward_per_eps            | -40        |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00125    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.29      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 188        |
| stats_o/mean                   | 0.20415755 |
| stats_o/std                    | 0.07898882 |
| test/episodes                  | 1890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0075     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0459    |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -31.978115 |
| test/Q_plus_P                  | -31.978115 |
| test/reward_per_eps            | -39.7      |
| test/steps                     | 75600      |
| train/episodes                 | 7560       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0213     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0881    |
| train/info_shaping_reward_mean | -0.167     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 302400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.20415618  |
| stats_o/std                    | 0.078995414 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.199      |
| test/Q                         | -31.72543   |
| test/Q_plus_P                  | -31.72543   |
| test/reward_per_eps            | -40         |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0131      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 190        |
| stats_o/mean                   | 0.2041775  |
| stats_o/std                    | 0.07905173 |
| test/episodes                  | 1910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -32.19238  |
| test/Q_plus_P                  | -32.19238  |
| test/reward_per_eps            | -40        |
| test/steps                     | 76400      |
| train/episodes                 | 7640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 305600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 191        |
| stats_o/mean                   | 0.2041942  |
| stats_o/std                    | 0.07909623 |
| test/episodes                  | 1920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0325     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0243    |
| test/info_shaping_reward_mean  | -0.154     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -31.83897  |
| test/Q_plus_P                  | -31.83897  |
| test/reward_per_eps            | -38.7      |
| test/steps                     | 76800      |
| train/episodes                 | 7680       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.12      |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 307200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.20419699  |
| stats_o/std                    | 0.079261206 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.181      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -32.512253  |
| test/Q_plus_P                  | -32.512253  |
| test/reward_per_eps            | -40         |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 193         |
| stats_o/mean                   | 0.2041916   |
| stats_o/std                    | 0.079307444 |
| test/episodes                  | 1940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.02        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.035      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.333      |
| test/Q                         | -31.977444  |
| test/Q_plus_P                  | -31.977444  |
| test/reward_per_eps            | -39.2       |
| test/steps                     | 77600       |
| train/episodes                 | 7760        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0112      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.17       |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 310400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 194        |
| stats_o/mean                   | 0.20418827 |
| stats_o/std                    | 0.07936538 |
| test/episodes                  | 1950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.236     |
| test/Q                         | -33.011597 |
| test/Q_plus_P                  | -33.011597 |
| test/reward_per_eps            | -40        |
| test/steps                     | 78000      |
| train/episodes                 | 7800       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.035      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0957    |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 312000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 195        |
| stats_o/mean                   | 0.2042169  |
| stats_o/std                    | 0.07955561 |
| test/episodes                  | 1960       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.131     |
| test/info_shaping_reward_mean  | -0.208     |
| test/info_shaping_reward_min   | -0.77      |
| test/Q                         | -33.088467 |
| test/Q_plus_P                  | -33.088467 |
| test/reward_per_eps            | -40        |
| test/steps                     | 78400      |
| train/episodes                 | 7840       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00125    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 313600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 196        |
| stats_o/mean                   | 0.2042074  |
| stats_o/std                    | 0.07960928 |
| test/episodes                  | 1970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0672    |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.366     |
| test/Q                         | -31.794395 |
| test/Q_plus_P                  | -31.794395 |
| test/reward_per_eps            | -40        |
| test/steps                     | 78800      |
| train/episodes                 | 7880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.291     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 315200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 197         |
| stats_o/mean                   | 0.20418467  |
| stats_o/std                    | 0.079656266 |
| test/episodes                  | 1980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.187      |
| test/info_shaping_reward_min   | -0.602      |
| test/Q                         | -32.883198  |
| test/Q_plus_P                  | -32.883198  |
| test/reward_per_eps            | -40         |
| test/steps                     | 79200       |
| train/episodes                 | 7920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.189      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 316800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.204179    |
| stats_o/std                    | 0.079904445 |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0886     |
| test/info_shaping_reward_mean  | -0.168      |
| test/info_shaping_reward_min   | -0.209      |
| test/Q                         | -32.201694  |
| test/Q_plus_P                  | -32.201694  |
| test/reward_per_eps            | -40         |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.181      |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.20420682 |
| stats_o/std                    | 0.08001572 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0575     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0287    |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -32.54377  |
| test/Q_plus_P                  | -32.54377  |
| test/reward_per_eps            | -37.7      |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.005      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 200        |
| stats_o/mean                   | 0.20423657 |
| stats_o/std                    | 0.08020048 |
| test/episodes                  | 2010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0695    |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -33.04913  |
| test/Q_plus_P                  | -33.04913  |
| test/reward_per_eps            | -40        |
| test/steps                     | 80400      |
| train/episodes                 | 8040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.186     |
| train/info_shaping_reward_min  | -0.287     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 321600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 201         |
| stats_o/mean                   | 0.20423432  |
| stats_o/std                    | 0.080223836 |
| test/episodes                  | 2020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.133      |
| test/info_shaping_reward_mean  | -0.167      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -32.754295  |
| test/Q_plus_P                  | -32.754295  |
| test/reward_per_eps            | -40         |
| test/steps                     | 80800       |
| train/episodes                 | 8080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.183      |
| train/info_shaping_reward_min  | -0.295      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 323200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 202        |
| stats_o/mean                   | 0.20426905 |
| stats_o/std                    | 0.08020105 |
| test/episodes                  | 2030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.181     |
| test/info_shaping_reward_min   | -0.283     |
| test/Q                         | -33.244328 |
| test/Q_plus_P                  | -33.244328 |
| test/reward_per_eps            | -40        |
| test/steps                     | 81200      |
| train/episodes                 | 8120       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00437    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 324800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 203        |
| stats_o/mean                   | 0.20430157 |
| stats_o/std                    | 0.08035614 |
| test/episodes                  | 2040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0973    |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -33.053196 |
| test/Q_plus_P                  | -33.053196 |
| test/reward_per_eps            | -40        |
| test/steps                     | 81600      |
| train/episodes                 | 8160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 326400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 204        |
| stats_o/mean                   | 0.20429811 |
| stats_o/std                    | 0.08036034 |
| test/episodes                  | 2050       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.123     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -33.05992  |
| test/Q_plus_P                  | -33.05992  |
| test/reward_per_eps            | -40        |
| test/steps                     | 82000      |
| train/episodes                 | 8200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 328000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 205        |
| stats_o/mean                   | 0.20430349 |
| stats_o/std                    | 0.08040905 |
| test/episodes                  | 2060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -33.13625  |
| test/Q_plus_P                  | -33.13625  |
| test/reward_per_eps            | -40        |
| test/steps                     | 82400      |
| train/episodes                 | 8240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 329600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 206        |
| stats_o/mean                   | 0.20430614 |
| stats_o/std                    | 0.08049511 |
| test/episodes                  | 2070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0791    |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -32.958496 |
| test/Q_plus_P                  | -32.958496 |
| test/reward_per_eps            | -40        |
| test/steps                     | 82800      |
| train/episodes                 | 8280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 331200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.20431721 |
| stats_o/std                    | 0.08056131 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0805    |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -32.707798 |
| test/Q_plus_P                  | -32.707798 |
| test/reward_per_eps            | -40        |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 208        |
| stats_o/mean                   | 0.20432234 |
| stats_o/std                    | 0.08075597 |
| test/episodes                  | 2090       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -33.383156 |
| test/Q_plus_P                  | -33.383156 |
| test/reward_per_eps            | -40        |
| test/steps                     | 83600      |
| train/episodes                 | 8360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 334400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 209        |
| stats_o/mean                   | 0.20430607 |
| stats_o/std                    | 0.08075962 |
| test/episodes                  | 2100       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.118     |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -33.38581  |
| test/Q_plus_P                  | -33.38581  |
| test/reward_per_eps            | -40        |
| test/steps                     | 84000      |
| train/episodes                 | 8400       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0119     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 336000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 210         |
| stats_o/mean                   | 0.20430055  |
| stats_o/std                    | 0.080774546 |
| test/episodes                  | 2110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0475      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0121     |
| test/info_shaping_reward_mean  | -0.165      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -32.714386  |
| test/Q_plus_P                  | -32.714386  |
| test/reward_per_eps            | -38.1       |
| test/steps                     | 84400       |
| train/episodes                 | 8440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 337600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 211         |
| stats_o/mean                   | 0.20428774  |
| stats_o/std                    | 0.080752686 |
| test/episodes                  | 2120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -33.459618  |
| test/Q_plus_P                  | -33.459618  |
| test/reward_per_eps            | -40         |
| test/steps                     | 84800       |
| train/episodes                 | 8480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 339200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 212        |
| stats_o/mean                   | 0.20426005 |
| stats_o/std                    | 0.08086539 |
| test/episodes                  | 2130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0025     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0441    |
| test/info_shaping_reward_mean  | -0.158     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -32.485615 |
| test/Q_plus_P                  | -32.485615 |
| test/reward_per_eps            | -39.9      |
| test/steps                     | 85200      |
| train/episodes                 | 8520       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00625    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.116     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 340800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 213        |
| stats_o/mean                   | 0.20422934 |
| stats_o/std                    | 0.08080461 |
| test/episodes                  | 2140       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.104     |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -33.76118  |
| test/Q_plus_P                  | -33.76118  |
| test/reward_per_eps            | -40        |
| test/steps                     | 85600      |
| train/episodes                 | 8560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 342400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 214        |
| stats_o/mean                   | 0.20425029 |
| stats_o/std                    | 0.08079649 |
| test/episodes                  | 2150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.13      |
| test/info_shaping_reward_mean  | -0.193     |
| test/info_shaping_reward_min   | -0.605     |
| test/Q                         | -33.259438 |
| test/Q_plus_P                  | -33.259438 |
| test/reward_per_eps            | -40        |
| test/steps                     | 86000      |
| train/episodes                 | 8600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.121     |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 344000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 215        |
| stats_o/mean                   | 0.20423496 |
| stats_o/std                    | 0.08090714 |
| test/episodes                  | 2160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.163      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00484   |
| test/info_shaping_reward_mean  | -0.143     |
| test/info_shaping_reward_min   | -0.213     |
| test/Q                         | -31.829872 |
| test/Q_plus_P                  | -31.829872 |
| test/reward_per_eps            | -33.5      |
| test/steps                     | 86400      |
| train/episodes                 | 8640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 345600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 216        |
| stats_o/mean                   | 0.20422383 |
| stats_o/std                    | 0.08096743 |
| test/episodes                  | 2170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.105     |
| test/info_shaping_reward_mean  | -0.19      |
| test/info_shaping_reward_min   | -0.356     |
| test/Q                         | -32.71333  |
| test/Q_plus_P                  | -32.71333  |
| test/reward_per_eps            | -40        |
| test/steps                     | 86800      |
| train/episodes                 | 8680       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00187    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 347200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 217         |
| stats_o/mean                   | 0.20423125  |
| stats_o/std                    | 0.081217915 |
| test/episodes                  | 2180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0894     |
| test/info_shaping_reward_mean  | -0.169      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -33.16955   |
| test/Q_plus_P                  | -33.16955   |
| test/reward_per_eps            | -40         |
| test/steps                     | 87200       |
| train/episodes                 | 8720        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0138      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0978     |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 348800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 218        |
| stats_o/mean                   | 0.2042395  |
| stats_o/std                    | 0.08129204 |
| test/episodes                  | 2190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.186     |
| test/info_shaping_reward_min   | -0.547     |
| test/Q                         | -34.024662 |
| test/Q_plus_P                  | -34.024662 |
| test/reward_per_eps            | -40        |
| test/steps                     | 87600      |
| train/episodes                 | 8760       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0106     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0946    |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 350400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 219        |
| stats_o/mean                   | 0.20422105 |
| stats_o/std                    | 0.0813056  |
| test/episodes                  | 2200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0525     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0159    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -33.50668  |
| test/Q_plus_P                  | -33.50668  |
| test/reward_per_eps            | -37.9      |
| test/steps                     | 88000      |
| train/episodes                 | 8800       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 352000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 220        |
| stats_o/mean                   | 0.20421456 |
| stats_o/std                    | 0.08136259 |
| test/episodes                  | 2210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0569    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -33.466152 |
| test/Q_plus_P                  | -33.466152 |
| test/reward_per_eps            | -40        |
| test/steps                     | 88400      |
| train/episodes                 | 8840       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0181     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 353600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 221        |
| stats_o/mean                   | 0.20422752 |
| stats_o/std                    | 0.08158478 |
| test/episodes                  | 2220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.055      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.014     |
| test/info_shaping_reward_mean  | -0.181     |
| test/info_shaping_reward_min   | -0.699     |
| test/Q                         | -33.004852 |
| test/Q_plus_P                  | -33.004852 |
| test/reward_per_eps            | -37.8      |
| test/steps                     | 88800      |
| train/episodes                 | 8880       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0175     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0939    |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 355200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.2042053  |
| stats_o/std                    | 0.08168066 |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0525     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00436   |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.327     |
| test/Q                         | -33.234673 |
| test/Q_plus_P                  | -33.234673 |
| test/reward_per_eps            | -37.9      |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0462     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0964    |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.1      |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 223         |
| stats_o/mean                   | 0.20421383  |
| stats_o/std                    | 0.081778005 |
| test/episodes                  | 2240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.01        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0272     |
| test/info_shaping_reward_mean  | -0.156      |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -32.641075  |
| test/Q_plus_P                  | -32.641075  |
| test/reward_per_eps            | -39.6       |
| test/steps                     | 89600       |
| train/episodes                 | 8960        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00187     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.184      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 358400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.20418917 |
| stats_o/std                    | 0.08196455 |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0525    |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -33.29066  |
| test/Q_plus_P                  | -33.29066  |
| test/reward_per_eps            | -40        |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0131     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.12      |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 225        |
| stats_o/mean                   | 0.20418407 |
| stats_o/std                    | 0.08225311 |
| test/episodes                  | 2260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.172      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0243    |
| test/info_shaping_reward_mean  | -0.14      |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -31.925531 |
| test/Q_plus_P                  | -31.925531 |
| test/reward_per_eps            | -33.1      |
| test/steps                     | 90400      |
| train/episodes                 | 9040       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0112     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0925    |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 361600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 226        |
| stats_o/mean                   | 0.2041712  |
| stats_o/std                    | 0.08226111 |
| test/episodes                  | 2270       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.129     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -33.32239  |
| test/Q_plus_P                  | -33.32239  |
| test/reward_per_eps            | -40        |
| test/steps                     | 90800      |
| train/episodes                 | 9080       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0394     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.099     |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.21      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 363200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 227        |
| stats_o/mean                   | 0.20417608 |
| stats_o/std                    | 0.08247003 |
| test/episodes                  | 2280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.005      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0411    |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -33.39767  |
| test/Q_plus_P                  | -33.39767  |
| test/reward_per_eps            | -39.8      |
| test/steps                     | 91200      |
| train/episodes                 | 9120       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0119     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.109     |
| train/info_shaping_reward_mean | -0.187     |
| train/info_shaping_reward_min  | -0.311     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 364800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 228         |
| stats_o/mean                   | 0.20417038  |
| stats_o/std                    | 0.082655504 |
| test/episodes                  | 2290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0925      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0312     |
| test/info_shaping_reward_mean  | -0.156      |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -32.107704  |
| test/Q_plus_P                  | -32.107704  |
| test/reward_per_eps            | -36.3       |
| test/steps                     | 91600       |
| train/episodes                 | 9160        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.000625    |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.11       |
| train/info_shaping_reward_mean | -0.168      |
| train/info_shaping_reward_min  | -0.204      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 366400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 229        |
| stats_o/mean                   | 0.20415013 |
| stats_o/std                    | 0.08279767 |
| test/episodes                  | 2300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0879    |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -33.069195 |
| test/Q_plus_P                  | -33.069195 |
| test/reward_per_eps            | -40        |
| test/steps                     | 92000      |
| train/episodes                 | 9200       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0187     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0939    |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 368000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 230        |
| stats_o/mean                   | 0.20414999 |
| stats_o/std                    | 0.08285836 |
| test/episodes                  | 2310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.11       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0221    |
| test/info_shaping_reward_mean  | -0.136     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -31.265131 |
| test/Q_plus_P                  | -31.265131 |
| test/reward_per_eps            | -35.6      |
| test/steps                     | 92400      |
| train/episodes                 | 9240       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00625    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.101     |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 369600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 231        |
| stats_o/mean                   | 0.20414372 |
| stats_o/std                    | 0.08303284 |
| test/episodes                  | 2320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0563    |
| test/info_shaping_reward_mean  | -0.179     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -34.06194  |
| test/Q_plus_P                  | -34.06194  |
| test/reward_per_eps            | -40        |
| test/steps                     | 92800      |
| train/episodes                 | 9280       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0187     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0866    |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 371200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 232        |
| stats_o/mean                   | 0.20406778 |
| stats_o/std                    | 0.08326417 |
| test/episodes                  | 2330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.115      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0174    |
| test/info_shaping_reward_mean  | -0.165     |
| test/info_shaping_reward_min   | -0.277     |
| test/Q                         | -33.243652 |
| test/Q_plus_P                  | -33.243652 |
| test/reward_per_eps            | -35.4      |
| test/steps                     | 93200      |
| train/episodes                 | 9320       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0119     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.102     |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.274     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 372800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 233         |
| stats_o/mean                   | 0.2040403   |
| stats_o/std                    | 0.083518915 |
| test/episodes                  | 2340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.147      |
| test/info_shaping_reward_mean  | -0.177      |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -34.132877  |
| test/Q_plus_P                  | -34.132877  |
| test/reward_per_eps            | -40         |
| test/steps                     | 93600       |
| train/episodes                 | 9360        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.00937     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0999     |
| train/info_shaping_reward_mean | -0.165      |
| train/info_shaping_reward_min  | -0.219      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 374400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.2039975  |
| stats_o/std                    | 0.08363645 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0778    |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -32.269447 |
| test/Q_plus_P                  | -32.269447 |
| test/reward_per_eps            | -40        |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.005      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0998    |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.20399073  |
| stats_o/std                    | 0.083634235 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0675      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0189     |
| test/info_shaping_reward_mean  | -0.159      |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -33.503273  |
| test/Q_plus_P                  | -33.503273  |
| test/reward_per_eps            | -37.3       |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.015       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.122      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 236        |
| stats_o/mean                   | 0.2039823  |
| stats_o/std                    | 0.08364994 |
| test/episodes                  | 2370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.005      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0408    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.22      |
| test/Q                         | -31.93315  |
| test/Q_plus_P                  | -31.93315  |
| test/reward_per_eps            | -39.8      |
| test/steps                     | 94800      |
| train/episodes                 | 9480       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0138     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.113     |
| train/info_shaping_reward_mean | -0.167     |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 379200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 237         |
| stats_o/mean                   | 0.20397574  |
| stats_o/std                    | 0.083825864 |
| test/episodes                  | 2380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0561     |
| test/info_shaping_reward_mean  | -0.17       |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -33.632748  |
| test/Q_plus_P                  | -33.632748  |
| test/reward_per_eps            | -40         |
| test/steps                     | 95200       |
| train/episodes                 | 9520        |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0238      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.111      |
| train/info_shaping_reward_mean | -0.19       |
| train/info_shaping_reward_min  | -0.396      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39         |
| train/steps                    | 380800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 238        |
| stats_o/mean                   | 0.20397788 |
| stats_o/std                    | 0.08393703 |
| test/episodes                  | 2390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0075     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0133    |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.606     |
| test/Q                         | -32.685    |
| test/Q_plus_P                  | -32.685    |
| test/reward_per_eps            | -39.7      |
| test/steps                     | 95600      |
| train/episodes                 | 9560       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0144     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.102     |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 382400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 239        |
| stats_o/mean                   | 0.20396307 |
| stats_o/std                    | 0.08417299 |
| test/episodes                  | 2400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0619    |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -32.83934  |
| test/Q_plus_P                  | -32.83934  |
| test/reward_per_eps            | -40        |
| test/steps                     | 96000      |
| train/episodes                 | 9600       |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0119     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0842    |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 384000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 240        |
| stats_o/mean                   | 0.20394702 |
| stats_o/std                    | 0.08430556 |
| test/episodes                  | 2410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.102      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00964   |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.292     |
| test/Q                         | -33.060947 |
| test/Q_plus_P                  | -33.060947 |
| test/reward_per_eps            | -35.9      |
| test/steps                     | 96400      |
| train/episodes                 | 9640       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0181     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0877    |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 385600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 241        |
| stats_o/mean                   | 0.20391755 |
| stats_o/std                    | 0.08445354 |
| test/episodes                  | 2420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.366     |
| test/Q                         | -32.92101  |
| test/Q_plus_P                  | -32.92101  |
| test/reward_per_eps            | -40        |
| test/steps                     | 96800      |
| train/episodes                 | 9680       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0225     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.111     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 387200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 242        |
| stats_o/mean                   | 0.20391443 |
| stats_o/std                    | 0.08461072 |
| test/episodes                  | 2430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.038     |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -32.470142 |
| test/Q_plus_P                  | -32.470142 |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 97200      |
| train/episodes                 | 9720       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.11      |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 388800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 243         |
| stats_o/mean                   | 0.20388386  |
| stats_o/std                    | 0.084636815 |
| test/episodes                  | 2440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.06        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0391     |
| test/info_shaping_reward_mean  | -0.158      |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -31.869421  |
| test/Q_plus_P                  | -31.869421  |
| test/reward_per_eps            | -37.6       |
| test/steps                     | 97600       |
| train/episodes                 | 9760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 390400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.20390244 |
| stats_o/std                    | 0.08489343 |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0175     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0125    |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.186     |
| test/Q                         | -30.184547 |
| test/Q_plus_P                  | -30.184547 |
| test/reward_per_eps            | -39.3      |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.005      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0822    |
| train/info_shaping_reward_mean | -0.161     |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 245        |
| stats_o/mean                   | 0.20387806 |
| stats_o/std                    | 0.0850792  |
| test/episodes                  | 2460       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.059     |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -31.81824  |
| test/Q_plus_P                  | -31.81824  |
| test/reward_per_eps            | -40        |
| test/steps                     | 98400      |
| train/episodes                 | 9840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.121     |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 393600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 246         |
| stats_o/mean                   | 0.20386957  |
| stats_o/std                    | 0.085167274 |
| test/episodes                  | 2470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0685     |
| test/info_shaping_reward_mean  | -0.169      |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -31.87356   |
| test/Q_plus_P                  | -31.87356   |
| test/reward_per_eps            | -40         |
| test/steps                     | 98800       |
| train/episodes                 | 9880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.116      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 395200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 247         |
| stats_o/mean                   | 0.20386389  |
| stats_o/std                    | 0.085328385 |
| test/episodes                  | 2480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.172      |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -34.5464    |
| test/Q_plus_P                  | -34.5464    |
| test/reward_per_eps            | -40         |
| test/steps                     | 99200       |
| train/episodes                 | 9920        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0169      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0998     |
| train/info_shaping_reward_mean | -0.169      |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 396800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.20385283 |
| stats_o/std                    | 0.08544724 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0375     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0101    |
| test/info_shaping_reward_mean  | -0.148     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -31.817114 |
| test/Q_plus_P                  | -31.817114 |
| test/reward_per_eps            | -38.5      |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0131     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 249        |
| stats_o/mean                   | 0.20384344 |
| stats_o/std                    | 0.08565437 |
| test/episodes                  | 2500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0685    |
| test/info_shaping_reward_mean  | -0.188     |
| test/info_shaping_reward_min   | -0.611     |
| test/Q                         | -32.739857 |
| test/Q_plus_P                  | -32.739857 |
| test/reward_per_eps            | -40        |
| test/steps                     | 100000     |
| train/episodes                 | 10000      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0187     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0952    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 400000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 250        |
| stats_o/mean                   | 0.20383094 |
| stats_o/std                    | 0.08593259 |
| test/episodes                  | 2510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.097     |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -33.781803 |
| test/Q_plus_P                  | -33.781803 |
| test/reward_per_eps            | -40        |
| test/steps                     | 100400     |
| train/episodes                 | 10040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.12      |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.297     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 401600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 251        |
| stats_o/mean                   | 0.20383579 |
| stats_o/std                    | 0.08629166 |
| test/episodes                  | 2520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0375     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0429    |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -32.678757 |
| test/Q_plus_P                  | -32.678757 |
| test/reward_per_eps            | -38.5      |
| test/steps                     | 100800     |
| train/episodes                 | 10080      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0387     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0728    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 403200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 252        |
| stats_o/mean                   | 0.20381798 |
| stats_o/std                    | 0.0864748  |
| test/episodes                  | 2530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.005      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0363    |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -30.977549 |
| test/Q_plus_P                  | -30.977549 |
| test/reward_per_eps            | -39.8      |
| test/steps                     | 101200     |
| train/episodes                 | 10120      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0306     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.079     |
| train/info_shaping_reward_mean | -0.165     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 404800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 253        |
| stats_o/mean                   | 0.20378487 |
| stats_o/std                    | 0.08671504 |
| test/episodes                  | 2540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.113      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.024     |
| test/info_shaping_reward_mean  | -0.142     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -31.559893 |
| test/Q_plus_P                  | -31.559893 |
| test/reward_per_eps            | -35.5      |
| test/steps                     | 101600     |
| train/episodes                 | 10160      |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.115     |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.548     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 406400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 254        |
| stats_o/mean                   | 0.20375334 |
| stats_o/std                    | 0.08696451 |
| test/episodes                  | 2550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.074     |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -32.63713  |
| test/Q_plus_P                  | -32.63713  |
| test/reward_per_eps            | -40        |
| test/steps                     | 102000     |
| train/episodes                 | 10200      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0525     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0627    |
| train/info_shaping_reward_mean | -0.162     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.9      |
| train/steps                    | 408000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 255        |
| stats_o/mean                   | 0.20373261 |
| stats_o/std                    | 0.08715467 |
| test/episodes                  | 2560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.107      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0133    |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.321     |
| test/Q                         | -30.486078 |
| test/Q_plus_P                  | -30.486078 |
| test/reward_per_eps            | -35.7      |
| test/steps                     | 102400     |
| train/episodes                 | 10240      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0338     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0825    |
| train/info_shaping_reward_mean | -0.173     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 409600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 256        |
| stats_o/mean                   | 0.2037137  |
| stats_o/std                    | 0.08741513 |
| test/episodes                  | 2570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.05       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0226    |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -31.999344 |
| test/Q_plus_P                  | -31.999344 |
| test/reward_per_eps            | -38        |
| test/steps                     | 102800     |
| train/episodes                 | 10280      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0631     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0803    |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.43      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 411200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 257         |
| stats_o/mean                   | 0.20369388  |
| stats_o/std                    | 0.087741606 |
| test/episodes                  | 2580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0925      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0179     |
| test/info_shaping_reward_mean  | -0.157      |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -31.261856  |
| test/Q_plus_P                  | -31.261856  |
| test/reward_per_eps            | -36.3       |
| test/steps                     | 103200      |
| train/episodes                 | 10320       |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0225      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.08       |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.296      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.1       |
| train/steps                    | 412800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 258        |
| stats_o/mean                   | 0.20369461 |
| stats_o/std                    | 0.08790988 |
| test/episodes                  | 2590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.065      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0421    |
| test/info_shaping_reward_mean  | -0.158     |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -31.047867 |
| test/Q_plus_P                  | -31.047867 |
| test/reward_per_eps            | -37.4      |
| test/steps                     | 103600     |
| train/episodes                 | 10360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 414400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 259        |
| stats_o/mean                   | 0.20364068 |
| stats_o/std                    | 0.08809092 |
| test/episodes                  | 2600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0175     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0181    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -31.744892 |
| test/Q_plus_P                  | -31.744892 |
| test/reward_per_eps            | -39.3      |
| test/steps                     | 104000     |
| train/episodes                 | 10400      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.01       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.109     |
| train/info_shaping_reward_mean | -0.199     |
| train/info_shaping_reward_min  | -0.392     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 416000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 260         |
| stats_o/mean                   | 0.20362215  |
| stats_o/std                    | 0.088305816 |
| test/episodes                  | 2610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0527     |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.859      |
| test/Q                         | -32.082245  |
| test/Q_plus_P                  | -32.082245  |
| test/reward_per_eps            | -40         |
| test/steps                     | 104400      |
| train/episodes                 | 10440       |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0256      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0768     |
| train/info_shaping_reward_mean | -0.18       |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39         |
| train/steps                    | 417600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 261        |
| stats_o/mean                   | 0.20357049 |
| stats_o/std                    | 0.08850581 |
| test/episodes                  | 2620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.137     |
| test/info_shaping_reward_mean  | -0.187     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -31.924484 |
| test/Q_plus_P                  | -31.924484 |
| test/reward_per_eps            | -40        |
| test/steps                     | 104800     |
| train/episodes                 | 10480      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0325     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0752    |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.471     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 419200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 262        |
| stats_o/mean                   | 0.20351438 |
| stats_o/std                    | 0.08892939 |
| test/episodes                  | 2630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0175     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0312    |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.566     |
| test/Q                         | -31.14985  |
| test/Q_plus_P                  | -31.14985  |
| test/reward_per_eps            | -39.3      |
| test/steps                     | 105200     |
| train/episodes                 | 10520      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0219     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0929    |
| train/info_shaping_reward_mean | -0.201     |
| train/info_shaping_reward_min  | -0.372     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 420800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 263        |
| stats_o/mean                   | 0.2034882  |
| stats_o/std                    | 0.0891288  |
| test/episodes                  | 2640       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.057     |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -0.815     |
| test/Q                         | -32.178097 |
| test/Q_plus_P                  | -32.178097 |
| test/reward_per_eps            | -40        |
| test/steps                     | 105600     |
| train/episodes                 | 10560      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0556     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0665    |
| train/info_shaping_reward_mean | -0.183     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 422400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 264         |
| stats_o/mean                   | 0.2034371   |
| stats_o/std                    | 0.089444056 |
| test/episodes                  | 2650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0025      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0134     |
| test/info_shaping_reward_mean  | -0.162      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -30.967592  |
| test/Q_plus_P                  | -30.967592  |
| test/reward_per_eps            | -39.9       |
| test/steps                     | 106000      |
| train/episodes                 | 10600       |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0188      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0771     |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 424000      |
------------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 265       |
| stats_o/mean                   | 0.2033855 |
| stats_o/std                    | 0.0897399 |
| test/episodes                  | 2660      |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.1       |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.00866  |
| test/info_shaping_reward_mean  | -0.235    |
| test/info_shaping_reward_min   | -0.821    |
| test/Q                         | -29.10212 |
| test/Q_plus_P                  | -29.10212 |
| test/reward_per_eps            | -36       |
| test/steps                     | 106400    |
| train/episodes                 | 10640     |
| train/info_is_success_max      | 0.2       |
| train/info_is_success_mean     | 0.00188   |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.0968   |
| train/info_shaping_reward_mean | -0.218    |
| train/info_shaping_reward_min  | -0.452    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -39.9     |
| train/steps                    | 425600    |
----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.2033636   |
| stats_o/std                    | 0.090072244 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0075      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.039      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.555      |
| test/Q                         | -32.985664  |
| test/Q_plus_P                  | -32.985664  |
| test/reward_per_eps            | -39.7       |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0112      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0802     |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 267        |
| stats_o/mean                   | 0.20332968 |
| stats_o/std                    | 0.09060433 |
| test/episodes                  | 2680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0877    |
| test/info_shaping_reward_mean  | -0.172     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -31.654888 |
| test/Q_plus_P                  | -31.654888 |
| test/reward_per_eps            | -40        |
| test/steps                     | 107200     |
| train/episodes                 | 10720      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0287     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0688    |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.302     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 428800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.20329762  |
| stats_o/std                    | 0.090929106 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0541     |
| test/info_shaping_reward_mean  | -0.162      |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -30.578716  |
| test/Q_plus_P                  | -30.578716  |
| test/reward_per_eps            | -40         |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0225      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0727     |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.1       |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 269        |
| stats_o/mean                   | 0.20329243 |
| stats_o/std                    | 0.0911856  |
| test/episodes                  | 2700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0425     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0181    |
| test/info_shaping_reward_mean  | -0.158     |
| test/info_shaping_reward_min   | -0.197     |
| test/Q                         | -30.688047 |
| test/Q_plus_P                  | -30.688047 |
| test/reward_per_eps            | -38.3      |
| test/steps                     | 108000     |
| train/episodes                 | 10800      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0163     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0576    |
| train/info_shaping_reward_mean | -0.178     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 432000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 270        |
| stats_o/mean                   | 0.2032854  |
| stats_o/std                    | 0.09152453 |
| test/episodes                  | 2710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.06       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.042     |
| test/info_shaping_reward_mean  | -0.157     |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -30.738106 |
| test/Q_plus_P                  | -30.738106 |
| test/reward_per_eps            | -37.6      |
| test/steps                     | 108400     |
| train/episodes                 | 10840      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0175     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.106     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 433600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 271        |
| stats_o/mean                   | 0.20327815 |
| stats_o/std                    | 0.09173643 |
| test/episodes                  | 2720       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.125     |
| test/info_shaping_reward_mean  | -0.197     |
| test/info_shaping_reward_min   | -0.613     |
| test/Q                         | -31.622858 |
| test/Q_plus_P                  | -31.622858 |
| test/reward_per_eps            | -40        |
| test/steps                     | 108800     |
| train/episodes                 | 10880      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0369     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0897    |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 435200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 272        |
| stats_o/mean                   | 0.20324188 |
| stats_o/std                    | 0.09185086 |
| test/episodes                  | 2730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0191    |
| test/info_shaping_reward_mean  | -0.135     |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -28.18816  |
| test/Q_plus_P                  | -28.18816  |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 109200     |
| train/episodes                 | 10920      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00688    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.112     |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 436800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 273        |
| stats_o/mean                   | 0.20320858 |
| stats_o/std                    | 0.09210495 |
| test/episodes                  | 2740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0519    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -29.748423 |
| test/Q_plus_P                  | -29.748423 |
| test/reward_per_eps            | -40        |
| test/steps                     | 109600     |
| train/episodes                 | 10960      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0119     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0821    |
| train/info_shaping_reward_mean | -0.171     |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 438400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 274         |
| stats_o/mean                   | 0.20320305  |
| stats_o/std                    | 0.092331275 |
| test/episodes                  | 2750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -32.84517   |
| test/Q_plus_P                  | -32.84517   |
| test/reward_per_eps            | -40         |
| test/steps                     | 110000      |
| train/episodes                 | 11000       |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0075      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.101      |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 440000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 275        |
| stats_o/mean                   | 0.20321009 |
| stats_o/std                    | 0.09257074 |
| test/episodes                  | 2760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.123     |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -31.164074 |
| test/Q_plus_P                  | -31.164074 |
| test/reward_per_eps            | -40        |
| test/steps                     | 110400     |
| train/episodes                 | 11040      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0169     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0959    |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 441600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 276         |
| stats_o/mean                   | 0.20322134  |
| stats_o/std                    | 0.092747234 |
| test/episodes                  | 2770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0625      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0202     |
| test/info_shaping_reward_mean  | -0.159      |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -30.379742  |
| test/Q_plus_P                  | -30.379742  |
| test/reward_per_eps            | -37.5       |
| test/steps                     | 110800      |
| train/episodes                 | 11080       |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.00688     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.102      |
| train/info_shaping_reward_mean | -0.173      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 443200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 277        |
| stats_o/mean                   | 0.20323591 |
| stats_o/std                    | 0.09291481 |
| test/episodes                  | 2780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0575     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0453    |
| test/info_shaping_reward_mean  | -0.151     |
| test/info_shaping_reward_min   | -0.209     |
| test/Q                         | -29.553455 |
| test/Q_plus_P                  | -29.553455 |
| test/reward_per_eps            | -37.7      |
| test/steps                     | 111200     |
| train/episodes                 | 11120      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00688    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0787    |
| train/info_shaping_reward_mean | -0.157     |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 444800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 278         |
| stats_o/mean                   | 0.2032278   |
| stats_o/std                    | 0.092949286 |
| test/episodes                  | 2790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.134      |
| test/info_shaping_reward_mean  | -0.17       |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -32.203537  |
| test/Q_plus_P                  | -32.203537  |
| test/reward_per_eps            | -40         |
| test/steps                     | 111600      |
| train/episodes                 | 11160       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0131      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.168      |
| train/info_shaping_reward_min  | -0.197      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 446400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 279        |
| stats_o/mean                   | 0.20320801 |
| stats_o/std                    | 0.09302866 |
| test/episodes                  | 2800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.095      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.023     |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -30.425135 |
| test/Q_plus_P                  | -30.425135 |
| test/reward_per_eps            | -36.2      |
| test/steps                     | 112000     |
| train/episodes                 | 11200      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0563     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0664    |
| train/info_shaping_reward_mean | -0.157     |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 448000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 280         |
| stats_o/mean                   | 0.2031605   |
| stats_o/std                    | 0.093191124 |
| test/episodes                  | 2810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0903     |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -31.483828  |
| test/Q_plus_P                  | -31.483828  |
| test/reward_per_eps            | -40         |
| test/steps                     | 112400      |
| train/episodes                 | 11240       |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0456      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0866     |
| train/info_shaping_reward_mean | -0.154      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.2       |
| train/steps                    | 449600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.2031429  |
| stats_o/std                    | 0.09334913 |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.147      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0271    |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -26.794182 |
| test/Q_plus_P                  | -26.794182 |
| test/reward_per_eps            | -34.1      |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0163     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.104     |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.276     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 282         |
| stats_o/mean                   | 0.20314845  |
| stats_o/std                    | 0.093445495 |
| test/episodes                  | 2830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0568     |
| test/info_shaping_reward_mean  | -0.156      |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -30.254189  |
| test/Q_plus_P                  | -30.254189  |
| test/reward_per_eps            | -40         |
| test/steps                     | 113200      |
| train/episodes                 | 11320       |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0256      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0842     |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39         |
| train/steps                    | 452800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.20312072 |
| stats_o/std                    | 0.09359375 |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00446   |
| test/info_shaping_reward_mean  | -0.157     |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -29.250591 |
| test/Q_plus_P                  | -29.250591 |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0238     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0602    |
| train/info_shaping_reward_mean | -0.162     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 284        |
| stats_o/mean                   | 0.20311415 |
| stats_o/std                    | 0.09374901 |
| test/episodes                  | 2850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.015      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0287    |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -31.588264 |
| test/Q_plus_P                  | -31.588264 |
| test/reward_per_eps            | -39.4      |
| test/steps                     | 114000     |
| train/episodes                 | 11400      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0175     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0808    |
| train/info_shaping_reward_mean | -0.164     |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 456000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 285        |
| stats_o/mean                   | 0.20310348 |
| stats_o/std                    | 0.09387201 |
| test/episodes                  | 2860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0725     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.012     |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.565     |
| test/Q                         | -28.450186 |
| test/Q_plus_P                  | -28.450186 |
| test/reward_per_eps            | -37.1      |
| test/steps                     | 114400     |
| train/episodes                 | 11440      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.00812    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0886    |
| train/info_shaping_reward_mean | -0.162     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 457600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 286         |
| stats_o/mean                   | 0.20308767  |
| stats_o/std                    | 0.093953535 |
| test/episodes                  | 2870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.128       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0192     |
| test/info_shaping_reward_mean  | -0.148      |
| test/info_shaping_reward_min   | -0.191      |
| test/Q                         | -29.570791  |
| test/Q_plus_P                  | -29.570791  |
| test/reward_per_eps            | -34.9       |
| test/steps                     | 114800      |
| train/episodes                 | 11480       |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.104       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0569     |
| train/info_shaping_reward_mean | -0.153      |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.8       |
| train/steps                    | 459200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.20308515 |
| stats_o/std                    | 0.09403762 |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0425     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0241    |
| test/info_shaping_reward_mean  | -0.161     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -30.340872 |
| test/Q_plus_P                  | -30.340872 |
| test/reward_per_eps            | -38.3      |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0163     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0836    |
| train/info_shaping_reward_mean | -0.158     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 288        |
| stats_o/mean                   | 0.20306416 |
| stats_o/std                    | 0.09429982 |
| test/episodes                  | 2890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0413    |
| test/info_shaping_reward_mean  | -0.152     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -28.266895 |
| test/Q_plus_P                  | -28.266895 |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 115600     |
| train/episodes                 | 11560      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0269     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0696    |
| train/info_shaping_reward_mean | -0.157     |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.9      |
| train/steps                    | 462400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 289        |
| stats_o/mean                   | 0.2030491  |
| stats_o/std                    | 0.09433035 |
| test/episodes                  | 2900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0075     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0288    |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.749     |
| test/Q                         | -31.467102 |
| test/Q_plus_P                  | -31.467102 |
| test/reward_per_eps            | -39.7      |
| test/steps                     | 116000     |
| train/episodes                 | 11600      |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0262     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.112     |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 464000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 290         |
| stats_o/mean                   | 0.20304     |
| stats_o/std                    | 0.094366066 |
| test/episodes                  | 2910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0075      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0432     |
| test/info_shaping_reward_mean  | -0.15       |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -30.514938  |
| test/Q_plus_P                  | -30.514938  |
| test/reward_per_eps            | -39.7       |
| test/steps                     | 116400      |
| train/episodes                 | 11640       |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0531      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0812     |
| train/info_shaping_reward_mean | -0.157      |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.9       |
| train/steps                    | 465600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 291        |
| stats_o/mean                   | 0.20306887 |
| stats_o/std                    | 0.09450971 |
| test/episodes                  | 2920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.17       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00926   |
| test/info_shaping_reward_mean  | -0.133     |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -26.594612 |
| test/Q_plus_P                  | -26.594612 |
| test/reward_per_eps            | -33.2      |
| test/steps                     | 116800     |
| train/episodes                 | 11680      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0613     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0684    |
| train/info_shaping_reward_mean | -0.175     |
| train/info_shaping_reward_min  | -0.297     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 467200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 292        |
| stats_o/mean                   | 0.20304082 |
| stats_o/std                    | 0.09466269 |
| test/episodes                  | 2930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0339    |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -27.63614  |
| test/Q_plus_P                  | -27.63614  |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 117200     |
| train/episodes                 | 11720      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0638     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0742    |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.297     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 468800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.20302276  |
| stats_o/std                    | 0.094712906 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.075       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0143     |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.601      |
| test/Q                         | -29.703821  |
| test/Q_plus_P                  | -29.703821  |
| test/reward_per_eps            | -37         |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0512      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0908     |
| train/info_shaping_reward_mean | -0.161      |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38         |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 294        |
| stats_o/mean                   | 0.2029927  |
| stats_o/std                    | 0.0949125  |
| test/episodes                  | 2950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.135      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0293    |
| test/info_shaping_reward_mean  | -0.137     |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -26.793316 |
| test/Q_plus_P                  | -26.793316 |
| test/reward_per_eps            | -34.6      |
| test/steps                     | 118000     |
| train/episodes                 | 11800      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0538     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0567    |
| train/info_shaping_reward_mean | -0.16      |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.9      |
| train/steps                    | 472000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 295        |
| stats_o/mean                   | 0.20298794 |
| stats_o/std                    | 0.09501397 |
| test/episodes                  | 2960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.095      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0119    |
| test/info_shaping_reward_mean  | -0.149     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -27.75356  |
| test/Q_plus_P                  | -27.75356  |
| test/reward_per_eps            | -36.2      |
| test/steps                     | 118400     |
| train/episodes                 | 11840      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0338     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0867    |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.389     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 473600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 296         |
| stats_o/mean                   | 0.20299505  |
| stats_o/std                    | 0.095278785 |
| test/episodes                  | 2970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.193       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0224     |
| test/info_shaping_reward_mean  | -0.13       |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -24.786606  |
| test/Q_plus_P                  | -24.786606  |
| test/reward_per_eps            | -32.3       |
| test/steps                     | 118800      |
| train/episodes                 | 11880       |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.045       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0537     |
| train/info_shaping_reward_mean | -0.159      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.2       |
| train/steps                    | 475200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 297        |
| stats_o/mean                   | 0.20297962 |
| stats_o/std                    | 0.09531659 |
| test/episodes                  | 2980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0725     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0321    |
| test/info_shaping_reward_mean  | -0.155     |
| test/info_shaping_reward_min   | -0.223     |
| test/Q                         | -29.861528 |
| test/Q_plus_P                  | -29.861528 |
| test/reward_per_eps            | -37.1      |
| test/steps                     | 119200     |
| train/episodes                 | 11920      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0863     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0777    |
| train/info_shaping_reward_mean | -0.16      |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.5      |
| train/steps                    | 476800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 298        |
| stats_o/mean                   | 0.2029569  |
| stats_o/std                    | 0.09549776 |
| test/episodes                  | 2990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.343      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0122    |
| test/info_shaping_reward_mean  | -0.123     |
| test/info_shaping_reward_min   | -0.328     |
| test/Q                         | -23.218714 |
| test/Q_plus_P                  | -23.218714 |
| test/reward_per_eps            | -26.3      |
| test/steps                     | 119600     |
| train/episodes                 | 11960      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.158      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0515    |
| train/info_shaping_reward_mean | -0.154     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.7      |
| train/steps                    | 478400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 299        |
| stats_o/mean                   | 0.20292084 |
| stats_o/std                    | 0.09558256 |
| test/episodes                  | 3000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.08       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0217    |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -29.146328 |
| test/Q_plus_P                  | -29.146328 |
| test/reward_per_eps            | -36.8      |
| test/steps                     | 120000     |
| train/episodes                 | 12000      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.153      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0349    |
| train/info_shaping_reward_mean | -0.15      |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.9      |
| train/steps                    | 480000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 300        |
| stats_o/mean                   | 0.2029083  |
| stats_o/std                    | 0.09564668 |
| test/episodes                  | 3010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.217      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0133    |
| test/info_shaping_reward_mean  | -0.128     |
| test/info_shaping_reward_min   | -0.211     |
| test/Q                         | -24.024508 |
| test/Q_plus_P                  | -24.024508 |
| test/reward_per_eps            | -31.3      |
| test/steps                     | 120400     |
| train/episodes                 | 12040      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0875     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0574    |
| train/info_shaping_reward_mean | -0.155     |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.5      |
| train/steps                    | 481600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.202878   |
| stats_o/std                    | 0.0957583  |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.12       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0266    |
| test/info_shaping_reward_mean  | -0.152     |
| test/info_shaping_reward_min   | -0.214     |
| test/Q                         | -26.814566 |
| test/Q_plus_P                  | -26.814566 |
| test/reward_per_eps            | -35.2      |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.04       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0904    |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 302         |
| stats_o/mean                   | 0.20288195  |
| stats_o/std                    | 0.095978335 |
| test/episodes                  | 3030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.142       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0218     |
| test/info_shaping_reward_mean  | -0.147      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -27.909958  |
| test/Q_plus_P                  | -27.909958  |
| test/reward_per_eps            | -34.3       |
| test/steps                     | 121200      |
| train/episodes                 | 12120       |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.105       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0466     |
| train/info_shaping_reward_mean | -0.157      |
| train/info_shaping_reward_min  | -0.293      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.8       |
| train/steps                    | 484800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 303         |
| stats_o/mean                   | 0.20289747  |
| stats_o/std                    | 0.096120425 |
| test/episodes                  | 3040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0775      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0109     |
| test/info_shaping_reward_mean  | -0.146      |
| test/info_shaping_reward_min   | -0.19       |
| test/Q                         | -28.26521   |
| test/Q_plus_P                  | -28.26521   |
| test/reward_per_eps            | -36.9       |
| test/steps                     | 121600      |
| train/episodes                 | 12160       |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0419      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0632     |
| train/info_shaping_reward_mean | -0.156      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.3       |
| train/steps                    | 486400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 304        |
| stats_o/mean                   | 0.20287807 |
| stats_o/std                    | 0.0963102  |
| test/episodes                  | 3050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.095      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0164    |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -26.616846 |
| test/Q_plus_P                  | -26.616846 |
| test/reward_per_eps            | -36.2      |
| test/steps                     | 122000     |
| train/episodes                 | 12200      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0319     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0824    |
| train/info_shaping_reward_mean | -0.185     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 488000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.2028754  |
| stats_o/std                    | 0.09640228 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.242      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0062    |
| test/info_shaping_reward_mean  | -0.128     |
| test/info_shaping_reward_min   | -0.219     |
| test/Q                         | -24.698828 |
| test/Q_plus_P                  | -24.698828 |
| test/reward_per_eps            | -30.3      |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0325     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0867    |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 306        |
| stats_o/mean                   | 0.20286408 |
| stats_o/std                    | 0.0965005  |
| test/episodes                  | 3070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.128      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0165    |
| test/info_shaping_reward_mean  | -0.144     |
| test/info_shaping_reward_min   | -0.625     |
| test/Q                         | -22.553955 |
| test/Q_plus_P                  | -22.553955 |
| test/reward_per_eps            | -34.9      |
| test/steps                     | 122800     |
| train/episodes                 | 12280      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0412     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0596    |
| train/info_shaping_reward_mean | -0.156     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 491200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.20286818 |
| stats_o/std                    | 0.09663854 |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.07       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.042     |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -29.092497 |
| test/Q_plus_P                  | -29.092497 |
| test/reward_per_eps            | -37.2      |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0462     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0746    |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.309     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.1      |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 308        |
| stats_o/mean                   | 0.20287843 |
| stats_o/std                    | 0.09683392 |
| test/episodes                  | 3090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.135      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0163    |
| test/info_shaping_reward_mean  | -0.138     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -26.533297 |
| test/Q_plus_P                  | -26.533297 |
| test/reward_per_eps            | -34.6      |
| test/steps                     | 123600     |
| train/episodes                 | 12360      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0569     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0766    |
| train/info_shaping_reward_mean | -0.153     |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.7      |
| train/steps                    | 494400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 309        |
| stats_o/mean                   | 0.20290539 |
| stats_o/std                    | 0.09696323 |
| test/episodes                  | 3100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.268      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0122    |
| test/info_shaping_reward_mean  | -0.13      |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -23.457535 |
| test/Q_plus_P                  | -23.457535 |
| test/reward_per_eps            | -29.3      |
| test/steps                     | 124000     |
| train/episodes                 | 12400      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0894     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0404    |
| train/info_shaping_reward_mean | -0.158     |
| train/info_shaping_reward_min  | -0.297     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.4      |
| train/steps                    | 496000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 310        |
| stats_o/mean                   | 0.2028938  |
| stats_o/std                    | 0.09705205 |
| test/episodes                  | 3110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.39       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0144    |
| test/info_shaping_reward_mean  | -0.113     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -20.606524 |
| test/Q_plus_P                  | -20.606524 |
| test/reward_per_eps            | -24.4      |
| test/steps                     | 124400     |
| train/episodes                 | 12440      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.106      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0638    |
| train/info_shaping_reward_mean | -0.153     |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.8      |
| train/steps                    | 497600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.20285946 |
| stats_o/std                    | 0.09707552 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.22       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0151    |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -25.496544 |
| test/Q_plus_P                  | -25.496544 |
| test/reward_per_eps            | -31.2      |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0481     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.1       |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.3       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.1      |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 312        |
| stats_o/mean                   | 0.20285574 |
| stats_o/std                    | 0.09703511 |
| test/episodes                  | 3130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.177      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0109    |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.194     |
| test/Q                         | -23.993427 |
| test/Q_plus_P                  | -23.993427 |
| test/reward_per_eps            | -32.9      |
| test/steps                     | 125200     |
| train/episodes                 | 12520      |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0381     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.085     |
| train/info_shaping_reward_mean | -0.162     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.5      |
| train/steps                    | 500800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 313        |
| stats_o/mean                   | 0.20283121 |
| stats_o/std                    | 0.09701238 |
| test/episodes                  | 3140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.133      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0154    |
| test/info_shaping_reward_mean  | -0.156     |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -28.220156 |
| test/Q_plus_P                  | -28.220156 |
| test/reward_per_eps            | -34.7      |
| test/steps                     | 125600     |
| train/episodes                 | 12560      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0838     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.075     |
| train/info_shaping_reward_mean | -0.154     |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.6      |
| train/steps                    | 502400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 314        |
| stats_o/mean                   | 0.20282085 |
| stats_o/std                    | 0.09701803 |
| test/episodes                  | 3150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.198      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0174    |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -24.887442 |
| test/Q_plus_P                  | -24.887442 |
| test/reward_per_eps            | -32.1      |
| test/steps                     | 126000     |
| train/episodes                 | 12600      |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0406     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.099     |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 504000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 315        |
| stats_o/mean                   | 0.20279838 |
| stats_o/std                    | 0.09701573 |
| test/episodes                  | 3160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.128      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00422   |
| test/info_shaping_reward_mean  | -0.145     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -27.121233 |
| test/Q_plus_P                  | -27.121233 |
| test/reward_per_eps            | -34.9      |
| test/steps                     | 126400     |
| train/episodes                 | 12640      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.108      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0479    |
| train/info_shaping_reward_mean | -0.144     |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.7      |
| train/steps                    | 505600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.20279768  |
| stats_o/std                    | 0.096976765 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.203       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00226    |
| test/info_shaping_reward_mean  | -0.139      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -25.441628  |
| test/Q_plus_P                  | -25.441628  |
| test/reward_per_eps            | -31.9       |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.174       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0317     |
| train/info_shaping_reward_mean | -0.136      |
| train/info_shaping_reward_min  | -0.176      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33         |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 317        |
| stats_o/mean                   | 0.20278557 |
| stats_o/std                    | 0.09695754 |
| test/episodes                  | 3180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.022     |
| test/info_shaping_reward_mean  | -0.151     |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -26.414268 |
| test/Q_plus_P                  | -26.414268 |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 127200     |
| train/episodes                 | 12720      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.119      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0511    |
| train/info_shaping_reward_mean | -0.15      |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.2      |
| train/steps                    | 508800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 318         |
| stats_o/mean                   | 0.20277676  |
| stats_o/std                    | 0.096940234 |
| test/episodes                  | 3190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.203       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.005      |
| test/info_shaping_reward_mean  | -0.121      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -21.76356   |
| test/Q_plus_P                  | -21.76356   |
| test/reward_per_eps            | -31.9       |
| test/steps                     | 127600      |
| train/episodes                 | 12760       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.0825      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0487     |
| train/info_shaping_reward_mean | -0.148      |
| train/info_shaping_reward_min  | -0.187      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.7       |
| train/steps                    | 510400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 319         |
| stats_o/mean                   | 0.20276691  |
| stats_o/std                    | 0.096968174 |
| test/episodes                  | 3200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.21        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00553    |
| test/info_shaping_reward_mean  | -0.124      |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -22.908812  |
| test/Q_plus_P                  | -22.908812  |
| test/reward_per_eps            | -31.6       |
| test/steps                     | 128000      |
| train/episodes                 | 12800       |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0837      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.064      |
| train/info_shaping_reward_mean | -0.154      |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.6       |
| train/steps                    | 512000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 320        |
| stats_o/mean                   | 0.20271452 |
| stats_o/std                    | 0.09705418 |
| test/episodes                  | 3210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.04       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0288    |
| test/info_shaping_reward_mean  | -0.16      |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -29.350103 |
| test/Q_plus_P                  | -29.350103 |
| test/reward_per_eps            | -38.4      |
| test/steps                     | 128400     |
| train/episodes                 | 12840      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0906     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0369    |
| train/info_shaping_reward_mean | -0.16      |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.4      |
| train/steps                    | 513600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 321        |
| stats_o/mean                   | 0.20272076 |
| stats_o/std                    | 0.09706269 |
| test/episodes                  | 3220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0825     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00454   |
| test/info_shaping_reward_mean  | -0.158     |
| test/info_shaping_reward_min   | -0.567     |
| test/Q                         | -26.449697 |
| test/Q_plus_P                  | -26.449697 |
| test/reward_per_eps            | -36.7      |
| test/steps                     | 128800     |
| train/episodes                 | 12880      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.106      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0762    |
| train/info_shaping_reward_mean | -0.16      |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.8      |
| train/steps                    | 515200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 322         |
| stats_o/mean                   | 0.20271735  |
| stats_o/std                    | 0.097064115 |
| test/episodes                  | 3230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0875      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00296    |
| test/info_shaping_reward_mean  | -0.149      |
| test/info_shaping_reward_min   | -0.203      |
| test/Q                         | -28.173456  |
| test/Q_plus_P                  | -28.173456  |
| test/reward_per_eps            | -36.5       |
| test/steps                     | 129200      |
| train/episodes                 | 12920       |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0794      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0626     |
| train/info_shaping_reward_mean | -0.163      |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.8       |
| train/steps                    | 516800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 323         |
| stats_o/mean                   | 0.20271139  |
| stats_o/std                    | 0.097049035 |
| test/episodes                  | 3240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.14        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00926    |
| test/info_shaping_reward_mean  | -0.144      |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -26.354897  |
| test/Q_plus_P                  | -26.354897  |
| test/reward_per_eps            | -34.4       |
| test/steps                     | 129600      |
| train/episodes                 | 12960       |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.065       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0695     |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.4       |
| train/steps                    | 518400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.20268054  |
| stats_o/std                    | 0.097056836 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.133       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0323     |
| test/info_shaping_reward_mean  | -0.152      |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -25.397728  |
| test/Q_plus_P                  | -25.397728  |
| test/reward_per_eps            | -34.7       |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0544      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0919     |
| train/info_shaping_reward_mean | -0.158      |
| train/info_shaping_reward_min  | -0.223      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.8       |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 325        |
| stats_o/mean                   | 0.20265718 |
| stats_o/std                    | 0.09708858 |
| test/episodes                  | 3260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.26       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00464   |
| test/info_shaping_reward_mean  | -0.122     |
| test/info_shaping_reward_min   | -0.192     |
| test/Q                         | -21.14265  |
| test/Q_plus_P                  | -21.14265  |
| test/reward_per_eps            | -29.6      |
| test/steps                     | 130400     |
| train/episodes                 | 13040      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.095      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0442    |
| train/info_shaping_reward_mean | -0.15      |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.2      |
| train/steps                    | 521600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 326        |
| stats_o/mean                   | 0.2026735  |
| stats_o/std                    | 0.09716606 |
| test/episodes                  | 3270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.168      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0147    |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -24.845448 |
| test/Q_plus_P                  | -24.845448 |
| test/reward_per_eps            | -33.3      |
| test/steps                     | 130800     |
| train/episodes                 | 13080      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.176      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0225    |
| train/info_shaping_reward_mean | -0.134     |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33        |
| train/steps                    | 523200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 327        |
| stats_o/mean                   | 0.20267355 |
| stats_o/std                    | 0.09715286 |
| test/episodes                  | 3280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.14       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0267    |
| test/info_shaping_reward_mean  | -0.146     |
| test/info_shaping_reward_min   | -0.212     |
| test/Q                         | -25.311317 |
| test/Q_plus_P                  | -25.311317 |
| test/reward_per_eps            | -34.4      |
| test/steps                     | 131200     |
| train/episodes                 | 13120      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.126      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.031     |
| train/info_shaping_reward_mean | -0.14      |
| train/info_shaping_reward_min  | -0.2       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35        |
| train/steps                    | 524800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 328        |
| stats_o/mean                   | 0.20266113 |
| stats_o/std                    | 0.09718252 |
| test/episodes                  | 3290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.31       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0125    |
| test/info_shaping_reward_mean  | -0.123     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -21.179802 |
| test/Q_plus_P                  | -21.179802 |
| test/reward_per_eps            | -27.6      |
| test/steps                     | 131600     |
| train/episodes                 | 13160      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0994     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0512    |
| train/info_shaping_reward_mean | -0.151     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36        |
| train/steps                    | 526400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 329        |
| stats_o/mean                   | 0.20264542 |
| stats_o/std                    | 0.09737007 |
| test/episodes                  | 3300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.318      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00948   |
| test/info_shaping_reward_mean  | -0.12      |
| test/info_shaping_reward_min   | -0.212     |
| test/Q                         | -21.263855 |
| test/Q_plus_P                  | -21.263855 |
| test/reward_per_eps            | -27.3      |
| test/steps                     | 132000     |
| train/episodes                 | 13200      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0762     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0555    |
| train/info_shaping_reward_mean | -0.172     |
| train/info_shaping_reward_min  | -0.373     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37        |
| train/steps                    | 528000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 330        |
| stats_o/mean                   | 0.20261379 |
| stats_o/std                    | 0.09750411 |
| test/episodes                  | 3310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.212      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00725   |
| test/info_shaping_reward_mean  | -0.14      |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -25.39361  |
| test/Q_plus_P                  | -25.39361  |
| test/reward_per_eps            | -31.5      |
| test/steps                     | 132400     |
| train/episodes                 | 13240      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.119      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0565    |
| train/info_shaping_reward_mean | -0.143     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.2      |
| train/steps                    | 529600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 331        |
| stats_o/mean                   | 0.20260462 |
| stats_o/std                    | 0.09749594 |
| test/episodes                  | 3320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.163      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00433   |
| test/info_shaping_reward_mean  | -0.138     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -23.36329  |
| test/Q_plus_P                  | -23.36329  |
| test/reward_per_eps            | -33.5      |
| test/steps                     | 132800     |
| train/episodes                 | 13280      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.133      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0582    |
| train/info_shaping_reward_mean | -0.147     |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.7      |
| train/steps                    | 531200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 332        |
| stats_o/mean                   | 0.20256172 |
| stats_o/std                    | 0.09756072 |
| test/episodes                  | 3330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.28       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00514   |
| test/info_shaping_reward_mean  | -0.128     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -22.465172 |
| test/Q_plus_P                  | -22.465172 |
| test/reward_per_eps            | -28.8      |
| test/steps                     | 133200     |
| train/episodes                 | 13320      |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.106      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0558    |
| train/info_shaping_reward_mean | -0.156     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.8      |
| train/steps                    | 532800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 333        |
| stats_o/mean                   | 0.20253411 |
| stats_o/std                    | 0.09761481 |
| test/episodes                  | 3340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.205      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0203    |
| test/info_shaping_reward_mean  | -0.137     |
| test/info_shaping_reward_min   | -0.213     |
| test/Q                         | -24.31944  |
| test/Q_plus_P                  | -24.31944  |
| test/reward_per_eps            | -31.8      |
| test/steps                     | 133600     |
| train/episodes                 | 13360      |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.103      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0442    |
| train/info_shaping_reward_mean | -0.138     |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.9      |
| train/steps                    | 534400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 334        |
| stats_o/mean                   | 0.20250933 |
| stats_o/std                    | 0.0977876  |
| test/episodes                  | 3350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.448      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.01      |
| test/info_shaping_reward_mean  | -0.0884    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -13.267697 |
| test/Q_plus_P                  | -13.267697 |
| test/reward_per_eps            | -22.1      |
| test/steps                     | 134000     |
| train/episodes                 | 13400      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.188      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0339    |
| train/info_shaping_reward_mean | -0.144     |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.5      |
| train/steps                    | 536000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.20248282 |
| stats_o/std                    | 0.09782506 |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.26       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0142    |
| test/info_shaping_reward_mean  | -0.159     |
| test/info_shaping_reward_min   | -0.424     |
| test/Q                         | -21.397566 |
| test/Q_plus_P                  | -21.397566 |
| test/reward_per_eps            | -29.6      |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.232      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.021     |
| train/info_shaping_reward_mean | -0.136     |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.7      |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.2024711   |
| stats_o/std                    | 0.097918294 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.367       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00993    |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -1.32       |
| test/Q                         | -17.892387  |
| test/Q_plus_P                  | -17.892387  |
| test/reward_per_eps            | -25.3       |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.142       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0741     |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.3       |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 337        |
| stats_o/mean                   | 0.2024553  |
| stats_o/std                    | 0.09801869 |
| test/episodes                  | 3380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.335      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.008     |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.588     |
| test/Q                         | -20.443449 |
| test/Q_plus_P                  | -20.443449 |
| test/reward_per_eps            | -26.6      |
| test/steps                     | 135200     |
| train/episodes                 | 13520      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.118      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0428    |
| train/info_shaping_reward_mean | -0.176     |
| train/info_shaping_reward_min  | -0.404     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.3      |
| train/steps                    | 540800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.2024423  |
| stats_o/std                    | 0.09819299 |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.19       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0132    |
| test/info_shaping_reward_mean  | -0.17      |
| test/info_shaping_reward_min   | -0.587     |
| test/Q                         | -22.944561 |
| test/Q_plus_P                  | -22.944561 |
| test/reward_per_eps            | -32.4      |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.104      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0322    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.8      |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.202422    |
| stats_o/std                    | 0.098224014 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0375      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0106     |
| test/info_shaping_reward_mean  | -0.161      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -25.098162  |
| test/Q_plus_P                  | -25.098162  |
| test/reward_per_eps            | -38.5       |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.113       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0548     |
| train/info_shaping_reward_mean | -0.163      |
| train/info_shaping_reward_min  | -0.299      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.5       |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 340        |
| stats_o/mean                   | 0.20240988 |
| stats_o/std                    | 0.09826763 |
| test/episodes                  | 3410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.205      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00836   |
| test/info_shaping_reward_mean  | -0.141     |
| test/info_shaping_reward_min   | -0.202     |
| test/Q                         | -23.917574 |
| test/Q_plus_P                  | -23.917574 |
| test/reward_per_eps            | -31.8      |
| test/steps                     | 136400     |
| train/episodes                 | 13640      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.145      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0274    |
| train/info_shaping_reward_mean | -0.152     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.2      |
| train/steps                    | 545600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 341         |
| stats_o/mean                   | 0.20240277  |
| stats_o/std                    | 0.098338224 |
| test/episodes                  | 3420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.133       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0119     |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.601      |
| test/Q                         | -25.96174   |
| test/Q_plus_P                  | -25.96174   |
| test/reward_per_eps            | -34.7       |
| test/steps                     | 136800      |
| train/episodes                 | 13680       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.22        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0315     |
| train/info_shaping_reward_mean | -0.141      |
| train/info_shaping_reward_min  | -0.273      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.2       |
| train/steps                    | 547200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 342        |
| stats_o/mean                   | 0.20238756 |
| stats_o/std                    | 0.09843604 |
| test/episodes                  | 3430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.36       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0114    |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.837     |
| test/Q                         | -16.887571 |
| test/Q_plus_P                  | -16.887571 |
| test/reward_per_eps            | -25.6      |
| test/steps                     | 137200     |
| train/episodes                 | 13720      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.177      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0294    |
| train/info_shaping_reward_mean | -0.169     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.9      |
| train/steps                    | 548800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 343        |
| stats_o/mean                   | 0.2023562  |
| stats_o/std                    | 0.09847087 |
| test/episodes                  | 3440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.255      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00397   |
| test/info_shaping_reward_mean  | -0.132     |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -20.962358 |
| test/Q_plus_P                  | -20.962358 |
| test/reward_per_eps            | -29.8      |
| test/steps                     | 137600     |
| train/episodes                 | 13760      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.147      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0393    |
| train/info_shaping_reward_mean | -0.149     |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.1      |
| train/steps                    | 550400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 344        |
| stats_o/mean                   | 0.20234217 |
| stats_o/std                    | 0.09847684 |
| test/episodes                  | 3450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.138      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00827   |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -23.210503 |
| test/Q_plus_P                  | -23.210503 |
| test/reward_per_eps            | -34.5      |
| test/steps                     | 138000     |
| train/episodes                 | 13800      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.188      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0199    |
| train/info_shaping_reward_mean | -0.129     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.5      |
| train/steps                    | 552000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 345        |
| stats_o/mean                   | 0.20233786 |
| stats_o/std                    | 0.09849719 |
| test/episodes                  | 3460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.475      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00493   |
| test/info_shaping_reward_mean  | -0.084     |
| test/info_shaping_reward_min   | -0.201     |
| test/Q                         | -10.780662 |
| test/Q_plus_P                  | -10.780662 |
| test/reward_per_eps            | -21        |
| test/steps                     | 138400     |
| train/episodes                 | 13840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.219      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0105    |
| train/info_shaping_reward_mean | -0.132     |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.2      |
| train/steps                    | 553600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 346        |
| stats_o/mean                   | 0.2023404  |
| stats_o/std                    | 0.09854854 |
| test/episodes                  | 3470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.333      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000158  |
| test/info_shaping_reward_mean  | -0.106     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -16.216032 |
| test/Q_plus_P                  | -16.216032 |
| test/reward_per_eps            | -26.7      |
| test/steps                     | 138800     |
| train/episodes                 | 13880      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.188      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0232    |
| train/info_shaping_reward_mean | -0.129     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.5      |
| train/steps                    | 555200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 347        |
| stats_o/mean                   | 0.20230989 |
| stats_o/std                    | 0.09862427 |
| test/episodes                  | 3480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.47       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00279   |
| test/info_shaping_reward_mean  | -0.0912    |
| test/info_shaping_reward_min   | -0.223     |
| test/Q                         | -12.836173 |
| test/Q_plus_P                  | -12.836173 |
| test/reward_per_eps            | -21.2      |
| test/steps                     | 139200     |
| train/episodes                 | 13920      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.165      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0357    |
| train/info_shaping_reward_mean | -0.142     |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.4      |
| train/steps                    | 556800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 348        |
| stats_o/mean                   | 0.2022909  |
| stats_o/std                    | 0.09862863 |
| test/episodes                  | 3490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.242      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0044    |
| test/info_shaping_reward_mean  | -0.124     |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -19.246098 |
| test/Q_plus_P                  | -19.246098 |
| test/reward_per_eps            | -30.3      |
| test/steps                     | 139600     |
| train/episodes                 | 13960      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.159      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0212    |
| train/info_shaping_reward_mean | -0.143     |
| train/info_shaping_reward_min  | -0.297     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.6      |
| train/steps                    | 558400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 349        |
| stats_o/mean                   | 0.20231633 |
| stats_o/std                    | 0.09877138 |
| test/episodes                  | 3500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.355      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00322   |
| test/info_shaping_reward_mean  | -0.131     |
| test/info_shaping_reward_min   | -0.681     |
| test/Q                         | -14.797515 |
| test/Q_plus_P                  | -14.797515 |
| test/reward_per_eps            | -25.8      |
| test/steps                     | 140000     |
| train/episodes                 | 14000      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.199      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.035     |
| train/info_shaping_reward_mean | -0.153     |
| train/info_shaping_reward_min  | -0.375     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32        |
| train/steps                    | 560000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 350         |
| stats_o/mean                   | 0.20229714  |
| stats_o/std                    | 0.098771006 |
| test/episodes                  | 3510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.333       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0074     |
| test/info_shaping_reward_mean  | -0.1        |
| test/info_shaping_reward_min   | -0.189      |
| test/Q                         | -14.546012  |
| test/Q_plus_P                  | -14.546012  |
| test/reward_per_eps            | -26.7       |
| test/steps                     | 140400      |
| train/episodes                 | 14040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.252       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0181     |
| train/info_shaping_reward_mean | -0.124      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.9       |
| train/steps                    | 561600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 351        |
| stats_o/mean                   | 0.20228536 |
| stats_o/std                    | 0.09884092 |
| test/episodes                  | 3520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.435      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0147    |
| test/info_shaping_reward_mean  | -0.095     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -12.793926 |
| test/Q_plus_P                  | -12.793926 |
| test/reward_per_eps            | -22.6      |
| test/steps                     | 140800     |
| train/episodes                 | 14080      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.146      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0314    |
| train/info_shaping_reward_mean | -0.139     |
| train/info_shaping_reward_min  | -0.286     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.1      |
| train/steps                    | 563200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 352        |
| stats_o/mean                   | 0.20229238 |
| stats_o/std                    | 0.09892637 |
| test/episodes                  | 3530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.388      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0071    |
| test/info_shaping_reward_mean  | -0.125     |
| test/info_shaping_reward_min   | -0.513     |
| test/Q                         | -16.191732 |
| test/Q_plus_P                  | -16.191732 |
| test/reward_per_eps            | -24.5      |
| test/steps                     | 141200     |
| train/episodes                 | 14120      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.205      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0196    |
| train/info_shaping_reward_mean | -0.147     |
| train/info_shaping_reward_min  | -0.291     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.8      |
| train/steps                    | 564800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 353        |
| stats_o/mean                   | 0.20227438 |
| stats_o/std                    | 0.09895237 |
| test/episodes                  | 3540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.323      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00786   |
| test/info_shaping_reward_mean  | -0.104     |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -12.997371 |
| test/Q_plus_P                  | -12.997371 |
| test/reward_per_eps            | -27.1      |
| test/steps                     | 141600     |
| train/episodes                 | 14160      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.149      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.021     |
| train/info_shaping_reward_mean | -0.144     |
| train/info_shaping_reward_min  | -0.28      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34        |
| train/steps                    | 566400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 354        |
| stats_o/mean                   | 0.20225756 |
| stats_o/std                    | 0.099052   |
| test/episodes                  | 3550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.335      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0145    |
| test/info_shaping_reward_mean  | -0.111     |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -15.526123 |
| test/Q_plus_P                  | -15.526123 |
| test/reward_per_eps            | -26.6      |
| test/steps                     | 142000     |
| train/episodes                 | 14200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.263      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0155    |
| train/info_shaping_reward_mean | -0.145     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.5      |
| train/steps                    | 568000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 355        |
| stats_o/mean                   | 0.20224267 |
| stats_o/std                    | 0.09910807 |
| test/episodes                  | 3560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.38       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00412   |
| test/info_shaping_reward_mean  | -0.102     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -14.398746 |
| test/Q_plus_P                  | -14.398746 |
| test/reward_per_eps            | -24.8      |
| test/steps                     | 142400     |
| train/episodes                 | 14240      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.237      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.016     |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.5      |
| train/steps                    | 569600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 356        |
| stats_o/mean                   | 0.20221844 |
| stats_o/std                    | 0.0991078  |
| test/episodes                  | 3570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.357      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00065   |
| test/info_shaping_reward_mean  | -0.101     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -14.54728  |
| test/Q_plus_P                  | -14.54728  |
| test/reward_per_eps            | -25.7      |
| test/steps                     | 142800     |
| train/episodes                 | 14280      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.202      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0204    |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.9      |
| train/steps                    | 571200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 357        |
| stats_o/mean                   | 0.20220375 |
| stats_o/std                    | 0.09912409 |
| test/episodes                  | 3580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.32       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0177    |
| test/info_shaping_reward_mean  | -0.107     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -12.62711  |
| test/Q_plus_P                  | -12.62711  |
| test/reward_per_eps            | -27.2      |
| test/steps                     | 143200     |
| train/episodes                 | 14320      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.227      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0262    |
| train/info_shaping_reward_mean | -0.14      |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.9      |
| train/steps                    | 572800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 358        |
| stats_o/mean                   | 0.2022209  |
| stats_o/std                    | 0.09917878 |
| test/episodes                  | 3590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.345      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00958   |
| test/info_shaping_reward_mean  | -0.108     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -14.977992 |
| test/Q_plus_P                  | -14.977992 |
| test/reward_per_eps            | -26.2      |
| test/steps                     | 143600     |
| train/episodes                 | 14360      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.193      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0241    |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.3      |
| train/steps                    | 574400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 359        |
| stats_o/mean                   | 0.20221314 |
| stats_o/std                    | 0.09930966 |
| test/episodes                  | 3600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0197    |
| test/info_shaping_reward_mean  | -0.117     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -14.593598 |
| test/Q_plus_P                  | -14.593598 |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 144000     |
| train/episodes                 | 14400      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.19       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0201    |
| train/info_shaping_reward_mean | -0.15      |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.4      |
| train/steps                    | 576000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 360        |
| stats_o/mean                   | 0.20220594 |
| stats_o/std                    | 0.09936404 |
| test/episodes                  | 3610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.4        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0137    |
| test/info_shaping_reward_mean  | -0.108     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -15.480256 |
| test/Q_plus_P                  | -15.480256 |
| test/reward_per_eps            | -24        |
| test/steps                     | 144400     |
| train/episodes                 | 14440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.251      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0153    |
| train/info_shaping_reward_mean | -0.124     |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.9      |
| train/steps                    | 577600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 361         |
| stats_o/mean                   | 0.2021947   |
| stats_o/std                    | 0.099436216 |
| test/episodes                  | 3620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.333       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00564    |
| test/info_shaping_reward_mean  | -0.107      |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -16.647045  |
| test/Q_plus_P                  | -16.647045  |
| test/reward_per_eps            | -26.7       |
| test/steps                     | 144800      |
| train/episodes                 | 14480       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.148       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0305     |
| train/info_shaping_reward_mean | -0.155      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.1       |
| train/steps                    | 579200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 362        |
| stats_o/mean                   | 0.20219654 |
| stats_o/std                    | 0.09943735 |
| test/episodes                  | 3630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.247      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0119    |
| test/info_shaping_reward_mean  | -0.126     |
| test/info_shaping_reward_min   | -0.196     |
| test/Q                         | -19.548386 |
| test/Q_plus_P                  | -19.548386 |
| test/reward_per_eps            | -30.1      |
| test/steps                     | 145200     |
| train/episodes                 | 14520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.234      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00835   |
| train/info_shaping_reward_mean | -0.124     |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.6      |
| train/steps                    | 580800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.20219201  |
| stats_o/std                    | 0.099446155 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.415       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0125     |
| test/info_shaping_reward_mean  | -0.103      |
| test/info_shaping_reward_min   | -0.202      |
| test/Q                         | -13.943018  |
| test/Q_plus_P                  | -13.943018  |
| test/reward_per_eps            | -23.4       |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.201       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0229     |
| train/info_shaping_reward_mean | -0.136      |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32         |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 364        |
| stats_o/mean                   | 0.20218167 |
| stats_o/std                    | 0.09947937 |
| test/episodes                  | 3650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.228      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0164    |
| test/info_shaping_reward_mean  | -0.106     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -14.617744 |
| test/Q_plus_P                  | -14.617744 |
| test/reward_per_eps            | -30.9      |
| test/steps                     | 146000     |
| train/episodes                 | 14600      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.146      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.045     |
| train/info_shaping_reward_mean | -0.145     |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.2      |
| train/steps                    | 584000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.20216864  |
| stats_o/std                    | 0.09954578  |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.217       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00732    |
| test/info_shaping_reward_mean  | -0.112      |
| test/info_shaping_reward_min   | -0.218      |
| test/Q                         | -15.0430765 |
| test/Q_plus_P                  | -15.0430765 |
| test/reward_per_eps            | -31.3       |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.187       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0457     |
| train/info_shaping_reward_mean | -0.13       |
| train/info_shaping_reward_min  | -0.203      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.5       |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 366        |
| stats_o/mean                   | 0.20216042 |
| stats_o/std                    | 0.09961645 |
| test/episodes                  | 3670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00998   |
| test/info_shaping_reward_mean  | -0.143     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -20.445717 |
| test/Q_plus_P                  | -20.445717 |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 146800     |
| train/episodes                 | 14680      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.163      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0309    |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.5      |
| train/steps                    | 587200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 367        |
| stats_o/mean                   | 0.20216873 |
| stats_o/std                    | 0.09970147 |
| test/episodes                  | 3680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.133      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0289    |
| test/info_shaping_reward_mean  | -0.154     |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -23.145603 |
| test/Q_plus_P                  | -23.145603 |
| test/reward_per_eps            | -34.7      |
| test/steps                     | 147200     |
| train/episodes                 | 14720      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.143      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0246    |
| train/info_shaping_reward_mean | -0.139     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.3      |
| train/steps                    | 588800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 368         |
| stats_o/mean                   | 0.20218426  |
| stats_o/std                    | 0.099741526 |
| test/episodes                  | 3690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.26        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0285     |
| test/info_shaping_reward_mean  | -0.104      |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -12.232361  |
| test/Q_plus_P                  | -12.232361  |
| test/reward_per_eps            | -29.6       |
| test/steps                     | 147600      |
| train/episodes                 | 14760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.233       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0154     |
| train/info_shaping_reward_mean | -0.122      |
| train/info_shaping_reward_min  | -0.185      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.7       |
| train/steps                    | 590400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 369        |
| stats_o/mean                   | 0.20216395 |
| stats_o/std                    | 0.09978346 |
| test/episodes                  | 3700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.35       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00903   |
| test/info_shaping_reward_mean  | -0.108     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -13.344213 |
| test/Q_plus_P                  | -13.344213 |
| test/reward_per_eps            | -26        |
| test/steps                     | 148000     |
| train/episodes                 | 14800      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.191      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.02      |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.4      |
| train/steps                    | 592000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 370        |
| stats_o/mean                   | 0.20216638 |
| stats_o/std                    | 0.09980529 |
| test/episodes                  | 3710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.25       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0241    |
| test/info_shaping_reward_mean  | -0.109     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -14.555059 |
| test/Q_plus_P                  | -14.555059 |
| test/reward_per_eps            | -30        |
| test/steps                     | 148400     |
| train/episodes                 | 14840      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.12       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0473    |
| train/info_shaping_reward_mean | -0.147     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.2      |
| train/steps                    | 593600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 371        |
| stats_o/mean                   | 0.20217001 |
| stats_o/std                    | 0.09982648 |
| test/episodes                  | 3720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.258      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00832   |
| test/info_shaping_reward_mean  | -0.127     |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -15.155525 |
| test/Q_plus_P                  | -15.155525 |
| test/reward_per_eps            | -29.7      |
| test/steps                     | 148800     |
| train/episodes                 | 14880      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.163      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0188    |
| train/info_shaping_reward_mean | -0.139     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.5      |
| train/steps                    | 595200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 372        |
| stats_o/mean                   | 0.20217367 |
| stats_o/std                    | 0.09983845 |
| test/episodes                  | 3730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.405      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00743   |
| test/info_shaping_reward_mean  | -0.1       |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -11.063307 |
| test/Q_plus_P                  | -11.063307 |
| test/reward_per_eps            | -23.8      |
| test/steps                     | 149200     |
| train/episodes                 | 14920      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.234      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0331    |
| train/info_shaping_reward_mean | -0.129     |
| train/info_shaping_reward_min  | -0.275     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.6      |
| train/steps                    | 596800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 373         |
| stats_o/mean                   | 0.20217726  |
| stats_o/std                    | 0.099810556 |
| test/episodes                  | 3740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.29        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00736    |
| test/info_shaping_reward_mean  | -0.115      |
| test/info_shaping_reward_min   | -0.199      |
| test/Q                         | -14.879963  |
| test/Q_plus_P                  | -14.879963  |
| test/reward_per_eps            | -28.4       |
| test/steps                     | 149600      |
| train/episodes                 | 14960       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.233       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0299     |
| train/info_shaping_reward_mean | -0.123      |
| train/info_shaping_reward_min  | -0.2        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.7       |
| train/steps                    | 598400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 374         |
| stats_o/mean                   | 0.20216274  |
| stats_o/std                    | 0.09981856  |
| test/episodes                  | 3750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.475       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00745    |
| test/info_shaping_reward_mean  | -0.0946     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -10.4100485 |
| test/Q_plus_P                  | -10.4100485 |
| test/reward_per_eps            | -21         |
| test/steps                     | 150000      |
| train/episodes                 | 15000       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.246       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0362     |
| train/info_shaping_reward_mean | -0.128      |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.1       |
| train/steps                    | 600000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.20217542  |
| stats_o/std                    | 0.099936895 |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.24        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00799    |
| test/info_shaping_reward_mean  | -0.156      |
| test/info_shaping_reward_min   | -0.649      |
| test/Q                         | -16.493862  |
| test/Q_plus_P                  | -16.493862  |
| test/reward_per_eps            | -30.4       |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.188       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0255     |
| train/info_shaping_reward_mean | -0.126      |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.5       |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.2021774  |
| stats_o/std                    | 0.09999743 |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.347      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00959   |
| test/info_shaping_reward_mean  | -0.111     |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -12.127803 |
| test/Q_plus_P                  | -12.127803 |
| test/reward_per_eps            | -26.1      |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.196      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0151    |
| train/info_shaping_reward_mean | -0.134     |
| train/info_shaping_reward_min  | -0.293     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.1      |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.20218424  |
| stats_o/std                    | 0.099998504 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.128       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0157     |
| test/info_shaping_reward_mean  | -0.126      |
| test/info_shaping_reward_min   | -0.198      |
| test/Q                         | -12.732314  |
| test/Q_plus_P                  | -12.732314  |
| test/reward_per_eps            | -34.9       |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.27        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0128     |
| train/info_shaping_reward_mean | -0.121      |
| train/info_shaping_reward_min  | -0.195      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.2       |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 378        |
| stats_o/mean                   | 0.20218438 |
| stats_o/std                    | 0.09999106 |
| test/episodes                  | 3790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.223      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00774   |
| test/info_shaping_reward_mean  | -0.13      |
| test/info_shaping_reward_min   | -0.204     |
| test/Q                         | -15.234642 |
| test/Q_plus_P                  | -15.234642 |
| test/reward_per_eps            | -31.1      |
| test/steps                     | 151600     |
| train/episodes                 | 15160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.284      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0163    |
| train/info_shaping_reward_mean | -0.117     |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.6      |
| train/steps                    | 606400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 379        |
| stats_o/mean                   | 0.20216037 |
| stats_o/std                    | 0.10000105 |
| test/episodes                  | 3800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.21       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00607   |
| test/info_shaping_reward_mean  | -0.136     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -14.777652 |
| test/Q_plus_P                  | -14.777652 |
| test/reward_per_eps            | -31.6      |
| test/steps                     | 152000     |
| train/episodes                 | 15200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.205      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0148    |
| train/info_shaping_reward_mean | -0.14      |
| train/info_shaping_reward_min  | -0.304     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.8      |
| train/steps                    | 608000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 380        |
| stats_o/mean                   | 0.20215136 |
| stats_o/std                    | 0.10004467 |
| test/episodes                  | 3810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.25       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0105    |
| test/info_shaping_reward_mean  | -0.115     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -11.48179  |
| test/Q_plus_P                  | -11.48179  |
| test/reward_per_eps            | -30        |
| test/steps                     | 152400     |
| train/episodes                 | 15240      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.156      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0235    |
| train/info_shaping_reward_mean | -0.138     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.8      |
| train/steps                    | 609600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 381        |
| stats_o/mean                   | 0.20215489 |
| stats_o/std                    | 0.10008559 |
| test/episodes                  | 3820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.333      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00265   |
| test/info_shaping_reward_mean  | -0.1       |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -9.557084  |
| test/Q_plus_P                  | -9.557084  |
| test/reward_per_eps            | -26.7      |
| test/steps                     | 152800     |
| train/episodes                 | 15280      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.206      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.025     |
| train/info_shaping_reward_mean | -0.135     |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.8      |
| train/steps                    | 611200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.20216383 |
| stats_o/std                    | 0.1000809  |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0375     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0151    |
| test/info_shaping_reward_mean  | -0.155     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -19.529135 |
| test/Q_plus_P                  | -19.529135 |
| test/reward_per_eps            | -38.5      |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.223      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0319    |
| train/info_shaping_reward_mean | -0.133     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.1      |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 383        |
| stats_o/mean                   | 0.20216976 |
| stats_o/std                    | 0.10007007 |
| test/episodes                  | 3840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.383      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00315   |
| test/info_shaping_reward_mean  | -0.105     |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -10.653892 |
| test/Q_plus_P                  | -10.653892 |
| test/reward_per_eps            | -24.7      |
| test/steps                     | 153600     |
| train/episodes                 | 15360      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.226      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0373    |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31        |
| train/steps                    | 614400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 384        |
| stats_o/mean                   | 0.20216946 |
| stats_o/std                    | 0.10005113 |
| test/episodes                  | 3850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.26       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00969   |
| test/info_shaping_reward_mean  | -0.129     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -17.422718 |
| test/Q_plus_P                  | -17.422718 |
| test/reward_per_eps            | -29.6      |
| test/steps                     | 154000     |
| train/episodes                 | 15400      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.184      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0248    |
| train/info_shaping_reward_mean | -0.139     |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.6      |
| train/steps                    | 616000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 385        |
| stats_o/mean                   | 0.20216362 |
| stats_o/std                    | 0.10005563 |
| test/episodes                  | 3860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.328      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00805   |
| test/info_shaping_reward_mean  | -0.115     |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -13.241033 |
| test/Q_plus_P                  | -13.241033 |
| test/reward_per_eps            | -26.9      |
| test/steps                     | 154400     |
| train/episodes                 | 15440      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.145      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0251    |
| train/info_shaping_reward_mean | -0.147     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.2      |
| train/steps                    | 617600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 386        |
| stats_o/mean                   | 0.20214191 |
| stats_o/std                    | 0.10002954 |
| test/episodes                  | 3870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.245      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00848   |
| test/info_shaping_reward_mean  | -0.127     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -14.745818 |
| test/Q_plus_P                  | -14.745818 |
| test/reward_per_eps            | -30.2      |
| test/steps                     | 154800     |
| train/episodes                 | 15480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.169      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0204    |
| train/info_shaping_reward_mean | -0.137     |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.2      |
| train/steps                    | 619200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.20214027  |
| stats_o/std                    | 0.099997066 |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.345       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0142     |
| test/info_shaping_reward_mean  | -0.116      |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -12.432527  |
| test/Q_plus_P                  | -12.432527  |
| test/reward_per_eps            | -26.2       |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.201       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0439     |
| train/info_shaping_reward_mean | -0.129      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.9       |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 388        |
| stats_o/mean                   | 0.20213552 |
| stats_o/std                    | 0.09999465 |
| test/episodes                  | 3890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.405      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00841   |
| test/info_shaping_reward_mean  | -0.0967    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -8.833839  |
| test/Q_plus_P                  | -8.833839  |
| test/reward_per_eps            | -23.8      |
| test/steps                     | 155600     |
| train/episodes                 | 15560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.26       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0149    |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.6      |
| train/steps                    | 622400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 389        |
| stats_o/mean                   | 0.20215328 |
| stats_o/std                    | 0.09999939 |
| test/episodes                  | 3900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.26       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0203    |
| test/info_shaping_reward_mean  | -0.122     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -13.384839 |
| test/Q_plus_P                  | -13.384839 |
| test/reward_per_eps            | -29.6      |
| test/steps                     | 156000     |
| train/episodes                 | 15600      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.186      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0262    |
| train/info_shaping_reward_mean | -0.131     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.5      |
| train/steps                    | 624000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 390         |
| stats_o/mean                   | 0.20215368  |
| stats_o/std                    | 0.099944614 |
| test/episodes                  | 3910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.403       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0139     |
| test/info_shaping_reward_mean  | -0.108      |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -10.953283  |
| test/Q_plus_P                  | -10.953283  |
| test/reward_per_eps            | -23.9       |
| test/steps                     | 156400      |
| train/episodes                 | 15640       |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.182       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0544     |
| train/info_shaping_reward_mean | -0.134      |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.7       |
| train/steps                    | 625600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.20214975  |
| stats_o/std                    | 0.099890254 |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.172       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0255     |
| test/info_shaping_reward_mean  | -0.135      |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -15.162561  |
| test/Q_plus_P                  | -15.162561  |
| test/reward_per_eps            | -33.1       |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.219       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0414     |
| train/info_shaping_reward_mean | -0.132      |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.2       |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 392        |
| stats_o/mean                   | 0.20214096 |
| stats_o/std                    | 0.0998471  |
| test/episodes                  | 3930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.205      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00349   |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -15.306882 |
| test/Q_plus_P                  | -15.306882 |
| test/reward_per_eps            | -31.8      |
| test/steps                     | 157200     |
| train/episodes                 | 15720      |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.151      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0427    |
| train/info_shaping_reward_mean | -0.138     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34        |
| train/steps                    | 628800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 393         |
| stats_o/mean                   | 0.20213908  |
| stats_o/std                    | 0.099837035 |
| test/episodes                  | 3940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.492       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0143     |
| test/info_shaping_reward_mean  | -0.0708     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -3.4715824  |
| test/Q_plus_P                  | -3.4715824  |
| test/reward_per_eps            | -20.3       |
| test/steps                     | 157600      |
| train/episodes                 | 15760       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.194       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0245     |
| train/info_shaping_reward_mean | -0.129      |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.2       |
| train/steps                    | 630400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 394        |
| stats_o/mean                   | 0.20213252 |
| stats_o/std                    | 0.09977436 |
| test/episodes                  | 3950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.47       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00493   |
| test/info_shaping_reward_mean  | -0.0933    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -7.319779  |
| test/Q_plus_P                  | -7.319779  |
| test/reward_per_eps            | -21.2      |
| test/steps                     | 158000     |
| train/episodes                 | 15800      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.144      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0438    |
| train/info_shaping_reward_mean | -0.14      |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.2      |
| train/steps                    | 632000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 395         |
| stats_o/mean                   | 0.20214164  |
| stats_o/std                    | 0.099727504 |
| test/episodes                  | 3960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.378       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0137     |
| test/info_shaping_reward_mean  | -0.111      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -13.419507  |
| test/Q_plus_P                  | -13.419507  |
| test/reward_per_eps            | -24.9       |
| test/steps                     | 158400      |
| train/episodes                 | 15840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.331       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0139     |
| train/info_shaping_reward_mean | -0.115      |
| train/info_shaping_reward_min  | -0.214      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.8       |
| train/steps                    | 633600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 396        |
| stats_o/mean                   | 0.20213409 |
| stats_o/std                    | 0.09970342 |
| test/episodes                  | 3970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.343      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0154    |
| test/info_shaping_reward_mean  | -0.115     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -11.230683 |
| test/Q_plus_P                  | -11.230683 |
| test/reward_per_eps            | -26.3      |
| test/steps                     | 158800     |
| train/episodes                 | 15880      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.217      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.025     |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.3      |
| train/steps                    | 635200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 397        |
| stats_o/mean                   | 0.20213918 |
| stats_o/std                    | 0.09967116 |
| test/episodes                  | 3980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.588      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.02      |
| test/info_shaping_reward_mean  | -0.0797    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -5.315187  |
| test/Q_plus_P                  | -5.315187  |
| test/reward_per_eps            | -16.5      |
| test/steps                     | 159200     |
| train/episodes                 | 15920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.248      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0176    |
| train/info_shaping_reward_mean | -0.133     |
| train/info_shaping_reward_min  | -0.224     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.1      |
| train/steps                    | 636800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 398        |
| stats_o/mean                   | 0.20213605 |
| stats_o/std                    | 0.09961558 |
| test/episodes                  | 3990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.398      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00365   |
| test/info_shaping_reward_mean  | -0.101     |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -8.956815  |
| test/Q_plus_P                  | -8.956815  |
| test/reward_per_eps            | -24.1      |
| test/steps                     | 159600     |
| train/episodes                 | 15960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.278      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0182    |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.9      |
| train/steps                    | 638400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 399        |
| stats_o/mean                   | 0.20214218 |
| stats_o/std                    | 0.09958408 |
| test/episodes                  | 4000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.328      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00276   |
| test/info_shaping_reward_mean  | -0.109     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -12.371941 |
| test/Q_plus_P                  | -12.371941 |
| test/reward_per_eps            | -26.9      |
| test/steps                     | 160000     |
| train/episodes                 | 16000      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.251      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0348    |
| train/info_shaping_reward_mean | -0.127     |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.9      |
| train/steps                    | 640000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 400        |
| stats_o/mean                   | 0.2021259  |
| stats_o/std                    | 0.09954957 |
| test/episodes                  | 4010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.325      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00719   |
| test/info_shaping_reward_mean  | -0.121     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -12.932012 |
| test/Q_plus_P                  | -12.932012 |
| test/reward_per_eps            | -27        |
| test/steps                     | 160400     |
| train/episodes                 | 16040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.244      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0169    |
| train/info_shaping_reward_mean | -0.122     |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.2      |
| train/steps                    | 641600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 401        |
| stats_o/mean                   | 0.20212087 |
| stats_o/std                    | 0.09953247 |
| test/episodes                  | 4020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.463      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00496   |
| test/info_shaping_reward_mean  | -0.0948    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -8.31327   |
| test/Q_plus_P                  | -8.31327   |
| test/reward_per_eps            | -21.5      |
| test/steps                     | 160800     |
| train/episodes                 | 16080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.278      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0173    |
| train/info_shaping_reward_mean | -0.129     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.9      |
| train/steps                    | 643200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.20211548 |
| stats_o/std                    | 0.09948065 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.307      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00749   |
| test/info_shaping_reward_mean  | -0.116     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -12.040497 |
| test/Q_plus_P                  | -12.040497 |
| test/reward_per_eps            | -27.7      |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.195      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0335    |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.2      |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 403        |
| stats_o/mean                   | 0.2021097  |
| stats_o/std                    | 0.09943275 |
| test/episodes                  | 4040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.2        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.01      |
| test/info_shaping_reward_mean  | -0.134     |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -17.457176 |
| test/Q_plus_P                  | -17.457176 |
| test/reward_per_eps            | -32        |
| test/steps                     | 161600     |
| train/episodes                 | 16160      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.251      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.028     |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.278     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30        |
| train/steps                    | 646400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.20210655  |
| stats_o/std                    | 0.099398896 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.177       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00979    |
| test/info_shaping_reward_mean  | -0.138      |
| test/info_shaping_reward_min   | -0.184      |
| test/Q                         | -17.382837  |
| test/Q_plus_P                  | -17.382837  |
| test/reward_per_eps            | -32.9       |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.186       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0359     |
| train/info_shaping_reward_mean | -0.134      |
| train/info_shaping_reward_min  | -0.196      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.5       |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 405        |
| stats_o/mean                   | 0.2021195  |
| stats_o/std                    | 0.09934902 |
| test/episodes                  | 4060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.505      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00851   |
| test/info_shaping_reward_mean  | -0.0914    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -7.551405  |
| test/Q_plus_P                  | -7.551405  |
| test/reward_per_eps            | -19.8      |
| test/steps                     | 162400     |
| train/episodes                 | 16240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.179      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0197    |
| train/info_shaping_reward_mean | -0.138     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.9      |
| train/steps                    | 649600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 406         |
| stats_o/mean                   | 0.20211428  |
| stats_o/std                    | 0.099339694 |
| test/episodes                  | 4070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.325       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000356   |
| test/info_shaping_reward_mean  | -0.12       |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -15.164783  |
| test/Q_plus_P                  | -15.164783  |
| test/reward_per_eps            | -27         |
| test/steps                     | 162800      |
| train/episodes                 | 16280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.275       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.019      |
| train/info_shaping_reward_mean | -0.119      |
| train/info_shaping_reward_min  | -0.19       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29         |
| train/steps                    | 651200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 407        |
| stats_o/mean                   | 0.20210336 |
| stats_o/std                    | 0.09929202 |
| test/episodes                  | 4080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.34       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00854   |
| test/info_shaping_reward_mean  | -0.117     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -13.09737  |
| test/Q_plus_P                  | -13.09737  |
| test/reward_per_eps            | -26.4      |
| test/steps                     | 163200     |
| train/episodes                 | 16320      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.224      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0238    |
| train/info_shaping_reward_mean | -0.131     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31        |
| train/steps                    | 652800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 408        |
| stats_o/mean                   | 0.20211716 |
| stats_o/std                    | 0.09929685 |
| test/episodes                  | 4090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.333      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0222    |
| test/info_shaping_reward_mean  | -0.113     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -10.429685 |
| test/Q_plus_P                  | -10.429685 |
| test/reward_per_eps            | -26.7      |
| test/steps                     | 163600     |
| train/episodes                 | 16360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.282      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0129    |
| train/info_shaping_reward_mean | -0.119     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.7      |
| train/steps                    | 654400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 409        |
| stats_o/mean                   | 0.20213817 |
| stats_o/std                    | 0.09931632 |
| test/episodes                  | 4100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.432      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0168    |
| test/info_shaping_reward_mean  | -0.102     |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -11.113452 |
| test/Q_plus_P                  | -11.113452 |
| test/reward_per_eps            | -22.7      |
| test/steps                     | 164000     |
| train/episodes                 | 16400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.301      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.012     |
| train/info_shaping_reward_mean | -0.119     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.9      |
| train/steps                    | 656000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 410         |
| stats_o/mean                   | 0.20213221  |
| stats_o/std                    | 0.099281825 |
| test/episodes                  | 4110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.347       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0102     |
| test/info_shaping_reward_mean  | -0.116      |
| test/info_shaping_reward_min   | -0.187      |
| test/Q                         | -12.732906  |
| test/Q_plus_P                  | -12.732906  |
| test/reward_per_eps            | -26.1       |
| test/steps                     | 164400      |
| train/episodes                 | 16440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.252       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0156     |
| train/info_shaping_reward_mean | -0.124      |
| train/info_shaping_reward_min  | -0.201      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.9       |
| train/steps                    | 657600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 411         |
| stats_o/mean                   | 0.20213535  |
| stats_o/std                    | 0.099316455 |
| test/episodes                  | 4120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.135       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0192     |
| test/info_shaping_reward_mean  | -0.149      |
| test/info_shaping_reward_min   | -0.202      |
| test/Q                         | -17.574352  |
| test/Q_plus_P                  | -17.574352  |
| test/reward_per_eps            | -34.6       |
| test/steps                     | 164800      |
| train/episodes                 | 16480       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.157       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0239     |
| train/info_shaping_reward_mean | -0.153      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.7       |
| train/steps                    | 659200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 412        |
| stats_o/mean                   | 0.20211987 |
| stats_o/std                    | 0.09929104 |
| test/episodes                  | 4130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.41       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00334   |
| test/info_shaping_reward_mean  | -0.106     |
| test/info_shaping_reward_min   | -0.19      |
| test/Q                         | -9.939299  |
| test/Q_plus_P                  | -9.939299  |
| test/reward_per_eps            | -23.6      |
| test/steps                     | 165200     |
| train/episodes                 | 16520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.262      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0118    |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.5      |
| train/steps                    | 660800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 413        |
| stats_o/mean                   | 0.20211422 |
| stats_o/std                    | 0.09924843 |
| test/episodes                  | 4140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.515      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0218    |
| test/info_shaping_reward_mean  | -0.0938    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -7.7707715 |
| test/Q_plus_P                  | -7.7707715 |
| test/reward_per_eps            | -19.4      |
| test/steps                     | 165600     |
| train/episodes                 | 16560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.244      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0108    |
| train/info_shaping_reward_mean | -0.122     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.2      |
| train/steps                    | 662400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 414         |
| stats_o/mean                   | 0.2020925   |
| stats_o/std                    | 0.099246435 |
| test/episodes                  | 4150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.38        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0166     |
| test/info_shaping_reward_mean  | -0.112      |
| test/info_shaping_reward_min   | -0.207      |
| test/Q                         | -10.235249  |
| test/Q_plus_P                  | -10.235249  |
| test/reward_per_eps            | -24.8       |
| test/steps                     | 166000      |
| train/episodes                 | 16600       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.217       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.031      |
| train/info_shaping_reward_mean | -0.134      |
| train/info_shaping_reward_min  | -0.188      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.3       |
| train/steps                    | 664000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 415        |
| stats_o/mean                   | 0.20209707 |
| stats_o/std                    | 0.0992224  |
| test/episodes                  | 4160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.407      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00808   |
| test/info_shaping_reward_mean  | -0.0946    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -7.467056  |
| test/Q_plus_P                  | -7.467056  |
| test/reward_per_eps            | -23.7      |
| test/steps                     | 166400     |
| train/episodes                 | 16640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.299      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0119    |
| train/info_shaping_reward_mean | -0.123     |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.1      |
| train/steps                    | 665600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 416        |
| stats_o/mean                   | 0.20210572 |
| stats_o/std                    | 0.09919347 |
| test/episodes                  | 4170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.37       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0213    |
| test/info_shaping_reward_mean  | -0.097     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -8.911718  |
| test/Q_plus_P                  | -8.911718  |
| test/reward_per_eps            | -25.2      |
| test/steps                     | 166800     |
| train/episodes                 | 16680      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.264      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0196    |
| train/info_shaping_reward_mean | -0.119     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.4      |
| train/steps                    | 667200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 417        |
| stats_o/mean                   | 0.20210156 |
| stats_o/std                    | 0.09916815 |
| test/episodes                  | 4180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.477      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0155    |
| test/info_shaping_reward_mean  | -0.0967    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -8.816315  |
| test/Q_plus_P                  | -8.816315  |
| test/reward_per_eps            | -20.9      |
| test/steps                     | 167200     |
| train/episodes                 | 16720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.336      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0122    |
| train/info_shaping_reward_mean | -0.115     |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.6      |
| train/steps                    | 668800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.20209894  |
| stats_o/std                    | 0.099180736 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.445       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00605    |
| test/info_shaping_reward_mean  | -0.0986     |
| test/info_shaping_reward_min   | -0.183      |
| test/Q                         | -9.126178   |
| test/Q_plus_P                  | -9.126178   |
| test/reward_per_eps            | -22.2       |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.276       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0105     |
| train/info_shaping_reward_mean | -0.117      |
| train/info_shaping_reward_min  | -0.192      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29         |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 419        |
| stats_o/mean                   | 0.20210479 |
| stats_o/std                    | 0.09914794 |
| test/episodes                  | 4200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.458      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.021     |
| test/info_shaping_reward_mean  | -0.0931    |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -6.9402313 |
| test/Q_plus_P                  | -6.9402313 |
| test/reward_per_eps            | -21.7      |
| test/steps                     | 168000     |
| train/episodes                 | 16800      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.306      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0308    |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.8      |
| train/steps                    | 672000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 420        |
| stats_o/mean                   | 0.20209856 |
| stats_o/std                    | 0.09913803 |
| test/episodes                  | 4210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.605      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00357   |
| test/info_shaping_reward_mean  | -0.0756    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -4.4369984 |
| test/Q_plus_P                  | -4.4369984 |
| test/reward_per_eps            | -15.8      |
| test/steps                     | 168400     |
| train/episodes                 | 16840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.318      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0152    |
| train/info_shaping_reward_mean | -0.118     |
| train/info_shaping_reward_min  | -0.217     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.3      |
| train/steps                    | 673600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 421         |
| stats_o/mean                   | 0.2020916   |
| stats_o/std                    | 0.099164784 |
| test/episodes                  | 4220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.29        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00311    |
| test/info_shaping_reward_mean  | -0.113      |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -10.396004  |
| test/Q_plus_P                  | -10.396004  |
| test/reward_per_eps            | -28.4       |
| test/steps                     | 168800      |
| train/episodes                 | 16880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.326       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0139     |
| train/info_shaping_reward_mean | -0.108      |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.9       |
| train/steps                    | 675200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.20208845  |
| stats_o/std                    | 0.099145405 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.635       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0156     |
| test/info_shaping_reward_mean  | -0.0692     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -4.1885576  |
| test/Q_plus_P                  | -4.1885576  |
| test/reward_per_eps            | -14.6       |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.323       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0149     |
| train/info_shaping_reward_mean | -0.112      |
| train/info_shaping_reward_min  | -0.203      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.1       |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 423        |
| stats_o/mean                   | 0.20208874 |
| stats_o/std                    | 0.09916131 |
| test/episodes                  | 4240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.305      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00363   |
| test/info_shaping_reward_mean  | -0.113     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.779663  |
| test/Q_plus_P                  | -9.779663  |
| test/reward_per_eps            | -27.8      |
| test/steps                     | 169600     |
| train/episodes                 | 16960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.274      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0156    |
| train/info_shaping_reward_mean | -0.114     |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.1      |
| train/steps                    | 678400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 424        |
| stats_o/mean                   | 0.20209475 |
| stats_o/std                    | 0.09914677 |
| test/episodes                  | 4250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.4        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0244    |
| test/info_shaping_reward_mean  | -0.0945    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -6.4174414 |
| test/Q_plus_P                  | -6.4174414 |
| test/reward_per_eps            | -24        |
| test/steps                     | 170000     |
| train/episodes                 | 17000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.356      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0109    |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.8      |
| train/steps                    | 680000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.20209584  |
| stats_o/std                    | 0.099154614 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.398       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0213     |
| test/info_shaping_reward_mean  | -0.111      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -9.32706    |
| test/Q_plus_P                  | -9.32706    |
| test/reward_per_eps            | -24.1       |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.296       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0147     |
| train/info_shaping_reward_mean | -0.11       |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.1       |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 426        |
| stats_o/mean                   | 0.20210077 |
| stats_o/std                    | 0.099143   |
| test/episodes                  | 4270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.207      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0177    |
| test/info_shaping_reward_mean  | -0.128     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -12.758298 |
| test/Q_plus_P                  | -12.758298 |
| test/reward_per_eps            | -31.7      |
| test/steps                     | 170800     |
| train/episodes                 | 17080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.331      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0134    |
| train/info_shaping_reward_mean | -0.113     |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.8      |
| train/steps                    | 683200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 427        |
| stats_o/mean                   | 0.20209865 |
| stats_o/std                    | 0.09919394 |
| test/episodes                  | 4280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.32       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000936  |
| test/info_shaping_reward_mean  | -0.11      |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -8.172766  |
| test/Q_plus_P                  | -8.172766  |
| test/reward_per_eps            | -27.2      |
| test/steps                     | 171200     |
| train/episodes                 | 17120      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.24       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0156    |
| train/info_shaping_reward_mean | -0.131     |
| train/info_shaping_reward_min  | -0.277     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.4      |
| train/steps                    | 684800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 428        |
| stats_o/mean                   | 0.20209762 |
| stats_o/std                    | 0.0992106  |
| test/episodes                  | 4290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.28       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0141    |
| test/info_shaping_reward_mean  | -0.124     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -10.359912 |
| test/Q_plus_P                  | -10.359912 |
| test/reward_per_eps            | -28.8      |
| test/steps                     | 171600     |
| train/episodes                 | 17160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.276      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0108    |
| train/info_shaping_reward_mean | -0.122     |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29        |
| train/steps                    | 686400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 429        |
| stats_o/mean                   | 0.20208596 |
| stats_o/std                    | 0.09921029 |
| test/episodes                  | 4300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.432      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00817   |
| test/info_shaping_reward_mean  | -0.0952    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -6.232706  |
| test/Q_plus_P                  | -6.232706  |
| test/reward_per_eps            | -22.7      |
| test/steps                     | 172000     |
| train/episodes                 | 17200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.233      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.016     |
| train/info_shaping_reward_mean | -0.122     |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.7      |
| train/steps                    | 688000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 430         |
| stats_o/mean                   | 0.20207798  |
| stats_o/std                    | 0.099222735 |
| test/episodes                  | 4310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.253       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00412    |
| test/info_shaping_reward_mean  | -0.109      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -8.969588   |
| test/Q_plus_P                  | -8.969588   |
| test/reward_per_eps            | -29.9       |
| test/steps                     | 172400      |
| train/episodes                 | 17240       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.302       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0174     |
| train/info_shaping_reward_mean | -0.119      |
| train/info_shaping_reward_min  | -0.209      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.9       |
| train/steps                    | 689600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 431        |
| stats_o/mean                   | 0.20206621 |
| stats_o/std                    | 0.0992501  |
| test/episodes                  | 4320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.32       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00428   |
| test/info_shaping_reward_mean  | -0.113     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -8.032877  |
| test/Q_plus_P                  | -8.032877  |
| test/reward_per_eps            | -27.2      |
| test/steps                     | 172800     |
| train/episodes                 | 17280      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.315      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0183    |
| train/info_shaping_reward_mean | -0.112     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.4      |
| train/steps                    | 691200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 432        |
| stats_o/mean                   | 0.20206098 |
| stats_o/std                    | 0.09925911 |
| test/episodes                  | 4330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.315      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00938   |
| test/info_shaping_reward_mean  | -0.112     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.348728  |
| test/Q_plus_P                  | -9.348728  |
| test/reward_per_eps            | -27.4      |
| test/steps                     | 173200     |
| train/episodes                 | 17320      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.319      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0183    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.2      |
| train/steps                    | 692800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 433        |
| stats_o/mean                   | 0.20206308 |
| stats_o/std                    | 0.0992877  |
| test/episodes                  | 4340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.235      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0103    |
| test/info_shaping_reward_mean  | -0.117     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -8.264954  |
| test/Q_plus_P                  | -8.264954  |
| test/reward_per_eps            | -30.6      |
| test/steps                     | 173600     |
| train/episodes                 | 17360      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.328      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0251    |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.9      |
| train/steps                    | 694400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 434        |
| stats_o/mean                   | 0.20205417 |
| stats_o/std                    | 0.09927617 |
| test/episodes                  | 4350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.432      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0148    |
| test/info_shaping_reward_mean  | -0.102     |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -6.843004  |
| test/Q_plus_P                  | -6.843004  |
| test/reward_per_eps            | -22.7      |
| test/steps                     | 174000     |
| train/episodes                 | 17400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.297      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0164    |
| train/info_shaping_reward_mean | -0.118     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.1      |
| train/steps                    | 696000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 435        |
| stats_o/mean                   | 0.20205785 |
| stats_o/std                    | 0.0992797  |
| test/episodes                  | 4360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.455      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00495   |
| test/info_shaping_reward_mean  | -0.0903    |
| test/info_shaping_reward_min   | -0.187     |
| test/Q                         | -6.1731734 |
| test/Q_plus_P                  | -6.1731734 |
| test/reward_per_eps            | -21.8      |
| test/steps                     | 174400     |
| train/episodes                 | 17440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.284      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0109    |
| train/info_shaping_reward_mean | -0.112     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.6      |
| train/steps                    | 697600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 436        |
| stats_o/mean                   | 0.20206022 |
| stats_o/std                    | 0.09931143 |
| test/episodes                  | 4370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.41       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00601   |
| test/info_shaping_reward_mean  | -0.104     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -7.8611965 |
| test/Q_plus_P                  | -7.8611965 |
| test/reward_per_eps            | -23.6      |
| test/steps                     | 174800     |
| train/episodes                 | 17480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.336      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0144    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.6      |
| train/steps                    | 699200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 437        |
| stats_o/mean                   | 0.20205298 |
| stats_o/std                    | 0.09932107 |
| test/episodes                  | 4380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.388      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00735   |
| test/info_shaping_reward_mean  | -0.0938    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -6.6226916 |
| test/Q_plus_P                  | -6.6226916 |
| test/reward_per_eps            | -24.5      |
| test/steps                     | 175200     |
| train/episodes                 | 17520      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.249      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0273    |
| train/info_shaping_reward_mean | -0.127     |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30        |
| train/steps                    | 700800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 438        |
| stats_o/mean                   | 0.20204563 |
| stats_o/std                    | 0.09935831 |
| test/episodes                  | 4390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.362      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0219    |
| test/info_shaping_reward_mean  | -0.117     |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -8.237128  |
| test/Q_plus_P                  | -8.237128  |
| test/reward_per_eps            | -25.5      |
| test/steps                     | 175600     |
| train/episodes                 | 17560      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.23       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0249    |
| train/info_shaping_reward_mean | -0.132     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.8      |
| train/steps                    | 702400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 439         |
| stats_o/mean                   | 0.2020362   |
| stats_o/std                    | 0.099388465 |
| test/episodes                  | 4400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.203       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0182     |
| test/info_shaping_reward_mean  | -0.141      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -11.1884575 |
| test/Q_plus_P                  | -11.1884575 |
| test/reward_per_eps            | -31.9       |
| test/steps                     | 176000      |
| train/episodes                 | 17600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.257       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0123     |
| train/info_shaping_reward_mean | -0.122      |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.7       |
| train/steps                    | 704000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 440        |
| stats_o/mean                   | 0.20203131 |
| stats_o/std                    | 0.09940062 |
| test/episodes                  | 4410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.48       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00746   |
| test/info_shaping_reward_mean  | -0.0871    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -4.626412  |
| test/Q_plus_P                  | -4.626412  |
| test/reward_per_eps            | -20.8      |
| test/steps                     | 176400     |
| train/episodes                 | 17640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.318      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0171    |
| train/info_shaping_reward_mean | -0.115     |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.3      |
| train/steps                    | 705600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.2020363   |
| stats_o/std                    | 0.099401824 |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.52        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00534    |
| test/info_shaping_reward_mean  | -0.0847     |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -5.1336775  |
| test/Q_plus_P                  | -5.1336775  |
| test/reward_per_eps            | -19.2       |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.261       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0138     |
| train/info_shaping_reward_mean | -0.118      |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.6       |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 442        |
| stats_o/mean                   | 0.20203482 |
| stats_o/std                    | 0.09944912 |
| test/episodes                  | 4430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.477      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00708   |
| test/info_shaping_reward_mean  | -0.0862    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -5.0264235 |
| test/Q_plus_P                  | -5.0264235 |
| test/reward_per_eps            | -20.9      |
| test/steps                     | 177200     |
| train/episodes                 | 17720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.264      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0173    |
| train/info_shaping_reward_mean | -0.12      |
| train/info_shaping_reward_min  | -0.193     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.4      |
| train/steps                    | 708800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.20203461  |
| stats_o/std                    | 0.099443525 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.5         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0191     |
| test/info_shaping_reward_mean  | -0.0963     |
| test/info_shaping_reward_min   | -0.514      |
| test/Q                         | -5.616963   |
| test/Q_plus_P                  | -5.616963   |
| test/reward_per_eps            | -20         |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.251       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0126     |
| train/info_shaping_reward_mean | -0.12       |
| train/info_shaping_reward_min  | -0.189      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.9       |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 444        |
| stats_o/mean                   | 0.20203325 |
| stats_o/std                    | 0.09943273 |
| test/episodes                  | 4450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.555      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00644   |
| test/info_shaping_reward_mean  | -0.0723    |
| test/info_shaping_reward_min   | -0.193     |
| test/Q                         | -3.675485  |
| test/Q_plus_P                  | -3.675485  |
| test/reward_per_eps            | -17.8      |
| test/steps                     | 178000     |
| train/episodes                 | 17800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.374      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0128    |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.216     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25        |
| train/steps                    | 712000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 445        |
| stats_o/mean                   | 0.20202155 |
| stats_o/std                    | 0.09944756 |
| test/episodes                  | 4460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.405      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00903   |
| test/info_shaping_reward_mean  | -0.108     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -7.090831  |
| test/Q_plus_P                  | -7.090831  |
| test/reward_per_eps            | -23.8      |
| test/steps                     | 178400     |
| train/episodes                 | 17840      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.257      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0177    |
| train/info_shaping_reward_mean | -0.124     |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.7      |
| train/steps                    | 713600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 446         |
| stats_o/mean                   | 0.20202263  |
| stats_o/std                    | 0.099449046 |
| test/episodes                  | 4470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.627       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00657    |
| test/info_shaping_reward_mean  | -0.0685     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -3.814895   |
| test/Q_plus_P                  | -3.814895   |
| test/reward_per_eps            | -14.9       |
| test/steps                     | 178800      |
| train/episodes                 | 17880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.291       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0146     |
| train/info_shaping_reward_mean | -0.108      |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.4       |
| train/steps                    | 715200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 447         |
| stats_o/mean                   | 0.20201449  |
| stats_o/std                    | 0.099458255 |
| test/episodes                  | 4480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.215       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00379    |
| test/info_shaping_reward_mean  | -0.131      |
| test/info_shaping_reward_min   | -0.181      |
| test/Q                         | -9.266187   |
| test/Q_plus_P                  | -9.266187   |
| test/reward_per_eps            | -31.4       |
| test/steps                     | 179200      |
| train/episodes                 | 17920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.261       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0128     |
| train/info_shaping_reward_mean | -0.119      |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.6       |
| train/steps                    | 716800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 448        |
| stats_o/mean                   | 0.20200798 |
| stats_o/std                    | 0.09947688 |
| test/episodes                  | 4490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.472      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00146   |
| test/info_shaping_reward_mean  | -0.0926    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -5.418927  |
| test/Q_plus_P                  | -5.418927  |
| test/reward_per_eps            | -21.1      |
| test/steps                     | 179600     |
| train/episodes                 | 17960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.312      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0114    |
| train/info_shaping_reward_mean | -0.117     |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.5      |
| train/steps                    | 718400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 449        |
| stats_o/mean                   | 0.20200068 |
| stats_o/std                    | 0.09953369 |
| test/episodes                  | 4500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.152      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.026     |
| test/info_shaping_reward_mean  | -0.147     |
| test/info_shaping_reward_min   | -0.212     |
| test/Q                         | -11.137263 |
| test/Q_plus_P                  | -11.137263 |
| test/reward_per_eps            | -33.9      |
| test/steps                     | 180000     |
| train/episodes                 | 18000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.351      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0146    |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.9      |
| train/steps                    | 720000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.20198537 |
| stats_o/std                    | 0.09952946 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.468      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00948   |
| test/info_shaping_reward_mean  | -0.0875    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -5.073796  |
| test/Q_plus_P                  | -5.073796  |
| test/reward_per_eps            | -21.3      |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.291      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0111    |
| train/info_shaping_reward_mean | -0.117     |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.4      |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 451         |
| stats_o/mean                   | 0.2019786   |
| stats_o/std                    | 0.099518254 |
| test/episodes                  | 4520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.552       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00971    |
| test/info_shaping_reward_mean  | -0.0837     |
| test/info_shaping_reward_min   | -0.192      |
| test/Q                         | -4.876764   |
| test/Q_plus_P                  | -4.876764   |
| test/reward_per_eps            | -17.9       |
| test/steps                     | 180800      |
| train/episodes                 | 18080       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.286       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0267     |
| train/info_shaping_reward_mean | -0.122      |
| train/info_shaping_reward_min  | -0.222      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.6       |
| train/steps                    | 723200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 452        |
| stats_o/mean                   | 0.20198397 |
| stats_o/std                    | 0.09951224 |
| test/episodes                  | 4530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.583      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00252   |
| test/info_shaping_reward_mean  | -0.0674    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.015599  |
| test/Q_plus_P                  | -3.015599  |
| test/reward_per_eps            | -16.7      |
| test/steps                     | 181200     |
| train/episodes                 | 18120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.307      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00984   |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.7      |
| train/steps                    | 724800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 453        |
| stats_o/mean                   | 0.2019804  |
| stats_o/std                    | 0.09953501 |
| test/episodes                  | 4540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.517      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00887   |
| test/info_shaping_reward_mean  | -0.0742    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -3.7996728 |
| test/Q_plus_P                  | -3.7996728 |
| test/reward_per_eps            | -19.3      |
| test/steps                     | 181600     |
| train/episodes                 | 18160      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.206      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0379    |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.8      |
| train/steps                    | 726400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 454        |
| stats_o/mean                   | 0.20198275 |
| stats_o/std                    | 0.0995014  |
| test/episodes                  | 4550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.487      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00583   |
| test/info_shaping_reward_mean  | -0.0725    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -4.0294523 |
| test/Q_plus_P                  | -4.0294523 |
| test/reward_per_eps            | -20.5      |
| test/steps                     | 182000     |
| train/episodes                 | 18200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.338      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0103    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.5      |
| train/steps                    | 728000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 455        |
| stats_o/mean                   | 0.20198005 |
| stats_o/std                    | 0.09947643 |
| test/episodes                  | 4560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.502      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00728   |
| test/info_shaping_reward_mean  | -0.0916    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -5.6948023 |
| test/Q_plus_P                  | -5.6948023 |
| test/reward_per_eps            | -19.9      |
| test/steps                     | 182400     |
| train/episodes                 | 18240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.378      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0108    |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.212     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.9      |
| train/steps                    | 729600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 456         |
| stats_o/mean                   | 0.20197766  |
| stats_o/std                    | 0.099481575 |
| test/episodes                  | 4570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.578       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00234    |
| test/info_shaping_reward_mean  | -0.0765     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -4.6229353  |
| test/Q_plus_P                  | -4.6229353  |
| test/reward_per_eps            | -16.9       |
| test/steps                     | 182800      |
| train/episodes                 | 18280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.298       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0134     |
| train/info_shaping_reward_mean | -0.121      |
| train/info_shaping_reward_min  | -0.212      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.1       |
| train/steps                    | 731200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.20196968  |
| stats_o/std                    | 0.099471465 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.487       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00831    |
| test/info_shaping_reward_mean  | -0.0851     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -5.9652057  |
| test/Q_plus_P                  | -5.9652057  |
| test/reward_per_eps            | -20.5       |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.287       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0284     |
| train/info_shaping_reward_mean | -0.122      |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.5       |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.20197344  |
| stats_o/std                    | 0.099492714 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.475       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0121     |
| test/info_shaping_reward_mean  | -0.0841     |
| test/info_shaping_reward_min   | -0.18       |
| test/Q                         | -5.4084344  |
| test/Q_plus_P                  | -5.4084344  |
| test/reward_per_eps            | -21         |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.324       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00943    |
| train/info_shaping_reward_mean | -0.121      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.1       |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 459        |
| stats_o/mean                   | 0.20197451 |
| stats_o/std                    | 0.09951906 |
| test/episodes                  | 4600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.212      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00842   |
| test/info_shaping_reward_mean  | -0.139     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -10.952934 |
| test/Q_plus_P                  | -10.952934 |
| test/reward_per_eps            | -31.5      |
| test/steps                     | 184000     |
| train/episodes                 | 18400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.31       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0118    |
| train/info_shaping_reward_mean | -0.118     |
| train/info_shaping_reward_min  | -0.227     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.6      |
| train/steps                    | 736000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 460        |
| stats_o/mean                   | 0.20197196 |
| stats_o/std                    | 0.09948903 |
| test/episodes                  | 4610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.375      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0042    |
| test/info_shaping_reward_mean  | -0.103     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -7.371966  |
| test/Q_plus_P                  | -7.371966  |
| test/reward_per_eps            | -25        |
| test/steps                     | 184400     |
| train/episodes                 | 18440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.355      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00865   |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.8      |
| train/steps                    | 737600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 461        |
| stats_o/mean                   | 0.2019598  |
| stats_o/std                    | 0.09950884 |
| test/episodes                  | 4620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.3        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00276   |
| test/info_shaping_reward_mean  | -0.119     |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -9.39078   |
| test/Q_plus_P                  | -9.39078   |
| test/reward_per_eps            | -28        |
| test/steps                     | 184800     |
| train/episodes                 | 18480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.328      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.012     |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.9      |
| train/steps                    | 739200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 462        |
| stats_o/mean                   | 0.20195033 |
| stats_o/std                    | 0.09951194 |
| test/episodes                  | 4630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.598      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00242   |
| test/info_shaping_reward_mean  | -0.0722    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -3.8588293 |
| test/Q_plus_P                  | -3.8588293 |
| test/reward_per_eps            | -16.1      |
| test/steps                     | 185200     |
| train/episodes                 | 18520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.262      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00829   |
| train/info_shaping_reward_mean | -0.118     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.5      |
| train/steps                    | 740800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 463        |
| stats_o/mean                   | 0.20194712 |
| stats_o/std                    | 0.09948448 |
| test/episodes                  | 4640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.42       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00374   |
| test/info_shaping_reward_mean  | -0.0999    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -7.415161  |
| test/Q_plus_P                  | -7.415161  |
| test/reward_per_eps            | -23.2      |
| test/steps                     | 185600     |
| train/episodes                 | 18560      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.307      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0286    |
| train/info_shaping_reward_mean | -0.118     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.7      |
| train/steps                    | 742400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 464        |
| stats_o/mean                   | 0.20193924 |
| stats_o/std                    | 0.09946975 |
| test/episodes                  | 4650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.502      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00383   |
| test/info_shaping_reward_mean  | -0.0882    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -6.1916003 |
| test/Q_plus_P                  | -6.1916003 |
| test/reward_per_eps            | -19.9      |
| test/steps                     | 186000     |
| train/episodes                 | 18600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.346      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0104    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.211     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.1      |
| train/steps                    | 744000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 465         |
| stats_o/mean                   | 0.20192969  |
| stats_o/std                    | 0.099483944 |
| test/episodes                  | 4660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.578       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0131     |
| test/info_shaping_reward_mean  | -0.0829     |
| test/info_shaping_reward_min   | -0.2        |
| test/Q                         | -4.387818   |
| test/Q_plus_P                  | -4.387818   |
| test/reward_per_eps            | -16.9       |
| test/steps                     | 186400      |
| train/episodes                 | 18640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.294       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00742    |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.2       |
| train/steps                    | 745600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.20192662  |
| stats_o/std                    | 0.099496506 |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.468       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00546    |
| test/info_shaping_reward_mean  | -0.0946     |
| test/info_shaping_reward_min   | -0.207      |
| test/Q                         | -7.0057096  |
| test/Q_plus_P                  | -7.0057096  |
| test/reward_per_eps            | -21.3       |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.257       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0201     |
| train/info_shaping_reward_mean | -0.125      |
| train/info_shaping_reward_min  | -0.221      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.7       |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 467        |
| stats_o/mean                   | 0.20192766 |
| stats_o/std                    | 0.09944089 |
| test/episodes                  | 4680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.555      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00748   |
| test/info_shaping_reward_mean  | -0.0816    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -5.0576777 |
| test/Q_plus_P                  | -5.0576777 |
| test/reward_per_eps            | -17.8      |
| test/steps                     | 187200     |
| train/episodes                 | 18720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.371      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00841   |
| train/info_shaping_reward_mean | -0.0989    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.2      |
| train/steps                    | 748800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 468        |
| stats_o/mean                   | 0.2019256  |
| stats_o/std                    | 0.09942344 |
| test/episodes                  | 4690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.37       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00953   |
| test/info_shaping_reward_mean  | -0.102     |
| test/info_shaping_reward_min   | -0.199     |
| test/Q                         | -7.650319  |
| test/Q_plus_P                  | -7.650319  |
| test/reward_per_eps            | -25.2      |
| test/steps                     | 187600     |
| train/episodes                 | 18760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.352      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00993   |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.9      |
| train/steps                    | 750400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 469        |
| stats_o/mean                   | 0.20192187 |
| stats_o/std                    | 0.09940198 |
| test/episodes                  | 4700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.365      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.011     |
| test/info_shaping_reward_mean  | -0.1       |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -7.3982787 |
| test/Q_plus_P                  | -7.3982787 |
| test/reward_per_eps            | -25.4      |
| test/steps                     | 188000     |
| train/episodes                 | 18800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.341      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0115    |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.4      |
| train/steps                    | 752000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 470         |
| stats_o/mean                   | 0.20191166  |
| stats_o/std                    | 0.099428006 |
| test/episodes                  | 4710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.347       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0178     |
| test/info_shaping_reward_mean  | -0.101      |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -7.771803   |
| test/Q_plus_P                  | -7.771803   |
| test/reward_per_eps            | -26.1       |
| test/steps                     | 188400      |
| train/episodes                 | 18840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.372       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00982    |
| train/info_shaping_reward_mean | -0.109      |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.1       |
| train/steps                    | 753600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.20190485 |
| stats_o/std                    | 0.0994232  |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.552      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00844   |
| test/info_shaping_reward_mean  | -0.0824    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -5.347445  |
| test/Q_plus_P                  | -5.347445  |
| test/reward_per_eps            | -17.9      |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.304      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0122    |
| train/info_shaping_reward_mean | -0.115     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.9      |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.20189236 |
| stats_o/std                    | 0.09941788 |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.292      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0104    |
| test/info_shaping_reward_mean  | -0.119     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -9.814169  |
| test/Q_plus_P                  | -9.814169  |
| test/reward_per_eps            | -28.3      |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.312      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0113    |
| train/info_shaping_reward_mean | -0.115     |
| train/info_shaping_reward_min  | -0.192     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.5      |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 473        |
| stats_o/mean                   | 0.20188937 |
| stats_o/std                    | 0.09939037 |
| test/episodes                  | 4740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.645      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00401   |
| test/info_shaping_reward_mean  | -0.067     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.77239   |
| test/Q_plus_P                  | -3.77239   |
| test/reward_per_eps            | -14.2      |
| test/steps                     | 189600     |
| train/episodes                 | 18960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.465      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.009     |
| train/info_shaping_reward_mean | -0.0874    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 758400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 474        |
| stats_o/mean                   | 0.20187761 |
| stats_o/std                    | 0.09939106 |
| test/episodes                  | 4750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.365      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0104    |
| test/info_shaping_reward_mean  | -0.104     |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -8.201979  |
| test/Q_plus_P                  | -8.201979  |
| test/reward_per_eps            | -25.4      |
| test/steps                     | 190000     |
| train/episodes                 | 19000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.298      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0074    |
| train/info_shaping_reward_mean | -0.115     |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.1      |
| train/steps                    | 760000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 475        |
| stats_o/mean                   | 0.20188347 |
| stats_o/std                    | 0.09941831 |
| test/episodes                  | 4760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.443      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00365   |
| test/info_shaping_reward_mean  | -0.0929    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -6.9484086 |
| test/Q_plus_P                  | -6.9484086 |
| test/reward_per_eps            | -22.3      |
| test/steps                     | 190400     |
| train/episodes                 | 19040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.379      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0132    |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.213     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.9      |
| train/steps                    | 761600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 476        |
| stats_o/mean                   | 0.20188953 |
| stats_o/std                    | 0.09941377 |
| test/episodes                  | 4770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.43       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00848   |
| test/info_shaping_reward_mean  | -0.103     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -7.935835  |
| test/Q_plus_P                  | -7.935835  |
| test/reward_per_eps            | -22.8      |
| test/steps                     | 190800     |
| train/episodes                 | 19080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.409      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0106    |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.221     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.6      |
| train/steps                    | 763200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 477        |
| stats_o/mean                   | 0.20189387 |
| stats_o/std                    | 0.09939598 |
| test/episodes                  | 4780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00796   |
| test/info_shaping_reward_mean  | -0.0534    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.7844396 |
| test/Q_plus_P                  | -1.7844396 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 191200     |
| train/episodes                 | 19120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.384      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0079    |
| train/info_shaping_reward_mean | -0.0959    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.6      |
| train/steps                    | 764800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 478         |
| stats_o/mean                   | 0.20188236  |
| stats_o/std                    | 0.099378034 |
| test/episodes                  | 4790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.58        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00248    |
| test/info_shaping_reward_mean  | -0.0685     |
| test/info_shaping_reward_min   | -0.185      |
| test/Q                         | -4.2392235  |
| test/Q_plus_P                  | -4.2392235  |
| test/reward_per_eps            | -16.8       |
| test/steps                     | 191600      |
| train/episodes                 | 19160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.404       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00754    |
| train/info_shaping_reward_mean | -0.101      |
| train/info_shaping_reward_min  | -0.216      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.8       |
| train/steps                    | 766400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.20188653  |
| stats_o/std                    | 0.099356435 |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.435       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00489    |
| test/info_shaping_reward_mean  | -0.101      |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -7.7360015  |
| test/Q_plus_P                  | -7.7360015  |
| test/reward_per_eps            | -22.6       |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.418       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00603    |
| train/info_shaping_reward_mean | -0.097      |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.3       |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 480        |
| stats_o/mean                   | 0.20188895 |
| stats_o/std                    | 0.09933927 |
| test/episodes                  | 4810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.502      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00517   |
| test/info_shaping_reward_mean  | -0.0806    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -5.795377  |
| test/Q_plus_P                  | -5.795377  |
| test/reward_per_eps            | -19.9      |
| test/steps                     | 192400     |
| train/episodes                 | 19240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.319      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.011     |
| train/info_shaping_reward_mean | -0.111     |
| train/info_shaping_reward_min  | -0.197     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.2      |
| train/steps                    | 769600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 481        |
| stats_o/mean                   | 0.20189247 |
| stats_o/std                    | 0.09935336 |
| test/episodes                  | 4820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.497      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00315   |
| test/info_shaping_reward_mean  | -0.084     |
| test/info_shaping_reward_min   | -0.181     |
| test/Q                         | -5.6930847 |
| test/Q_plus_P                  | -5.6930847 |
| test/reward_per_eps            | -20.1      |
| test/steps                     | 192800     |
| train/episodes                 | 19280      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.342      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0257    |
| train/info_shaping_reward_mean | -0.125     |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.3      |
| train/steps                    | 771200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 482        |
| stats_o/mean                   | 0.20189367 |
| stats_o/std                    | 0.09931749 |
| test/episodes                  | 4830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.487      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00289   |
| test/info_shaping_reward_mean  | -0.0881    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -7.0015035 |
| test/Q_plus_P                  | -7.0015035 |
| test/reward_per_eps            | -20.5      |
| test/steps                     | 193200     |
| train/episodes                 | 19320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.414      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00733   |
| train/info_shaping_reward_mean | -0.097     |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.4      |
| train/steps                    | 772800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 483        |
| stats_o/mean                   | 0.20189406 |
| stats_o/std                    | 0.09930339 |
| test/episodes                  | 4840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.505      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00653   |
| test/info_shaping_reward_mean  | -0.085     |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -5.809121  |
| test/Q_plus_P                  | -5.809121  |
| test/reward_per_eps            | -19.8      |
| test/steps                     | 193600     |
| train/episodes                 | 19360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.354      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00824   |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.8      |
| train/steps                    | 774400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.2018733   |
| stats_o/std                    | 0.099292725 |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.487       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00643    |
| test/info_shaping_reward_mean  | -0.0845     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -6.3043513  |
| test/Q_plus_P                  | -6.3043513  |
| test/reward_per_eps            | -20.5       |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.392       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.01       |
| train/info_shaping_reward_mean | -0.0981     |
| train/info_shaping_reward_min  | -0.18       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.3       |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.20187174  |
| stats_o/std                    | 0.099264815 |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.693       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00394    |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -1.8940738  |
| test/Q_plus_P                  | -1.8940738  |
| test/reward_per_eps            | -12.3       |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.394       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00669    |
| train/info_shaping_reward_mean | -0.0982     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.2       |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 486        |
| stats_o/mean                   | 0.20185904 |
| stats_o/std                    | 0.09926872 |
| test/episodes                  | 4870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00268   |
| test/info_shaping_reward_mean  | -0.0557    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -1.7024786 |
| test/Q_plus_P                  | -1.7024786 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 194800     |
| train/episodes                 | 19480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.327      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0103    |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.195     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.9      |
| train/steps                    | 779200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 487         |
| stats_o/mean                   | 0.20186293  |
| stats_o/std                    | 0.099244565 |
| test/episodes                  | 4880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.205       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0187     |
| test/info_shaping_reward_mean  | -0.106      |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -9.053167   |
| test/Q_plus_P                  | -9.053167   |
| test/reward_per_eps            | -31.8       |
| test/steps                     | 195200      |
| train/episodes                 | 19520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.411       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00819    |
| train/info_shaping_reward_mean | -0.0944     |
| train/info_shaping_reward_min  | -0.211      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.6       |
| train/steps                    | 780800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 488         |
| stats_o/mean                   | 0.20186402  |
| stats_o/std                    | 0.099220164 |
| test/episodes                  | 4890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.545       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00461    |
| test/info_shaping_reward_mean  | -0.0721     |
| test/info_shaping_reward_min   | -0.186      |
| test/Q                         | -4.6370845  |
| test/Q_plus_P                  | -4.6370845  |
| test/reward_per_eps            | -18.2       |
| test/steps                     | 195600      |
| train/episodes                 | 19560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.412       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0109     |
| train/info_shaping_reward_mean | -0.0942     |
| train/info_shaping_reward_min  | -0.179      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.5       |
| train/steps                    | 782400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 489        |
| stats_o/mean                   | 0.20186484 |
| stats_o/std                    | 0.09918464 |
| test/episodes                  | 4900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.552      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00493   |
| test/info_shaping_reward_mean  | -0.0807    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -5.6279454 |
| test/Q_plus_P                  | -5.6279454 |
| test/reward_per_eps            | -17.9      |
| test/steps                     | 196000     |
| train/episodes                 | 19600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.365      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00981   |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.4      |
| train/steps                    | 784000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 490        |
| stats_o/mean                   | 0.20185791 |
| stats_o/std                    | 0.09913878 |
| test/episodes                  | 4910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.593      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00659   |
| test/info_shaping_reward_mean  | -0.0752    |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -4.478324  |
| test/Q_plus_P                  | -4.478324  |
| test/reward_per_eps            | -16.3      |
| test/steps                     | 196400     |
| train/episodes                 | 19640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.404      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00798   |
| train/info_shaping_reward_mean | -0.0979    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.9      |
| train/steps                    | 785600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 491        |
| stats_o/mean                   | 0.20184903 |
| stats_o/std                    | 0.09913684 |
| test/episodes                  | 4920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.565      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0115    |
| test/info_shaping_reward_mean  | -0.0812    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -5.2681103 |
| test/Q_plus_P                  | -5.2681103 |
| test/reward_per_eps            | -17.4      |
| test/steps                     | 196800     |
| train/episodes                 | 19680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.406      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0119    |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.8      |
| train/steps                    | 787200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 492        |
| stats_o/mean                   | 0.2018499  |
| stats_o/std                    | 0.09914719 |
| test/episodes                  | 4930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.618      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0109    |
| test/info_shaping_reward_mean  | -0.0697    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -3.7790651 |
| test/Q_plus_P                  | -3.7790651 |
| test/reward_per_eps            | -15.3      |
| test/steps                     | 197200     |
| train/episodes                 | 19720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.336      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0114    |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.6      |
| train/steps                    | 788800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.20184176 |
| stats_o/std                    | 0.09910911 |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.52       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0168    |
| test/info_shaping_reward_mean  | -0.0815    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -5.719066  |
| test/Q_plus_P                  | -5.719066  |
| test/reward_per_eps            | -19.2      |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.387      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00857   |
| train/info_shaping_reward_mean | -0.103     |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.5      |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.20184015  |
| stats_o/std                    | 0.099131994 |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.485       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00524    |
| test/info_shaping_reward_mean  | -0.0888     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -7.4566526  |
| test/Q_plus_P                  | -7.4566526  |
| test/reward_per_eps            | -20.6       |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.393       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00696    |
| train/info_shaping_reward_mean | -0.109      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.3       |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 495        |
| stats_o/mean                   | 0.20185196 |
| stats_o/std                    | 0.09914024 |
| test/episodes                  | 4960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.54       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00595   |
| test/info_shaping_reward_mean  | -0.0799    |
| test/info_shaping_reward_min   | -0.198     |
| test/Q                         | -5.539398  |
| test/Q_plus_P                  | -5.539398  |
| test/reward_per_eps            | -18.4      |
| test/steps                     | 198400     |
| train/episodes                 | 19840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.451      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00737   |
| train/info_shaping_reward_mean | -0.0862    |
| train/info_shaping_reward_min  | -0.208     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22        |
| train/steps                    | 793600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 496        |
| stats_o/mean                   | 0.2018473  |
| stats_o/std                    | 0.09914483 |
| test/episodes                  | 4970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.49       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00395   |
| test/info_shaping_reward_mean  | -0.0805    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -6.5630584 |
| test/Q_plus_P                  | -6.5630584 |
| test/reward_per_eps            | -20.4      |
| test/steps                     | 198800     |
| train/episodes                 | 19880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.399      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00695   |
| train/info_shaping_reward_mean | -0.0928    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.1      |
| train/steps                    | 795200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 497        |
| stats_o/mean                   | 0.20184797 |
| stats_o/std                    | 0.09913983 |
| test/episodes                  | 4980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0035    |
| test/info_shaping_reward_mean  | -0.0587    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -3.134663  |
| test/Q_plus_P                  | -3.134663  |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 199200     |
| train/episodes                 | 19920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.378      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00824   |
| train/info_shaping_reward_mean | -0.1       |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.9      |
| train/steps                    | 796800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 498        |
| stats_o/mean                   | 0.20184126 |
| stats_o/std                    | 0.0991494  |
| test/episodes                  | 4990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00805   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.7176481 |
| test/Q_plus_P                  | -1.7176481 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 199600     |
| train/episodes                 | 19960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.419      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00926   |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.2      |
| train/steps                    | 798400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 499        |
| stats_o/mean                   | 0.20184836 |
| stats_o/std                    | 0.09911908 |
| test/episodes                  | 5000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00221   |
| test/info_shaping_reward_mean  | -0.0664    |
| test/info_shaping_reward_min   | -0.195     |
| test/Q                         | -3.701861  |
| test/Q_plus_P                  | -3.701861  |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 200000     |
| train/episodes                 | 20000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.441      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00786   |
| train/info_shaping_reward_mean | -0.0932    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.4      |
| train/steps                    | 800000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 500         |
| stats_o/mean                   | 0.20184542  |
| stats_o/std                    | 0.099140786 |
| test/episodes                  | 5010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.65        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0092     |
| test/info_shaping_reward_mean  | -0.0663     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -3.8396206  |
| test/Q_plus_P                  | -3.8396206  |
| test/reward_per_eps            | -14         |
| test/steps                     | 200400      |
| train/episodes                 | 20040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.442       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00536    |
| train/info_shaping_reward_mean | -0.0882     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.3       |
| train/steps                    | 801600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 501        |
| stats_o/mean                   | 0.20185037 |
| stats_o/std                    | 0.09915697 |
| test/episodes                  | 5020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.585      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00309   |
| test/info_shaping_reward_mean  | -0.0807    |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -4.970904  |
| test/Q_plus_P                  | -4.970904  |
| test/reward_per_eps            | -16.6      |
| test/steps                     | 200800     |
| train/episodes                 | 20080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.4        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00928   |
| train/info_shaping_reward_mean | -0.09      |
| train/info_shaping_reward_min  | -0.196     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24        |
| train/steps                    | 803200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.20184201  |
| stats_o/std                    | 0.099135056 |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00512    |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5209476  |
| test/Q_plus_P                  | -1.5209476  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.36        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0133     |
| train/info_shaping_reward_mean | -0.0991     |
| train/info_shaping_reward_min  | -0.198      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.6       |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 503         |
| stats_o/mean                   | 0.20184489  |
| stats_o/std                    | 0.099098854 |
| test/episodes                  | 5040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.468       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00261    |
| test/info_shaping_reward_mean  | -0.0829     |
| test/info_shaping_reward_min   | -0.188      |
| test/Q                         | -4.0490417  |
| test/Q_plus_P                  | -4.0490417  |
| test/reward_per_eps            | -21.3       |
| test/steps                     | 201600      |
| train/episodes                 | 20160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.461       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00472    |
| train/info_shaping_reward_mean | -0.0873     |
| train/info_shaping_reward_min  | -0.198      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.6       |
| train/steps                    | 806400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 504        |
| stats_o/mean                   | 0.20184907 |
| stats_o/std                    | 0.09907375 |
| test/episodes                  | 5050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.5        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00287   |
| test/info_shaping_reward_mean  | -0.0718    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -5.3228564 |
| test/Q_plus_P                  | -5.3228564 |
| test/reward_per_eps            | -20        |
| test/steps                     | 202000     |
| train/episodes                 | 20200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.456      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00694   |
| train/info_shaping_reward_mean | -0.09      |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 808000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 505         |
| stats_o/mean                   | 0.2018557   |
| stats_o/std                    | 0.099082954 |
| test/episodes                  | 5060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.59        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00628    |
| test/info_shaping_reward_mean  | -0.0645     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -3.6257453  |
| test/Q_plus_P                  | -3.6257453  |
| test/reward_per_eps            | -16.4       |
| test/steps                     | 202400      |
| train/episodes                 | 20240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.415       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00829    |
| train/info_shaping_reward_mean | -0.0929     |
| train/info_shaping_reward_min  | -0.218      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.4       |
| train/steps                    | 809600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.20185713  |
| stats_o/std                    | 0.099129535 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.56        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00401    |
| test/info_shaping_reward_mean  | -0.0648     |
| test/info_shaping_reward_min   | -0.179      |
| test/Q                         | -4.1204753  |
| test/Q_plus_P                  | -4.1204753  |
| test/reward_per_eps            | -17.6       |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.363       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00806    |
| train/info_shaping_reward_mean | -0.105      |
| train/info_shaping_reward_min  | -0.225      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.5       |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 507        |
| stats_o/mean                   | 0.20185718 |
| stats_o/std                    | 0.09913782 |
| test/episodes                  | 5080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00733   |
| test/info_shaping_reward_mean  | -0.0616    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -3.2616472 |
| test/Q_plus_P                  | -3.2616472 |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 203200     |
| train/episodes                 | 20320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.443      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00827   |
| train/info_shaping_reward_mean | -0.0826    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.3      |
| train/steps                    | 812800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 508        |
| stats_o/mean                   | 0.20185265 |
| stats_o/std                    | 0.09910983 |
| test/episodes                  | 5090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00287   |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.3780706 |
| test/Q_plus_P                  | -2.3780706 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 203600     |
| train/episodes                 | 20360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.487      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0079    |
| train/info_shaping_reward_mean | -0.0872    |
| train/info_shaping_reward_min  | -0.209     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.5      |
| train/steps                    | 814400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 509        |
| stats_o/mean                   | 0.2018488  |
| stats_o/std                    | 0.099146   |
| test/episodes                  | 5100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0015    |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.0525506 |
| test/Q_plus_P                  | -2.0525506 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 204000     |
| train/episodes                 | 20400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.403      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00719   |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.9      |
| train/steps                    | 816000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 510        |
| stats_o/mean                   | 0.2018587  |
| stats_o/std                    | 0.09923467 |
| test/episodes                  | 5110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00386   |
| test/info_shaping_reward_mean  | -0.0635    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -3.706563  |
| test/Q_plus_P                  | -3.706563  |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 204400     |
| train/episodes                 | 20440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.367      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0064    |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.3      |
| train/steps                    | 817600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 511         |
| stats_o/mean                   | 0.20184618  |
| stats_o/std                    | 0.099210784 |
| test/episodes                  | 5120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.585       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00553    |
| test/info_shaping_reward_mean  | -0.068      |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -3.8073847  |
| test/Q_plus_P                  | -3.8073847  |
| test/reward_per_eps            | -16.6       |
| test/steps                     | 204800      |
| train/episodes                 | 20480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.444       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00555    |
| train/info_shaping_reward_mean | -0.087      |
| train/info_shaping_reward_min  | -0.182      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.2       |
| train/steps                    | 819200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 512        |
| stats_o/mean                   | 0.20185396 |
| stats_o/std                    | 0.09915933 |
| test/episodes                  | 5130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00249   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -2.053121  |
| test/Q_plus_P                  | -2.053121  |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 205200     |
| train/episodes                 | 20520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.501      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00884   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20        |
| train/steps                    | 820800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 513        |
| stats_o/mean                   | 0.20185265 |
| stats_o/std                    | 0.09911371 |
| test/episodes                  | 5140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.575      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00613   |
| test/info_shaping_reward_mean  | -0.0771    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -4.9831214 |
| test/Q_plus_P                  | -4.9831214 |
| test/reward_per_eps            | -17        |
| test/steps                     | 205600     |
| train/episodes                 | 20560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0057    |
| train/info_shaping_reward_mean | -0.0754    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 822400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 514        |
| stats_o/mean                   | 0.20186472 |
| stats_o/std                    | 0.09911069 |
| test/episodes                  | 5150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.45       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.001     |
| test/info_shaping_reward_mean  | -0.0831    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -7.068494  |
| test/Q_plus_P                  | -7.068494  |
| test/reward_per_eps            | -22        |
| test/steps                     | 206000     |
| train/episodes                 | 20600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.411      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00859   |
| train/info_shaping_reward_mean | -0.0853    |
| train/info_shaping_reward_min  | -0.19      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.6      |
| train/steps                    | 824000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 515        |
| stats_o/mean                   | 0.20186165 |
| stats_o/std                    | 0.09911999 |
| test/episodes                  | 5160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.618      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0039    |
| test/info_shaping_reward_mean  | -0.0708    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -4.6744413 |
| test/Q_plus_P                  | -4.6744413 |
| test/reward_per_eps            | -15.3      |
| test/steps                     | 206400     |
| train/episodes                 | 20640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.458      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00801   |
| train/info_shaping_reward_mean | -0.0912    |
| train/info_shaping_reward_min  | -0.207     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.7      |
| train/steps                    | 825600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 516         |
| stats_o/mean                   | 0.20185605  |
| stats_o/std                    | 0.099127166 |
| test/episodes                  | 5170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.583       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000438   |
| test/info_shaping_reward_mean  | -0.069      |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -5.109058   |
| test/Q_plus_P                  | -5.109058   |
| test/reward_per_eps            | -16.7       |
| test/steps                     | 206800      |
| train/episodes                 | 20680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.434       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00822    |
| train/info_shaping_reward_mean | -0.091      |
| train/info_shaping_reward_min  | -0.197      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.6       |
| train/steps                    | 827200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 517        |
| stats_o/mean                   | 0.20186621 |
| stats_o/std                    | 0.09911384 |
| test/episodes                  | 5180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00595   |
| test/info_shaping_reward_mean  | -0.0615    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.0195167 |
| test/Q_plus_P                  | -3.0195167 |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 207200     |
| train/episodes                 | 20720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.436      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00603   |
| train/info_shaping_reward_mean | -0.0878    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.6      |
| train/steps                    | 828800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 518        |
| stats_o/mean                   | 0.20186397 |
| stats_o/std                    | 0.09914262 |
| test/episodes                  | 5190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.568      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00635   |
| test/info_shaping_reward_mean  | -0.069     |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -4.9438434 |
| test/Q_plus_P                  | -4.9438434 |
| test/reward_per_eps            | -17.3      |
| test/steps                     | 207600     |
| train/episodes                 | 20760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.408      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0093    |
| train/info_shaping_reward_mean | -0.0846    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.7      |
| train/steps                    | 830400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 519        |
| stats_o/mean                   | 0.20186579 |
| stats_o/std                    | 0.09909729 |
| test/episodes                  | 5200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.61       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00398   |
| test/info_shaping_reward_mean  | -0.066     |
| test/info_shaping_reward_min   | -0.189     |
| test/Q                         | -4.045624  |
| test/Q_plus_P                  | -4.045624  |
| test/reward_per_eps            | -15.6      |
| test/steps                     | 208000     |
| train/episodes                 | 20800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.502      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00865   |
| train/info_shaping_reward_mean | -0.0797    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 832000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 520        |
| stats_o/mean                   | 0.20187846 |
| stats_o/std                    | 0.09908624 |
| test/episodes                  | 5210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00347   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.6340392 |
| test/Q_plus_P                  | -1.6340392 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 208400     |
| train/episodes                 | 20840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.432      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00898   |
| train/info_shaping_reward_mean | -0.0885    |
| train/info_shaping_reward_min  | -0.22      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.7      |
| train/steps                    | 833600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 521        |
| stats_o/mean                   | 0.20187986 |
| stats_o/std                    | 0.0990484  |
| test/episodes                  | 5220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00246   |
| test/info_shaping_reward_mean  | -0.0651    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -3.710522  |
| test/Q_plus_P                  | -3.710522  |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 208800     |
| train/episodes                 | 20880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.492      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00615   |
| train/info_shaping_reward_mean | -0.0803    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.3      |
| train/steps                    | 835200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 522        |
| stats_o/mean                   | 0.20188637 |
| stats_o/std                    | 0.09902607 |
| test/episodes                  | 5230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.62       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00256   |
| test/info_shaping_reward_mean  | -0.0616    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -3.7868903 |
| test/Q_plus_P                  | -3.7868903 |
| test/reward_per_eps            | -15.2      |
| test/steps                     | 209200     |
| train/episodes                 | 20920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.473      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00799   |
| train/info_shaping_reward_mean | -0.0828    |
| train/info_shaping_reward_min  | -0.205     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 836800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 523        |
| stats_o/mean                   | 0.20187628 |
| stats_o/std                    | 0.09903069 |
| test/episodes                  | 5240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00272   |
| test/info_shaping_reward_mean  | -0.0594    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.9548876 |
| test/Q_plus_P                  | -2.9548876 |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 209600     |
| train/episodes                 | 20960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.441      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00682   |
| train/info_shaping_reward_mean | -0.0855    |
| train/info_shaping_reward_min  | -0.191     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.4      |
| train/steps                    | 838400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 524        |
| stats_o/mean                   | 0.20186947 |
| stats_o/std                    | 0.0990073  |
| test/episodes                  | 5250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00261   |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.1959872 |
| test/Q_plus_P                  | -2.1959872 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 210000     |
| train/episodes                 | 21000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.465      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00881   |
| train/info_shaping_reward_mean | -0.0852    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 840000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 525        |
| stats_o/mean                   | 0.20186967 |
| stats_o/std                    | 0.0990096  |
| test/episodes                  | 5260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00199   |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -2.323777  |
| test/Q_plus_P                  | -2.323777  |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 210400     |
| train/episodes                 | 21040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.47       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00727   |
| train/info_shaping_reward_mean | -0.0925    |
| train/info_shaping_reward_min  | -0.274     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 841600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.201874    |
| stats_o/std                    | 0.098979466 |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.603       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00631    |
| test/info_shaping_reward_mean  | -0.0645     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -4.6626253  |
| test/Q_plus_P                  | -4.6626253  |
| test/reward_per_eps            | -15.9       |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.439       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00771    |
| train/info_shaping_reward_mean | -0.0842     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.4       |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 527         |
| stats_o/mean                   | 0.2018681   |
| stats_o/std                    | 0.098951705 |
| test/episodes                  | 5280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.618       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00924    |
| test/info_shaping_reward_mean  | -0.0785     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -4.7635436  |
| test/Q_plus_P                  | -4.7635436  |
| test/reward_per_eps            | -15.3       |
| test/steps                     | 211200      |
| train/episodes                 | 21120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.492       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00713    |
| train/info_shaping_reward_mean | -0.0777     |
| train/info_shaping_reward_min  | -0.183      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.3       |
| train/steps                    | 844800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 528        |
| stats_o/mean                   | 0.20187148 |
| stats_o/std                    | 0.09894663 |
| test/episodes                  | 5290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.485      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00766   |
| test/info_shaping_reward_mean  | -0.0778    |
| test/info_shaping_reward_min   | -0.208     |
| test/Q                         | -5.571213  |
| test/Q_plus_P                  | -5.571213  |
| test/reward_per_eps            | -20.6      |
| test/steps                     | 211600     |
| train/episodes                 | 21160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.423      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00893   |
| train/info_shaping_reward_mean | -0.0914    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.1      |
| train/steps                    | 846400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.2018806   |
| stats_o/std                    | 0.098951474 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.51        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00637    |
| test/info_shaping_reward_mean  | -0.0733     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -4.9698405  |
| test/Q_plus_P                  | -4.9698405  |
| test/reward_per_eps            | -19.6       |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.48        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00644    |
| train/info_shaping_reward_mean | -0.0817     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.8       |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 530        |
| stats_o/mean                   | 0.20187923 |
| stats_o/std                    | 0.09893607 |
| test/episodes                  | 5310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.0577    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -3.2046905 |
| test/Q_plus_P                  | -3.2046905 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 212400     |
| train/episodes                 | 21240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00765   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.206     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 849600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 531        |
| stats_o/mean                   | 0.20188291 |
| stats_o/std                    | 0.09889799 |
| test/episodes                  | 5320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00189   |
| test/info_shaping_reward_mean  | -0.0615    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -2.479822  |
| test/Q_plus_P                  | -2.479822  |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 212800     |
| train/episodes                 | 21280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.524      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00785   |
| train/info_shaping_reward_mean | -0.0765    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 851200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.20188214  |
| stats_o/std                    | 0.098888606 |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.625       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000765   |
| test/info_shaping_reward_mean  | -0.0646     |
| test/info_shaping_reward_min   | -0.182      |
| test/Q                         | -3.0436354  |
| test/Q_plus_P                  | -3.0436354  |
| test/reward_per_eps            | -15         |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.464       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0072     |
| train/info_shaping_reward_mean | -0.0864     |
| train/info_shaping_reward_min  | -0.225      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.4       |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 533        |
| stats_o/mean                   | 0.20187855 |
| stats_o/std                    | 0.09884494 |
| test/episodes                  | 5340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000892  |
| test/info_shaping_reward_mean  | -0.0601    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.8291078 |
| test/Q_plus_P                  | -2.8291078 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 213600     |
| train/episodes                 | 21360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.454      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00812   |
| train/info_shaping_reward_mean | -0.0869    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 854400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 534        |
| stats_o/mean                   | 0.20188455 |
| stats_o/std                    | 0.09884398 |
| test/episodes                  | 5350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.623      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00763   |
| test/info_shaping_reward_mean  | -0.0656    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -3.4884198 |
| test/Q_plus_P                  | -3.4884198 |
| test/reward_per_eps            | -15.1      |
| test/steps                     | 214000     |
| train/episodes                 | 21400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.456      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00583   |
| train/info_shaping_reward_mean | -0.0871    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 856000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 535        |
| stats_o/mean                   | 0.20189433 |
| stats_o/std                    | 0.0988548  |
| test/episodes                  | 5360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00742   |
| test/info_shaping_reward_mean  | -0.0556    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.1958506 |
| test/Q_plus_P                  | -2.1958506 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 214400     |
| train/episodes                 | 21440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.44       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00585   |
| train/info_shaping_reward_mean | -0.0913    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.4      |
| train/steps                    | 857600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 536        |
| stats_o/mean                   | 0.20189501 |
| stats_o/std                    | 0.09884095 |
| test/episodes                  | 5370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.58       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00541   |
| test/info_shaping_reward_mean  | -0.0877    |
| test/info_shaping_reward_min   | -0.553     |
| test/Q                         | -5.573829  |
| test/Q_plus_P                  | -5.573829  |
| test/reward_per_eps            | -16.8      |
| test/steps                     | 214800     |
| train/episodes                 | 21480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.507      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00896   |
| train/info_shaping_reward_mean | -0.0822    |
| train/info_shaping_reward_min  | -0.199     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.7      |
| train/steps                    | 859200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 537        |
| stats_o/mean                   | 0.20190221 |
| stats_o/std                    | 0.09881472 |
| test/episodes                  | 5380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.575      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00276   |
| test/info_shaping_reward_mean  | -0.071     |
| test/info_shaping_reward_min   | -0.185     |
| test/Q                         | -3.84844   |
| test/Q_plus_P                  | -3.84844   |
| test/reward_per_eps            | -17        |
| test/steps                     | 215200     |
| train/episodes                 | 21520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.493      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00663   |
| train/info_shaping_reward_mean | -0.0854    |
| train/info_shaping_reward_min  | -0.201     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.3      |
| train/steps                    | 860800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 538        |
| stats_o/mean                   | 0.20190667 |
| stats_o/std                    | 0.09878856 |
| test/episodes                  | 5390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00709   |
| test/info_shaping_reward_mean  | -0.0593    |
| test/info_shaping_reward_min   | -0.191     |
| test/Q                         | -2.5617297 |
| test/Q_plus_P                  | -2.5617297 |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 215600     |
| train/episodes                 | 21560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.419      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00727   |
| train/info_shaping_reward_mean | -0.0871    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.2      |
| train/steps                    | 862400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 539        |
| stats_o/mean                   | 0.20190328 |
| stats_o/std                    | 0.09878644 |
| test/episodes                  | 5400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00499   |
| test/info_shaping_reward_mean  | -0.0592    |
| test/info_shaping_reward_min   | -0.2       |
| test/Q                         | -2.6196601 |
| test/Q_plus_P                  | -2.6196601 |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 216000     |
| train/episodes                 | 21600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.465      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00703   |
| train/info_shaping_reward_mean | -0.0841    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 864000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 540        |
| stats_o/mean                   | 0.20189978 |
| stats_o/std                    | 0.09874651 |
| test/episodes                  | 5410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.675      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00674   |
| test/info_shaping_reward_mean  | -0.0565    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.4792337 |
| test/Q_plus_P                  | -2.4792337 |
| test/reward_per_eps            | -13        |
| test/steps                     | 216400     |
| train/episodes                 | 21640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.504      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00592   |
| train/info_shaping_reward_mean | -0.0803    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 865600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 541        |
| stats_o/mean                   | 0.20190611 |
| stats_o/std                    | 0.09871455 |
| test/episodes                  | 5420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0024    |
| test/info_shaping_reward_mean  | -0.0588    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.4146917 |
| test/Q_plus_P                  | -2.4146917 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 216800     |
| train/episodes                 | 21680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00844   |
| train/info_shaping_reward_mean | -0.0778    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 867200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 542        |
| stats_o/mean                   | 0.20190953 |
| stats_o/std                    | 0.09871081 |
| test/episodes                  | 5430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00108   |
| test/info_shaping_reward_mean  | -0.0608    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -3.4550664 |
| test/Q_plus_P                  | -3.4550664 |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 217200     |
| train/episodes                 | 21720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.469      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00646   |
| train/info_shaping_reward_mean | -0.0829    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 868800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 543        |
| stats_o/mean                   | 0.20192039 |
| stats_o/std                    | 0.09871519 |
| test/episodes                  | 5440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00547   |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.8470525 |
| test/Q_plus_P                  | -1.8470525 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 217600     |
| train/episodes                 | 21760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.473      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00579   |
| train/info_shaping_reward_mean | -0.0866    |
| train/info_shaping_reward_min  | -0.204     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 870400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.20192751  |
| stats_o/std                    | 0.098713785 |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.66        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00742    |
| test/info_shaping_reward_mean  | -0.0581     |
| test/info_shaping_reward_min   | -0.176      |
| test/Q                         | -2.4028742  |
| test/Q_plus_P                  | -2.4028742  |
| test/reward_per_eps            | -13.6       |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.473       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0107     |
| train/info_shaping_reward_mean | -0.0912     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.1       |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 545        |
| stats_o/mean                   | 0.20193236 |
| stats_o/std                    | 0.09869776 |
| test/episodes                  | 5460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.627      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00221   |
| test/info_shaping_reward_mean  | -0.0608    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -3.1746747 |
| test/Q_plus_P                  | -3.1746747 |
| test/reward_per_eps            | -14.9      |
| test/steps                     | 218400     |
| train/episodes                 | 21840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.458      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0107    |
| train/info_shaping_reward_mean | -0.0859    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.7      |
| train/steps                    | 873600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 546        |
| stats_o/mean                   | 0.20193659 |
| stats_o/std                    | 0.09869413 |
| test/episodes                  | 5470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00145   |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -1.7426451 |
| test/Q_plus_P                  | -1.7426451 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 218800     |
| train/episodes                 | 21880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.468      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00681   |
| train/info_shaping_reward_mean | -0.0994    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.3      |
| train/steps                    | 875200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 547        |
| stats_o/mean                   | 0.20193473 |
| stats_o/std                    | 0.09865043 |
| test/episodes                  | 5480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00233   |
| test/info_shaping_reward_mean  | -0.0564    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -2.0802026 |
| test/Q_plus_P                  | -2.0802026 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 219200     |
| train/episodes                 | 21920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00786   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 876800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 548        |
| stats_o/mean                   | 0.20194359 |
| stats_o/std                    | 0.09861463 |
| test/episodes                  | 5490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00556   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.6733027 |
| test/Q_plus_P                  | -1.6733027 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 219600     |
| train/episodes                 | 21960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.515      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0081    |
| train/info_shaping_reward_mean | -0.0793    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 878400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 549        |
| stats_o/mean                   | 0.2019526  |
| stats_o/std                    | 0.09855869 |
| test/episodes                  | 5500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000767  |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.0889857 |
| test/Q_plus_P                  | -2.0889857 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 220000     |
| train/episodes                 | 22000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.536      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00696   |
| train/info_shaping_reward_mean | -0.0797    |
| train/info_shaping_reward_min  | -0.214     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 880000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 550        |
| stats_o/mean                   | 0.20194972 |
| stats_o/std                    | 0.09853417 |
| test/episodes                  | 5510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0101    |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.982735  |
| test/Q_plus_P                  | -1.982735  |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 220400     |
| train/episodes                 | 22040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.411      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0107    |
| train/info_shaping_reward_mean | -0.0928    |
| train/info_shaping_reward_min  | -0.198     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.6      |
| train/steps                    | 881600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 551        |
| stats_o/mean                   | 0.20195001 |
| stats_o/std                    | 0.09848322 |
| test/episodes                  | 5520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.665      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00341   |
| test/info_shaping_reward_mean  | -0.0603    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.70794   |
| test/Q_plus_P                  | -2.70794   |
| test/reward_per_eps            | -13.4      |
| test/steps                     | 220800     |
| train/episodes                 | 22080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.535      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00788   |
| train/info_shaping_reward_mean | -0.0773    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 883200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 552        |
| stats_o/mean                   | 0.20196302 |
| stats_o/std                    | 0.09845775 |
| test/episodes                  | 5530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.595      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00995   |
| test/info_shaping_reward_mean  | -0.0599    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.7451336 |
| test/Q_plus_P                  | -2.7451336 |
| test/reward_per_eps            | -16.2      |
| test/steps                     | 221200     |
| train/episodes                 | 22120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.478      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00818   |
| train/info_shaping_reward_mean | -0.0792    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 884800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 553        |
| stats_o/mean                   | 0.20196465 |
| stats_o/std                    | 0.09844616 |
| test/episodes                  | 5540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00402   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.1233795 |
| test/Q_plus_P                  | -2.1233795 |
| test/reward_per_eps            | -11        |
| test/steps                     | 221600     |
| train/episodes                 | 22160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.486      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00591   |
| train/info_shaping_reward_mean | -0.0852    |
| train/info_shaping_reward_min  | -0.219     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 886400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 554         |
| stats_o/mean                   | 0.20196593  |
| stats_o/std                    | 0.098405436 |
| test/episodes                  | 5550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.657       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00647    |
| test/info_shaping_reward_mean  | -0.056      |
| test/info_shaping_reward_min   | -0.177      |
| test/Q                         | -2.1597106  |
| test/Q_plus_P                  | -2.1597106  |
| test/reward_per_eps            | -13.7       |
| test/steps                     | 222000      |
| train/episodes                 | 22200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.536       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00776    |
| train/info_shaping_reward_mean | -0.0764     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 888000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 555        |
| stats_o/mean                   | 0.20196928 |
| stats_o/std                    | 0.09836487 |
| test/episodes                  | 5560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00643   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.7929491 |
| test/Q_plus_P                  | -1.7929491 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 222400     |
| train/episodes                 | 22240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.516      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00742   |
| train/info_shaping_reward_mean | -0.0783    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 889600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 556        |
| stats_o/mean                   | 0.20196664 |
| stats_o/std                    | 0.09835767 |
| test/episodes                  | 5570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00374   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.798772  |
| test/Q_plus_P                  | -1.798772  |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 222800     |
| train/episodes                 | 22280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.54       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00511   |
| train/info_shaping_reward_mean | -0.0845    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 891200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 557        |
| stats_o/mean                   | 0.2019705  |
| stats_o/std                    | 0.09831307 |
| test/episodes                  | 5580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00126   |
| test/info_shaping_reward_mean  | -0.0587    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.4456887 |
| test/Q_plus_P                  | -2.4456887 |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 223200     |
| train/episodes                 | 22320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.547      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00683   |
| train/info_shaping_reward_mean | -0.0792    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 892800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 558        |
| stats_o/mean                   | 0.2019686  |
| stats_o/std                    | 0.09827644 |
| test/episodes                  | 5590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.008     |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.6443983 |
| test/Q_plus_P                  | -1.6443983 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 223600     |
| train/episodes                 | 22360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.514      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00892   |
| train/info_shaping_reward_mean | -0.0818    |
| train/info_shaping_reward_min  | -0.189     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 894400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 559        |
| stats_o/mean                   | 0.2019658  |
| stats_o/std                    | 0.09826749 |
| test/episodes                  | 5600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00298   |
| test/info_shaping_reward_mean  | -0.0592    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.8689535 |
| test/Q_plus_P                  | -2.8689535 |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 224000     |
| train/episodes                 | 22400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.463      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00989   |
| train/info_shaping_reward_mean | -0.0906    |
| train/info_shaping_reward_min  | -0.202     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.5      |
| train/steps                    | 896000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 560        |
| stats_o/mean                   | 0.20197214 |
| stats_o/std                    | 0.09825989 |
| test/episodes                  | 5610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.613      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00827   |
| test/info_shaping_reward_mean  | -0.0607    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -2.6778038 |
| test/Q_plus_P                  | -2.6778038 |
| test/reward_per_eps            | -15.5      |
| test/steps                     | 224400     |
| train/episodes                 | 22440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.483      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00836   |
| train/info_shaping_reward_mean | -0.0832    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.7      |
| train/steps                    | 897600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 561        |
| stats_o/mean                   | 0.20197378 |
| stats_o/std                    | 0.09823868 |
| test/episodes                  | 5620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0106    |
| test/info_shaping_reward_mean  | -0.0523    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -1.8181391 |
| test/Q_plus_P                  | -1.8181391 |
| test/reward_per_eps            | -11        |
| test/steps                     | 224800     |
| train/episodes                 | 22480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.473      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00995   |
| train/info_shaping_reward_mean | -0.0851    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 899200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 562        |
| stats_o/mean                   | 0.20197813 |
| stats_o/std                    | 0.09822139 |
| test/episodes                  | 5630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00418   |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.0677767 |
| test/Q_plus_P                  | -2.0677767 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 225200     |
| train/episodes                 | 22520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.551      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00827   |
| train/info_shaping_reward_mean | -0.0782    |
| train/info_shaping_reward_min  | -0.187     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 900800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 563        |
| stats_o/mean                   | 0.20198719 |
| stats_o/std                    | 0.098205   |
| test/episodes                  | 5640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.605      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0026    |
| test/info_shaping_reward_mean  | -0.0622    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -2.3624277 |
| test/Q_plus_P                  | -2.3624277 |
| test/reward_per_eps            | -15.8      |
| test/steps                     | 225600     |
| train/episodes                 | 22560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.483      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00646   |
| train/info_shaping_reward_mean | -0.0836    |
| train/info_shaping_reward_min  | -0.188     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.7      |
| train/steps                    | 902400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 564         |
| stats_o/mean                   | 0.20199804  |
| stats_o/std                    | 0.098147556 |
| test/episodes                  | 5650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.608       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00404    |
| test/info_shaping_reward_mean  | -0.0623     |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -3.1574347  |
| test/Q_plus_P                  | -3.1574347  |
| test/reward_per_eps            | -15.7       |
| test/steps                     | 226000      |
| train/episodes                 | 22600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.535       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0051     |
| train/info_shaping_reward_mean | -0.0774     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 904000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 565        |
| stats_o/mean                   | 0.20199688 |
| stats_o/std                    | 0.09812065 |
| test/episodes                  | 5660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00567   |
| test/info_shaping_reward_mean  | -0.0604    |
| test/info_shaping_reward_min   | -0.178     |
| test/Q                         | -2.6991377 |
| test/Q_plus_P                  | -2.6991377 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 226400     |
| train/episodes                 | 22640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.488      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00727   |
| train/info_shaping_reward_mean | -0.0897    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.5      |
| train/steps                    | 905600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 566        |
| stats_o/mean                   | 0.20199497 |
| stats_o/std                    | 0.09806479 |
| test/episodes                  | 5670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.655      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00499   |
| test/info_shaping_reward_mean  | -0.0677    |
| test/info_shaping_reward_min   | -0.21      |
| test/Q                         | -3.2908204 |
| test/Q_plus_P                  | -3.2908204 |
| test/reward_per_eps            | -13.8      |
| test/steps                     | 226800     |
| train/episodes                 | 22680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00696   |
| train/info_shaping_reward_mean | -0.0745    |
| train/info_shaping_reward_min  | -0.177     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 907200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 567        |
| stats_o/mean                   | 0.20199607 |
| stats_o/std                    | 0.09806234 |
| test/episodes                  | 5680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0106    |
| test/info_shaping_reward_mean  | -0.0578    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.521093  |
| test/Q_plus_P                  | -2.521093  |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 227200     |
| train/episodes                 | 22720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.453      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00521   |
| train/info_shaping_reward_mean | -0.0882    |
| train/info_shaping_reward_min  | -0.222     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.9      |
| train/steps                    | 908800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 568        |
| stats_o/mean                   | 0.20200016 |
| stats_o/std                    | 0.09803707 |
| test/episodes                  | 5690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0131    |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -2.430478  |
| test/Q_plus_P                  | -2.430478  |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 227600     |
| train/episodes                 | 22760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.515      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00568   |
| train/info_shaping_reward_mean | -0.0811    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 910400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.20200127  |
| stats_o/std                    | 0.098025516 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.618       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0077     |
| test/info_shaping_reward_mean  | -0.062      |
| test/info_shaping_reward_min   | -0.178      |
| test/Q                         | -2.6683512  |
| test/Q_plus_P                  | -2.6683512  |
| test/reward_per_eps            | -15.3       |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.503       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00952    |
| train/info_shaping_reward_mean | -0.0784     |
| train/info_shaping_reward_min  | -0.181      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.9       |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.20200117  |
| stats_o/std                    | 0.098011464 |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.657       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.0556     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -2.4445672  |
| test/Q_plus_P                  | -2.4445672  |
| test/reward_per_eps            | -13.7       |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.531       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00529    |
| train/info_shaping_reward_mean | -0.081      |
| train/info_shaping_reward_min  | -0.191      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.8       |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 571        |
| stats_o/mean                   | 0.20200197 |
| stats_o/std                    | 0.09797334 |
| test/episodes                  | 5720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.57       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00352   |
| test/info_shaping_reward_mean  | -0.0667    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -4.082144  |
| test/Q_plus_P                  | -4.082144  |
| test/reward_per_eps            | -17.2      |
| test/steps                     | 228800     |
| train/episodes                 | 22880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.579      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00685   |
| train/info_shaping_reward_mean | -0.0712    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 915200     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 572       |
| stats_o/mean                   | 0.2019977 |
| stats_o/std                    | 0.097965  |
| test/episodes                  | 5730      |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.738     |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.00146  |
| test/info_shaping_reward_mean  | -0.0524   |
| test/info_shaping_reward_min   | -0.178    |
| test/Q                         | -2.064641 |
| test/Q_plus_P                  | -2.064641 |
| test/reward_per_eps            | -10.5     |
| test/steps                     | 229200    |
| train/episodes                 | 22920     |
| train/info_is_success_max      | 1         |
| train/info_is_success_mean     | 0.468     |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.00792  |
| train/info_shaping_reward_mean | -0.0826   |
| train/info_shaping_reward_min  | -0.187    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -21.3     |
| train/steps                    | 916800    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 573        |
| stats_o/mean                   | 0.20200664 |
| stats_o/std                    | 0.09792298 |
| test/episodes                  | 5740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00533   |
| test/info_shaping_reward_mean  | -0.0639    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -2.7308915 |
| test/Q_plus_P                  | -2.7308915 |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 229600     |
| train/episodes                 | 22960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.507      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00735   |
| train/info_shaping_reward_mean | -0.0829    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.7      |
| train/steps                    | 918400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.20202273  |
| stats_o/std                    | 0.097922705 |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.645       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0124     |
| test/info_shaping_reward_mean  | -0.0672     |
| test/info_shaping_reward_min   | -0.174      |
| test/Q                         | -3.3208418  |
| test/Q_plus_P                  | -3.3208418  |
| test/reward_per_eps            | -14.2       |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.509       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0081     |
| train/info_shaping_reward_mean | -0.0815     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.6       |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 575        |
| stats_o/mean                   | 0.20202179 |
| stats_o/std                    | 0.09791176 |
| test/episodes                  | 5760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0581    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.8142891 |
| test/Q_plus_P                  | -1.8142891 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 230400     |
| train/episodes                 | 23040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.514      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00999   |
| train/info_shaping_reward_mean | -0.0811    |
| train/info_shaping_reward_min  | -0.194     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 921600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 576        |
| stats_o/mean                   | 0.20201153 |
| stats_o/std                    | 0.09787451 |
| test/episodes                  | 5770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00503   |
| test/info_shaping_reward_mean  | -0.0468    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.5481344 |
| test/Q_plus_P                  | -1.5481344 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 230800     |
| train/episodes                 | 23080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.54       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00893   |
| train/info_shaping_reward_mean | -0.0769    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 923200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 577         |
| stats_o/mean                   | 0.20201702  |
| stats_o/std                    | 0.097851664 |
| test/episodes                  | 5780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -2.9059887  |
| test/Q_plus_P                  | -2.9059887  |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 231200      |
| train/episodes                 | 23120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.519       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00953    |
| train/info_shaping_reward_mean | -0.0792     |
| train/info_shaping_reward_min  | -0.178      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.2       |
| train/steps                    | 924800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.20202869 |
| stats_o/std                    | 0.09781738 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00478   |
| test/info_shaping_reward_mean  | -0.0499    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5338022 |
| test/Q_plus_P                  | -1.5338022 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00657   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.176     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 579        |
| stats_o/mean                   | 0.20203781 |
| stats_o/std                    | 0.09778081 |
| test/episodes                  | 5800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00162   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.5367458 |
| test/Q_plus_P                  | -1.5367458 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 232000     |
| train/episodes                 | 23200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00861   |
| train/info_shaping_reward_mean | -0.078     |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 928000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 580        |
| stats_o/mean                   | 0.20203611 |
| stats_o/std                    | 0.09775298 |
| test/episodes                  | 5810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.642      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0128    |
| test/info_shaping_reward_mean  | -0.0673    |
| test/info_shaping_reward_min   | -0.177     |
| test/Q                         | -3.1690476 |
| test/Q_plus_P                  | -3.1690476 |
| test/reward_per_eps            | -14.3      |
| test/steps                     | 232400     |
| train/episodes                 | 23240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.532      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00604   |
| train/info_shaping_reward_mean | -0.0739    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 929600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 581        |
| stats_o/mean                   | 0.2020333  |
| stats_o/std                    | 0.09774426 |
| test/episodes                  | 5820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00295   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.7528877 |
| test/Q_plus_P                  | -1.7528877 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 232800     |
| train/episodes                 | 23280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.463      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00956   |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.5      |
| train/steps                    | 931200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 582        |
| stats_o/mean                   | 0.20202956 |
| stats_o/std                    | 0.09770571 |
| test/episodes                  | 5830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00205   |
| test/info_shaping_reward_mean  | -0.0609    |
| test/info_shaping_reward_min   | -0.18      |
| test/Q                         | -2.7975085 |
| test/Q_plus_P                  | -2.7975085 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 233200     |
| train/episodes                 | 23320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.494      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00862   |
| train/info_shaping_reward_mean | -0.0829    |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 932800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 583        |
| stats_o/mean                   | 0.20202953 |
| stats_o/std                    | 0.09767944 |
| test/episodes                  | 5840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0136    |
| test/info_shaping_reward_mean  | -0.0564    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -1.8233911 |
| test/Q_plus_P                  | -1.8233911 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 233600     |
| train/episodes                 | 23360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.538      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00755   |
| train/info_shaping_reward_mean | -0.076     |
| train/info_shaping_reward_min  | -0.186     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 934400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 584         |
| stats_o/mean                   | 0.20203176  |
| stats_o/std                    | 0.097638644 |
| test/episodes                  | 5850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00464    |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -2.0495672  |
| test/Q_plus_P                  | -2.0495672  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 234000      |
| train/episodes                 | 23400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.501       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00599    |
| train/info_shaping_reward_mean | -0.0821     |
| train/info_shaping_reward_min  | -0.225      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20         |
| train/steps                    | 936000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 585        |
| stats_o/mean                   | 0.20202734 |
| stats_o/std                    | 0.09759154 |
| test/episodes                  | 5860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000765  |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.4532056 |
| test/Q_plus_P                  | -1.4532056 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 234400     |
| train/episodes                 | 23440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.495      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00841   |
| train/info_shaping_reward_mean | -0.0827    |
| train/info_shaping_reward_min  | -0.184     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 937600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.20203316  |
| stats_o/std                    | 0.097547844 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00237    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.175      |
| test/Q                         | -1.4280804  |
| test/Q_plus_P                  | -1.4280804  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.54        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00663    |
| train/info_shaping_reward_mean | -0.0782     |
| train/info_shaping_reward_min  | -0.193      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 587        |
| stats_o/mean                   | 0.20203358 |
| stats_o/std                    | 0.09750101 |
| test/episodes                  | 5880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00315   |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.773897  |
| test/Q_plus_P                  | -1.773897  |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 235200     |
| train/episodes                 | 23520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00834   |
| train/info_shaping_reward_mean | -0.074     |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 940800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 588        |
| stats_o/mean                   | 0.20203581 |
| stats_o/std                    | 0.09745442 |
| test/episodes                  | 5890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0102    |
| test/info_shaping_reward_mean  | -0.0541    |
| test/info_shaping_reward_min   | -0.174     |
| test/Q                         | -1.7278731 |
| test/Q_plus_P                  | -1.7278731 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 235600     |
| train/episodes                 | 23560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.524      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00649   |
| train/info_shaping_reward_mean | -0.079     |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 942400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 589        |
| stats_o/mean                   | 0.20204668 |
| stats_o/std                    | 0.09742468 |
| test/episodes                  | 5900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00751   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -1.3597428 |
| test/Q_plus_P                  | -1.3597428 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 236000     |
| train/episodes                 | 23600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00571   |
| train/info_shaping_reward_mean | -0.0732    |
| train/info_shaping_reward_min  | -0.182     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 944000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 590        |
| stats_o/mean                   | 0.2020524  |
| stats_o/std                    | 0.09738454 |
| test/episodes                  | 5910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00382   |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.182     |
| test/Q                         | -1.8626882 |
| test/Q_plus_P                  | -1.8626882 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 236400     |
| train/episodes                 | 23640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00759   |
| train/info_shaping_reward_mean | -0.0734    |
| train/info_shaping_reward_min  | -0.179     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 945600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 591        |
| stats_o/mean                   | 0.20205574 |
| stats_o/std                    | 0.09736834 |
| test/episodes                  | 5920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000743  |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.175     |
| test/Q                         | -1.8393375 |
| test/Q_plus_P                  | -1.8393375 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 236800     |
| train/episodes                 | 23680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00734   |
| train/info_shaping_reward_mean | -0.085     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 947200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 592        |
| stats_o/mean                   | 0.20205776 |
| stats_o/std                    | 0.09736154 |
| test/episodes                  | 5930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00389   |
| test/info_shaping_reward_mean  | -0.0557    |
| test/info_shaping_reward_min   | -0.176     |
| test/Q                         | -2.0369267 |
| test/Q_plus_P                  | -2.0369267 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 237200     |
| train/episodes                 | 23720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00788   |
| train/info_shaping_reward_mean | -0.0824    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 948800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 593        |
| stats_o/mean                   | 0.20206553 |
| stats_o/std                    | 0.09732577 |
| test/episodes                  | 5940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00349   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.184     |
| test/Q                         | -1.530206  |
| test/Q_plus_P                  | -1.530206  |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 237600     |
| train/episodes                 | 23760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.519      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00553   |
| train/info_shaping_reward_mean | -0.0769    |
| train/info_shaping_reward_min  | -0.18      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.2      |
| train/steps                    | 950400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 594         |
| stats_o/mean                   | 0.20206185  |
| stats_o/std                    | 0.097301595 |
| test/episodes                  | 5950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0101     |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.173      |
| test/Q                         | -1.5368344  |
| test/Q_plus_P                  | -1.5368344  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 238000      |
| train/episodes                 | 23800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.528       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00711    |
| train/info_shaping_reward_mean | -0.0791     |
| train/info_shaping_reward_min  | -0.184      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 952000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 595        |
| stats_o/mean                   | 0.2020744  |
| stats_o/std                    | 0.09727635 |
| test/episodes                  | 5960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00243   |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.188     |
| test/Q                         | -1.437141  |
| test/Q_plus_P                  | -1.437141  |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 238400     |
| train/episodes                 | 23840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.541      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00628   |
| train/info_shaping_reward_mean | -0.0798    |
| train/info_shaping_reward_min  | -0.215     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 953600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 596        |
| stats_o/mean                   | 0.20208064 |
| stats_o/std                    | 0.09722535 |
| test/episodes                  | 5970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00856   |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.183     |
| test/Q                         | -1.6341393 |
| test/Q_plus_P                  | -1.6341393 |
| test/reward_per_eps            | -11        |
| test/steps                     | 238800     |
| train/episodes                 | 23880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00575   |
| train/info_shaping_reward_mean | -0.0678    |
| train/info_shaping_reward_min  | -0.181     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 955200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 597        |
| stats_o/mean                   | 0.20207852 |
| stats_o/std                    | 0.0971788  |
| test/episodes                  | 5980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.64       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00474   |
| test/info_shaping_reward_mean  | -0.0725    |
| test/info_shaping_reward_min   | -0.179     |
| test/Q                         | -2.7223768 |
| test/Q_plus_P                  | -2.7223768 |
| test/reward_per_eps            | -14.4      |
| test/steps                     | 239200     |
| train/episodes                 | 23920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.563      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00817   |
| train/info_shaping_reward_mean | -0.0756    |
| train/info_shaping_reward_min  | -0.178     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 956800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 598        |
| stats_o/mean                   | 0.20207843 |
| stats_o/std                    | 0.097152   |
| test/episodes                  | 5990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0101    |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.750371  |
| test/Q_plus_P                  | -1.750371  |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 239600     |
| train/episodes                 | 23960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0069    |
| train/info_shaping_reward_mean | -0.0756    |
| train/info_shaping_reward_min  | -0.185     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 958400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.20208254 |
| stats_o/std                    | 0.09712509 |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0105    |
| test/info_shaping_reward_mean  | -0.0577    |
| test/info_shaping_reward_min   | -0.173     |
| test/Q                         | -1.8054355 |
| test/Q_plus_P                  | -1.8054355 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00654   |
| train/info_shaping_reward_mean | -0.0721    |
| train/info_shaping_reward_min  | -0.183     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
