Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/GymMujoco/HalfCheetah-v2/config_MAGE/seed_0
Current potential weight:  1.0
---------------------------------------------
| epoch                       | 0           |
| stats_g/mean                | nan         |
| stats_g/std                 | nan         |
| stats_o/mean                | 0.061221503 |
| stats_o/std                 | 2.4762733   |
| test/episodes               | 10          |
| test/info_reward_ctrl_max   | -0.0387     |
| test/info_reward_ctrl_mean  | -0.0854     |
| test/info_reward_ctrl_min   | -0.345      |
| test/info_reward_run_max    | 1.01        |
| test/info_reward_run_mean   | 0.00235     |
| test/info_reward_run_min    | -0.928      |
| test/reward_per_eps         | -83.1       |
| test/steps                  | 10000       |
| train/episodes              | 6           |
| train/info_reward_ctrl_max  | -0.291      |
| train/info_reward_ctrl_mean | -0.291      |
| train/info_reward_ctrl_min  | -0.291      |
| train/info_reward_run_max   | 0.0182      |
| train/info_reward_run_mean  | 0.0182      |
| train/info_reward_run_min   | 0.0182      |
| train/reward_per_eps        | nan         |
| train/steps                 | 6000        |
---------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 1          |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.07155826 |
| stats_o/std                 | 2.321695   |
| test/episodes               | 20         |
| test/info_reward_ctrl_max   | -0.0229    |
| test/info_reward_ctrl_mean  | -0.331     |
| test/info_reward_ctrl_min   | -0.593     |
| test/info_reward_run_max    | 3.1        |
| test/info_reward_run_mean   | 0.62       |
| test/info_reward_run_min    | -1.51      |
| test/reward_per_eps         | 290        |
| test/steps                  | 20000      |
| train/episodes              | 7          |
| train/info_reward_ctrl_max  | -0.394     |
| train/info_reward_ctrl_mean | -0.394     |
| train/info_reward_ctrl_min  | -0.394     |
| train/info_reward_run_max   | 0.000921   |
| train/info_reward_run_mean  | 0.000921   |
| train/info_reward_run_min   | 0.000921   |
| train/reward_per_eps        | nan        |
| train/steps                 | 7000       |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 2          |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.08999009 |
| stats_o/std                 | 2.5291028  |
| test/episodes               | 30         |
| test/info_reward_ctrl_max   | -0.0431    |
| test/info_reward_ctrl_mean  | -0.433     |
| test/info_reward_ctrl_min   | -0.596     |
| test/info_reward_run_max    | 2.94       |
| test/info_reward_run_mean   | 0.0213     |
| test/info_reward_run_min    | -2.33      |
| test/reward_per_eps         | -411       |
| test/steps                  | 30000      |
| train/episodes              | 8          |
| train/info_reward_ctrl_max  | -0.411     |
| train/info_reward_ctrl_mean | -0.411     |
| train/info_reward_ctrl_min  | -0.411     |
| train/info_reward_run_max   | 0.645      |
| train/info_reward_run_mean  | 0.645      |
| train/info_reward_run_min   | 0.645      |
| train/reward_per_eps        | nan        |
| train/steps                 | 8000       |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
---------------------------------------------
| epoch                       | 3           |
| stats_g/mean                | nan         |
| stats_g/std                 | nan         |
| stats_o/mean                | 0.104476176 |
| stats_o/std                 | 2.736981    |
| test/episodes               | 40          |
| test/info_reward_ctrl_max   | -0.1        |
| test/info_reward_ctrl_mean  | -0.468      |
| test/info_reward_ctrl_min   | -0.599      |
| test/info_reward_run_max    | 3.39        |
| test/info_reward_run_mean   | 0.981       |
| test/info_reward_run_min    | -1.33       |
| test/reward_per_eps         | 513         |
| test/steps                  | 40000       |
| train/episodes              | 9           |
| train/info_reward_ctrl_max  | -0.413      |
| train/info_reward_ctrl_mean | -0.413      |
| train/info_reward_ctrl_min  | -0.413      |
| train/info_reward_run_max   | 0.849       |
| train/info_reward_run_mean  | 0.849       |
| train/info_reward_run_min   | 0.849       |
| train/reward_per_eps        | nan         |
| train/steps                 | 9000        |
---------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 4          |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.11764964 |
| stats_o/std                 | 2.9196503  |
| test/episodes               | 50         |
| test/info_reward_ctrl_max   | -0.052     |
| test/info_reward_ctrl_mean  | -0.402     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 3.93       |
| test/info_reward_run_mean   | 0.0414     |
| test/info_reward_run_min    | -4.04      |
| test/reward_per_eps         | -360       |
| test/steps                  | 50000      |
| train/episodes              | 10         |
| train/info_reward_ctrl_max  | -0.412     |
| train/info_reward_ctrl_mean | -0.412     |
| train/info_reward_ctrl_min  | -0.412     |
| train/info_reward_run_max   | 0.857      |
| train/info_reward_run_mean  | 0.857      |
| train/info_reward_run_min   | 0.857      |
| train/reward_per_eps        | nan        |
| train/steps                 | 10000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 5          |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.12461896 |
| stats_o/std                 | 3.035271   |
| test/episodes               | 60         |
| test/info_reward_ctrl_max   | -0.0908    |
| test/info_reward_ctrl_mean  | -0.439     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 4.21       |
| test/info_reward_run_mean   | 0.764      |
| test/info_reward_run_min    | -2.31      |
| test/reward_per_eps         | 326        |
| test/steps                  | 60000      |
| train/episodes              | 11         |
| train/info_reward_ctrl_max  | -0.403     |
| train/info_reward_ctrl_mean | -0.403     |
| train/info_reward_ctrl_min  | -0.403     |
| train/info_reward_run_max   | 0.987      |
| train/info_reward_run_mean  | 0.987      |
| train/info_reward_run_min   | 0.987      |
| train/reward_per_eps        | nan        |
| train/steps                 | 11000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 6          |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.13240702 |
| stats_o/std                 | 3.106372   |
| test/episodes               | 70         |
| test/info_reward_ctrl_max   | -0.0579    |
| test/info_reward_ctrl_mean  | -0.433     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 4.97       |
| test/info_reward_run_mean   | 1.06       |
| test/info_reward_run_min    | -3.29      |
| test/reward_per_eps         | 626        |
| test/steps                  | 70000      |
| train/episodes              | 12         |
| train/info_reward_ctrl_max  | -0.358     |
| train/info_reward_ctrl_mean | -0.358     |
| train/info_reward_ctrl_min  | -0.358     |
| train/info_reward_run_max   | 1.14       |
| train/info_reward_run_mean  | 1.14       |
| train/info_reward_run_min   | 1.14       |
| train/reward_per_eps        | nan        |
| train/steps                 | 12000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 7          |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.12814343 |
| stats_o/std                 | 3.1580822  |
| test/episodes               | 80         |
| test/info_reward_ctrl_max   | -0.122     |
| test/info_reward_ctrl_mean  | -0.479     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 4.67       |
| test/info_reward_run_mean   | 1.37       |
| test/info_reward_run_min    | -2.22      |
| test/reward_per_eps         | 893        |
| test/steps                  | 80000      |
| train/episodes              | 13         |
| train/info_reward_ctrl_max  | -0.453     |
| train/info_reward_ctrl_mean | -0.453     |
| train/info_reward_ctrl_min  | -0.453     |
| train/info_reward_run_max   | 1.44       |
| train/info_reward_run_mean  | 1.44       |
| train/info_reward_run_min   | 1.44       |
| train/reward_per_eps        | nan        |
| train/steps                 | 13000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 8          |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.13322769 |
| stats_o/std                 | 3.226354   |
| test/episodes               | 90         |
| test/info_reward_ctrl_max   | -0.151     |
| test/info_reward_ctrl_mean  | -0.473     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 4.75       |
| test/info_reward_run_mean   | 1.1        |
| test/info_reward_run_min    | -2.49      |
| test/reward_per_eps         | 622        |
| test/steps                  | 90000      |
| train/episodes              | 14         |
| train/info_reward_ctrl_max  | -0.393     |
| train/info_reward_ctrl_mean | -0.393     |
| train/info_reward_ctrl_min  | -0.393     |
| train/info_reward_run_max   | 1.29       |
| train/info_reward_run_mean  | 1.29       |
| train/info_reward_run_min   | 1.29       |
| train/reward_per_eps        | nan        |
| train/steps                 | 14000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 9          |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.13178721 |
| stats_o/std                 | 3.2528322  |
| test/episodes               | 100        |
| test/info_reward_ctrl_max   | -0.0809    |
| test/info_reward_ctrl_mean  | -0.479     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 5.06       |
| test/info_reward_run_mean   | 1.93       |
| test/info_reward_run_min    | -1.38      |
| test/reward_per_eps         | 1.45e+03   |
| test/steps                  | 100000     |
| train/episodes              | 15         |
| train/info_reward_ctrl_max  | -0.446     |
| train/info_reward_ctrl_mean | -0.446     |
| train/info_reward_ctrl_min  | -0.446     |
| train/info_reward_run_max   | 0.885      |
| train/info_reward_run_mean  | 0.885      |
| train/info_reward_run_min   | 0.885      |
| train/reward_per_eps        | nan        |
| train/steps                 | 15000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
------------------------------------------
| epoch                       | 10       |
| stats_g/mean                | nan      |
| stats_g/std                 | nan      |
| stats_o/mean                | 0.130683 |
| stats_o/std                 | 3.279227 |
| test/episodes               | 110      |
| test/info_reward_ctrl_max   | -0.136   |
| test/info_reward_ctrl_mean  | -0.512   |
| test/info_reward_ctrl_min   | -0.6     |
| test/info_reward_run_max    | 5.87     |
| test/info_reward_run_mean   | 2.27     |
| test/info_reward_run_min    | -1.79    |
| test/reward_per_eps         | 1.76e+03 |
| test/steps                  | 110000   |
| train/episodes              | 16       |
| train/info_reward_ctrl_max  | -0.477   |
| train/info_reward_ctrl_mean | -0.477   |
| train/info_reward_ctrl_min  | -0.477   |
| train/info_reward_run_max   | 1.3      |
| train/info_reward_run_mean  | 1.3      |
| train/info_reward_run_min   | 1.3      |
| train/reward_per_eps        | nan      |
| train/steps                 | 16000    |
------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 11         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.12919651 |
| stats_o/std                 | 3.3046606  |
| test/episodes               | 120        |
| test/info_reward_ctrl_max   | -0.1       |
| test/info_reward_ctrl_mean  | -0.506     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 5.87       |
| test/info_reward_run_mean   | 1.89       |
| test/info_reward_run_min    | -2.15      |
| test/reward_per_eps         | 1.38e+03   |
| test/steps                  | 120000     |
| train/episodes              | 17         |
| train/info_reward_ctrl_max  | -0.46      |
| train/info_reward_ctrl_mean | -0.46      |
| train/info_reward_ctrl_min  | -0.46      |
| train/info_reward_run_max   | 2          |
| train/info_reward_run_mean  | 2          |
| train/info_reward_run_min   | 2          |
| train/reward_per_eps        | nan        |
| train/steps                 | 17000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 12         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.12965457 |
| stats_o/std                 | 3.3243737  |
| test/episodes               | 130        |
| test/info_reward_ctrl_max   | -0.157     |
| test/info_reward_ctrl_mean  | -0.51      |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 5.15       |
| test/info_reward_run_mean   | 1.01       |
| test/info_reward_run_min    | -2.42      |
| test/reward_per_eps         | 497        |
| test/steps                  | 130000     |
| train/episodes              | 18         |
| train/info_reward_ctrl_max  | -0.43      |
| train/info_reward_ctrl_mean | -0.43      |
| train/info_reward_ctrl_min  | -0.43      |
| train/info_reward_run_max   | 2.14       |
| train/info_reward_run_mean  | 2.14       |
| train/info_reward_run_min   | 2.14       |
| train/reward_per_eps        | nan        |
| train/steps                 | 18000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
---------------------------------------------
| epoch                       | 13          |
| stats_g/mean                | nan         |
| stats_g/std                 | nan         |
| stats_o/mean                | 0.112285495 |
| stats_o/std                 | 3.3261616   |
| test/episodes               | 140         |
| test/info_reward_ctrl_max   | -0.176      |
| test/info_reward_ctrl_mean  | -0.547      |
| test/info_reward_ctrl_min   | -0.6        |
| test/info_reward_run_max    | 5.27        |
| test/info_reward_run_mean   | 1.53        |
| test/info_reward_run_min    | -2.48       |
| test/reward_per_eps         | 979         |
| test/steps                  | 140000      |
| train/episodes              | 19          |
| train/info_reward_ctrl_max  | -0.553      |
| train/info_reward_ctrl_mean | -0.553      |
| train/info_reward_ctrl_min  | -0.553      |
| train/info_reward_run_max   | 0.00739     |
| train/info_reward_run_mean  | 0.00739     |
| train/info_reward_run_min   | 0.00739     |
| train/reward_per_eps        | nan         |
| train/steps                 | 19000       |
---------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 14         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.09970131 |
| stats_o/std                 | 3.36492    |
| test/episodes               | 150        |
| test/info_reward_ctrl_max   | -0.105     |
| test/info_reward_ctrl_mean  | -0.517     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 6.17       |
| test/info_reward_run_mean   | 2.75       |
| test/info_reward_run_min    | -1.59      |
| test/reward_per_eps         | 2.24e+03   |
| test/steps                  | 150000     |
| train/episodes              | 20         |
| train/info_reward_ctrl_max  | -0.528     |
| train/info_reward_ctrl_mean | -0.528     |
| train/info_reward_ctrl_min  | -0.528     |
| train/info_reward_run_max   | 0.0665     |
| train/info_reward_run_mean  | 0.0665     |
| train/info_reward_run_min   | 0.0665     |
| train/reward_per_eps        | nan        |
| train/steps                 | 20000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 15         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.10096308 |
| stats_o/std                 | 3.3903549  |
| test/episodes               | 160        |
| test/info_reward_ctrl_max   | -0.182     |
| test/info_reward_ctrl_mean  | -0.494     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 5.97       |
| test/info_reward_run_mean   | 3.21       |
| test/info_reward_run_min    | -1.46      |
| test/reward_per_eps         | 2.71e+03   |
| test/steps                  | 160000     |
| train/episodes              | 21         |
| train/info_reward_ctrl_max  | -0.477     |
| train/info_reward_ctrl_mean | -0.477     |
| train/info_reward_ctrl_min  | -0.477     |
| train/info_reward_run_max   | 2.67       |
| train/info_reward_run_mean  | 2.67       |
| train/info_reward_run_min   | 2.67       |
| train/reward_per_eps        | nan        |
| train/steps                 | 21000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 16         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.10386914 |
| stats_o/std                 | 3.4009001  |
| test/episodes               | 170        |
| test/info_reward_ctrl_max   | -0.0961    |
| test/info_reward_ctrl_mean  | -0.481     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 5.43       |
| test/info_reward_run_mean   | 2.54       |
| test/info_reward_run_min    | -1.4       |
| test/reward_per_eps         | 2.06e+03   |
| test/steps                  | 170000     |
| train/episodes              | 22         |
| train/info_reward_ctrl_max  | -0.443     |
| train/info_reward_ctrl_mean | -0.443     |
| train/info_reward_ctrl_min  | -0.443     |
| train/info_reward_run_max   | 3.02       |
| train/info_reward_run_mean  | 3.02       |
| train/info_reward_run_min   | 3.02       |
| train/reward_per_eps        | nan        |
| train/steps                 | 22000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 17         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.10680577 |
| stats_o/std                 | 3.4020174  |
| test/episodes               | 180        |
| test/info_reward_ctrl_max   | -0.102     |
| test/info_reward_ctrl_mean  | -0.413     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 4.76       |
| test/info_reward_run_mean   | 3          |
| test/info_reward_run_min    | -2.62      |
| test/reward_per_eps         | 2.59e+03   |
| test/steps                  | 180000     |
| train/episodes              | 23         |
| train/info_reward_ctrl_max  | -0.435     |
| train/info_reward_ctrl_mean | -0.435     |
| train/info_reward_ctrl_min  | -0.435     |
| train/info_reward_run_max   | 3.23       |
| train/info_reward_run_mean  | 3.23       |
| train/info_reward_run_min   | 3.23       |
| train/reward_per_eps        | nan        |
| train/steps                 | 23000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 18         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.11002896 |
| stats_o/std                 | 3.4092112  |
| test/episodes               | 190        |
| test/info_reward_ctrl_max   | -0.0605    |
| test/info_reward_ctrl_mean  | -0.465     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 5.82       |
| test/info_reward_run_mean   | 2.91       |
| test/info_reward_run_min    | -2.1       |
| test/reward_per_eps         | 2.45e+03   |
| test/steps                  | 190000     |
| train/episodes              | 24         |
| train/info_reward_ctrl_max  | -0.422     |
| train/info_reward_ctrl_mean | -0.422     |
| train/info_reward_ctrl_min  | -0.422     |
| train/info_reward_run_max   | 3.48       |
| train/info_reward_run_mean  | 3.48       |
| train/info_reward_run_min   | 3.48       |
| train/reward_per_eps        | nan        |
| train/steps                 | 24000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
---------------------------------------------
| epoch                       | 19          |
| stats_g/mean                | nan         |
| stats_g/std                 | nan         |
| stats_o/mean                | 0.113658845 |
| stats_o/std                 | 3.4227538   |
| test/episodes               | 200         |
| test/info_reward_ctrl_max   | -0.0799     |
| test/info_reward_ctrl_mean  | -0.492      |
| test/info_reward_ctrl_min   | -0.6        |
| test/info_reward_run_max    | 6.85        |
| test/info_reward_run_mean   | 2.54        |
| test/info_reward_run_min    | -2.04       |
| test/reward_per_eps         | 2.05e+03    |
| test/steps                  | 200000      |
| train/episodes              | 25          |
| train/info_reward_ctrl_max  | -0.457      |
| train/info_reward_ctrl_mean | -0.457      |
| train/info_reward_ctrl_min  | -0.457      |
| train/info_reward_run_max   | 2.2         |
| train/info_reward_run_mean  | 2.2         |
| train/info_reward_run_min   | 2.2         |
| train/reward_per_eps        | nan         |
| train/steps                 | 25000       |
---------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 20         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.11725044 |
| stats_o/std                 | 3.4606676  |
| test/episodes               | 210        |
| test/info_reward_ctrl_max   | -0.0512    |
| test/info_reward_ctrl_mean  | -0.477     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 6.62       |
| test/info_reward_run_mean   | 3.48       |
| test/info_reward_run_min    | -1.78      |
| test/reward_per_eps         | 3e+03      |
| test/steps                  | 210000     |
| train/episodes              | 26         |
| train/info_reward_ctrl_max  | -0.49      |
| train/info_reward_ctrl_mean | -0.49      |
| train/info_reward_ctrl_min  | -0.49      |
| train/info_reward_run_max   | 1.02       |
| train/info_reward_run_mean  | 1.02       |
| train/info_reward_run_min   | 1.02       |
| train/reward_per_eps        | nan        |
| train/steps                 | 26000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 21         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.12119132 |
| stats_o/std                 | 3.4743109  |
| test/episodes               | 220        |
| test/info_reward_ctrl_max   | -0.095     |
| test/info_reward_ctrl_mean  | -0.479     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 6.92       |
| test/info_reward_run_mean   | 3.67       |
| test/info_reward_run_min    | -1.7       |
| test/reward_per_eps         | 3.19e+03   |
| test/steps                  | 220000     |
| train/episodes              | 27         |
| train/info_reward_ctrl_max  | -0.433     |
| train/info_reward_ctrl_mean | -0.433     |
| train/info_reward_ctrl_min  | -0.433     |
| train/info_reward_run_max   | 3.93       |
| train/info_reward_run_mean  | 3.93       |
| train/info_reward_run_min   | 3.93       |
| train/reward_per_eps        | nan        |
| train/steps                 | 27000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 22         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.12478322 |
| stats_o/std                 | 3.4921439  |
| test/episodes               | 230        |
| test/info_reward_ctrl_max   | -0.0742    |
| test/info_reward_ctrl_mean  | -0.466     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 6.74       |
| test/info_reward_run_mean   | 4.01       |
| test/info_reward_run_min    | -1.16      |
| test/reward_per_eps         | 3.55e+03   |
| test/steps                  | 230000     |
| train/episodes              | 28         |
| train/info_reward_ctrl_max  | -0.451     |
| train/info_reward_ctrl_mean | -0.451     |
| train/info_reward_ctrl_min  | -0.451     |
| train/info_reward_run_max   | 3.67       |
| train/info_reward_run_mean  | 3.67       |
| train/info_reward_run_min   | 3.67       |
| train/reward_per_eps        | nan        |
| train/steps                 | 28000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 23         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.12826958 |
| stats_o/std                 | 3.5056236  |
| test/episodes               | 240        |
| test/info_reward_ctrl_max   | -0.0317    |
| test/info_reward_ctrl_mean  | -0.458     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.07       |
| test/info_reward_run_mean   | 4.24       |
| test/info_reward_run_min    | -1.58      |
| test/reward_per_eps         | 3.78e+03   |
| test/steps                  | 240000     |
| train/episodes              | 29         |
| train/info_reward_ctrl_max  | -0.429     |
| train/info_reward_ctrl_mean | -0.429     |
| train/info_reward_ctrl_min  | -0.429     |
| train/info_reward_run_max   | 3.78       |
| train/info_reward_run_mean  | 3.78       |
| train/info_reward_run_min   | 3.78       |
| train/reward_per_eps        | nan        |
| train/steps                 | 29000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 24         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.13245212 |
| stats_o/std                 | 3.5190861  |
| test/episodes               | 250        |
| test/info_reward_ctrl_max   | -0.093     |
| test/info_reward_ctrl_mean  | -0.464     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 6.55       |
| test/info_reward_run_mean   | 3.86       |
| test/info_reward_run_min    | -1.55      |
| test/reward_per_eps         | 3.39e+03   |
| test/steps                  | 250000     |
| train/episodes              | 30         |
| train/info_reward_ctrl_max  | -0.431     |
| train/info_reward_ctrl_mean | -0.431     |
| train/info_reward_ctrl_min  | -0.431     |
| train/info_reward_run_max   | 3.98       |
| train/info_reward_run_mean  | 3.98       |
| train/info_reward_run_min   | 3.98       |
| train/reward_per_eps        | nan        |
| train/steps                 | 30000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 25         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.13658349 |
| stats_o/std                 | 3.5285366  |
| test/episodes               | 260        |
| test/info_reward_ctrl_max   | -0.143     |
| test/info_reward_ctrl_mean  | -0.461     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.43       |
| test/info_reward_run_mean   | 4.38       |
| test/info_reward_run_min    | -2.09      |
| test/reward_per_eps         | 3.92e+03   |
| test/steps                  | 260000     |
| train/episodes              | 31         |
| train/info_reward_ctrl_max  | -0.434     |
| train/info_reward_ctrl_mean | -0.434     |
| train/info_reward_ctrl_min  | -0.434     |
| train/info_reward_run_max   | 4.13       |
| train/info_reward_run_mean  | 4.13       |
| train/info_reward_run_min   | 4.13       |
| train/reward_per_eps        | nan        |
| train/steps                 | 31000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 26         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.14067557 |
| stats_o/std                 | 3.5450368  |
| test/episodes               | 270        |
| test/info_reward_ctrl_max   | -0.145     |
| test/info_reward_ctrl_mean  | -0.467     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 6.76       |
| test/info_reward_run_mean   | 3.41       |
| test/info_reward_run_min    | -3.4       |
| test/reward_per_eps         | 2.94e+03   |
| test/steps                  | 270000     |
| train/episodes              | 32         |
| train/info_reward_ctrl_max  | -0.484     |
| train/info_reward_ctrl_mean | -0.484     |
| train/info_reward_ctrl_min  | -0.484     |
| train/info_reward_run_max   | 1.12       |
| train/info_reward_run_mean  | 1.12       |
| train/info_reward_run_min   | 1.12       |
| train/reward_per_eps        | nan        |
| train/steps                 | 32000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 27         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.14436111 |
| stats_o/std                 | 3.558315   |
| test/episodes               | 280        |
| test/info_reward_ctrl_max   | -0.0891    |
| test/info_reward_ctrl_mean  | -0.44      |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.16       |
| test/info_reward_run_mean   | 4.09       |
| test/info_reward_run_min    | -1.12      |
| test/reward_per_eps         | 3.65e+03   |
| test/steps                  | 280000     |
| train/episodes              | 33         |
| train/info_reward_ctrl_max  | -0.437     |
| train/info_reward_ctrl_mean | -0.437     |
| train/info_reward_ctrl_min  | -0.437     |
| train/info_reward_run_max   | 4.15       |
| train/info_reward_run_mean  | 4.15       |
| train/info_reward_run_min   | 4.15       |
| train/reward_per_eps        | nan        |
| train/steps                 | 33000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 28         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.14836065 |
| stats_o/std                 | 3.5723789  |
| test/episodes               | 290        |
| test/info_reward_ctrl_max   | -0.114     |
| test/info_reward_ctrl_mean  | -0.467     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.12       |
| test/info_reward_run_mean   | 4.34       |
| test/info_reward_run_min    | -1.09      |
| test/reward_per_eps         | 3.87e+03   |
| test/steps                  | 290000     |
| train/episodes              | 34         |
| train/info_reward_ctrl_max  | -0.436     |
| train/info_reward_ctrl_mean | -0.436     |
| train/info_reward_ctrl_min  | -0.436     |
| train/info_reward_run_max   | 4.41       |
| train/info_reward_run_mean  | 4.41       |
| train/info_reward_run_min   | 4.41       |
| train/reward_per_eps        | nan        |
| train/steps                 | 34000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 29         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.15203948 |
| stats_o/std                 | 3.5864902  |
| test/episodes               | 300        |
| test/info_reward_ctrl_max   | -0.125     |
| test/info_reward_ctrl_mean  | -0.489     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.12       |
| test/info_reward_run_mean   | 4.36       |
| test/info_reward_run_min    | -1.11      |
| test/reward_per_eps         | 3.87e+03   |
| test/steps                  | 300000     |
| train/episodes              | 35         |
| train/info_reward_ctrl_max  | -0.463     |
| train/info_reward_ctrl_mean | -0.463     |
| train/info_reward_ctrl_min  | -0.463     |
| train/info_reward_run_max   | 4.42       |
| train/info_reward_run_mean  | 4.42       |
| train/info_reward_run_min   | 4.42       |
| train/reward_per_eps        | nan        |
| train/steps                 | 35000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 30         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.15507254 |
| stats_o/std                 | 3.6001072  |
| test/episodes               | 310        |
| test/info_reward_ctrl_max   | -0.135     |
| test/info_reward_ctrl_mean  | -0.492     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.29       |
| test/info_reward_run_mean   | 4.62       |
| test/info_reward_run_min    | -2.25      |
| test/reward_per_eps         | 4.12e+03   |
| test/steps                  | 310000     |
| train/episodes              | 36         |
| train/info_reward_ctrl_max  | -0.468     |
| train/info_reward_ctrl_mean | -0.468     |
| train/info_reward_ctrl_min  | -0.468     |
| train/info_reward_run_max   | 4.11       |
| train/info_reward_run_mean  | 4.11       |
| train/info_reward_run_min   | 4.11       |
| train/reward_per_eps        | nan        |
| train/steps                 | 36000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 31        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.1586253 |
| stats_o/std                 | 3.6096423 |
| test/episodes               | 320       |
| test/info_reward_ctrl_max   | -0.156    |
| test/info_reward_ctrl_mean  | -0.475    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 7.54      |
| test/info_reward_run_mean   | 4.77      |
| test/info_reward_run_min    | -1.42     |
| test/reward_per_eps         | 4.29e+03  |
| test/steps                  | 320000    |
| train/episodes              | 37        |
| train/info_reward_ctrl_max  | -0.449    |
| train/info_reward_ctrl_mean | -0.449    |
| train/info_reward_ctrl_min  | -0.449    |
| train/info_reward_run_max   | 4.51      |
| train/info_reward_run_mean  | 4.51      |
| train/info_reward_run_min   | 4.51      |
| train/reward_per_eps        | nan       |
| train/steps                 | 37000     |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 32         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.16207913 |
| stats_o/std                 | 3.617985   |
| test/episodes               | 330        |
| test/info_reward_ctrl_max   | -0.167     |
| test/info_reward_ctrl_mean  | -0.478     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.31       |
| test/info_reward_run_mean   | 4.21       |
| test/info_reward_run_min    | -0.978     |
| test/reward_per_eps         | 3.74e+03   |
| test/steps                  | 330000     |
| train/episodes              | 38         |
| train/info_reward_ctrl_max  | -0.45      |
| train/info_reward_ctrl_mean | -0.45      |
| train/info_reward_ctrl_min  | -0.45      |
| train/info_reward_run_max   | 4.64       |
| train/info_reward_run_mean  | 4.64       |
| train/info_reward_run_min   | 4.64       |
| train/reward_per_eps        | nan        |
| train/steps                 | 38000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 33         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.16505545 |
| stats_o/std                 | 3.624638   |
| test/episodes               | 340        |
| test/info_reward_ctrl_max   | -0.103     |
| test/info_reward_ctrl_mean  | -0.461     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.91       |
| test/info_reward_run_mean   | 4.83       |
| test/info_reward_run_min    | -1.3       |
| test/reward_per_eps         | 4.37e+03   |
| test/steps                  | 340000     |
| train/episodes              | 39         |
| train/info_reward_ctrl_max  | -0.444     |
| train/info_reward_ctrl_mean | -0.444     |
| train/info_reward_ctrl_min  | -0.444     |
| train/info_reward_run_max   | 4.47       |
| train/info_reward_run_mean  | 4.47       |
| train/info_reward_run_min   | 4.47       |
| train/reward_per_eps        | nan        |
| train/steps                 | 39000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 34         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.16768304 |
| stats_o/std                 | 3.6329288  |
| test/episodes               | 350        |
| test/info_reward_ctrl_max   | -0.156     |
| test/info_reward_ctrl_mean  | -0.488     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.98       |
| test/info_reward_run_mean   | 4.77       |
| test/info_reward_run_min    | -1.33      |
| test/reward_per_eps         | 4.28e+03   |
| test/steps                  | 350000     |
| train/episodes              | 40         |
| train/info_reward_ctrl_max  | -0.447     |
| train/info_reward_ctrl_mean | -0.447     |
| train/info_reward_ctrl_min  | -0.447     |
| train/info_reward_run_max   | 4.15       |
| train/info_reward_run_mean  | 4.15       |
| train/info_reward_run_min   | 4.15       |
| train/reward_per_eps        | nan        |
| train/steps                 | 40000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 35        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.1705555 |
| stats_o/std                 | 3.6417286 |
| test/episodes               | 360       |
| test/info_reward_ctrl_max   | -0.148    |
| test/info_reward_ctrl_mean  | -0.492    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 7.97      |
| test/info_reward_run_mean   | 5.11      |
| test/info_reward_run_min    | -1.25     |
| test/reward_per_eps         | 4.62e+03  |
| test/steps                  | 360000    |
| train/episodes              | 41        |
| train/info_reward_ctrl_max  | -0.468    |
| train/info_reward_ctrl_mean | -0.468    |
| train/info_reward_ctrl_min  | -0.468    |
| train/info_reward_run_max   | 4.69      |
| train/info_reward_run_mean  | 4.69      |
| train/info_reward_run_min   | 4.69      |
| train/reward_per_eps        | nan       |
| train/steps                 | 41000     |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 36         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.17310017 |
| stats_o/std                 | 3.6496081  |
| test/episodes               | 370        |
| test/info_reward_ctrl_max   | -0.126     |
| test/info_reward_ctrl_mean  | -0.505     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.12       |
| test/info_reward_run_mean   | 4.85       |
| test/info_reward_run_min    | -1.69      |
| test/reward_per_eps         | 4.35e+03   |
| test/steps                  | 370000     |
| train/episodes              | 42         |
| train/info_reward_ctrl_max  | -0.466     |
| train/info_reward_ctrl_mean | -0.466     |
| train/info_reward_ctrl_min  | -0.466     |
| train/info_reward_run_max   | 4.61       |
| train/info_reward_run_mean  | 4.61       |
| train/info_reward_run_min   | 4.61       |
| train/reward_per_eps        | nan        |
| train/steps                 | 42000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 37         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.17558582 |
| stats_o/std                 | 3.658548   |
| test/episodes               | 380        |
| test/info_reward_ctrl_max   | -0.0658    |
| test/info_reward_ctrl_mean  | -0.465     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.74       |
| test/info_reward_run_mean   | 4.79       |
| test/info_reward_run_min    | -1.29      |
| test/reward_per_eps         | 4.33e+03   |
| test/steps                  | 380000     |
| train/episodes              | 43         |
| train/info_reward_ctrl_max  | -0.453     |
| train/info_reward_ctrl_mean | -0.453     |
| train/info_reward_ctrl_min  | -0.453     |
| train/info_reward_run_max   | 4.65       |
| train/info_reward_run_mean  | 4.65       |
| train/info_reward_run_min   | 4.65       |
| train/reward_per_eps        | nan        |
| train/steps                 | 43000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 38         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.17821634 |
| stats_o/std                 | 3.6653156  |
| test/episodes               | 390        |
| test/info_reward_ctrl_max   | -0.152     |
| test/info_reward_ctrl_mean  | -0.463     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.53       |
| test/info_reward_run_mean   | 4.82       |
| test/info_reward_run_min    | -1.05      |
| test/reward_per_eps         | 4.35e+03   |
| test/steps                  | 390000     |
| train/episodes              | 44         |
| train/info_reward_ctrl_max  | -0.454     |
| train/info_reward_ctrl_mean | -0.454     |
| train/info_reward_ctrl_min  | -0.454     |
| train/info_reward_run_max   | 4.9        |
| train/info_reward_run_mean  | 4.9        |
| train/info_reward_run_min   | 4.9        |
| train/reward_per_eps        | nan        |
| train/steps                 | 44000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 39         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.18070492 |
| stats_o/std                 | 3.6701198  |
| test/episodes               | 400        |
| test/info_reward_ctrl_max   | -0.117     |
| test/info_reward_ctrl_mean  | -0.467     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.37       |
| test/info_reward_run_mean   | 4.69       |
| test/info_reward_run_min    | -0.981     |
| test/reward_per_eps         | 4.22e+03   |
| test/steps                  | 400000     |
| train/episodes              | 45         |
| train/info_reward_ctrl_max  | -0.443     |
| train/info_reward_ctrl_mean | -0.443     |
| train/info_reward_ctrl_min  | -0.443     |
| train/info_reward_run_max   | 4.54       |
| train/info_reward_run_mean  | 4.54       |
| train/info_reward_run_min   | 4.54       |
| train/reward_per_eps        | nan        |
| train/steps                 | 45000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 40        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.1831987 |
| stats_o/std                 | 3.6756313 |
| test/episodes               | 410       |
| test/info_reward_ctrl_max   | -0.124    |
| test/info_reward_ctrl_mean  | -0.466    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 7.62      |
| test/info_reward_run_mean   | 4.85      |
| test/info_reward_run_min    | -1.01     |
| test/reward_per_eps         | 4.38e+03  |
| test/steps                  | 410000    |
| train/episodes              | 46        |
| train/info_reward_ctrl_max  | -0.433    |
| train/info_reward_ctrl_mean | -0.433    |
| train/info_reward_ctrl_min  | -0.433    |
| train/info_reward_run_max   | 5.11      |
| train/info_reward_run_mean  | 5.11      |
| train/info_reward_run_min   | 5.11      |
| train/reward_per_eps        | nan       |
| train/steps                 | 46000     |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 41         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.18528591 |
| stats_o/std                 | 3.6784837  |
| test/episodes               | 420        |
| test/info_reward_ctrl_max   | -0.054     |
| test/info_reward_ctrl_mean  | -0.454     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.1        |
| test/info_reward_run_mean   | 4.52       |
| test/info_reward_run_min    | -1.27      |
| test/reward_per_eps         | 4.07e+03   |
| test/steps                  | 420000     |
| train/episodes              | 47         |
| train/info_reward_ctrl_max  | -0.423     |
| train/info_reward_ctrl_mean | -0.423     |
| train/info_reward_ctrl_min  | -0.423     |
| train/info_reward_run_max   | 4.44       |
| train/info_reward_run_mean  | 4.44       |
| train/info_reward_run_min   | 4.44       |
| train/reward_per_eps        | nan        |
| train/steps                 | 47000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 42         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.18742019 |
| stats_o/std                 | 3.6795645  |
| test/episodes               | 430        |
| test/info_reward_ctrl_max   | -0.0407    |
| test/info_reward_ctrl_mean  | -0.471     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.08       |
| test/info_reward_run_mean   | 4.66       |
| test/info_reward_run_min    | -1.16      |
| test/reward_per_eps         | 4.19e+03   |
| test/steps                  | 430000     |
| train/episodes              | 48         |
| train/info_reward_ctrl_max  | -0.443     |
| train/info_reward_ctrl_mean | -0.443     |
| train/info_reward_ctrl_min  | -0.443     |
| train/info_reward_run_max   | 4.88       |
| train/info_reward_run_mean  | 4.88       |
| train/info_reward_run_min   | 4.88       |
| train/reward_per_eps        | nan        |
| train/steps                 | 48000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 43        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.1893755 |
| stats_o/std                 | 3.680103  |
| test/episodes               | 440       |
| test/info_reward_ctrl_max   | -0.141    |
| test/info_reward_ctrl_mean  | -0.477    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 7.31      |
| test/info_reward_run_mean   | 4.65      |
| test/info_reward_run_min    | -1.08     |
| test/reward_per_eps         | 4.17e+03  |
| test/steps                  | 440000    |
| train/episodes              | 49        |
| train/info_reward_ctrl_max  | -0.441    |
| train/info_reward_ctrl_mean | -0.441    |
| train/info_reward_ctrl_min  | -0.441    |
| train/info_reward_run_max   | 4.67      |
| train/info_reward_run_mean  | 4.67      |
| train/info_reward_run_min   | 4.67      |
| train/reward_per_eps        | nan       |
| train/steps                 | 49000     |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 44         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.19142273 |
| stats_o/std                 | 3.6809666  |
| test/episodes               | 450        |
| test/info_reward_ctrl_max   | -0.108     |
| test/info_reward_ctrl_mean  | -0.456     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.23       |
| test/info_reward_run_mean   | 4.72       |
| test/info_reward_run_min    | -1.24      |
| test/reward_per_eps         | 4.26e+03   |
| test/steps                  | 450000     |
| train/episodes              | 50         |
| train/info_reward_ctrl_max  | -0.42      |
| train/info_reward_ctrl_mean | -0.42      |
| train/info_reward_ctrl_min  | -0.42      |
| train/info_reward_run_max   | 4.64       |
| train/info_reward_run_mean  | 4.64       |
| train/info_reward_run_min   | 4.64       |
| train/reward_per_eps        | nan        |
| train/steps                 | 50000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 45         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.19332568 |
| stats_o/std                 | 3.6823406  |
| test/episodes               | 460        |
| test/info_reward_ctrl_max   | -0.16      |
| test/info_reward_ctrl_mean  | -0.478     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.18       |
| test/info_reward_run_mean   | 5.37       |
| test/info_reward_run_min    | -1.49      |
| test/reward_per_eps         | 4.89e+03   |
| test/steps                  | 460000     |
| train/episodes              | 51         |
| train/info_reward_ctrl_max  | -0.44      |
| train/info_reward_ctrl_mean | -0.44      |
| train/info_reward_ctrl_min  | -0.44      |
| train/info_reward_run_max   | 5.14       |
| train/info_reward_run_mean  | 5.14       |
| train/info_reward_run_min   | 5.14       |
| train/reward_per_eps        | nan        |
| train/steps                 | 51000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 46         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.19549754 |
| stats_o/std                 | 3.6842592  |
| test/episodes               | 470        |
| test/info_reward_ctrl_max   | -0.0528    |
| test/info_reward_ctrl_mean  | -0.443     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.53       |
| test/info_reward_run_mean   | 4.95       |
| test/info_reward_run_min    | -1.04      |
| test/reward_per_eps         | 4.51e+03   |
| test/steps                  | 470000     |
| train/episodes              | 52         |
| train/info_reward_ctrl_max  | -0.445     |
| train/info_reward_ctrl_mean | -0.445     |
| train/info_reward_ctrl_min  | -0.445     |
| train/info_reward_run_max   | 4.81       |
| train/info_reward_run_mean  | 4.81       |
| train/info_reward_run_min   | 4.81       |
| train/reward_per_eps        | nan        |
| train/steps                 | 52000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 47         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.19796202 |
| stats_o/std                 | 3.6847332  |
| test/episodes               | 480        |
| test/info_reward_ctrl_max   | -0.0542    |
| test/info_reward_ctrl_mean  | -0.437     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.88       |
| test/info_reward_run_mean   | 5.16       |
| test/info_reward_run_min    | -0.627     |
| test/reward_per_eps         | 4.73e+03   |
| test/steps                  | 480000     |
| train/episodes              | 53         |
| train/info_reward_ctrl_max  | -0.422     |
| train/info_reward_ctrl_mean | -0.422     |
| train/info_reward_ctrl_min  | -0.422     |
| train/info_reward_run_max   | 5.05       |
| train/info_reward_run_mean  | 5.05       |
| train/info_reward_run_min   | 5.05       |
| train/reward_per_eps        | nan        |
| train/steps                 | 53000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 48         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.20005785 |
| stats_o/std                 | 3.6859775  |
| test/episodes               | 490        |
| test/info_reward_ctrl_max   | -0.0472    |
| test/info_reward_ctrl_mean  | -0.454     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.74       |
| test/info_reward_run_mean   | 4.99       |
| test/info_reward_run_min    | -1.31      |
| test/reward_per_eps         | 4.54e+03   |
| test/steps                  | 490000     |
| train/episodes              | 54         |
| train/info_reward_ctrl_max  | -0.437     |
| train/info_reward_ctrl_mean | -0.437     |
| train/info_reward_ctrl_min  | -0.437     |
| train/info_reward_run_max   | 4.76       |
| train/info_reward_run_mean  | 4.76       |
| train/info_reward_run_min   | 4.76       |
| train/reward_per_eps        | nan        |
| train/steps                 | 54000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 49         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.20184976 |
| stats_o/std                 | 3.6879783  |
| test/episodes               | 500        |
| test/info_reward_ctrl_max   | -0.126     |
| test/info_reward_ctrl_mean  | -0.496     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.12       |
| test/info_reward_run_mean   | 4.91       |
| test/info_reward_run_min    | -1.88      |
| test/reward_per_eps         | 4.41e+03   |
| test/steps                  | 500000     |
| train/episodes              | 55         |
| train/info_reward_ctrl_max  | -0.45      |
| train/info_reward_ctrl_mean | -0.45      |
| train/info_reward_ctrl_min  | -0.45      |
| train/info_reward_run_max   | 4.8        |
| train/info_reward_run_mean  | 4.8        |
| train/info_reward_run_min   | 4.8        |
| train/reward_per_eps        | nan        |
| train/steps                 | 55000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 50         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.20379286 |
| stats_o/std                 | 3.6897068  |
| test/episodes               | 510        |
| test/info_reward_ctrl_max   | -0.144     |
| test/info_reward_ctrl_mean  | -0.484     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.28       |
| test/info_reward_run_mean   | 5.47       |
| test/info_reward_run_min    | -1.01      |
| test/reward_per_eps         | 4.98e+03   |
| test/steps                  | 510000     |
| train/episodes              | 56         |
| train/info_reward_ctrl_max  | -0.456     |
| train/info_reward_ctrl_mean | -0.456     |
| train/info_reward_ctrl_min  | -0.456     |
| train/info_reward_run_max   | 4.65       |
| train/info_reward_run_mean  | 4.65       |
| train/info_reward_run_min   | 4.65       |
| train/reward_per_eps        | nan        |
| train/steps                 | 56000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 51         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.20547128 |
| stats_o/std                 | 3.690663   |
| test/episodes               | 520        |
| test/info_reward_ctrl_max   | -0.0595    |
| test/info_reward_ctrl_mean  | -0.452     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.41       |
| test/info_reward_run_mean   | 4.78       |
| test/info_reward_run_min    | -0.552     |
| test/reward_per_eps         | 4.33e+03   |
| test/steps                  | 520000     |
| train/episodes              | 57         |
| train/info_reward_ctrl_max  | -0.422     |
| train/info_reward_ctrl_mean | -0.422     |
| train/info_reward_ctrl_min  | -0.422     |
| train/info_reward_run_max   | 4.9        |
| train/info_reward_run_mean  | 4.9        |
| train/info_reward_run_min   | 4.9        |
| train/reward_per_eps        | nan        |
| train/steps                 | 57000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 52        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2071687 |
| stats_o/std                 | 3.6917064 |
| test/episodes               | 530       |
| test/info_reward_ctrl_max   | -0.0442   |
| test/info_reward_ctrl_mean  | -0.439    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 8.18      |
| test/info_reward_run_mean   | 5.55      |
| test/info_reward_run_min    | -1.17     |
| test/reward_per_eps         | 5.11e+03  |
| test/steps                  | 530000    |
| train/episodes              | 58        |
| train/info_reward_ctrl_max  | -0.422    |
| train/info_reward_ctrl_mean | -0.422    |
| train/info_reward_ctrl_min  | -0.422    |
| train/info_reward_run_max   | 5.14      |
| train/info_reward_run_mean  | 5.14      |
| train/info_reward_run_min   | 5.14      |
| train/reward_per_eps        | nan       |
| train/steps                 | 58000     |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 53         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.20862724 |
| stats_o/std                 | 3.690602   |
| test/episodes               | 540        |
| test/info_reward_ctrl_max   | -0.102     |
| test/info_reward_ctrl_mean  | -0.454     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.36       |
| test/info_reward_run_mean   | 5.11       |
| test/info_reward_run_min    | -1.11      |
| test/reward_per_eps         | 4.66e+03   |
| test/steps                  | 540000     |
| train/episodes              | 59         |
| train/info_reward_ctrl_max  | -0.432     |
| train/info_reward_ctrl_mean | -0.432     |
| train/info_reward_ctrl_min  | -0.432     |
| train/info_reward_run_max   | 4.9        |
| train/info_reward_run_mean  | 4.9        |
| train/info_reward_run_min   | 4.9        |
| train/reward_per_eps        | nan        |
| train/steps                 | 59000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 54         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.21020831 |
| stats_o/std                 | 3.6907814  |
| test/episodes               | 550        |
| test/info_reward_ctrl_max   | -0.176     |
| test/info_reward_ctrl_mean  | -0.493     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.25       |
| test/info_reward_run_mean   | 5.47       |
| test/info_reward_run_min    | -1.4       |
| test/reward_per_eps         | 4.98e+03   |
| test/steps                  | 550000     |
| train/episodes              | 60         |
| train/info_reward_ctrl_max  | -0.449     |
| train/info_reward_ctrl_mean | -0.449     |
| train/info_reward_ctrl_min  | -0.449     |
| train/info_reward_run_max   | 5.28       |
| train/info_reward_run_mean  | 5.28       |
| train/info_reward_run_min   | 5.28       |
| train/reward_per_eps        | nan        |
| train/steps                 | 60000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 55        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2117285 |
| stats_o/std                 | 3.6916432 |
| test/episodes               | 560       |
| test/info_reward_ctrl_max   | -0.126    |
| test/info_reward_ctrl_mean  | -0.479    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 8.28      |
| test/info_reward_run_mean   | 5.38      |
| test/info_reward_run_min    | -0.88     |
| test/reward_per_eps         | 4.91e+03  |
| test/steps                  | 560000    |
| train/episodes              | 61        |
| train/info_reward_ctrl_max  | -0.455    |
| train/info_reward_ctrl_mean | -0.455    |
| train/info_reward_ctrl_min  | -0.455    |
| train/info_reward_run_max   | 4.75      |
| train/info_reward_run_mean  | 4.75      |
| train/info_reward_run_min   | 4.75      |
| train/reward_per_eps        | nan       |
| train/steps                 | 61000     |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 56         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.21346676 |
| stats_o/std                 | 3.6939793  |
| test/episodes               | 570        |
| test/info_reward_ctrl_max   | -0.118     |
| test/info_reward_ctrl_mean  | -0.489     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.22       |
| test/info_reward_run_mean   | 5.28       |
| test/info_reward_run_min    | -1.08      |
| test/reward_per_eps         | 4.79e+03   |
| test/steps                  | 570000     |
| train/episodes              | 62         |
| train/info_reward_ctrl_max  | -0.454     |
| train/info_reward_ctrl_mean | -0.454     |
| train/info_reward_ctrl_min  | -0.454     |
| train/info_reward_run_max   | 5.3        |
| train/info_reward_run_mean  | 5.3        |
| train/info_reward_run_min   | 5.3        |
| train/reward_per_eps        | nan        |
| train/steps                 | 62000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 57         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.21493141 |
| stats_o/std                 | 3.6945062  |
| test/episodes               | 580        |
| test/info_reward_ctrl_max   | -0.0776    |
| test/info_reward_ctrl_mean  | -0.468     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.95       |
| test/info_reward_run_mean   | 5.18       |
| test/info_reward_run_min    | -1.06      |
| test/reward_per_eps         | 4.72e+03   |
| test/steps                  | 580000     |
| train/episodes              | 63         |
| train/info_reward_ctrl_max  | -0.444     |
| train/info_reward_ctrl_mean | -0.444     |
| train/info_reward_ctrl_min  | -0.444     |
| train/info_reward_run_max   | 5.17       |
| train/info_reward_run_mean  | 5.17       |
| train/info_reward_run_min   | 5.17       |
| train/reward_per_eps        | nan        |
| train/steps                 | 63000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 58         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.21653424 |
| stats_o/std                 | 3.6954982  |
| test/episodes               | 590        |
| test/info_reward_ctrl_max   | -0.134     |
| test/info_reward_ctrl_mean  | -0.495     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.15       |
| test/info_reward_run_mean   | 4.95       |
| test/info_reward_run_min    | -1.72      |
| test/reward_per_eps         | 4.46e+03   |
| test/steps                  | 590000     |
| train/episodes              | 64         |
| train/info_reward_ctrl_max  | -0.459     |
| train/info_reward_ctrl_mean | -0.459     |
| train/info_reward_ctrl_min  | -0.459     |
| train/info_reward_run_max   | 5.32       |
| train/info_reward_run_mean  | 5.32       |
| train/info_reward_run_min   | 5.32       |
| train/reward_per_eps        | nan        |
| train/steps                 | 64000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 59         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.21794032 |
| stats_o/std                 | 3.6940637  |
| test/episodes               | 600        |
| test/info_reward_ctrl_max   | -0.16      |
| test/info_reward_ctrl_mean  | -0.488     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.4        |
| test/info_reward_run_mean   | 5.59       |
| test/info_reward_run_min    | -0.782     |
| test/reward_per_eps         | 5.1e+03    |
| test/steps                  | 600000     |
| train/episodes              | 65         |
| train/info_reward_ctrl_max  | -0.444     |
| train/info_reward_ctrl_mean | -0.444     |
| train/info_reward_ctrl_min  | -0.444     |
| train/info_reward_run_max   | 4.91       |
| train/info_reward_run_mean  | 4.91       |
| train/info_reward_run_min   | 4.91       |
| train/reward_per_eps        | nan        |
| train/steps                 | 65000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 60         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.21922544 |
| stats_o/std                 | 3.6939754  |
| test/episodes               | 610        |
| test/info_reward_ctrl_max   | -0.124     |
| test/info_reward_ctrl_mean  | -0.458     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.05       |
| test/info_reward_run_mean   | 5.34       |
| test/info_reward_run_min    | -0.79      |
| test/reward_per_eps         | 4.88e+03   |
| test/steps                  | 610000     |
| train/episodes              | 66         |
| train/info_reward_ctrl_max  | -0.43      |
| train/info_reward_ctrl_mean | -0.43      |
| train/info_reward_ctrl_min  | -0.43      |
| train/info_reward_run_max   | 5.24       |
| train/info_reward_run_mean  | 5.24       |
| train/info_reward_run_min   | 5.24       |
| train/reward_per_eps        | nan        |
| train/steps                 | 66000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 61         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.22073972 |
| stats_o/std                 | 3.6952615  |
| test/episodes               | 620        |
| test/info_reward_ctrl_max   | -0.12      |
| test/info_reward_ctrl_mean  | -0.471     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 9          |
| test/info_reward_run_mean   | 5.86       |
| test/info_reward_run_min    | -0.937     |
| test/reward_per_eps         | 5.39e+03   |
| test/steps                  | 620000     |
| train/episodes              | 67         |
| train/info_reward_ctrl_max  | -0.439     |
| train/info_reward_ctrl_mean | -0.439     |
| train/info_reward_ctrl_min  | -0.439     |
| train/info_reward_run_max   | 5.35       |
| train/info_reward_run_mean  | 5.35       |
| train/info_reward_run_min   | 5.35       |
| train/reward_per_eps        | nan        |
| train/steps                 | 67000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 62         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.22254707 |
| stats_o/std                 | 3.696606   |
| test/episodes               | 630        |
| test/info_reward_ctrl_max   | -0.133     |
| test/info_reward_ctrl_mean  | -0.465     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.88       |
| test/info_reward_run_mean   | 5.79       |
| test/info_reward_run_min    | -0.734     |
| test/reward_per_eps         | 5.32e+03   |
| test/steps                  | 630000     |
| train/episodes              | 68         |
| train/info_reward_ctrl_max  | -0.446     |
| train/info_reward_ctrl_mean | -0.446     |
| train/info_reward_ctrl_min  | -0.446     |
| train/info_reward_run_max   | 5.75       |
| train/info_reward_run_mean  | 5.75       |
| train/info_reward_run_min   | 5.75       |
| train/reward_per_eps        | nan        |
| train/steps                 | 68000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 63         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.22418396 |
| stats_o/std                 | 3.6984038  |
| test/episodes               | 640        |
| test/info_reward_ctrl_max   | -0.0636    |
| test/info_reward_ctrl_mean  | -0.477     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 7.99       |
| test/info_reward_run_mean   | 5.29       |
| test/info_reward_run_min    | -1.09      |
| test/reward_per_eps         | 4.81e+03   |
| test/steps                  | 640000     |
| train/episodes              | 69         |
| train/info_reward_ctrl_max  | -0.458     |
| train/info_reward_ctrl_mean | -0.458     |
| train/info_reward_ctrl_min  | -0.458     |
| train/info_reward_run_max   | 5.62       |
| train/info_reward_run_mean  | 5.62       |
| train/info_reward_run_min   | 5.62       |
| train/reward_per_eps        | nan        |
| train/steps                 | 69000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 64         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.22565784 |
| stats_o/std                 | 3.6990788  |
| test/episodes               | 650        |
| test/info_reward_ctrl_max   | -0.071     |
| test/info_reward_ctrl_mean  | -0.474     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.26       |
| test/info_reward_run_mean   | 5.67       |
| test/info_reward_run_min    | -0.489     |
| test/reward_per_eps         | 5.2e+03    |
| test/steps                  | 650000     |
| train/episodes              | 70         |
| train/info_reward_ctrl_max  | -0.459     |
| train/info_reward_ctrl_mean | -0.459     |
| train/info_reward_ctrl_min  | -0.459     |
| train/info_reward_run_max   | 5.41       |
| train/info_reward_run_mean  | 5.41       |
| train/info_reward_run_min   | 5.41       |
| train/reward_per_eps        | nan        |
| train/steps                 | 70000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 65        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2272907 |
| stats_o/std                 | 3.6997547 |
| test/episodes               | 660       |
| test/info_reward_ctrl_max   | -0.152    |
| test/info_reward_ctrl_mean  | -0.483    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 9.02      |
| test/info_reward_run_mean   | 5.91      |
| test/info_reward_run_min    | -0.972    |
| test/reward_per_eps         | 5.42e+03  |
| test/steps                  | 660000    |
| train/episodes              | 71        |
| train/info_reward_ctrl_max  | -0.452    |
| train/info_reward_ctrl_mean | -0.452    |
| train/info_reward_ctrl_min  | -0.452    |
| train/info_reward_run_max   | 6.2       |
| train/info_reward_run_mean  | 6.2       |
| train/info_reward_run_min   | 6.2       |
| train/reward_per_eps        | nan       |
| train/steps                 | 71000     |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 66         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.22876386 |
| stats_o/std                 | 3.699977   |
| test/episodes               | 670        |
| test/info_reward_ctrl_max   | -0.0897    |
| test/info_reward_ctrl_mean  | -0.476     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 9.11       |
| test/info_reward_run_mean   | 6.05       |
| test/info_reward_run_min    | -1.41      |
| test/reward_per_eps         | 5.58e+03   |
| test/steps                  | 670000     |
| train/episodes              | 72         |
| train/info_reward_ctrl_max  | -0.445     |
| train/info_reward_ctrl_mean | -0.445     |
| train/info_reward_ctrl_min  | -0.445     |
| train/info_reward_run_max   | 5.48       |
| train/info_reward_run_mean  | 5.48       |
| train/info_reward_run_min   | 5.48       |
| train/reward_per_eps        | nan        |
| train/steps                 | 72000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 67         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.23045564 |
| stats_o/std                 | 3.7011056  |
| test/episodes               | 680        |
| test/info_reward_ctrl_max   | -0.122     |
| test/info_reward_ctrl_mean  | -0.491     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.65       |
| test/info_reward_run_mean   | 5.52       |
| test/info_reward_run_min    | -0.992     |
| test/reward_per_eps         | 5.03e+03   |
| test/steps                  | 680000     |
| train/episodes              | 73         |
| train/info_reward_ctrl_max  | -0.46      |
| train/info_reward_ctrl_mean | -0.46      |
| train/info_reward_ctrl_min  | -0.46      |
| train/info_reward_run_max   | 5.58       |
| train/info_reward_run_mean  | 5.58       |
| train/info_reward_run_min   | 5.58       |
| train/reward_per_eps        | nan        |
| train/steps                 | 73000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 68         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.23197968 |
| stats_o/std                 | 3.7028596  |
| test/episodes               | 690        |
| test/info_reward_ctrl_max   | -0.079     |
| test/info_reward_ctrl_mean  | -0.467     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.9        |
| test/info_reward_run_mean   | 6.1        |
| test/info_reward_run_min    | -0.789     |
| test/reward_per_eps         | 5.64e+03   |
| test/steps                  | 690000     |
| train/episodes              | 74         |
| train/info_reward_ctrl_max  | -0.45      |
| train/info_reward_ctrl_mean | -0.45      |
| train/info_reward_ctrl_min  | -0.45      |
| train/info_reward_run_max   | 5.9        |
| train/info_reward_run_mean  | 5.9        |
| train/info_reward_run_min   | 5.9        |
| train/reward_per_eps        | nan        |
| train/steps                 | 74000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 69        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2336477 |
| stats_o/std                 | 3.7060452 |
| test/episodes               | 700       |
| test/info_reward_ctrl_max   | -0.133    |
| test/info_reward_ctrl_mean  | -0.479    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 9.21      |
| test/info_reward_run_mean   | 6.18      |
| test/info_reward_run_min    | -1.03     |
| test/reward_per_eps         | 5.71e+03  |
| test/steps                  | 700000    |
| train/episodes              | 75        |
| train/info_reward_ctrl_max  | -0.466    |
| train/info_reward_ctrl_mean | -0.466    |
| train/info_reward_ctrl_min  | -0.466    |
| train/info_reward_run_max   | 6.12      |
| train/info_reward_run_mean  | 6.12      |
| train/info_reward_run_min   | 6.12      |
| train/reward_per_eps        | nan       |
| train/steps                 | 75000     |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 70         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.23514934 |
| stats_o/std                 | 3.7076778  |
| test/episodes               | 710        |
| test/info_reward_ctrl_max   | -0.119     |
| test/info_reward_ctrl_mean  | -0.464     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.93       |
| test/info_reward_run_mean   | 5.96       |
| test/info_reward_run_min    | -0.77      |
| test/reward_per_eps         | 5.5e+03    |
| test/steps                  | 710000     |
| train/episodes              | 76         |
| train/info_reward_ctrl_max  | -0.441     |
| train/info_reward_ctrl_mean | -0.441     |
| train/info_reward_ctrl_min  | -0.441     |
| train/info_reward_run_max   | 5.91       |
| train/info_reward_run_mean  | 5.91       |
| train/info_reward_run_min   | 5.91       |
| train/reward_per_eps        | nan        |
| train/steps                 | 76000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 71         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.23639098 |
| stats_o/std                 | 3.7081814  |
| test/episodes               | 720        |
| test/info_reward_ctrl_max   | -0.144     |
| test/info_reward_ctrl_mean  | -0.477     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 9.24       |
| test/info_reward_run_mean   | 6.02       |
| test/info_reward_run_min    | -0.871     |
| test/reward_per_eps         | 5.54e+03   |
| test/steps                  | 720000     |
| train/episodes              | 77         |
| train/info_reward_ctrl_max  | -0.443     |
| train/info_reward_ctrl_mean | -0.443     |
| train/info_reward_ctrl_min  | -0.443     |
| train/info_reward_run_max   | 6.11       |
| train/info_reward_run_mean  | 6.11       |
| train/info_reward_run_min   | 6.11       |
| train/reward_per_eps        | nan        |
| train/steps                 | 77000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 72         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.23783849 |
| stats_o/std                 | 3.709915   |
| test/episodes               | 730        |
| test/info_reward_ctrl_max   | -0.159     |
| test/info_reward_ctrl_mean  | -0.481     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 9.06       |
| test/info_reward_run_mean   | 6.01       |
| test/info_reward_run_min    | -0.744     |
| test/reward_per_eps         | 5.53e+03   |
| test/steps                  | 730000     |
| train/episodes              | 78         |
| train/info_reward_ctrl_max  | -0.457     |
| train/info_reward_ctrl_mean | -0.457     |
| train/info_reward_ctrl_min  | -0.457     |
| train/info_reward_run_max   | 5.96       |
| train/info_reward_run_mean  | 5.96       |
| train/info_reward_run_min   | 5.96       |
| train/reward_per_eps        | nan        |
| train/steps                 | 78000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 73         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.23937789 |
| stats_o/std                 | 3.711833   |
| test/episodes               | 740        |
| test/info_reward_ctrl_max   | -0.149     |
| test/info_reward_ctrl_mean  | -0.503     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.1       |
| test/info_reward_run_mean   | 6.31       |
| test/info_reward_run_min    | -0.646     |
| test/reward_per_eps         | 5.81e+03   |
| test/steps                  | 740000     |
| train/episodes              | 79         |
| train/info_reward_ctrl_max  | -0.462     |
| train/info_reward_ctrl_mean | -0.462     |
| train/info_reward_ctrl_min  | -0.462     |
| train/info_reward_run_max   | 6.26       |
| train/info_reward_run_mean  | 6.26       |
| train/info_reward_run_min   | 6.26       |
| train/reward_per_eps        | nan        |
| train/steps                 | 79000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 74         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.24076395 |
| stats_o/std                 | 3.713268   |
| test/episodes               | 750        |
| test/info_reward_ctrl_max   | -0.0806    |
| test/info_reward_ctrl_mean  | -0.507     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 9.13       |
| test/info_reward_run_mean   | 5.86       |
| test/info_reward_run_min    | -0.888     |
| test/reward_per_eps         | 5.35e+03   |
| test/steps                  | 750000     |
| train/episodes              | 80         |
| train/info_reward_ctrl_max  | -0.455     |
| train/info_reward_ctrl_mean | -0.455     |
| train/info_reward_ctrl_min  | -0.455     |
| train/info_reward_run_max   | 6.2        |
| train/info_reward_run_mean  | 6.2        |
| train/info_reward_run_min   | 6.2        |
| train/reward_per_eps        | nan        |
| train/steps                 | 80000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 75         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.24231516 |
| stats_o/std                 | 3.7157192  |
| test/episodes               | 760        |
| test/info_reward_ctrl_max   | -0.149     |
| test/info_reward_ctrl_mean  | -0.497     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10         |
| test/info_reward_run_mean   | 6.69       |
| test/info_reward_run_min    | -0.789     |
| test/reward_per_eps         | 6.19e+03   |
| test/steps                  | 760000     |
| train/episodes              | 81         |
| train/info_reward_ctrl_max  | -0.459     |
| train/info_reward_ctrl_mean | -0.459     |
| train/info_reward_ctrl_min  | -0.459     |
| train/info_reward_run_max   | 6.4        |
| train/info_reward_run_mean  | 6.4        |
| train/info_reward_run_min   | 6.4        |
| train/reward_per_eps        | nan        |
| train/steps                 | 81000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 76         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.24370979 |
| stats_o/std                 | 3.7174363  |
| test/episodes               | 770        |
| test/info_reward_ctrl_max   | -0.126     |
| test/info_reward_ctrl_mean  | -0.492     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 9.79       |
| test/info_reward_run_mean   | 5.9        |
| test/info_reward_run_min    | -1.13      |
| test/reward_per_eps         | 5.41e+03   |
| test/steps                  | 770000     |
| train/episodes              | 82         |
| train/info_reward_ctrl_max  | -0.454     |
| train/info_reward_ctrl_mean | -0.454     |
| train/info_reward_ctrl_min  | -0.454     |
| train/info_reward_run_max   | 6.05       |
| train/info_reward_run_mean  | 6.05       |
| train/info_reward_run_min   | 6.05       |
| train/reward_per_eps        | nan        |
| train/steps                 | 82000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 77         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.24506515 |
| stats_o/std                 | 3.717751   |
| test/episodes               | 780        |
| test/info_reward_ctrl_max   | -0.0821    |
| test/info_reward_ctrl_mean  | -0.465     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 9.25       |
| test/info_reward_run_mean   | 6.15       |
| test/info_reward_run_min    | -0.701     |
| test/reward_per_eps         | 5.69e+03   |
| test/steps                  | 780000     |
| train/episodes              | 83         |
| train/info_reward_ctrl_max  | -0.439     |
| train/info_reward_ctrl_mean | -0.439     |
| train/info_reward_ctrl_min  | -0.439     |
| train/info_reward_run_max   | 6.07       |
| train/info_reward_run_mean  | 6.07       |
| train/info_reward_run_min   | 6.07       |
| train/reward_per_eps        | nan        |
| train/steps                 | 83000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 78         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.24648759 |
| stats_o/std                 | 3.7182117  |
| test/episodes               | 790        |
| test/info_reward_ctrl_max   | -0.114     |
| test/info_reward_ctrl_mean  | -0.454     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 8.99       |
| test/info_reward_run_mean   | 6.03       |
| test/info_reward_run_min    | -0.971     |
| test/reward_per_eps         | 5.58e+03   |
| test/steps                  | 790000     |
| train/episodes              | 84         |
| train/info_reward_ctrl_max  | -0.452     |
| train/info_reward_ctrl_mean | -0.452     |
| train/info_reward_ctrl_min  | -0.452     |
| train/info_reward_run_max   | 6.4        |
| train/info_reward_run_mean  | 6.4        |
| train/info_reward_run_min   | 6.4        |
| train/reward_per_eps        | nan        |
| train/steps                 | 84000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 79         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.24773093 |
| stats_o/std                 | 3.7184997  |
| test/episodes               | 800        |
| test/info_reward_ctrl_max   | -0.0978    |
| test/info_reward_ctrl_mean  | -0.471     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.2       |
| test/info_reward_run_mean   | 6.45       |
| test/info_reward_run_min    | -0.894     |
| test/reward_per_eps         | 5.98e+03   |
| test/steps                  | 800000     |
| train/episodes              | 85         |
| train/info_reward_ctrl_max  | -0.43      |
| train/info_reward_ctrl_mean | -0.43      |
| train/info_reward_ctrl_min  | -0.43      |
| train/info_reward_run_max   | 6.42       |
| train/info_reward_run_mean  | 6.42       |
| train/info_reward_run_min   | 6.42       |
| train/reward_per_eps        | nan        |
| train/steps                 | 85000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 80         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.24932317 |
| stats_o/std                 | 3.7204003  |
| test/episodes               | 810        |
| test/info_reward_ctrl_max   | -0.137     |
| test/info_reward_ctrl_mean  | -0.492     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 9.6        |
| test/info_reward_run_mean   | 6.09       |
| test/info_reward_run_min    | -0.868     |
| test/reward_per_eps         | 5.6e+03    |
| test/steps                  | 810000     |
| train/episodes              | 86         |
| train/info_reward_ctrl_max  | -0.443     |
| train/info_reward_ctrl_mean | -0.443     |
| train/info_reward_ctrl_min  | -0.443     |
| train/info_reward_run_max   | 6.44       |
| train/info_reward_run_mean  | 6.44       |
| train/info_reward_run_min   | 6.44       |
| train/reward_per_eps        | nan        |
| train/steps                 | 86000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 81         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.25102422 |
| stats_o/std                 | 3.7216935  |
| test/episodes               | 820        |
| test/info_reward_ctrl_max   | -0.157     |
| test/info_reward_ctrl_mean  | -0.477     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 9.96       |
| test/info_reward_run_mean   | 6.73       |
| test/info_reward_run_min    | -0.754     |
| test/reward_per_eps         | 6.25e+03   |
| test/steps                  | 820000     |
| train/episodes              | 87         |
| train/info_reward_ctrl_max  | -0.453     |
| train/info_reward_ctrl_mean | -0.453     |
| train/info_reward_ctrl_min  | -0.453     |
| train/info_reward_run_max   | 6.74       |
| train/info_reward_run_mean  | 6.74       |
| train/info_reward_run_min   | 6.74       |
| train/reward_per_eps        | nan        |
| train/steps                 | 87000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 82         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.25252423 |
| stats_o/std                 | 3.723187   |
| test/episodes               | 830        |
| test/info_reward_ctrl_max   | -0.148     |
| test/info_reward_ctrl_mean  | -0.502     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.3       |
| test/info_reward_run_mean   | 6.34       |
| test/info_reward_run_min    | -0.756     |
| test/reward_per_eps         | 5.84e+03   |
| test/steps                  | 830000     |
| train/episodes              | 88         |
| train/info_reward_ctrl_max  | -0.455     |
| train/info_reward_ctrl_mean | -0.455     |
| train/info_reward_ctrl_min  | -0.455     |
| train/info_reward_run_max   | 6.55       |
| train/info_reward_run_mean  | 6.55       |
| train/info_reward_run_min   | 6.55       |
| train/reward_per_eps        | nan        |
| train/steps                 | 88000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 83        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2539111 |
| stats_o/std                 | 3.724228  |
| test/episodes               | 840       |
| test/info_reward_ctrl_max   | -0.106    |
| test/info_reward_ctrl_mean  | -0.469    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 9.22      |
| test/info_reward_run_mean   | 6.26      |
| test/info_reward_run_min    | -0.847    |
| test/reward_per_eps         | 5.79e+03  |
| test/steps                  | 840000    |
| train/episodes              | 89        |
| train/info_reward_ctrl_max  | -0.457    |
| train/info_reward_ctrl_mean | -0.457    |
| train/info_reward_ctrl_min  | -0.457    |
| train/info_reward_run_max   | 6.21      |
| train/info_reward_run_mean  | 6.21      |
| train/info_reward_run_min   | 6.21      |
| train/reward_per_eps        | nan       |
| train/steps                 | 89000     |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 84        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2552797 |
| stats_o/std                 | 3.7249897 |
| test/episodes               | 850       |
| test/info_reward_ctrl_max   | -0.132    |
| test/info_reward_ctrl_mean  | -0.465    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 9.69      |
| test/info_reward_run_mean   | 6.72      |
| test/info_reward_run_min    | -0.8      |
| test/reward_per_eps         | 6.25e+03  |
| test/steps                  | 850000    |
| train/episodes              | 90        |
| train/info_reward_ctrl_max  | -0.443    |
| train/info_reward_ctrl_mean | -0.443    |
| train/info_reward_ctrl_min  | -0.443    |
| train/info_reward_run_max   | 6.48      |
| train/info_reward_run_mean  | 6.48      |
| train/info_reward_run_min   | 6.48      |
| train/reward_per_eps        | nan       |
| train/steps                 | 90000     |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 85        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2564528 |
| stats_o/std                 | 3.7251425 |
| test/episodes               | 860       |
| test/info_reward_ctrl_max   | -0.116    |
| test/info_reward_ctrl_mean  | -0.471    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 9.98      |
| test/info_reward_run_mean   | 6.29      |
| test/info_reward_run_min    | -0.655    |
| test/reward_per_eps         | 5.82e+03  |
| test/steps                  | 860000    |
| train/episodes              | 91        |
| train/info_reward_ctrl_max  | -0.443    |
| train/info_reward_ctrl_mean | -0.443    |
| train/info_reward_ctrl_min  | -0.443    |
| train/info_reward_run_max   | 5.9       |
| train/info_reward_run_mean  | 5.9       |
| train/info_reward_run_min   | 5.9       |
| train/reward_per_eps        | nan       |
| train/steps                 | 91000     |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 86         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.25801122 |
| stats_o/std                 | 3.7268507  |
| test/episodes               | 870        |
| test/info_reward_ctrl_max   | -0.0902    |
| test/info_reward_ctrl_mean  | -0.48      |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.3       |
| test/info_reward_run_mean   | 5.98       |
| test/info_reward_run_min    | -3.23      |
| test/reward_per_eps         | 5.5e+03    |
| test/steps                  | 870000     |
| train/episodes              | 92         |
| train/info_reward_ctrl_max  | -0.444     |
| train/info_reward_ctrl_mean | -0.444     |
| train/info_reward_ctrl_min  | -0.444     |
| train/info_reward_run_max   | 6.8        |
| train/info_reward_run_mean  | 6.8        |
| train/info_reward_run_min   | 6.8        |
| train/reward_per_eps        | nan        |
| train/steps                 | 92000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 87         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.25941116 |
| stats_o/std                 | 3.7278798  |
| test/episodes               | 880        |
| test/info_reward_ctrl_max   | -0.0723    |
| test/info_reward_ctrl_mean  | -0.479     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10         |
| test/info_reward_run_mean   | 6.71       |
| test/info_reward_run_min    | -0.82      |
| test/reward_per_eps         | 6.23e+03   |
| test/steps                  | 880000     |
| train/episodes              | 93         |
| train/info_reward_ctrl_max  | -0.446     |
| train/info_reward_ctrl_mean | -0.446     |
| train/info_reward_ctrl_min  | -0.446     |
| train/info_reward_run_max   | 6.51       |
| train/info_reward_run_mean  | 6.51       |
| train/info_reward_run_min   | 6.51       |
| train/reward_per_eps        | nan        |
| train/steps                 | 93000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 88         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.26071617 |
| stats_o/std                 | 3.72931    |
| test/episodes               | 890        |
| test/info_reward_ctrl_max   | -0.106     |
| test/info_reward_ctrl_mean  | -0.475     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.3       |
| test/info_reward_run_mean   | 6.99       |
| test/info_reward_run_min    | -0.914     |
| test/reward_per_eps         | 6.51e+03   |
| test/steps                  | 890000     |
| train/episodes              | 94         |
| train/info_reward_ctrl_max  | -0.446     |
| train/info_reward_ctrl_mean | -0.446     |
| train/info_reward_ctrl_min  | -0.446     |
| train/info_reward_run_max   | 6.32       |
| train/info_reward_run_mean  | 6.32       |
| train/info_reward_run_min   | 6.32       |
| train/reward_per_eps        | nan        |
| train/steps                 | 94000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 89        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2620416 |
| stats_o/std                 | 3.7304463 |
| test/episodes               | 900       |
| test/info_reward_ctrl_max   | -0.041    |
| test/info_reward_ctrl_mean  | -0.465    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 9.52      |
| test/info_reward_run_mean   | 6.25      |
| test/info_reward_run_min    | -0.78     |
| test/reward_per_eps         | 5.78e+03  |
| test/steps                  | 900000    |
| train/episodes              | 95        |
| train/info_reward_ctrl_max  | -0.44     |
| train/info_reward_ctrl_mean | -0.44     |
| train/info_reward_ctrl_min  | -0.44     |
| train/info_reward_run_max   | 6.38      |
| train/info_reward_run_mean  | 6.38      |
| train/info_reward_run_min   | 6.38      |
| train/reward_per_eps        | nan       |
| train/steps                 | 95000     |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 90         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.26328897 |
| stats_o/std                 | 3.7313251  |
| test/episodes               | 910        |
| test/info_reward_ctrl_max   | -0.0405    |
| test/info_reward_ctrl_mean  | -0.472     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 9.64       |
| test/info_reward_run_mean   | 6.2        |
| test/info_reward_run_min    | -1.81      |
| test/reward_per_eps         | 5.72e+03   |
| test/steps                  | 910000     |
| train/episodes              | 96         |
| train/info_reward_ctrl_max  | -0.435     |
| train/info_reward_ctrl_mean | -0.435     |
| train/info_reward_ctrl_min  | -0.435     |
| train/info_reward_run_max   | 6.62       |
| train/info_reward_run_mean  | 6.62       |
| train/info_reward_run_min   | 6.62       |
| train/reward_per_eps        | nan        |
| train/steps                 | 96000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 91        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2646056 |
| stats_o/std                 | 3.7329702 |
| test/episodes               | 920       |
| test/info_reward_ctrl_max   | -0.0381   |
| test/info_reward_ctrl_mean  | -0.479    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.1      |
| test/info_reward_run_mean   | 6.24      |
| test/info_reward_run_min    | -1.81     |
| test/reward_per_eps         | 5.76e+03  |
| test/steps                  | 920000    |
| train/episodes              | 97        |
| train/info_reward_ctrl_max  | -0.438    |
| train/info_reward_ctrl_mean | -0.438    |
| train/info_reward_ctrl_min  | -0.438    |
| train/info_reward_run_max   | 6.68      |
| train/info_reward_run_mean  | 6.68      |
| train/info_reward_run_min   | 6.68      |
| train/reward_per_eps        | nan       |
| train/steps                 | 97000     |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 92         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.26613066 |
| stats_o/std                 | 3.7343411  |
| test/episodes               | 930        |
| test/info_reward_ctrl_max   | -0.0957    |
| test/info_reward_ctrl_mean  | -0.478     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.3       |
| test/info_reward_run_mean   | 6.64       |
| test/info_reward_run_min    | -0.798     |
| test/reward_per_eps         | 6.16e+03   |
| test/steps                  | 930000     |
| train/episodes              | 98         |
| train/info_reward_ctrl_max  | -0.451     |
| train/info_reward_ctrl_mean | -0.451     |
| train/info_reward_ctrl_min  | -0.451     |
| train/info_reward_run_max   | 6.89       |
| train/info_reward_run_mean  | 6.89       |
| train/info_reward_run_min   | 6.89       |
| train/reward_per_eps        | nan        |
| train/steps                 | 98000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 93         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.26748317 |
| stats_o/std                 | 3.735781   |
| test/episodes               | 940        |
| test/info_reward_ctrl_max   | -0.0826    |
| test/info_reward_ctrl_mean  | -0.455     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.2       |
| test/info_reward_run_mean   | 6.89       |
| test/info_reward_run_min    | -1.51      |
| test/reward_per_eps         | 6.43e+03   |
| test/steps                  | 940000     |
| train/episodes              | 99         |
| train/info_reward_ctrl_max  | -0.442     |
| train/info_reward_ctrl_mean | -0.442     |
| train/info_reward_ctrl_min  | -0.442     |
| train/info_reward_run_max   | 6.85       |
| train/info_reward_run_mean  | 6.85       |
| train/info_reward_run_min   | 6.85       |
| train/reward_per_eps        | nan        |
| train/steps                 | 99000      |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 94        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2688456 |
| stats_o/std                 | 3.736452  |
| test/episodes               | 950       |
| test/info_reward_ctrl_max   | -0.118    |
| test/info_reward_ctrl_mean  | -0.458    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.5      |
| test/info_reward_run_mean   | 7.07      |
| test/info_reward_run_min    | -0.826    |
| test/reward_per_eps         | 6.61e+03  |
| test/steps                  | 950000    |
| train/episodes              | 100       |
| train/info_reward_ctrl_max  | -0.439    |
| train/info_reward_ctrl_mean | -0.439    |
| train/info_reward_ctrl_min  | -0.439    |
| train/info_reward_run_max   | 6.93      |
| train/info_reward_run_mean  | 6.93      |
| train/info_reward_run_min   | 6.93      |
| train/reward_per_eps        | nan       |
| train/steps                 | 100000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 95         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.27017426 |
| stats_o/std                 | 3.7374215  |
| test/episodes               | 960        |
| test/info_reward_ctrl_max   | -0.0804    |
| test/info_reward_ctrl_mean  | -0.464     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.5       |
| test/info_reward_run_mean   | 7.09       |
| test/info_reward_run_min    | -0.734     |
| test/reward_per_eps         | 6.62e+03   |
| test/steps                  | 960000     |
| train/episodes              | 101        |
| train/info_reward_ctrl_max  | -0.435     |
| train/info_reward_ctrl_mean | -0.435     |
| train/info_reward_ctrl_min  | -0.435     |
| train/info_reward_run_max   | 6.69       |
| train/info_reward_run_mean  | 6.69       |
| train/info_reward_run_min   | 6.69       |
| train/reward_per_eps        | nan        |
| train/steps                 | 101000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 96        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2715734 |
| stats_o/std                 | 3.7391982 |
| test/episodes               | 970       |
| test/info_reward_ctrl_max   | -0.0823   |
| test/info_reward_ctrl_mean  | -0.475    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.4      |
| test/info_reward_run_mean   | 7.16      |
| test/info_reward_run_min    | -0.981    |
| test/reward_per_eps         | 6.68e+03  |
| test/steps                  | 970000    |
| train/episodes              | 102       |
| train/info_reward_ctrl_max  | -0.452    |
| train/info_reward_ctrl_mean | -0.452    |
| train/info_reward_ctrl_min  | -0.452    |
| train/info_reward_run_max   | 7.3       |
| train/info_reward_run_mean  | 7.3       |
| train/info_reward_run_min   | 7.3       |
| train/reward_per_eps        | nan       |
| train/steps                 | 102000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 97        |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2728379 |
| stats_o/std                 | 3.739772  |
| test/episodes               | 980       |
| test/info_reward_ctrl_max   | -0.0481   |
| test/info_reward_ctrl_mean  | -0.45     |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.6      |
| test/info_reward_run_mean   | 7.02      |
| test/info_reward_run_min    | -1.05     |
| test/reward_per_eps         | 6.57e+03  |
| test/steps                  | 980000    |
| train/episodes              | 103       |
| train/info_reward_ctrl_max  | -0.435    |
| train/info_reward_ctrl_mean | -0.435    |
| train/info_reward_ctrl_min  | -0.435    |
| train/info_reward_run_max   | 7         |
| train/info_reward_run_mean  | 7         |
| train/info_reward_run_min   | 7         |
| train/reward_per_eps        | nan       |
| train/steps                 | 103000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 98         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.27403504 |
| stats_o/std                 | 3.740427   |
| test/episodes               | 990        |
| test/info_reward_ctrl_max   | -0.136     |
| test/info_reward_ctrl_mean  | -0.489     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11         |
| test/info_reward_run_mean   | 6.44       |
| test/info_reward_run_min    | -2.01      |
| test/reward_per_eps         | 5.95e+03   |
| test/steps                  | 990000     |
| train/episodes              | 104        |
| train/info_reward_ctrl_max  | -0.444     |
| train/info_reward_ctrl_mean | -0.444     |
| train/info_reward_ctrl_min  | -0.444     |
| train/info_reward_run_max   | 6.37       |
| train/info_reward_run_mean  | 6.37       |
| train/info_reward_run_min   | 6.37       |
| train/reward_per_eps        | nan        |
| train/steps                 | 104000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 99         |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.27530083 |
| stats_o/std                 | 3.7409153  |
| test/episodes               | 1000       |
| test/info_reward_ctrl_max   | -0.136     |
| test/info_reward_ctrl_mean  | -0.475     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.8       |
| test/info_reward_run_mean   | 7.18       |
| test/info_reward_run_min    | -0.607     |
| test/reward_per_eps         | 6.7e+03    |
| test/steps                  | 1000000    |
| train/episodes              | 105        |
| train/info_reward_ctrl_max  | -0.445     |
| train/info_reward_ctrl_mean | -0.445     |
| train/info_reward_ctrl_min  | -0.445     |
| train/info_reward_run_max   | 6.44       |
| train/info_reward_run_mean  | 6.44       |
| train/info_reward_run_min   | 6.44       |
| train/reward_per_eps        | nan        |
| train/steps                 | 105000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 100        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.27641922 |
| stats_o/std                 | 3.741384   |
| test/episodes               | 1010       |
| test/info_reward_ctrl_max   | -0.069     |
| test/info_reward_ctrl_mean  | -0.471     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.9       |
| test/info_reward_run_mean   | 6.93       |
| test/info_reward_run_min    | -0.848     |
| test/reward_per_eps         | 6.46e+03   |
| test/steps                  | 1010000    |
| train/episodes              | 106        |
| train/info_reward_ctrl_max  | -0.449     |
| train/info_reward_ctrl_mean | -0.449     |
| train/info_reward_ctrl_min  | -0.449     |
| train/info_reward_run_max   | 6.68       |
| train/info_reward_run_mean  | 6.68       |
| train/info_reward_run_min   | 6.68       |
| train/reward_per_eps        | nan        |
| train/steps                 | 106000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 101       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2774619 |
| stats_o/std                 | 3.741875  |
| test/episodes               | 1020      |
| test/info_reward_ctrl_max   | -0.118    |
| test/info_reward_ctrl_mean  | -0.484    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.6      |
| test/info_reward_run_mean   | 6.62      |
| test/info_reward_run_min    | -2.6      |
| test/reward_per_eps         | 6.14e+03  |
| test/steps                  | 1020000   |
| train/episodes              | 107       |
| train/info_reward_ctrl_max  | -0.46     |
| train/info_reward_ctrl_mean | -0.46     |
| train/info_reward_ctrl_min  | -0.46     |
| train/info_reward_run_max   | 6.78      |
| train/info_reward_run_mean  | 6.78      |
| train/info_reward_run_min   | 6.78      |
| train/reward_per_eps        | nan       |
| train/steps                 | 107000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 102        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.27871817 |
| stats_o/std                 | 3.742787   |
| test/episodes               | 1030       |
| test/info_reward_ctrl_max   | -0.146     |
| test/info_reward_ctrl_mean  | -0.469     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.1       |
| test/info_reward_run_mean   | 6.66       |
| test/info_reward_run_min    | -0.816     |
| test/reward_per_eps         | 6.19e+03   |
| test/steps                  | 1030000    |
| train/episodes              | 108        |
| train/info_reward_ctrl_max  | -0.432     |
| train/info_reward_ctrl_mean | -0.432     |
| train/info_reward_ctrl_min  | -0.432     |
| train/info_reward_run_max   | 6.72       |
| train/info_reward_run_mean  | 6.72       |
| train/info_reward_run_min   | 6.72       |
| train/reward_per_eps        | nan        |
| train/steps                 | 108000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 103        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.27996108 |
| stats_o/std                 | 3.7443345  |
| test/episodes               | 1040       |
| test/info_reward_ctrl_max   | -0.0512    |
| test/info_reward_ctrl_mean  | -0.46      |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11         |
| test/info_reward_run_mean   | 7.34       |
| test/info_reward_run_min    | -0.783     |
| test/reward_per_eps         | 6.88e+03   |
| test/steps                  | 1040000    |
| train/episodes              | 109        |
| train/info_reward_ctrl_max  | -0.446     |
| train/info_reward_ctrl_mean | -0.446     |
| train/info_reward_ctrl_min  | -0.446     |
| train/info_reward_run_max   | 6.89       |
| train/info_reward_run_mean  | 6.89       |
| train/info_reward_run_min   | 6.89       |
| train/reward_per_eps        | nan        |
| train/steps                 | 109000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 104        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.28041962 |
| stats_o/std                 | 3.7440603  |
| test/episodes               | 1050       |
| test/info_reward_ctrl_max   | -0.0579    |
| test/info_reward_ctrl_mean  | -0.484     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.5       |
| test/info_reward_run_mean   | 7.15       |
| test/info_reward_run_min    | -0.848     |
| test/reward_per_eps         | 6.67e+03   |
| test/steps                  | 1050000    |
| train/episodes              | 110        |
| train/info_reward_ctrl_max  | -0.529     |
| train/info_reward_ctrl_mean | -0.529     |
| train/info_reward_ctrl_min  | -0.529     |
| train/info_reward_run_max   | 0.0085     |
| train/info_reward_run_mean  | 0.0085     |
| train/info_reward_run_min   | 0.0085     |
| train/reward_per_eps        | nan        |
| train/steps                 | 110000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 105        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.28174576 |
| stats_o/std                 | 3.745921   |
| test/episodes               | 1060       |
| test/info_reward_ctrl_max   | -0.102     |
| test/info_reward_ctrl_mean  | -0.467     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.1       |
| test/info_reward_run_mean   | 7.45       |
| test/info_reward_run_min    | -1.16      |
| test/reward_per_eps         | 6.98e+03   |
| test/steps                  | 1060000    |
| train/episodes              | 111        |
| train/info_reward_ctrl_max  | -0.441     |
| train/info_reward_ctrl_mean | -0.441     |
| train/info_reward_ctrl_min  | -0.441     |
| train/info_reward_run_max   | 7.15       |
| train/info_reward_run_mean  | 7.15       |
| train/info_reward_run_min   | 7.15       |
| train/reward_per_eps        | nan        |
| train/steps                 | 111000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 106        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.28302643 |
| stats_o/std                 | 3.74744    |
| test/episodes               | 1070       |
| test/info_reward_ctrl_max   | -0.108     |
| test/info_reward_ctrl_mean  | -0.474     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.2       |
| test/info_reward_run_mean   | 6.6        |
| test/info_reward_run_min    | -0.841     |
| test/reward_per_eps         | 6.12e+03   |
| test/steps                  | 1070000    |
| train/episodes              | 112        |
| train/info_reward_ctrl_max  | -0.444     |
| train/info_reward_ctrl_mean | -0.444     |
| train/info_reward_ctrl_min  | -0.444     |
| train/info_reward_run_max   | 7.11       |
| train/info_reward_run_mean  | 7.11       |
| train/info_reward_run_min   | 7.11       |
| train/reward_per_eps        | nan        |
| train/steps                 | 112000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 107        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.28413162 |
| stats_o/std                 | 3.7486687  |
| test/episodes               | 1080       |
| test/info_reward_ctrl_max   | -0.0917    |
| test/info_reward_ctrl_mean  | -0.475     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11         |
| test/info_reward_run_mean   | 7.11       |
| test/info_reward_run_min    | -0.841     |
| test/reward_per_eps         | 6.63e+03   |
| test/steps                  | 1080000    |
| train/episodes              | 113        |
| train/info_reward_ctrl_max  | -0.445     |
| train/info_reward_ctrl_mean | -0.445     |
| train/info_reward_ctrl_min  | -0.445     |
| train/info_reward_run_max   | 7.02       |
| train/info_reward_run_mean  | 7.02       |
| train/info_reward_run_min   | 7.02       |
| train/reward_per_eps        | nan        |
| train/steps                 | 113000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 108       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2853746 |
| stats_o/std                 | 3.749776  |
| test/episodes               | 1090      |
| test/info_reward_ctrl_max   | -0.0996   |
| test/info_reward_ctrl_mean  | -0.489    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 11        |
| test/info_reward_run_mean   | 6.79      |
| test/info_reward_run_min    | -0.661    |
| test/reward_per_eps         | 6.3e+03   |
| test/steps                  | 1090000   |
| train/episodes              | 114       |
| train/info_reward_ctrl_max  | -0.443    |
| train/info_reward_ctrl_mean | -0.443    |
| train/info_reward_ctrl_min  | -0.443    |
| train/info_reward_run_max   | 7.2       |
| train/info_reward_run_mean  | 7.2       |
| train/info_reward_run_min   | 7.2       |
| train/reward_per_eps        | nan       |
| train/steps                 | 114000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 109       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2864925 |
| stats_o/std                 | 3.7505074 |
| test/episodes               | 1100      |
| test/info_reward_ctrl_max   | -0.114    |
| test/info_reward_ctrl_mean  | -0.477    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.4      |
| test/info_reward_run_mean   | 6.82      |
| test/info_reward_run_min    | -0.726    |
| test/reward_per_eps         | 6.34e+03  |
| test/steps                  | 1100000   |
| train/episodes              | 115       |
| train/info_reward_ctrl_max  | -0.447    |
| train/info_reward_ctrl_mean | -0.447    |
| train/info_reward_ctrl_min  | -0.447    |
| train/info_reward_run_max   | 6.94      |
| train/info_reward_run_mean  | 6.94      |
| train/info_reward_run_min   | 6.94      |
| train/reward_per_eps        | nan       |
| train/steps                 | 115000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 110       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2877035 |
| stats_o/std                 | 3.751638  |
| test/episodes               | 1110      |
| test/info_reward_ctrl_max   | -0.104    |
| test/info_reward_ctrl_mean  | -0.475    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.2      |
| test/info_reward_run_mean   | 6.66      |
| test/info_reward_run_min    | -0.897    |
| test/reward_per_eps         | 6.18e+03  |
| test/steps                  | 1110000   |
| train/episodes              | 116       |
| train/info_reward_ctrl_max  | -0.452    |
| train/info_reward_ctrl_mean | -0.452    |
| train/info_reward_ctrl_min  | -0.452    |
| train/info_reward_run_max   | 7.42      |
| train/info_reward_run_mean  | 7.42      |
| train/info_reward_run_min   | 7.42      |
| train/reward_per_eps        | nan       |
| train/steps                 | 116000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 111        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.28874612 |
| stats_o/std                 | 3.7521324  |
| test/episodes               | 1120       |
| test/info_reward_ctrl_max   | -0.0974    |
| test/info_reward_ctrl_mean  | -0.466     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.7       |
| test/info_reward_run_mean   | 7.35       |
| test/info_reward_run_min    | -0.547     |
| test/reward_per_eps         | 6.89e+03   |
| test/steps                  | 1120000    |
| train/episodes              | 117        |
| train/info_reward_ctrl_max  | -0.442     |
| train/info_reward_ctrl_mean | -0.442     |
| train/info_reward_ctrl_min  | -0.442     |
| train/info_reward_run_max   | 6.7        |
| train/info_reward_run_mean  | 6.7        |
| train/info_reward_run_min   | 6.7        |
| train/reward_per_eps        | nan        |
| train/steps                 | 117000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
------------------------------------------
| epoch                       | 112      |
| stats_g/mean                | nan      |
| stats_g/std                 | nan      |
| stats_o/mean                | 0.289874 |
| stats_o/std                 | 3.753395 |
| test/episodes               | 1130     |
| test/info_reward_ctrl_max   | -0.105   |
| test/info_reward_ctrl_mean  | -0.471   |
| test/info_reward_ctrl_min   | -0.6     |
| test/info_reward_run_max    | 10.9     |
| test/info_reward_run_mean   | 7.26     |
| test/info_reward_run_min    | -0.529   |
| test/reward_per_eps         | 6.79e+03 |
| test/steps                  | 1130000  |
| train/episodes              | 118      |
| train/info_reward_ctrl_max  | -0.439   |
| train/info_reward_ctrl_mean | -0.439   |
| train/info_reward_ctrl_min  | -0.439   |
| train/info_reward_run_max   | 6.83     |
| train/info_reward_run_mean  | 6.83     |
| train/info_reward_run_min   | 6.83     |
| train/reward_per_eps        | nan      |
| train/steps                 | 118000   |
------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 113       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2910053 |
| stats_o/std                 | 3.7546835 |
| test/episodes               | 1140      |
| test/info_reward_ctrl_max   | -0.0928   |
| test/info_reward_ctrl_mean  | -0.458    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 11.1      |
| test/info_reward_run_mean   | 7.61      |
| test/info_reward_run_min    | -0.686    |
| test/reward_per_eps         | 7.16e+03  |
| test/steps                  | 1140000   |
| train/episodes              | 119       |
| train/info_reward_ctrl_max  | -0.435    |
| train/info_reward_ctrl_mean | -0.435    |
| train/info_reward_ctrl_min  | -0.435    |
| train/info_reward_run_max   | 7.81      |
| train/info_reward_run_mean  | 7.81      |
| train/info_reward_run_min   | 7.81      |
| train/reward_per_eps        | nan       |
| train/steps                 | 119000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 114       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2922707 |
| stats_o/std                 | 3.7559571 |
| test/episodes               | 1150      |
| test/info_reward_ctrl_max   | -0.142    |
| test/info_reward_ctrl_mean  | -0.477    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.7      |
| test/info_reward_run_mean   | 7.19      |
| test/info_reward_run_min    | -0.748    |
| test/reward_per_eps         | 6.71e+03  |
| test/steps                  | 1150000   |
| train/episodes              | 120       |
| train/info_reward_ctrl_max  | -0.446    |
| train/info_reward_ctrl_mean | -0.446    |
| train/info_reward_ctrl_min  | -0.446    |
| train/info_reward_run_max   | 7.12      |
| train/info_reward_run_mean  | 7.12      |
| train/info_reward_run_min   | 7.12      |
| train/reward_per_eps        | nan       |
| train/steps                 | 120000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 115        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.29338458 |
| stats_o/std                 | 3.7564487  |
| test/episodes               | 1160       |
| test/info_reward_ctrl_max   | -0.145     |
| test/info_reward_ctrl_mean  | -0.47      |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.8       |
| test/info_reward_run_mean   | 7.28       |
| test/info_reward_run_min    | -0.61      |
| test/reward_per_eps         | 6.81e+03   |
| test/steps                  | 1160000    |
| train/episodes              | 121        |
| train/info_reward_ctrl_max  | -0.452     |
| train/info_reward_ctrl_mean | -0.452     |
| train/info_reward_ctrl_min  | -0.452     |
| train/info_reward_run_max   | 6.98       |
| train/info_reward_run_mean  | 6.98       |
| train/info_reward_run_min   | 6.98       |
| train/reward_per_eps        | nan        |
| train/steps                 | 121000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 116        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.29449463 |
| stats_o/std                 | 3.7584486  |
| test/episodes               | 1170       |
| test/info_reward_ctrl_max   | -0.105     |
| test/info_reward_ctrl_mean  | -0.474     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.7       |
| test/info_reward_run_mean   | 7.27       |
| test/info_reward_run_min    | -0.821     |
| test/reward_per_eps         | 6.8e+03    |
| test/steps                  | 1170000    |
| train/episodes              | 122        |
| train/info_reward_ctrl_max  | -0.455     |
| train/info_reward_ctrl_mean | -0.455     |
| train/info_reward_ctrl_min  | -0.455     |
| train/info_reward_run_max   | 7.51       |
| train/info_reward_run_mean  | 7.51       |
| train/info_reward_run_min   | 7.51       |
| train/reward_per_eps        | nan        |
| train/steps                 | 122000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 117        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.29557577 |
| stats_o/std                 | 3.7598448  |
| test/episodes               | 1180       |
| test/info_reward_ctrl_max   | -0.132     |
| test/info_reward_ctrl_mean  | -0.476     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.5       |
| test/info_reward_run_mean   | 6.96       |
| test/info_reward_run_min    | -0.894     |
| test/reward_per_eps         | 6.49e+03   |
| test/steps                  | 1180000    |
| train/episodes              | 123        |
| train/info_reward_ctrl_max  | -0.446     |
| train/info_reward_ctrl_mean | -0.446     |
| train/info_reward_ctrl_min  | -0.446     |
| train/info_reward_run_max   | 7.47       |
| train/info_reward_run_mean  | 7.47       |
| train/info_reward_run_min   | 7.47       |
| train/reward_per_eps        | nan        |
| train/steps                 | 123000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 118        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.29650265 |
| stats_o/std                 | 3.7616827  |
| test/episodes               | 1190       |
| test/info_reward_ctrl_max   | -0.0551    |
| test/info_reward_ctrl_mean  | -0.463     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.4       |
| test/info_reward_run_mean   | 6.95       |
| test/info_reward_run_min    | -0.826     |
| test/reward_per_eps         | 6.49e+03   |
| test/steps                  | 1190000    |
| train/episodes              | 124        |
| train/info_reward_ctrl_max  | -0.44      |
| train/info_reward_ctrl_mean | -0.44      |
| train/info_reward_ctrl_min  | -0.44      |
| train/info_reward_run_max   | 6.51       |
| train/info_reward_run_mean  | 6.51       |
| train/info_reward_run_min   | 6.51       |
| train/reward_per_eps        | nan        |
| train/steps                 | 124000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 119        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.29760692 |
| stats_o/std                 | 3.763646   |
| test/episodes               | 1200       |
| test/info_reward_ctrl_max   | -0.056     |
| test/info_reward_ctrl_mean  | -0.452     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.4       |
| test/info_reward_run_mean   | 6.75       |
| test/info_reward_run_min    | -0.69      |
| test/reward_per_eps         | 6.3e+03    |
| test/steps                  | 1200000    |
| train/episodes              | 125        |
| train/info_reward_ctrl_max  | -0.433     |
| train/info_reward_ctrl_mean | -0.433     |
| train/info_reward_ctrl_min  | -0.433     |
| train/info_reward_run_max   | 7.12       |
| train/info_reward_run_mean  | 7.12       |
| train/info_reward_run_min   | 7.12       |
| train/reward_per_eps        | nan        |
| train/steps                 | 125000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 120       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.2983956 |
| stats_o/std                 | 3.7643456 |
| test/episodes               | 1210      |
| test/info_reward_ctrl_max   | -0.115    |
| test/info_reward_ctrl_mean  | -0.457    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 11.1      |
| test/info_reward_run_mean   | 7.4       |
| test/info_reward_run_min    | -0.815    |
| test/reward_per_eps         | 6.94e+03  |
| test/steps                  | 1210000   |
| train/episodes              | 126       |
| train/info_reward_ctrl_max  | -0.433    |
| train/info_reward_ctrl_mean | -0.433    |
| train/info_reward_ctrl_min  | -0.433    |
| train/info_reward_run_max   | 6.62      |
| train/info_reward_run_mean  | 6.62      |
| train/info_reward_run_min   | 6.62      |
| train/reward_per_eps        | nan       |
| train/steps                 | 126000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 121        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.29931444 |
| stats_o/std                 | 3.7647822  |
| test/episodes               | 1220       |
| test/info_reward_ctrl_max   | -0.113     |
| test/info_reward_ctrl_mean  | -0.465     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.8       |
| test/info_reward_run_mean   | 6.88       |
| test/info_reward_run_min    | -0.854     |
| test/reward_per_eps         | 6.41e+03   |
| test/steps                  | 1220000    |
| train/episodes              | 127        |
| train/info_reward_ctrl_max  | -0.434     |
| train/info_reward_ctrl_mean | -0.434     |
| train/info_reward_ctrl_min  | -0.434     |
| train/info_reward_run_max   | 7.35       |
| train/info_reward_run_mean  | 7.35       |
| train/info_reward_run_min   | 7.35       |
| train/reward_per_eps        | nan        |
| train/steps                 | 127000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 122        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.29963067 |
| stats_o/std                 | 3.7623591  |
| test/episodes               | 1230       |
| test/info_reward_ctrl_max   | -0.0989    |
| test/info_reward_ctrl_mean  | -0.458     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.6       |
| test/info_reward_run_mean   | 7.04       |
| test/info_reward_run_min    | -0.528     |
| test/reward_per_eps         | 6.58e+03   |
| test/steps                  | 1230000    |
| train/episodes              | 128        |
| train/info_reward_ctrl_max  | -0.553     |
| train/info_reward_ctrl_mean | -0.553     |
| train/info_reward_ctrl_min  | -0.553     |
| train/info_reward_run_max   | 0.000158   |
| train/info_reward_run_mean  | 0.000158   |
| train/info_reward_run_min   | 0.000158   |
| train/reward_per_eps        | nan        |
| train/steps                 | 128000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 123        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.30043402 |
| stats_o/std                 | 3.7636144  |
| test/episodes               | 1240       |
| test/info_reward_ctrl_max   | -0.0663    |
| test/info_reward_ctrl_mean  | -0.452     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.8       |
| test/info_reward_run_mean   | 7.1        |
| test/info_reward_run_min    | -0.615     |
| test/reward_per_eps         | 6.65e+03   |
| test/steps                  | 1240000    |
| train/episodes              | 129        |
| train/info_reward_ctrl_max  | -0.444     |
| train/info_reward_ctrl_mean | -0.444     |
| train/info_reward_ctrl_min  | -0.444     |
| train/info_reward_run_max   | 7.04       |
| train/info_reward_run_mean  | 7.04       |
| train/info_reward_run_min   | 7.04       |
| train/reward_per_eps        | nan        |
| train/steps                 | 129000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 124        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.30043024 |
| stats_o/std                 | 3.7613175  |
| test/episodes               | 1250       |
| test/info_reward_ctrl_max   | -0.106     |
| test/info_reward_ctrl_mean  | -0.467     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.6       |
| test/info_reward_run_mean   | 6.68       |
| test/info_reward_run_min    | -0.763     |
| test/reward_per_eps         | 6.21e+03   |
| test/steps                  | 1250000    |
| train/episodes              | 130        |
| train/info_reward_ctrl_max  | -0.556     |
| train/info_reward_ctrl_mean | -0.556     |
| train/info_reward_ctrl_min  | -0.556     |
| train/info_reward_run_max   | 0.000256   |
| train/info_reward_run_mean  | 0.000256   |
| train/info_reward_run_min   | 0.000256   |
| train/reward_per_eps        | nan        |
| train/steps                 | 130000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 125        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.30113626 |
| stats_o/std                 | 3.761075   |
| test/episodes               | 1260       |
| test/info_reward_ctrl_max   | -0.129     |
| test/info_reward_ctrl_mean  | -0.448     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.4       |
| test/info_reward_run_mean   | 6.83       |
| test/info_reward_run_min    | -0.515     |
| test/reward_per_eps         | 6.39e+03   |
| test/steps                  | 1260000    |
| train/episodes              | 131        |
| train/info_reward_ctrl_max  | -0.436     |
| train/info_reward_ctrl_mean | -0.436     |
| train/info_reward_ctrl_min  | -0.436     |
| train/info_reward_run_max   | 6.6        |
| train/info_reward_run_mean  | 6.6        |
| train/info_reward_run_min   | 6.6        |
| train/reward_per_eps        | nan        |
| train/steps                 | 131000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 126        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.30183902 |
| stats_o/std                 | 3.761027   |
| test/episodes               | 1270       |
| test/info_reward_ctrl_max   | -0.0774    |
| test/info_reward_ctrl_mean  | -0.435     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 9.74       |
| test/info_reward_run_mean   | 6.79       |
| test/info_reward_run_min    | -0.554     |
| test/reward_per_eps         | 6.35e+03   |
| test/steps                  | 1270000    |
| train/episodes              | 132        |
| train/info_reward_ctrl_max  | -0.428     |
| train/info_reward_ctrl_mean | -0.428     |
| train/info_reward_ctrl_min  | -0.428     |
| train/info_reward_run_max   | 6.94       |
| train/info_reward_run_mean  | 6.94       |
| train/info_reward_run_min   | 6.94       |
| train/reward_per_eps        | nan        |
| train/steps                 | 132000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 127        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.30269724 |
| stats_o/std                 | 3.761864   |
| test/episodes               | 1280       |
| test/info_reward_ctrl_max   | -0.0656    |
| test/info_reward_ctrl_mean  | -0.461     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.1       |
| test/info_reward_run_mean   | 7.44       |
| test/info_reward_run_min    | -0.999     |
| test/reward_per_eps         | 6.98e+03   |
| test/steps                  | 1280000    |
| train/episodes              | 133        |
| train/info_reward_ctrl_max  | -0.432     |
| train/info_reward_ctrl_mean | -0.432     |
| train/info_reward_ctrl_min  | -0.432     |
| train/info_reward_run_max   | 6.69       |
| train/info_reward_run_mean  | 6.69       |
| train/info_reward_run_min   | 6.69       |
| train/reward_per_eps        | nan        |
| train/steps                 | 133000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 128       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.3035559 |
| stats_o/std                 | 3.7619483 |
| test/episodes               | 1290      |
| test/info_reward_ctrl_max   | -0.111    |
| test/info_reward_ctrl_mean  | -0.473    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.1      |
| test/info_reward_run_mean   | 6.35      |
| test/info_reward_run_min    | -0.887    |
| test/reward_per_eps         | 5.88e+03  |
| test/steps                  | 1290000   |
| train/episodes              | 134       |
| train/info_reward_ctrl_max  | -0.438    |
| train/info_reward_ctrl_mean | -0.438    |
| train/info_reward_ctrl_min  | -0.438    |
| train/info_reward_run_max   | 6.97      |
| train/info_reward_run_mean  | 6.97      |
| train/info_reward_run_min   | 6.97      |
| train/reward_per_eps        | nan       |
| train/steps                 | 134000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 129        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.30437502 |
| stats_o/std                 | 3.7620077  |
| test/episodes               | 1300       |
| test/info_reward_ctrl_max   | -0.0809    |
| test/info_reward_ctrl_mean  | -0.452     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.5       |
| test/info_reward_run_mean   | 7.78       |
| test/info_reward_run_min    | -0.642     |
| test/reward_per_eps         | 7.33e+03   |
| test/steps                  | 1300000    |
| train/episodes              | 135        |
| train/info_reward_ctrl_max  | -0.43      |
| train/info_reward_ctrl_mean | -0.43      |
| train/info_reward_ctrl_min  | -0.43      |
| train/info_reward_run_max   | 7.4        |
| train/info_reward_run_mean  | 7.4        |
| train/info_reward_run_min   | 7.4        |
| train/reward_per_eps        | nan        |
| train/steps                 | 135000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 130        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.30525702 |
| stats_o/std                 | 3.7623036  |
| test/episodes               | 1310       |
| test/info_reward_ctrl_max   | -0.113     |
| test/info_reward_ctrl_mean  | -0.472     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.6       |
| test/info_reward_run_mean   | 7.52       |
| test/info_reward_run_min    | -0.661     |
| test/reward_per_eps         | 7.05e+03   |
| test/steps                  | 1310000    |
| train/episodes              | 136        |
| train/info_reward_ctrl_max  | -0.444     |
| train/info_reward_ctrl_mean | -0.444     |
| train/info_reward_ctrl_min  | -0.444     |
| train/info_reward_run_max   | 7.18       |
| train/info_reward_run_mean  | 7.18       |
| train/info_reward_run_min   | 7.18       |
| train/reward_per_eps        | nan        |
| train/steps                 | 136000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 131        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.30620933 |
| stats_o/std                 | 3.7628338  |
| test/episodes               | 1320       |
| test/info_reward_ctrl_max   | -0.0336    |
| test/info_reward_ctrl_mean  | -0.44      |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.4       |
| test/info_reward_run_mean   | 7.07       |
| test/info_reward_run_min    | -0.917     |
| test/reward_per_eps         | 6.63e+03   |
| test/steps                  | 1320000    |
| train/episodes              | 137        |
| train/info_reward_ctrl_max  | -0.435     |
| train/info_reward_ctrl_mean | -0.435     |
| train/info_reward_ctrl_min  | -0.435     |
| train/info_reward_run_max   | 7.5        |
| train/info_reward_run_mean  | 7.5        |
| train/info_reward_run_min   | 7.5        |
| train/reward_per_eps        | nan        |
| train/steps                 | 137000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 132        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.30709338 |
| stats_o/std                 | 3.7632587  |
| test/episodes               | 1330       |
| test/info_reward_ctrl_max   | -0.0745    |
| test/info_reward_ctrl_mean  | -0.473     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.3       |
| test/info_reward_run_mean   | 7.17       |
| test/info_reward_run_min    | -0.623     |
| test/reward_per_eps         | 6.7e+03    |
| test/steps                  | 1330000    |
| train/episodes              | 138        |
| train/info_reward_ctrl_max  | -0.437     |
| train/info_reward_ctrl_mean | -0.437     |
| train/info_reward_ctrl_min  | -0.437     |
| train/info_reward_run_max   | 7.35       |
| train/info_reward_run_mean  | 7.35       |
| train/info_reward_run_min   | 7.35       |
| train/reward_per_eps        | nan        |
| train/steps                 | 138000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 133        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.30802274 |
| stats_o/std                 | 3.7644384  |
| test/episodes               | 1340       |
| test/info_reward_ctrl_max   | -0.0844    |
| test/info_reward_ctrl_mean  | -0.462     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.4       |
| test/info_reward_run_mean   | 7.84       |
| test/info_reward_run_min    | -0.612     |
| test/reward_per_eps         | 7.38e+03   |
| test/steps                  | 1340000    |
| train/episodes              | 139        |
| train/info_reward_ctrl_max  | -0.438     |
| train/info_reward_ctrl_mean | -0.438     |
| train/info_reward_ctrl_min  | -0.438     |
| train/info_reward_run_max   | 7.35       |
| train/info_reward_run_mean  | 7.35       |
| train/info_reward_run_min   | 7.35       |
| train/reward_per_eps        | nan        |
| train/steps                 | 139000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 134       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.3088645 |
| stats_o/std                 | 3.7641258 |
| test/episodes               | 1350      |
| test/info_reward_ctrl_max   | -0.08     |
| test/info_reward_ctrl_mean  | -0.454    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.9      |
| test/info_reward_run_mean   | 7.1       |
| test/info_reward_run_min    | -0.496    |
| test/reward_per_eps         | 6.65e+03  |
| test/steps                  | 1350000   |
| train/episodes              | 140       |
| train/info_reward_ctrl_max  | -0.427    |
| train/info_reward_ctrl_mean | -0.427    |
| train/info_reward_ctrl_min  | -0.427    |
| train/info_reward_run_max   | 6.97      |
| train/info_reward_run_mean  | 6.97      |
| train/info_reward_run_min   | 6.97      |
| train/reward_per_eps        | nan       |
| train/steps                 | 140000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 135        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.30967027 |
| stats_o/std                 | 3.7636962  |
| test/episodes               | 1360       |
| test/info_reward_ctrl_max   | -0.0695    |
| test/info_reward_ctrl_mean  | -0.448     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.7       |
| test/info_reward_run_mean   | 7.25       |
| test/info_reward_run_min    | -0.535     |
| test/reward_per_eps         | 6.8e+03    |
| test/steps                  | 1360000    |
| train/episodes              | 141        |
| train/info_reward_ctrl_max  | -0.417     |
| train/info_reward_ctrl_mean | -0.417     |
| train/info_reward_ctrl_min  | -0.417     |
| train/info_reward_run_max   | 7.3        |
| train/info_reward_run_mean  | 7.3        |
| train/info_reward_run_min   | 7.3        |
| train/reward_per_eps        | nan        |
| train/steps                 | 141000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 136       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.3105808 |
| stats_o/std                 | 3.764471  |
| test/episodes               | 1370      |
| test/info_reward_ctrl_max   | -0.044    |
| test/info_reward_ctrl_mean  | -0.453    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 11        |
| test/info_reward_run_mean   | 7.39      |
| test/info_reward_run_min    | -0.554    |
| test/reward_per_eps         | 6.93e+03  |
| test/steps                  | 1370000   |
| train/episodes              | 142       |
| train/info_reward_ctrl_max  | -0.428    |
| train/info_reward_ctrl_mean | -0.428    |
| train/info_reward_ctrl_min  | -0.428    |
| train/info_reward_run_max   | 7.51      |
| train/info_reward_run_mean  | 7.51      |
| train/info_reward_run_min   | 7.51      |
| train/reward_per_eps        | nan       |
| train/steps                 | 142000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 137        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.31138784 |
| stats_o/std                 | 3.76524    |
| test/episodes               | 1380       |
| test/info_reward_ctrl_max   | -0.0168    |
| test/info_reward_ctrl_mean  | -0.435     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 9.48       |
| test/info_reward_run_mean   | 6.39       |
| test/info_reward_run_min    | -0.779     |
| test/reward_per_eps         | 5.96e+03   |
| test/steps                  | 1380000    |
| train/episodes              | 143        |
| train/info_reward_ctrl_max  | -0.417     |
| train/info_reward_ctrl_mean | -0.417     |
| train/info_reward_ctrl_min  | -0.417     |
| train/info_reward_run_max   | 7.16       |
| train/info_reward_run_mean  | 7.16       |
| train/info_reward_run_min   | 7.16       |
| train/reward_per_eps        | nan        |
| train/steps                 | 143000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 138        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.31194714 |
| stats_o/std                 | 3.7643056  |
| test/episodes               | 1390       |
| test/info_reward_ctrl_max   | -0.128     |
| test/info_reward_ctrl_mean  | -0.439     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.1       |
| test/info_reward_run_mean   | 7.12       |
| test/info_reward_run_min    | -0.875     |
| test/reward_per_eps         | 6.68e+03   |
| test/steps                  | 1390000    |
| train/episodes              | 144        |
| train/info_reward_ctrl_max  | -0.412     |
| train/info_reward_ctrl_mean | -0.412     |
| train/info_reward_ctrl_min  | -0.412     |
| train/info_reward_run_max   | 7.13       |
| train/info_reward_run_mean  | 7.13       |
| train/info_reward_run_min   | 7.13       |
| train/reward_per_eps        | nan        |
| train/steps                 | 144000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 139        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.31266695 |
| stats_o/std                 | 3.7640164  |
| test/episodes               | 1400       |
| test/info_reward_ctrl_max   | -0.0699    |
| test/info_reward_ctrl_mean  | -0.46      |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.6       |
| test/info_reward_run_mean   | 7.03       |
| test/info_reward_run_min    | -0.641     |
| test/reward_per_eps         | 6.57e+03   |
| test/steps                  | 1400000    |
| train/episodes              | 145        |
| train/info_reward_ctrl_max  | -0.432     |
| train/info_reward_ctrl_mean | -0.432     |
| train/info_reward_ctrl_min  | -0.432     |
| train/info_reward_run_max   | 6.87       |
| train/info_reward_run_mean  | 6.87       |
| train/info_reward_run_min   | 6.87       |
| train/reward_per_eps        | nan        |
| train/steps                 | 145000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 140       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.3133224 |
| stats_o/std                 | 3.7637181 |
| test/episodes               | 1410      |
| test/info_reward_ctrl_max   | -0.0753   |
| test/info_reward_ctrl_mean  | -0.442    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.5      |
| test/info_reward_run_mean   | 7.12      |
| test/info_reward_run_min    | -0.795    |
| test/reward_per_eps         | 6.68e+03  |
| test/steps                  | 1410000   |
| train/episodes              | 146       |
| train/info_reward_ctrl_max  | -0.418    |
| train/info_reward_ctrl_mean | -0.418    |
| train/info_reward_ctrl_min  | -0.418    |
| train/info_reward_run_max   | 7.1       |
| train/info_reward_run_mean  | 7.1       |
| train/info_reward_run_min   | 7.1       |
| train/reward_per_eps        | nan       |
| train/steps                 | 146000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 141       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.3139007 |
| stats_o/std                 | 3.7632053 |
| test/episodes               | 1420      |
| test/info_reward_ctrl_max   | -0.0624   |
| test/info_reward_ctrl_mean  | -0.455    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.6      |
| test/info_reward_run_mean   | 7.04      |
| test/info_reward_run_min    | -0.596    |
| test/reward_per_eps         | 6.58e+03  |
| test/steps                  | 1420000   |
| train/episodes              | 147       |
| train/info_reward_ctrl_max  | -0.424    |
| train/info_reward_ctrl_mean | -0.424    |
| train/info_reward_ctrl_min  | -0.424    |
| train/info_reward_run_max   | 7.28      |
| train/info_reward_run_mean  | 7.28      |
| train/info_reward_run_min   | 7.28      |
| train/reward_per_eps        | nan       |
| train/steps                 | 147000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 142        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.31460962 |
| stats_o/std                 | 3.763556   |
| test/episodes               | 1430       |
| test/info_reward_ctrl_max   | -0.0585    |
| test/info_reward_ctrl_mean  | -0.471     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.7       |
| test/info_reward_run_mean   | 7.15       |
| test/info_reward_run_min    | -0.649     |
| test/reward_per_eps         | 6.68e+03   |
| test/steps                  | 1430000    |
| train/episodes              | 148        |
| train/info_reward_ctrl_max  | -0.429     |
| train/info_reward_ctrl_mean | -0.429     |
| train/info_reward_ctrl_min  | -0.429     |
| train/info_reward_run_max   | 7.64       |
| train/info_reward_run_mean  | 7.64       |
| train/info_reward_run_min   | 7.64       |
| train/reward_per_eps        | nan        |
| train/steps                 | 148000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 143       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.3153387 |
| stats_o/std                 | 3.7634075 |
| test/episodes               | 1440      |
| test/info_reward_ctrl_max   | -0.075    |
| test/info_reward_ctrl_mean  | -0.452    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.7      |
| test/info_reward_run_mean   | 7.14      |
| test/info_reward_run_min    | -0.558    |
| test/reward_per_eps         | 6.69e+03  |
| test/steps                  | 1440000   |
| train/episodes              | 149       |
| train/info_reward_ctrl_max  | -0.428    |
| train/info_reward_ctrl_mean | -0.428    |
| train/info_reward_ctrl_min  | -0.428    |
| train/info_reward_run_max   | 7.38      |
| train/info_reward_run_mean  | 7.38      |
| train/info_reward_run_min   | 7.38      |
| train/reward_per_eps        | nan       |
| train/steps                 | 149000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 144       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.315924  |
| stats_o/std                 | 3.7633152 |
| test/episodes               | 1450      |
| test/info_reward_ctrl_max   | -0.0344   |
| test/info_reward_ctrl_mean  | -0.457    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 11.2      |
| test/info_reward_run_mean   | 7.7       |
| test/info_reward_run_min    | -0.687    |
| test/reward_per_eps         | 7.25e+03  |
| test/steps                  | 1450000   |
| train/episodes              | 150       |
| train/info_reward_ctrl_max  | -0.441    |
| train/info_reward_ctrl_mean | -0.441    |
| train/info_reward_ctrl_min  | -0.441    |
| train/info_reward_run_max   | 7.61      |
| train/info_reward_run_mean  | 7.61      |
| train/info_reward_run_min   | 7.61      |
| train/reward_per_eps        | nan       |
| train/steps                 | 150000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 145        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.31652555 |
| stats_o/std                 | 3.7629435  |
| test/episodes               | 1460       |
| test/info_reward_ctrl_max   | -0.027     |
| test/info_reward_ctrl_mean  | -0.436     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.5       |
| test/info_reward_run_mean   | 7.14       |
| test/info_reward_run_min    | -0.545     |
| test/reward_per_eps         | 6.7e+03    |
| test/steps                  | 1460000    |
| train/episodes              | 151        |
| train/info_reward_ctrl_max  | -0.417     |
| train/info_reward_ctrl_mean | -0.417     |
| train/info_reward_ctrl_min  | -0.417     |
| train/info_reward_run_max   | 7.14       |
| train/info_reward_run_mean  | 7.14       |
| train/info_reward_run_min   | 7.14       |
| train/reward_per_eps        | nan        |
| train/steps                 | 151000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 146        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.31711316 |
| stats_o/std                 | 3.762514   |
| test/episodes               | 1470       |
| test/info_reward_ctrl_max   | -0.0687    |
| test/info_reward_ctrl_mean  | -0.469     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.4       |
| test/info_reward_run_mean   | 7.3        |
| test/info_reward_run_min    | -0.636     |
| test/reward_per_eps         | 6.83e+03   |
| test/steps                  | 1470000    |
| train/episodes              | 152        |
| train/info_reward_ctrl_max  | -0.427     |
| train/info_reward_ctrl_mean | -0.427     |
| train/info_reward_ctrl_min  | -0.427     |
| train/info_reward_run_max   | 7.22       |
| train/info_reward_run_mean  | 7.22       |
| train/info_reward_run_min   | 7.22       |
| train/reward_per_eps        | nan        |
| train/steps                 | 152000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 147       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.3179805 |
| stats_o/std                 | 3.763039  |
| test/episodes               | 1480      |
| test/info_reward_ctrl_max   | -0.0907   |
| test/info_reward_ctrl_mean  | -0.458    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.2      |
| test/info_reward_run_mean   | 7.02      |
| test/info_reward_run_min    | -0.637    |
| test/reward_per_eps         | 6.56e+03  |
| test/steps                  | 1480000   |
| train/episodes              | 153       |
| train/info_reward_ctrl_max  | -0.432    |
| train/info_reward_ctrl_mean | -0.432    |
| train/info_reward_ctrl_min  | -0.432    |
| train/info_reward_run_max   | 8.2       |
| train/info_reward_run_mean  | 8.2       |
| train/info_reward_run_min   | 8.2       |
| train/reward_per_eps        | nan       |
| train/steps                 | 153000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 148        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.31884938 |
| stats_o/std                 | 3.7635148  |
| test/episodes               | 1490       |
| test/info_reward_ctrl_max   | -0.0868    |
| test/info_reward_ctrl_mean  | -0.461     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11         |
| test/info_reward_run_mean   | 7.55       |
| test/info_reward_run_min    | -0.552     |
| test/reward_per_eps         | 7.09e+03   |
| test/steps                  | 1490000    |
| train/episodes              | 154        |
| train/info_reward_ctrl_max  | -0.44      |
| train/info_reward_ctrl_mean | -0.44      |
| train/info_reward_ctrl_min  | -0.44      |
| train/info_reward_run_max   | 7.53       |
| train/info_reward_run_mean  | 7.53       |
| train/info_reward_run_min   | 7.53       |
| train/reward_per_eps        | nan        |
| train/steps                 | 154000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 149       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.319675  |
| stats_o/std                 | 3.7643685 |
| test/episodes               | 1500      |
| test/info_reward_ctrl_max   | -0.0581   |
| test/info_reward_ctrl_mean  | -0.464    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.7      |
| test/info_reward_run_mean   | 7.24      |
| test/info_reward_run_min    | -0.792    |
| test/reward_per_eps         | 6.78e+03  |
| test/steps                  | 1500000   |
| train/episodes              | 155       |
| train/info_reward_ctrl_max  | -0.435    |
| train/info_reward_ctrl_mean | -0.435    |
| train/info_reward_ctrl_min  | -0.435    |
| train/info_reward_run_max   | 7.74      |
| train/info_reward_run_mean  | 7.74      |
| train/info_reward_run_min   | 7.74      |
| train/reward_per_eps        | nan       |
| train/steps                 | 155000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 150        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.32052085 |
| stats_o/std                 | 3.7648509  |
| test/episodes               | 1510       |
| test/info_reward_ctrl_max   | -0.0808    |
| test/info_reward_ctrl_mean  | -0.46      |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.4       |
| test/info_reward_run_mean   | 7.53       |
| test/info_reward_run_min    | -0.63      |
| test/reward_per_eps         | 7.07e+03   |
| test/steps                  | 1510000    |
| train/episodes              | 156        |
| train/info_reward_ctrl_max  | -0.432     |
| train/info_reward_ctrl_mean | -0.432     |
| train/info_reward_ctrl_min  | -0.432     |
| train/info_reward_run_max   | 7.81       |
| train/info_reward_run_mean  | 7.81       |
| train/info_reward_run_min   | 7.81       |
| train/reward_per_eps        | nan        |
| train/steps                 | 156000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 151        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.32147756 |
| stats_o/std                 | 3.7661846  |
| test/episodes               | 1520       |
| test/info_reward_ctrl_max   | -0.076     |
| test/info_reward_ctrl_mean  | -0.474     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 12         |
| test/info_reward_run_mean   | 7.82       |
| test/info_reward_run_min    | -0.668     |
| test/reward_per_eps         | 7.35e+03   |
| test/steps                  | 1520000    |
| train/episodes              | 157        |
| train/info_reward_ctrl_max  | -0.435     |
| train/info_reward_ctrl_mean | -0.435     |
| train/info_reward_ctrl_min  | -0.435     |
| train/info_reward_run_max   | 8.67       |
| train/info_reward_run_mean  | 8.67       |
| train/info_reward_run_min   | 8.67       |
| train/reward_per_eps        | nan        |
| train/steps                 | 157000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 152        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.32254437 |
| stats_o/std                 | 3.767846   |
| test/episodes               | 1530       |
| test/info_reward_ctrl_max   | -0.111     |
| test/info_reward_ctrl_mean  | -0.468     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.4       |
| test/info_reward_run_mean   | 7.46       |
| test/info_reward_run_min    | -0.878     |
| test/reward_per_eps         | 7e+03      |
| test/steps                  | 1530000    |
| train/episodes              | 158        |
| train/info_reward_ctrl_max  | -0.446     |
| train/info_reward_ctrl_mean | -0.446     |
| train/info_reward_ctrl_min  | -0.446     |
| train/info_reward_run_max   | 8.1        |
| train/info_reward_run_mean  | 8.1        |
| train/info_reward_run_min   | 8.1        |
| train/reward_per_eps        | nan        |
| train/steps                 | 158000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 153        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.32342187 |
| stats_o/std                 | 3.768994   |
| test/episodes               | 1540       |
| test/info_reward_ctrl_max   | -0.0745    |
| test/info_reward_ctrl_mean  | -0.461     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.4       |
| test/info_reward_run_mean   | 7.7        |
| test/info_reward_run_min    | -0.821     |
| test/reward_per_eps         | 7.24e+03   |
| test/steps                  | 1540000    |
| train/episodes              | 159        |
| train/info_reward_ctrl_max  | -0.433     |
| train/info_reward_ctrl_mean | -0.433     |
| train/info_reward_ctrl_min  | -0.433     |
| train/info_reward_run_max   | 8.52       |
| train/info_reward_run_mean  | 8.52       |
| train/info_reward_run_min   | 8.52       |
| train/reward_per_eps        | nan        |
| train/steps                 | 159000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 154       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.3243491 |
| stats_o/std                 | 3.7699118 |
| test/episodes               | 1550      |
| test/info_reward_ctrl_max   | -0.0762   |
| test/info_reward_ctrl_mean  | -0.454    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 12        |
| test/info_reward_run_mean   | 8.37      |
| test/info_reward_run_min    | -0.779    |
| test/reward_per_eps         | 7.91e+03  |
| test/steps                  | 1550000   |
| train/episodes              | 160       |
| train/info_reward_ctrl_max  | -0.433    |
| train/info_reward_ctrl_mean | -0.433    |
| train/info_reward_ctrl_min  | -0.433    |
| train/info_reward_run_max   | 7.51      |
| train/info_reward_run_mean  | 7.51      |
| train/info_reward_run_min   | 7.51      |
| train/reward_per_eps        | nan       |
| train/steps                 | 160000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 155        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.32527003 |
| stats_o/std                 | 3.771307   |
| test/episodes               | 1560       |
| test/info_reward_ctrl_max   | -0.0737    |
| test/info_reward_ctrl_mean  | -0.477     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.8       |
| test/info_reward_run_mean   | 7.06       |
| test/info_reward_run_min    | -0.667     |
| test/reward_per_eps         | 6.58e+03   |
| test/steps                  | 1560000    |
| train/episodes              | 161        |
| train/info_reward_ctrl_max  | -0.437     |
| train/info_reward_ctrl_mean | -0.437     |
| train/info_reward_ctrl_min  | -0.437     |
| train/info_reward_run_max   | 7.49       |
| train/info_reward_run_mean  | 7.49       |
| train/info_reward_run_min   | 7.49       |
| train/reward_per_eps        | nan        |
| train/steps                 | 161000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 156        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.32610798 |
| stats_o/std                 | 3.7721195  |
| test/episodes               | 1570       |
| test/info_reward_ctrl_max   | -0.0958    |
| test/info_reward_ctrl_mean  | -0.494     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.8       |
| test/info_reward_run_mean   | 6.1        |
| test/info_reward_run_min    | -0.694     |
| test/reward_per_eps         | 5.61e+03   |
| test/steps                  | 1570000    |
| train/episodes              | 162        |
| train/info_reward_ctrl_max  | -0.441     |
| train/info_reward_ctrl_mean | -0.441     |
| train/info_reward_ctrl_min  | -0.441     |
| train/info_reward_run_max   | 7.5        |
| train/info_reward_run_mean  | 7.5        |
| train/info_reward_run_min   | 7.5        |
| train/reward_per_eps        | nan        |
| train/steps                 | 162000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 157       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.327038  |
| stats_o/std                 | 3.7728658 |
| test/episodes               | 1580      |
| test/info_reward_ctrl_max   | -0.0655   |
| test/info_reward_ctrl_mean  | -0.457    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 10.8      |
| test/info_reward_run_mean   | 7.57      |
| test/info_reward_run_min    | -0.921    |
| test/reward_per_eps         | 7.12e+03  |
| test/steps                  | 1580000   |
| train/episodes              | 163       |
| train/info_reward_ctrl_max  | -0.433    |
| train/info_reward_ctrl_mean | -0.433    |
| train/info_reward_ctrl_min  | -0.433    |
| train/info_reward_run_max   | 7.92      |
| train/info_reward_run_mean  | 7.92      |
| train/info_reward_run_min   | 7.92      |
| train/reward_per_eps        | nan       |
| train/steps                 | 163000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 158        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.32785502 |
| stats_o/std                 | 3.7737594  |
| test/episodes               | 1590       |
| test/info_reward_ctrl_max   | -0.0469    |
| test/info_reward_ctrl_mean  | -0.429     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.5       |
| test/info_reward_run_mean   | 7.96       |
| test/info_reward_run_min    | -0.717     |
| test/reward_per_eps         | 7.53e+03   |
| test/steps                  | 1590000    |
| train/episodes              | 164        |
| train/info_reward_ctrl_max  | -0.421     |
| train/info_reward_ctrl_mean | -0.421     |
| train/info_reward_ctrl_min  | -0.421     |
| train/info_reward_run_max   | 7.56       |
| train/info_reward_run_mean  | 7.56       |
| train/info_reward_run_min   | 7.56       |
| train/reward_per_eps        | nan        |
| train/steps                 | 164000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 159        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.32866028 |
| stats_o/std                 | 3.7741008  |
| test/episodes               | 1600       |
| test/info_reward_ctrl_max   | -0.0878    |
| test/info_reward_ctrl_mean  | -0.466     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.5       |
| test/info_reward_run_mean   | 7.77       |
| test/info_reward_run_min    | -0.516     |
| test/reward_per_eps         | 7.31e+03   |
| test/steps                  | 1600000    |
| train/episodes              | 165        |
| train/info_reward_ctrl_max  | -0.42      |
| train/info_reward_ctrl_mean | -0.42      |
| train/info_reward_ctrl_min  | -0.42      |
| train/info_reward_run_max   | 8.11       |
| train/info_reward_run_mean  | 8.11       |
| train/info_reward_run_min   | 8.11       |
| train/reward_per_eps        | nan        |
| train/steps                 | 165000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 160        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.32953513 |
| stats_o/std                 | 3.7743363  |
| test/episodes               | 1610       |
| test/info_reward_ctrl_max   | -0.0477    |
| test/info_reward_ctrl_mean  | -0.463     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.8       |
| test/info_reward_run_mean   | 7.91       |
| test/info_reward_run_min    | -0.618     |
| test/reward_per_eps         | 7.45e+03   |
| test/steps                  | 1610000    |
| train/episodes              | 166        |
| train/info_reward_ctrl_max  | -0.443     |
| train/info_reward_ctrl_mean | -0.443     |
| train/info_reward_ctrl_min  | -0.443     |
| train/info_reward_run_max   | 7.47       |
| train/info_reward_run_mean  | 7.47       |
| train/info_reward_run_min   | 7.47       |
| train/reward_per_eps        | nan        |
| train/steps                 | 166000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 161        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.33038813 |
| stats_o/std                 | 3.774874   |
| test/episodes               | 1620       |
| test/info_reward_ctrl_max   | -0.0679    |
| test/info_reward_ctrl_mean  | -0.459     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 12.3       |
| test/info_reward_run_mean   | 8.06       |
| test/info_reward_run_min    | -0.765     |
| test/reward_per_eps         | 7.6e+03    |
| test/steps                  | 1620000    |
| train/episodes              | 167        |
| train/info_reward_ctrl_max  | -0.438     |
| train/info_reward_ctrl_mean | -0.438     |
| train/info_reward_ctrl_min  | -0.438     |
| train/info_reward_run_max   | 7.67       |
| train/info_reward_run_mean  | 7.67       |
| train/info_reward_run_min   | 7.67       |
| train/reward_per_eps        | nan        |
| train/steps                 | 167000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 162       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.3313152 |
| stats_o/std                 | 3.775381  |
| test/episodes               | 1630      |
| test/info_reward_ctrl_max   | -0.0271   |
| test/info_reward_ctrl_mean  | -0.474    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 11.1      |
| test/info_reward_run_mean   | 7.66      |
| test/info_reward_run_min    | -0.588    |
| test/reward_per_eps         | 7.18e+03  |
| test/steps                  | 1630000   |
| train/episodes              | 168       |
| train/info_reward_ctrl_max  | -0.437    |
| train/info_reward_ctrl_mean | -0.437    |
| train/info_reward_ctrl_min  | -0.437    |
| train/info_reward_run_max   | 8.25      |
| train/info_reward_run_mean  | 8.25      |
| train/info_reward_run_min   | 8.25      |
| train/reward_per_eps        | nan       |
| train/steps                 | 168000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 163        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.33205092 |
| stats_o/std                 | 3.7754796  |
| test/episodes               | 1640       |
| test/info_reward_ctrl_max   | -0.0756    |
| test/info_reward_ctrl_mean  | -0.441     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.8       |
| test/info_reward_run_mean   | 7.31       |
| test/info_reward_run_min    | -0.833     |
| test/reward_per_eps         | 6.87e+03   |
| test/steps                  | 1640000    |
| train/episodes              | 169        |
| train/info_reward_ctrl_max  | -0.428     |
| train/info_reward_ctrl_mean | -0.428     |
| train/info_reward_ctrl_min  | -0.428     |
| train/info_reward_run_max   | 7.56       |
| train/info_reward_run_mean  | 7.56       |
| train/info_reward_run_min   | 7.56       |
| train/reward_per_eps        | nan        |
| train/steps                 | 169000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 164        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.33272976 |
| stats_o/std                 | 3.776264   |
| test/episodes               | 1650       |
| test/info_reward_ctrl_max   | -0.104     |
| test/info_reward_ctrl_mean  | -0.479     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 10.9       |
| test/info_reward_run_mean   | 6.75       |
| test/info_reward_run_min    | -0.702     |
| test/reward_per_eps         | 6.27e+03   |
| test/steps                  | 1650000    |
| train/episodes              | 170        |
| train/info_reward_ctrl_max  | -0.441     |
| train/info_reward_ctrl_mean | -0.441     |
| train/info_reward_ctrl_min  | -0.441     |
| train/info_reward_run_max   | 7.41       |
| train/info_reward_run_mean  | 7.41       |
| train/info_reward_run_min   | 7.41       |
| train/reward_per_eps        | nan        |
| train/steps                 | 170000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 165       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.3335572 |
| stats_o/std                 | 3.7766738 |
| test/episodes               | 1660      |
| test/info_reward_ctrl_max   | -0.0597   |
| test/info_reward_ctrl_mean  | -0.447    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 12.3      |
| test/info_reward_run_mean   | 8.05      |
| test/info_reward_run_min    | -0.658    |
| test/reward_per_eps         | 7.6e+03   |
| test/steps                  | 1660000   |
| train/episodes              | 171       |
| train/info_reward_ctrl_max  | -0.428    |
| train/info_reward_ctrl_mean | -0.428    |
| train/info_reward_ctrl_min  | -0.428    |
| train/info_reward_run_max   | 8.32      |
| train/info_reward_run_mean  | 8.32      |
| train/info_reward_run_min   | 8.32      |
| train/reward_per_eps        | nan       |
| train/steps                 | 171000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 166        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.33433503 |
| stats_o/std                 | 3.777174   |
| test/episodes               | 1670       |
| test/info_reward_ctrl_max   | -0.0534    |
| test/info_reward_ctrl_mean  | -0.454     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.5       |
| test/info_reward_run_mean   | 7.82       |
| test/info_reward_run_min    | -0.726     |
| test/reward_per_eps         | 7.36e+03   |
| test/steps                  | 1670000    |
| train/episodes              | 172        |
| train/info_reward_ctrl_max  | -0.434     |
| train/info_reward_ctrl_mean | -0.434     |
| train/info_reward_ctrl_min  | -0.434     |
| train/info_reward_run_max   | 8.25       |
| train/info_reward_run_mean  | 8.25       |
| train/info_reward_run_min   | 8.25       |
| train/reward_per_eps        | nan        |
| train/steps                 | 172000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 167        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.33507422 |
| stats_o/std                 | 3.7777584  |
| test/episodes               | 1680       |
| test/info_reward_ctrl_max   | -0.172     |
| test/info_reward_ctrl_mean  | -0.465     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 12.1       |
| test/info_reward_run_mean   | 8.55       |
| test/info_reward_run_min    | -0.663     |
| test/reward_per_eps         | 8.09e+03   |
| test/steps                  | 1680000    |
| train/episodes              | 173        |
| train/info_reward_ctrl_max  | -0.425     |
| train/info_reward_ctrl_mean | -0.425     |
| train/info_reward_ctrl_min  | -0.425     |
| train/info_reward_run_max   | 7.76       |
| train/info_reward_run_mean  | 7.76       |
| train/info_reward_run_min   | 7.76       |
| train/reward_per_eps        | nan        |
| train/steps                 | 173000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 168        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.33582214 |
| stats_o/std                 | 3.7786174  |
| test/episodes               | 1690       |
| test/info_reward_ctrl_max   | -0.0935    |
| test/info_reward_ctrl_mean  | -0.458     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 12         |
| test/info_reward_run_mean   | 8.15       |
| test/info_reward_run_min    | -0.71      |
| test/reward_per_eps         | 7.69e+03   |
| test/steps                  | 1690000    |
| train/episodes              | 174        |
| train/info_reward_ctrl_max  | -0.432     |
| train/info_reward_ctrl_mean | -0.432     |
| train/info_reward_ctrl_min  | -0.432     |
| train/info_reward_run_max   | 7.94       |
| train/info_reward_run_mean  | 7.94       |
| train/info_reward_run_min   | 7.94       |
| train/reward_per_eps        | nan        |
| train/steps                 | 174000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 169        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.33657858 |
| stats_o/std                 | 3.7795486  |
| test/episodes               | 1700       |
| test/info_reward_ctrl_max   | -0.0581    |
| test/info_reward_ctrl_mean  | -0.455     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.2       |
| test/info_reward_run_mean   | 7.9        |
| test/info_reward_run_min    | -0.921     |
| test/reward_per_eps         | 7.44e+03   |
| test/steps                  | 1700000    |
| train/episodes              | 175        |
| train/info_reward_ctrl_max  | -0.418     |
| train/info_reward_ctrl_mean | -0.418     |
| train/info_reward_ctrl_min  | -0.418     |
| train/info_reward_run_max   | 8.17       |
| train/info_reward_run_mean  | 8.17       |
| train/info_reward_run_min   | 8.17       |
| train/reward_per_eps        | nan        |
| train/steps                 | 175000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 170        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.33739325 |
| stats_o/std                 | 3.7801769  |
| test/episodes               | 1710       |
| test/info_reward_ctrl_max   | -0.0971    |
| test/info_reward_ctrl_mean  | -0.464     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.5       |
| test/info_reward_run_mean   | 7.33       |
| test/info_reward_run_min    | -1.11      |
| test/reward_per_eps         | 6.86e+03   |
| test/steps                  | 1710000    |
| train/episodes              | 176        |
| train/info_reward_ctrl_max  | -0.448     |
| train/info_reward_ctrl_mean | -0.448     |
| train/info_reward_ctrl_min  | -0.448     |
| train/info_reward_run_max   | 7.88       |
| train/info_reward_run_mean  | 7.88       |
| train/info_reward_run_min   | 7.88       |
| train/reward_per_eps        | nan        |
| train/steps                 | 176000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 171       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.338017  |
| stats_o/std                 | 3.7805974 |
| test/episodes               | 1720      |
| test/info_reward_ctrl_max   | -0.0568   |
| test/info_reward_ctrl_mean  | -0.457    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 11.9      |
| test/info_reward_run_mean   | 7.83      |
| test/info_reward_run_min    | -0.647    |
| test/reward_per_eps         | 7.37e+03  |
| test/steps                  | 1720000   |
| train/episodes              | 177       |
| train/info_reward_ctrl_max  | -0.436    |
| train/info_reward_ctrl_mean | -0.436    |
| train/info_reward_ctrl_min  | -0.436    |
| train/info_reward_run_max   | 7.72      |
| train/info_reward_run_mean  | 7.72      |
| train/info_reward_run_min   | 7.72      |
| train/reward_per_eps        | nan       |
| train/steps                 | 177000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
-------------------------------------------
| epoch                       | 172       |
| stats_g/mean                | nan       |
| stats_g/std                 | nan       |
| stats_o/mean                | 0.3386986 |
| stats_o/std                 | 3.7806091 |
| test/episodes               | 1730      |
| test/info_reward_ctrl_max   | -0.0809   |
| test/info_reward_ctrl_mean  | -0.461    |
| test/info_reward_ctrl_min   | -0.6      |
| test/info_reward_run_max    | 12.4      |
| test/info_reward_run_mean   | 8.14      |
| test/info_reward_run_min    | -0.545    |
| test/reward_per_eps         | 7.68e+03  |
| test/steps                  | 1730000   |
| train/episodes              | 178       |
| train/info_reward_ctrl_max  | -0.439    |
| train/info_reward_ctrl_mean | -0.439    |
| train/info_reward_ctrl_min  | -0.439    |
| train/info_reward_run_max   | 8.37      |
| train/info_reward_run_mean  | 8.37      |
| train/info_reward_run_min   | 8.37      |
| train/reward_per_eps        | nan       |
| train/steps                 | 178000    |
-------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 173        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.33936518 |
| stats_o/std                 | 3.7806365  |
| test/episodes               | 1740       |
| test/info_reward_ctrl_max   | -0.0651    |
| test/info_reward_ctrl_mean  | -0.465     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 11.9       |
| test/info_reward_run_mean   | 8.29       |
| test/info_reward_run_min    | -0.492     |
| test/reward_per_eps         | 7.82e+03   |
| test/steps                  | 1740000    |
| train/episodes              | 179        |
| train/info_reward_ctrl_max  | -0.43      |
| train/info_reward_ctrl_mean | -0.43      |
| train/info_reward_ctrl_min  | -0.43      |
| train/info_reward_run_max   | 7.75       |
| train/info_reward_run_mean  | 7.75       |
| train/info_reward_run_min   | 7.75       |
| train/reward_per_eps        | nan        |
| train/steps                 | 179000     |
--------------------------------------------
Saving latest policy.
Current potential weight:  1.0
--------------------------------------------
| epoch                       | 174        |
| stats_g/mean                | nan        |
| stats_g/std                 | nan        |
| stats_o/mean                | 0.34016126 |
| stats_o/std                 | 3.781341   |
| test/episodes               | 1750       |
| test/info_reward_ctrl_max   | -0.0678    |
| test/info_reward_ctrl_mean  | -0.468     |
| test/info_reward_ctrl_min   | -0.6       |
| test/info_reward_run_max    | 12.9       |
| test/info_reward_run_mean   | 8.78       |
| test/info_reward_run_min    | -0.57      |
| test/reward_per_eps         | 8.31e+03   |
| test/steps                  | 1750000    |
| train/episodes              | 180        |
| train/info_reward_ctrl_max  | -0.431     |
| train/info_reward_ctrl_mean | -0.431     |
| train/info_reward_ctrl_min  | -0.431     |
| train/info_reward_run_max   | 8.32       |
| train/info_reward_run_mean  | 8.32       |
| train/info_reward_run_min   | 8.32       |
| train/reward_per_eps        | nan        |
| train/steps                 | 180000     |
--------------------------------------------
Saving latest policy.
